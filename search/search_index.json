{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Viadot","text":"<p>A simple data ingestion library to guide data flows from some places to other places.</p>"},{"location":"#getting-data-from-a-source","title":"Getting data from a source","text":"<p>Viadot supports several API and database sources, private and public. Below is a snippet of how to get data from the UK Carbon Intensity API:</p> <pre><code>from viadot.sources import UKCarbonIntensity\n\nukci = UKCarbonIntensity()\nukci.query(\"/intensity\")\ndf = ukci.to_df()\n\nprint(df)\n</code></pre> <p>Output:</p> from to forecast actual index 0 2021-08-10T11:00Z 2021-08-10T11:30Z 211 216 moderate <p>The above <code>df</code> is a pandas <code>DataFrame</code> object. It contains data downloaded by <code>viadot</code> from the Carbon Intensity UK API.</p>"},{"location":"#loading-data-to-a-destination","title":"Loading data to a destination","text":"<p>Depending on the destination, <code>viadot</code> provides different methods of uploading data. For instance, for databases, this would be bulk inserts. For data lakes, it would be file uploads.</p> <p>For example:</p> <pre><code>from viadot.sources import UKCarbonIntensity\nfrom viadot.sources import AzureDataLake\n\nukci = UKCarbonIntensity()\nukci.query(\"/intensity\")\ndf = ukci.to_df()\n\nadls = AzureDataLake(config_key=\"my_adls_creds\")\nadls.from_df(df, \"my_folder/my_file.parquet\")\n</code></pre>"},{"location":"#next-steps","title":"Next steps","text":"<p>Head over to the Getting Started guide to learn how to set up <code>viadot</code>.</p>"},{"location":"getting_started/","title":"Getting started guide","text":""},{"location":"getting_started/#prerequisites","title":"Prerequisites","text":"<p>We use Rye. You can install it like so:</p> <pre><code>curl -sSf https://rye.astral.sh/get | bash\n</code></pre>"},{"location":"getting_started/#installation","title":"Installation","text":"<p>Note</p> <p><code>viadot2</code> installation requires installing some Linux libraries, which may be complex for less technical users. For those users, we recommend using the viadot container.</p>"},{"location":"getting_started/#os-dependencies","title":"OS dependencies","text":""},{"location":"getting_started/#core","title":"Core","text":"<p><code>viadot2</code> depends on some Linux system libraries. You can install them in the following way:</p> <pre><code>sudo apt update -q &amp;&amp; \\\n  yes | apt install -q gnupg unixodbc &amp;&amp; \\\n  curl https://packages.microsoft.com/keys/microsoft.asc | apt-key add - &amp;&amp; \\\n  curl https://packages.microsoft.com/config/debian/10/prod.list &gt; /etc/apt/sources.list.d/mssql-release.list &amp;&amp; \\\n  sudo apt update -q &amp;&amp; \\\n  sudo  apt install -q libsqliteodbc &amp;&amp; \\\n  ACCEPT_EULA=Y apt install -q -y msodbcsql17=17.10.1.1-1 &amp;&amp; \\\n  ACCEPT_EULA=Y apt install -q -y mssql-tools=17.10.1.1-1 &amp;&amp; \\\n  echo 'export PATH=\"$PATH:/opt/mssql-tools/bin\"' &gt;&gt; ~/.bashrc\n</code></pre> <p>Next, copy the SQL Server config from <code>docker/odbcinst.ini</code> file into your <code>/etc/odbcinst.ini</code> file.</p> <pre><code>cat docker/odbcinst.ini | sudo tee -a /etc/odbcinst.ini\n</code></pre>"},{"location":"getting_started/#sap-connector","title":"SAP connector","text":"<p>In order to work with the SAP connector, you must also install the SAP NetWeaver RFC SDK. You must have a SAP license in order to be able to download and use the driver.</p> <p>To install the driver, copy the <code>nwrfcsdk</code> folder to <code>/usr/local/sap/</code>. Then, also copy the <code>nwrfcsdk.conf</code> file to <code>/etc/ld.so.conf.d/</code>:</p> <pre><code>sudo cp -R ./sap_netweaver_rfc/ /usr/local/sap/\nsudo cp ./sap_netweaver_rfc/nwrfcsdk.conf /etc/ld.so.conf.d/\n</code></pre> <p>Next, configure the <code>SAPNWRFC_HOME</code> env variable, eg. by adding the following entry in your <code>~/.bashrc</code> file:</p> <pre><code>export SAPNWRFC_HOME=/usr/local/sap/nwrfcsdk\n</code></pre> <p>Make sure to reload the shell:</p> <pre><code>source ~/.bashrc\n</code></pre> <p>Next, configure the RFC driver with:</p> <pre><code>sudo ldconfig\n</code></pre> <p>Finally, you can install <code>pyrfc</code> by installing the viadot <code>sap</code> extra:</p> <pre><code>rye sync --features=sap\n</code></pre>"},{"location":"getting_started/#library","title":"Library","text":"<pre><code>git clone https://github.com/dyvenia/viadot.git &amp;&amp; \\\n  cd viadot &amp;&amp; \\\n  rye sync\n</code></pre> <p>Note</p> <p>Since <code>viadot</code> does not have an SDK, both adding new sources and flows requires contributing your code to the library. Hence, we install the library from source instead of just using <code>pip install</code>. However, installing <code>viadot2</code> with <code>pip install</code> is still possible:</p> <pre><code>pip install viadot2\n</code></pre> <p>or, with the <code>azure</code> extra as an example:</p> <pre><code>pip install viadot2[azure]\n</code></pre> <p>Note that the system dependencies are not installed via <code>pip</code> and must be installed separately a package manager such as <code>apt</code>.</p>"},{"location":"getting_started/#next-steps","title":"Next steps","text":"<p>Head over to the developer guide to learn how to use <code>viadot</code> to build data connectors and jobs.</p>"},{"location":"advanced_usage/","title":"Advanced usage guide","text":"<p>In this guide, we'll describe some of the more advanced <code>viadot</code> concepts and workflows, such as:</p> <ul> <li>working in a containerized environment</li> <li>using secrets stores instead of the config file</li> <li>etc.</li> </ul>"},{"location":"advanced_usage/containerized_env/","title":"Containerized development environment","text":"<p>Currently, viadot ships with three images:</p> <ul> <li><code>viadot-lite</code> - includes the core <code>viadot2</code> library and system dependencies</li> <li><code>viadot-azure</code> - includes <code>viadot2</code> with the <code>azure</code> extra, as well as Azure-related OS dependencies</li> <li><code>viadot-aws</code> - includes <code>viadot2</code> with the <code>aws</code> extra, as well as AWS-related OS dependencies</li> </ul> <p>You can use these images to avoid installing any OS dependencies on your local machine (for example, <code>msodbcsql17</code> and <code>mssql-tools</code> used by the <code>SQLServer</code> source).</p>"},{"location":"advanced_usage/containerized_env/#setup","title":"Setup","text":"<p>Spin up your container of choice with <code>docker compose</code>:</p> <pre><code>docker compose -f docker/docker-compose.yml up -d viadot-&lt;extra&gt;\n</code></pre> <p>For example, to start the <code>viadot-aws</code> container:</p> <pre><code>docker compose -f docker/docker-compose.yml up -d viadot-aws\n</code></pre>"},{"location":"advanced_usage/containerized_env/#usage","title":"Usage","text":""},{"location":"advanced_usage/containerized_env/#attaching-to-the-container","title":"Attaching to the container","text":"<p>Once you have a container running, use an IDE like VSCode to attach to it. Alternatively, you can also attach to the container using the CLI:</p> <pre><code>docker exec -it viadot-&lt;distro&gt; bash\n</code></pre>"},{"location":"advanced_usage/containerized_env/#building-a-custom-image-locally","title":"Building a custom image locally","text":"<p>If you need to build a custom image locally, you can do so using standard Docker commands. For example:</p> <pre><code>docker build --target viadot-&lt;distro&gt; --tag viadot-&lt;distro&gt;:&lt;your_tag&gt; -f docker/Dockerfile .\n</code></pre>"},{"location":"advanced_usage/containerized_env/#see-also","title":"See also","text":"<p>For more information on working with Docker containers and images, see Docker documentation.</p>"},{"location":"developer_guide/","title":"Developer guide","text":"<p>In this guide, we're going to be developing a new source and data orchestration job.</p> <p>We'll start by creating a <code>PostgreSQL</code> source connector. Then, we'll create a Prefect flow that downloads a PostgreSQL table into a pandas <code>DataFrame</code> and uploads the data as a Parquet file to Azure Data Lake.</p>"},{"location":"developer_guide/contributing_to_viadot/","title":"Contributing to viadot","text":""},{"location":"developer_guide/contributing_to_viadot/#how-to-contribute","title":"How to contribute","text":"<p>Before creating a PR, make sure to familiarize yourself with the Contributing Guide.</p>"},{"location":"developer_guide/contributing_to_viadot/#next-steps","title":"Next steps","text":"<p>To learn advanced <code>viadot</code> concepts and usage patterns, see the advanced usage guide.</p>"},{"location":"developer_guide/creating_a_prefect_flow/","title":"Creating jobs","text":"<p>Let's assume that we've finished our <code>PostgreSQL</code> source. We now want to use it to automatically download data from PostgreSQL on a schedule.</p> <p>Note</p> <p>Job scheduling is left outside of the scope of this guide - we only focus on creating the job itself.</p> <p>We create our ingestion job by utilizing Prefect as our orchestration tool. We will create a Prefect flow, <code>postgresql_to_adls</code>. This flow will utilize our new connector to download a PostgreSQL table into a pandas <code>DataFrame</code>, and then upload the data to Azure Data Lake. The flow will consist of two tasks:</p> <ul> <li><code>postgresql_to_df</code> - downloads a PostgreSQL table into a pandas <code>DataFrame</code></li> <li><code>df_to_adls</code> - uploads a pandas <code>DataFrame</code> to Azure Data Lake</li> </ul>"},{"location":"developer_guide/creating_a_prefect_flow/#creating-a-task","title":"Creating a task","text":"<p>Below is an example task:</p> <pre><code># orchestration/prefect/tasks/postgresql.py\n\nfrom viadot.sources import PostgreSQL\nfrom prefect import task\n\nfrom prefect_viadot.exceptions import MissingSourceCredentialsError\nfrom prefect_viadot.utils import get_credentials\n\nimport pandas as pd\n\n\n@task(retries=3, retry_delay_seconds=10, timeout_seconds=60 * 60)\ndef postgresql_to_df(config_key: str | None = None, credentials_secret: str | None = None, ...) -&gt; pd.DataFrame:\n    if not (credentials_secret or config_key):\n        raise MissingSourceCredentialsError\n\n    credentials = get_credentials(credentials_secret) if credentials_secret else None\n\n    postgres = PostgreSQL(credentials=credentials, config_key=config_key)\n    return postgres.to_df(...)\n</code></pre> <p>Best practices</p> <ol> <li>The task should be a thin wrapper over a <code>viadot</code> function or method (ie. it shouldn't contain any logic but simply use relevant <code>viadot</code> functions and classes).</li> <li>The task MUST NOT allow specifying the credentials directly. Credentials for the source must be passed via config key and/or secret. This is because Prefect stores the values of task parameters and sometimes even logs them in the UI, which means that passing the credentials directly creates a security risk.</li> <li>Validate the credentials and raise <code>MissingSourceCredentialsError</code> if needed.</li> <li>Utilize retries and timeouts to make the task and the entire system more robust.</li> </ol> <p>When you are done with the task, remember to import it in <code>tasks/__init__.py</code>, so that it can be imported with <code>from viadot.orchestration.prefect.tasks import postgresql_to_df</code> (instead of <code>from viadot.orchestration.prefect.tasks.postgresql_to_df import postgresql_to_df</code>):</p> <pre><code># orchestration/prefect/tasks/__init__.py\n\nfrom .postgresql import postgresql_to_df  # noqa: F401\n</code></pre> <p>Tests</p> <p>Note that since the orchestration layer is only a thin wrapper around <code>viadot</code> sources, we don't require unit or integration tests for tasks or flows. Instead, add all your unit and integration tests at the source connector level.</p>"},{"location":"developer_guide/creating_a_prefect_flow/#tasks-using-optional-dependencies","title":"Tasks using optional dependencies","text":"<p>Similar to sources, in case your task uses an optional dependency, it has to be escaped:</p> <pre><code># orchestration/prefect/tasks/adls.py\n\n\"\"\"Tasks for interacting with Azure Data Lake (gen2).\"\"\"\n\nimport contextlib\n...\n\nwith contextlib.suppress(ImportError):\n    from viadot.sources import AzureDataLake\n\n...\n</code></pre> <p>In case you're adding task/flow tests, remember to also escape the imports with <code>viadot.utils.skip_test_on_missing_extra()</code>!</p>"},{"location":"developer_guide/creating_a_prefect_flow/#creating-a-prefect-flow","title":"Creating a Prefect flow","text":"<p>Once the tasks are ready, the last step of our development is to define the flow:</p> <pre><code># orchestration/prefect/flows/postgresql_to_adls.py\n\nfrom viadot.orchestration.prefect.tasks import df_to_adls, postgresql_to_df\n\nfrom prefect import flow\n\n@flow(\n    name=\"extract--postgresql--adls\",\n    description=\"Extract data from PostgreSQL database and load it into Azure Data Lake.\",\n    retries=1,\n    retry_delay_seconds=60,\n    timeout_seconds=60*60,\n)\ndef postgresql_to_adls(\n    adls_path: str,\n    adls_credentials_secret: str | None,\n    adls_config_key: str | None,\n    overwrite: bool = False,\n    postgresql_config_key: str | None,\n    postgresql_credentials_secret: str | None,\n    sql_query: str | None,\n    ) -&gt; None:\n\n    df = postgresql_to_df(\n        credentials_secret=postgresql_credentials_secret,\n        config_key=postgresql_config_key,\n        sql_query=sql_query,\n    )\n    return df_to_adls(\n        df=df,\n        path=adls_path,\n        credentials_secret=adls_credentials_secret,\n        config_key=adls_config_key,\n        overwrite=overwrite,\n    )\n</code></pre> <p>Best practices</p> <ol> <li>The flow should be a thin wrapper over the tasks, and should contain minimal logic. If your flow is getting too complex, it means that you're probably working around the limitations for <code>viadot</code>. Instead of adding workarounds in the flow, simply add the missing functionality to the connector you're using. This will make the functionality easier to test. It will also make it reusable across different orchestrators (eg. Airflow).</li> <li>Utilize retries and timeouts to make the flow and the entire system more robust*.</li> </ol> <p>*if you do use retries, make sure the flow is idempotent</p> <p>When you are done with the flow, remember to import it in the init file, so that it can be imported with <code>from viadot.orchestration.prefect.flows import postgresql_to_adls</code> (instead of <code>from viadot.orchestration.prefect.flows.postgresql_to_adls import postgresql_to_adls</code>):</p> <pre><code># orchestration/prefect/flows/__init__.py\n\n...\nfrom .postgresql_to_adls import postgresql_to_adls\n\n__all__ = [\n    ...,\n    \"postgresql_to_adls\"\n]\n</code></pre>"},{"location":"developer_guide/creating_a_prefect_flow/#adding-docs","title":"Adding docs","text":"<p>To allow MkDocs to autogenerate and display documentation for your tasks and flows in reference docs, add relevant entries in the reference docs (<code>docs/references/orchestration/prefect</code>). For example:</p> <p>Task:</p> <pre><code># docs/references/orchestration/prefect/tasks.md\n\n...\n\n::: viadot.orchestration.prefect.tasks.postgresql_to_df\n</code></pre> <p>Flow:</p> <pre><code># docs/references/orchestration/prefect/flows.md\n\n...\n\n::: viadot.orchestration.prefect.flows.postgresql_to_adls\n</code></pre>"},{"location":"developer_guide/creating_a_source/","title":"Creating a source connector","text":""},{"location":"developer_guide/creating_a_source/#example","title":"Example","text":"<p>The first thing you need to do is create a class that inherits from the <code>SQL</code> class. You should also specify a pydantic model for the source's credentials:</p> <pre><code># sources/postgresql.py\n\n\"\"\"PostgreSQL connector.\"\"\"\n\nfrom viadot.sources.base import SQL\nfrom pydantic import BaseModel\n\nclass PostgreSQLCredentials(BaseModel):\n    host: str\n    port: int = 5432\n    database: str\n    user: str\n    password: str\n\nclass PostgreSQL(SQL):\n\n    def __init__(\n        self,\n        credentials: PostgreSQLCredentials | None = None,\n        config_key: str | None = None,\n        *args,\n        **kwargs,\n    ):\n        ...\n</code></pre> <p>Credentials can now be provided directly via the <code>credentials</code> parameter or by using the config key.</p> <p>viadot metadata - hardcoded schemas workaround</p> <p>The addition of viadot metadata columns (currently, <code>_viadot_source</code> and <code>_viadot_downloaded_at_utc</code>) should be done in the base class's <code>to_df()</code> method. However, due to some production uses of viadot relying on hardcoded DataFrame schemas (and not being able to either pin the <code>viadot</code> version or fix the hardcoding), this cannot currently be done. As a workaround, you need to implement the <code>to_df()</code> method in your source and add the columns yourself.</p> <p>Below is a an example for our Postgres connector. Since we can reuse the parent class's <code>to_df()</code> method, we're simply wrapping it with the <code>add_viadot_metadata_columns</code> decorator:</p> <pre><code># sources/postgresql.py\n\n\"\"\"PostgreSQL connector.\"\"\"\n\nfrom viadot.sources.base import SQL\nfrom viadot.utils import add_viadot_metadata_columns\nfrom pydantic import BaseModel\n\nclass PostgreSQLCredentials(BaseModel):\n    host: str\n    port: int = 5432\n    database: str\n    user: str\n    password: str\n\nclass PostgreSQL(SQL):\n\n    def __init__(\n        self,\n        credentials: PostgreSQLCredentials | None = None,\n        config_key: str | None = None,\n        *args,\n        **kwargs,\n    ):\n        ...\n\n    @add_viadot_metadata_columns\n    def to_df(\n        self,\n        query: str,\n        con: pyodbc.Connection | None = None,\n        if_empty: Literal[\"warn\", \"skip\", \"fail\"] = \"warn\",\n    ) -&gt; pd.DataFrame:\n        \"\"\"Execute a query and return the result as a pandas DataFrame.\"\"\"\n        super().to_df()\n</code></pre> <p>For more information, see this issue.</p> <p>Now, we also need to add a way to pass the credentials to the parent class:</p> <pre><code># sources/postgresql.py\n\n\"\"\"PostgreSQL connector.\"\"\"\n\nfrom viadot.sources.base import SQL\nfrom viadot.utils import add_viadot_metadata_columns\nfrom viadot.config import get_source_credentials\nfrom pydantic import BaseModel\n\nclass PostgreSQLCredentials(BaseModel):\n    host: str\n    port: int = 5432\n    database: str\n    user: str\n    password: str\n\nclass PostgreSQL(SQL):\n\n    def __init__(\n        self,\n        credentials: PostgreSQLCredentials | None = None,\n        config_key: str | None = None,\n        *args,\n        **kwargs,\n    ):\n        \"\"\"A PostgreSQL connector.\n\n        Args:\n            credentials (PostgreSQLCredentials, optional): Database credentials.\n            config_key (str, optional): The key in the viadot config holding relevant credentials.\n        \"\"\"\n        raw_creds = credentials or get_source_credentials(config_key) or {}\n        validated_creds = PostgreSQLCredentials(**raw_creds).dict(\n            by_alias=True\n        )\n        super().__init__(*args, credentials=validated_creds, **kwargs)\n\n    @add_viadot_metadata_columns\n    def to_df(\n        self,\n        query: str,\n        con: pyodbc.Connection | None = None,\n        if_empty: Literal[\"warn\", \"skip\", \"fail\"] = \"warn\",\n    ) -&gt; pd.DataFrame:\n        \"\"\"Execute a query and return the result as a pandas DataFrame.\"\"\"\n        super().to_df()\n</code></pre> <p>Once you're done with the source, remember to import it in <code>sources/__init__.py</code>, so that it can be imported with <code>from viadot.sources import PostgreSQL</code> (instead of <code>from viadot.sources.postgresql import PostgreSQL</code>):</p> <pre><code># sources/__init__.py\n\nfrom .postgresql import PostgreSQL\n\n__all__ = [\n    ...,\n    \"PostgreSQL\"\n]\n</code></pre>"},{"location":"developer_guide/creating_a_source/#sources-using-optional-dependencies","title":"Sources using optional dependencies","text":"<p>In case your source uses an optional dependency, you need to escape the import. In the example below, our source uses the optional <code>adlfs</code> package (part of the <code>azure</code> extra):</p> <pre><code># sources/azure_data_lake.py\n\nfrom viadot.sources.base import Source\n\ntry:\n    from adlfs import AzureBlobFileSystem, AzureDatalakeFileSystem\nexcept ModuleNotFoundError as e:\n    msg = \"Missing required modules to use AzureDataLake source.\"\n    raise ImportError(msg) from e\n\n\nclass AzureDataLake(Source):\n    ...\n</code></pre> <p>The import in <code>sources/__init__.py</code> also needs to be guarded:</p> <pre><code># sources/__init__.py\n\nfrom importlib.util import find_spec\n\nfrom .cloud_for_customers import CloudForCustomers\n...\n\n__all__ = [\n    \"CloudForCustomers\",\n    ...\n]\n\nif find_spec(\"adlfs\"):\n    from viadot.sources.azure_data_lake import AzureDataLake  # noqa: F401\n\n    __all__.extend([\"AzureDataLake\"])\n</code></pre>"},{"location":"developer_guide/creating_a_source/#adding-docs","title":"Adding docs","text":"<p>To allow MkDocs to autogenerate and display documentation for your source in reference docs, add an entry in the reference docs (<code>docs/references/sources</code>). For example:</p> <pre><code># docs/references/sources/sql_sources.md\n\n...\n\n::: viadot.sources.postgresql.PostgreSQL\n</code></pre>"},{"location":"developer_guide/creating_a_source/#adding-tests","title":"Adding tests","text":"<p>Make sure to add tests for your source!</p>"},{"location":"developer_guide/creating_a_source/#unit","title":"Unit","text":"<p>You can think of unit tests as tests which do not require internet connection or connectivity to the actual data source or destination. All unit tests are executed automatically on each PR to <code>viadot</code>'s default branch.</p> <p>A common practice to ensure above requirements are met is to mock the external systems. For example, if we wish to create a unit test for our <code>Sharepoint</code> source which will test the <code>to_df()</code> method, which in turn depends on the <code>_download_excel()</code> method, we must first mock the <code>_download_excel()</code> method so that it doesn't actually try to download any data. Below is an example of how you can accomplish this:</p> <pre><code># tests/unit/test_sharepoint.py\n\nimport pandas as pd\nfrom viadot.sources import Sharepoint\n\nTEST_CREDENTIALS = {\"site\": \"test\", \"username\": \"test2\", \"password\": \"test\"}\n\nclass SharepointMock(Sharepoint):\n    def _download_excel(self, url=None):\n        \"\"\"Returns a test DataFrame instead of calling a Sharepoint server.\"\"\"\n        return pd.ExcelFile(Path(\"tests/unit/test_file.xlsx\"))\n\ndef test_sharepoint():\n    s = SharepointMock(credentials=TEST_CREDENTIALS)\n    df = s.to_df(url=\"test\")\n\n    assert not df.empty\n</code></pre>"},{"location":"developer_guide/creating_a_source/#integration","title":"Integration","text":"<p>Integration tests connect to the actual systems. For these tests, you will need to set up your viadot config with proper credentials. For example, to test a <code>Sharepoint</code> source, our config could look like this:</p> <pre><code># ~/.config/viadot/config.yaml\nversion: 1\n\nsources:\n  - sharepoint_dev:\n      class: Sharepoint\n      credentials:\n        site: \"site.sharepoint.com\"\n        username: \"test_username\"\n        password: \"test_password\"\n</code></pre> <p>Then, in our integration tests, we can use the <code>Sharepoint</code> source with the <code>sharepoint_dev</code> config key:</p> <pre><code># tests/integration/test_sharepoint.py\n\nimport pytest\n...\n\n@pytest.fixture\ndef sharepoint():\n    from viadot.sources import Sharepoint\n\n    return Sharepoint(config_key=\"sharepoint_dev\")\n</code></pre> <p>Info</p> <p>For more information on viadot config, see this page.</p>"},{"location":"developer_guide/creating_a_source/#optional-dependencies","title":"Optional dependencies","text":"<p>Same as with the source, make sure to escape the imports of optional dependencies:</p> <pre><code>from viadot.utils import skip_test_on_missing_extra\n...\n\ntry:\n    from viadot.sources import AzureDataLake\n\nexcept ImportError:\n    skip_test_on_missing_extra(source_name=\"AzureDataLake\", extra=\"azure\")\n</code></pre>"},{"location":"developer_guide/creating_a_source/#using-viadot-config","title":"Using viadot config","text":"<p>In order to avoid storing and passing credentials through variables, source configuration should be stored in the viadot config file (by default, <code>~/.config/viadot/config.yaml</code>).</p> <p>You can find each source's configuration in the documentation.</p> <p>Below is an example config file, with configurations for two sources:</p> <pre><code>sources:\n  - exchange_rates:\n      class: ExchangeRates\n      credentials:\n        api_key: \"api123api123api123\"\n\n  - sharepoint:\n      class: Sharepoint\n      credentials:\n        site: \"site.sharepoint.com\"\n        username: \"user@email.com\"\n        password: \"password\"\n</code></pre> <p>In the above, <code>exchange_rates</code> and <code>sharepoint</code> are what we refer to as \"config keys\". For example, this is how to use the <code>exchange_rates</code> config key to pass credentials to the <code>ExchangeRates</code> source:</p> <pre><code># sources/exchange_rates.py\n\nsource = ExchangeRates(config_key=\"exchange_rates\")\n</code></pre> <p>This will pass the <code>credentials</code> key, including the <code>api_key</code> secret, to the instance.</p> <p>Info</p> <p>You can use any name for your config key, as long as it's unique. For example, we can have credentials for two different environments stored as <code>sharepoint_dev</code> and <code>sharepoint_prod</code> keys.</p>"},{"location":"developer_guide/creating_a_source/#conclusion","title":"Conclusion","text":"<p>And that's all you need to know to create your own <code>viadot</code> connectors!</p> <p>If you need inspiration, take a look at some of the existing sources.</p>"},{"location":"references/orchestration/prefect/flows/","title":"Flows","text":""},{"location":"references/orchestration/prefect/flows/#viadot.orchestration.prefect.flows.azure_sql_to_adls","title":"<code>viadot.orchestration.prefect.flows.azure_sql_to_adls</code>","text":"<p>Flows for downloading data from Azure SQL and uploading it to Azure ADLS.</p>"},{"location":"references/orchestration/prefect/flows/#viadot.orchestration.prefect.flows.azure_sql_to_adls.azure_sql_to_adls","title":"<code>azure_sql_to_adls(query=None, credentials_secret=None, validate_df_dict=None, convert_bytes=False, remove_special_characters=None, columns_to_clean=None, adls_config_key=None, adls_azure_key_vault_secret=None, adls_path=None, adls_path_overwrite=False)</code>","text":"<p>Download data from Azure SQL to a CSV file and uploading it to ADLS.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>Query to perform on a database. Defaults to None.</p> <code>None</code> <code>credentials_secret</code> <code>str</code> <p>The name of the Azure Key Vault secret containing a dictionary with database credentials. Defaults to None.</p> <code>None</code> <code>validate_df_dict</code> <code>Dict[str]</code> <p>A dictionary with optional list of tests to verify the output dataframe. If defined, triggers the <code>validate_df</code> task from task_utils. Defaults to None.</p> <code>None</code> <code>remove_special_characters</code> <code>str</code> <p>Call a function that remove special characters like escape symbols. Defaults to None.</p> <code>None</code> <code>columns_to_clean</code> <code>List(str)</code> <p>Select columns to clean, used with remove_special_characters. If None whole data frame will be processed. Defaults to None.</p> <code>None</code> <code>adls_config_key</code> <code>Optional[str]</code> <p>The key in the viadot config holding relevant credentials. Defaults to None.</p> <code>None</code> <code>adls_azure_key_vault_secret</code> <code>Optional[str]</code> <p>The name of the Azure Key Vault secret containing a dictionary with ACCOUNT_NAME and Service Principal credentials (TENANT_ID, CLIENT_ID, CLIENT_SECRET) for the Azure Data Lake. Defaults to None.</p> <code>None</code> <code>adls_path</code> <code>Optional[str]</code> <p>Azure Data Lake destination file path (with file name). Defaults to None.</p> <code>None</code> <code>adls_path_overwrite</code> <code>bool</code> <p>Whether to overwrite the file in ADLS. Defaults to True.</p> <code>False</code> Source code in <code>src/viadot/orchestration/prefect/flows/azure_sql_to_adls.py</code> <pre><code>@flow(\n    name=\"Azure SQL extraction to ADLS\",\n    description=\"Extract data from Azure SQL\"\n    + \" and load it into Azure Data Lake Storage.\",\n    retries=1,\n    retry_delay_seconds=60,\n    task_runner=ConcurrentTaskRunner,\n    log_prints=True,\n)\ndef azure_sql_to_adls(\n    query: str | None = None,\n    credentials_secret: str | None = None,\n    validate_df_dict: dict[str, Any] | None = None,\n    convert_bytes: bool = False,\n    remove_special_characters: bool | None = None,\n    columns_to_clean: list[str] | None = None,\n    adls_config_key: str | None = None,\n    adls_azure_key_vault_secret: str | None = None,\n    adls_path: str | None = None,\n    adls_path_overwrite: bool = False,\n) -&gt; None:\n    r\"\"\"Download data from Azure SQL to a CSV file and uploading it to ADLS.\n\n    Args:\n        query (str): Query to perform on a database. Defaults to None.\n        credentials_secret (str, optional): The name of the Azure Key Vault\n            secret containing a dictionary with database credentials.\n            Defaults to None.\n        validate_df_dict (Dict[str], optional): A dictionary with optional list of\n            tests to verify the output dataframe. If defined, triggers the `validate_df`\n            task from task_utils. Defaults to None.\n        convert_bytes (bool). A boolean value to trigger method df_converts_bytes_to_int\n            It is used to convert bytes data type into int, as pulling data with bytes\n            can lead to malformed data in data frame.\n            Defaults to False.\n        remove_special_characters (str, optional): Call a function that remove\n            special characters like escape symbols. Defaults to None.\n        columns_to_clean (List(str), optional): Select columns to clean, used with\n            remove_special_characters. If None whole data frame will be processed.\n            Defaults to None.\n        adls_config_key (Optional[str], optional): The key in the viadot config holding\n            relevant credentials. Defaults to None.\n        adls_azure_key_vault_secret (Optional[str], optional): The name of the Azure Key\n            Vault secret containing a dictionary with ACCOUNT_NAME and Service Principal\n            credentials (TENANT_ID, CLIENT_ID, CLIENT_SECRET) for the Azure Data Lake.\n            Defaults to None.\n        adls_path (Optional[str], optional): Azure Data Lake destination file path (with\n            file name). Defaults to None.\n        adls_path_overwrite (bool, optional): Whether to overwrite the file in ADLS.\n            Defaults to True.\n    \"\"\"\n    data_frame = azure_sql_to_df(\n        query=query,\n        credentials_secret=credentials_secret,\n        validate_df_dict=validate_df_dict,\n        convert_bytes=convert_bytes,\n        remove_special_characters=remove_special_characters,\n        columns_to_clean=columns_to_clean,\n    )\n\n    return df_to_adls(\n        df=data_frame,\n        path=adls_path,\n        credentials_secret=adls_azure_key_vault_secret,\n        config_key=adls_config_key,\n        overwrite=adls_path_overwrite,\n    )\n</code></pre>"},{"location":"references/orchestration/prefect/flows/#viadot.orchestration.prefect.flows.bigquery_to_adls","title":"<code>viadot.orchestration.prefect.flows.bigquery_to_adls</code>","text":"<p>'bigquery_to_adls.py'.</p>"},{"location":"references/orchestration/prefect/flows/#viadot.orchestration.prefect.flows.bigquery_to_adls.bigquery_to_adls","title":"<code>bigquery_to_adls(config_key=None, azure_key_vault_secret=None, query=None, dataset_name=None, table_name=None, date_column_name=None, start_date=None, end_date=None, columns=None, adls_config_key=None, adls_azure_key_vault_secret=None, adls_path=None, adls_path_overwrite=False)</code>","text":"<p>Download data from BigQuery to Azure Data Lake.</p> <p>Parameters:</p> Name Type Description Default <code>config_key</code> <code>str</code> <p>The key in the viadot config holding relevant credentials. Defaults to None.</p> <code>None</code> <code>azure_key_vault_secret</code> <code>str</code> <p>The name of the Azure Key Vault secret where credentials are stored. Defaults to None.</p> <code>None</code> <code>query</code> <code>str</code> <p>SQL query to querying data in BigQuery. Format of basic query: (SELECT * FROM <code>{project}.{dataset_name}.{table_name}</code>). Defaults to None.</p> <code>None</code> <code>dataset_name</code> <code>str</code> <p>Dataset name. Defaults to None.</p> <code>None</code> <code>table_name</code> <code>str</code> <p>Table name. Defaults to None.</p> <code>None</code> <code>dataset_name</code> <code>str</code> <p>Dataset name. Defaults to None.</p> <code>None</code> <code>date_column_name</code> <code>str</code> <p>The user can provide the name of the date column. If the user-specified column does not exist, all data will be retrieved from the table. Defaults to None.</p> <code>None</code> <code>start_date</code> <code>str</code> <p>Parameter to pass start date e.g. \"2022-01-01\". Defaults to None.</p> <code>None</code> <code>end_date</code> <code>str</code> <p>Parameter to pass end date e.g. \"2022-01-01\". Defaults to None.</p> <code>None</code> <code>columns</code> <code>list[str]</code> <p>List of columns from given table name. Defaults to None.</p> <code>None</code> <code>adls_config_key</code> <code>str</code> <p>The key in the viadot config holding relevant credentials. Defaults to None.</p> <code>None</code> <code>adls_azure_key_vault_secret</code> <code>str</code> <p>The name of the Azure Key Vault secret containing a dictionary with ACCOUNT_NAME and Service Principal credentials (TENANT_ID, CLIENT_ID, CLIENT_SECRET) for the Azure Data Lake. Defaults to None.</p> <code>None</code> <code>adls_path</code> <code>str</code> <p>Azure Data Lake destination file path. Defaults to None.</p> <code>None</code> <code>adls_path_overwrite</code> <code>bool</code> <p>Whether to overwrite the file in ADLS. Defaults to True.</p> <code>False</code> Source code in <code>src/viadot/orchestration/prefect/flows/bigquery_to_adls.py</code> <pre><code>@flow(\n    name=\"BigQuery extraction to ADLS\",\n    description=\"Extract data from BigQuery and load it into Azure Data Lake Storage.\",\n    retries=1,\n    retry_delay_seconds=60,\n    task_runner=ConcurrentTaskRunner,\n)\ndef bigquery_to_adls(  # noqa: PLR0913\n    config_key: str | None = None,\n    azure_key_vault_secret: str | None = None,\n    query: str | None = None,\n    dataset_name: str | None = None,\n    table_name: str | None = None,\n    date_column_name: str | None = None,\n    start_date: str | None = None,\n    end_date: str | None = None,\n    columns: list[str] | None = None,\n    adls_config_key: str | None = None,\n    adls_azure_key_vault_secret: str | None = None,\n    adls_path: str | None = None,\n    adls_path_overwrite: bool = False,\n) -&gt; None:\n    \"\"\"Download data from BigQuery to Azure Data Lake.\n\n    Args:\n        config_key (str, optional): The key in the viadot config holding relevant\n            credentials. Defaults to None.\n        azure_key_vault_secret (str, optional): The name of the Azure Key Vault secret\n            where credentials are stored. Defaults to None.\n        query (str, optional): SQL query to querying data in BigQuery. Format of basic\n            query: (SELECT * FROM `{project}.{dataset_name}.{table_name}`).\n            Defaults to None.\n        dataset_name (str, optional): Dataset name. Defaults to None.\n        table_name (str, optional): Table name. Defaults to None.\n        dataset_name (str, optional): Dataset name. Defaults to None.\n        date_column_name (str, optional): The user can provide the name of the date\n            column. If the user-specified column does not exist, all data will be\n            retrieved from the table. Defaults to None.\n        start_date (str, optional): Parameter to pass start date e.g.\n            \"2022-01-01\". Defaults to None.\n        end_date (str, optional): Parameter to pass end date e.g.\n            \"2022-01-01\". Defaults to None.\n        columns (list[str], optional): List of columns from given table name.\n            Defaults to None.\n        adls_config_key (str, optional): The key in the viadot config holding relevant\n            credentials. Defaults to None.\n        adls_azure_key_vault_secret (str, optional): The name of the Azure Key Vault\n            secret containing a dictionary with ACCOUNT_NAME and Service Principal\n            credentials (TENANT_ID, CLIENT_ID, CLIENT_SECRET) for the Azure Data Lake.\n            Defaults to None.\n        adls_path (str, optional): Azure Data Lake destination file path.\n            Defaults to None.\n        adls_path_overwrite (bool, optional): Whether to overwrite the file in ADLS.\n            Defaults to True.\n    \"\"\"\n    data_frame = bigquery_to_df(\n        config_key=config_key,\n        azure_key_vault_secret=azure_key_vault_secret,\n        query=query,\n        dataset_name=dataset_name,\n        table_name=table_name,\n        date_column_name=date_column_name,\n        start_date=start_date,\n        end_date=end_date,\n        columns=columns,\n    )\n\n    return df_to_adls(\n        df=data_frame,\n        path=adls_path,\n        credentials_secret=adls_azure_key_vault_secret,\n        config_key=adls_config_key,\n        overwrite=adls_path_overwrite,\n    )\n</code></pre>"},{"location":"references/orchestration/prefect/flows/#viadot.orchestration.prefect.flows.business_core_to_parquet","title":"<code>viadot.orchestration.prefect.flows.business_core_to_parquet</code>","text":"<p>Flow for downloading data from Business Core API to a Parquet file.</p>"},{"location":"references/orchestration/prefect/flows/#viadot.orchestration.prefect.flows.business_core_to_parquet.business_core_to_parquet","title":"<code>business_core_to_parquet(path=None, url=None, filters=None, credentials_secret=None, config_key=None, if_empty='skip', if_exists='replace', verify=True)</code>","text":"<p>Download data from Business Core API to a Parquet file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>(str, required)</code> <p>Path where to save the Parquet file. Defaults to None.</p> <code>None</code> <code>url</code> <code>(str, required)</code> <p>Base url to the view in Business Core API. Defaults to None.</p> <code>None</code> <code>filters</code> <code>dict[str, Any]</code> <p>Filters in form of dictionary. Available filters: 'BucketCount', 'BucketNo', 'FromDate', 'ToDate'. Defaults to None.</p> <code>None</code> <code>credentials_secret</code> <code>str</code> <p>The name of the secret that stores Business Core credentials. Defaults to None. More info on: https://docs.prefect.io/concepts/blocks/</p> <code>None</code> <code>config_key</code> <code>str</code> <p>The key in the viadot config holding relevant credentials. Defaults to None.</p> <code>None</code> <code>if_empty</code> <code>str</code> <p>What to do if output DataFrame is empty. Defaults to \"skip\".</p> <code>'skip'</code> <code>if_exists</code> <code>Literal['append', 'replace', 'skip']</code> <p>What to do if the table exists. Defaults to \"replace\".</p> <code>'replace'</code> <code>verify</code> <code>bool</code> <p>Whether or not verify certificates while connecting to an API. Defaults to True.</p> <code>True</code> Source code in <code>src/viadot/orchestration/prefect/flows/business_core_to_parquet.py</code> <pre><code>@flow(\n    name=\"extract--businesscore--parquet\",\n    description=\"Extract data from Business Core API and load it into Parquet file\",\n    retries=1,\n    retry_delay_seconds=60,\n)\ndef business_core_to_parquet(\n    path: str | None = None,\n    url: str | None = None,\n    filters: dict[str, Any] | None = None,\n    credentials_secret: str | None = None,\n    config_key: str | None = None,\n    if_empty: str = \"skip\",\n    if_exists: Literal[\"append\", \"replace\", \"skip\"] = \"replace\",\n    verify: bool = True,\n) -&gt; None:\n    \"\"\"Download data from Business Core API to a Parquet file.\n\n    Args:\n        path (str, required): Path where to save the Parquet file. Defaults to None.\n        url (str, required): Base url to the view in Business Core API.\n            Defaults to None.\n        filters (dict[str, Any], optional): Filters in form of dictionary.\n            Available filters: 'BucketCount', 'BucketNo', 'FromDate', 'ToDate'.\n            Defaults to None.\n        credentials_secret (str, optional): The name of the secret that stores Business\n            Core credentials. Defaults to None.\n            More info on: https://docs.prefect.io/concepts/blocks/\n        config_key (str, optional): The key in the viadot config holding relevant\n            credentials. Defaults to None.\n        if_empty (str, optional): What to do if output DataFrame is empty.\n            Defaults to \"skip\".\n        if_exists (Literal[\"append\", \"replace\", \"skip\"], optional):\n            What to do if the table exists. Defaults to \"replace\".\n        verify (bool, optional): Whether or not verify certificates while\n            connecting to an API. Defaults to True.\n    \"\"\"\n    df = business_core_to_df(\n        url=url,\n        credentials_secret=credentials_secret,\n        config_key=config_key,\n        filters=filters,\n        if_empty=if_empty,\n        verify=verify,\n    )\n    return df_to_parquet(\n        df=df,\n        path=path,\n        if_exists=if_exists,\n    )\n</code></pre>"},{"location":"references/orchestration/prefect/flows/#viadot.orchestration.prefect.flows.cloud_for_customers_to_adls","title":"<code>viadot.orchestration.prefect.flows.cloud_for_customers_to_adls</code>","text":"<p>Flow for pulling data from CloudForCustomers to Adls.</p>"},{"location":"references/orchestration/prefect/flows/#viadot.orchestration.prefect.flows.cloud_for_customers_to_adls.cloud_for_customers_to_adls","title":"<code>cloud_for_customers_to_adls(cloud_for_customers_url=None, fields=None, dtype=None, endpoint=None, report_url=None, filter_params=None, adls_path=None, overwrite=False, cloud_for_customers_credentials_secret=None, cloud_for_customers_config_key=None, adls_credentials_secret=None, adls_config_key=None, **kwargs)</code>","text":"<p>Download records from SAP Cloud for Customers and upload them to Azure Data Lake.</p> <p>Parameters:</p> Name Type Description Default <code>cloud_for_customers_url</code> <code>str</code> <p>The URL to the C4C API. For example, 'https://myNNNNNN.crm.ondemand.com/c4c/v1/'.</p> <code>None</code> <code>fields</code> <code>list[str]</code> <p>List of fields to put in DataFrame.</p> <code>None</code> <code>dtype</code> <code>dict</code> <p>The dtypes to use in the DataFrame.</p> <code>None</code> <code>endpoint</code> <code>str</code> <p>The API endpoint.</p> <code>None</code> <code>report_url</code> <code>str</code> <p>The API url in case of prepared report.</p> <code>None</code> <code>filter_params</code> <code>dict[str, Any]</code> <p>Query parameters.</p> <code>None</code> <code>adls_path</code> <code>str</code> <p>The destination path.</p> <code>None</code> <code>overwrite</code> <code>bool</code> <p>Whether to overwrite files in the lake. Defaults to False.</p> <code>False</code> <code>cloud_for_customers_credentials_secret</code> <code>str</code> <p>The name of the Azure Key Vault secret storing the C4C credentials. Defaults to None.</p> <code>None</code> <code>cloud_for_customers_config_key</code> <code>str</code> <p>The key in the viadot config holding relevant credentials. Defaults to None.</p> <code>None</code> <code>adls_credentials_secret</code> <code>str</code> <p>The name of the Azure Key Vault secret storing the ADLS credentials. Defaults to None.</p> <code>None</code> <code>adls_config_key</code> <code>str</code> <p>The key in the viadot config holding relevant credentials. Defaults to None.</p> <code>None</code> <code>kwargs</code> <code>dict[str, Any] | None</code> <p>The parameters to pass to the DataFrame constructor.</p> <code>{}</code> Source code in <code>src/viadot/orchestration/prefect/flows/cloud_for_customers_to_adls.py</code> <pre><code>@flow\ndef cloud_for_customers_to_adls(  # noqa: PLR0913\n    # C4C\n    cloud_for_customers_url: str | None = None,\n    fields: list[str] | None = None,\n    dtype: dict[str, Any] | None = None,\n    endpoint: str | None = None,\n    report_url: str | None = None,\n    filter_params: dict[str, Any] | None = None,\n    # ADLS\n    adls_path: str | None = None,\n    overwrite: bool = False,\n    # Auth\n    cloud_for_customers_credentials_secret: str | None = None,\n    cloud_for_customers_config_key: str | None = None,\n    adls_credentials_secret: str | None = None,\n    adls_config_key: str | None = None,\n    **kwargs: dict[str, Any] | None,\n) -&gt; None:\n    \"\"\"Download records from SAP Cloud for Customers and upload them to Azure Data Lake.\n\n    Args:\n        cloud_for_customers_url (str): The URL to the C4C API. For example,\n            'https://myNNNNNN.crm.ondemand.com/c4c/v1/'.\n        fields (list[str], optional): List of fields to put in DataFrame.\n        dtype (dict, optional): The dtypes to use in the DataFrame.\n        endpoint (str, optional): The API endpoint.\n        report_url (str, optional): The API url in case of prepared report.\n        filter_params (dict[str, Any], optional): Query parameters.\n        adls_path (str): The destination path.\n        overwrite (bool, optional): Whether to overwrite files in the lake. Defaults to\n            False.\n        cloud_for_customers_credentials_secret (str, optional): The name of the Azure\n            Key Vault secret storing the C4C credentials. Defaults to None.\n        cloud_for_customers_config_key (str, optional): The key in the viadot config\n            holding relevant credentials. Defaults to None.\n        adls_credentials_secret (str, optional): The name of the Azure Key Vault secret\n            storing the ADLS credentials. Defaults to None.\n        adls_config_key (str, optional): The key in the viadot config holding relevant\n            credentials. Defaults to None.\n        kwargs: The parameters to pass to the DataFrame constructor.\n    \"\"\"\n    df = cloud_for_customers_to_df(\n        url=cloud_for_customers_url,\n        fields=fields,\n        dtype=dtype,\n        endpoint=endpoint,\n        report_url=report_url,\n        credentials_secret=cloud_for_customers_credentials_secret,\n        config_key=cloud_for_customers_config_key,\n        filter_params=filter_params,\n        **kwargs,\n    )\n\n    return df_to_adls(\n        df=df,\n        path=adls_path,\n        credentials_secret=adls_credentials_secret,\n        config_key=adls_config_key,\n        overwrite=overwrite,\n    )\n</code></pre>"},{"location":"references/orchestration/prefect/flows/#viadot.orchestration.prefect.flows.cloud_for_customers_to_databricks","title":"<code>viadot.orchestration.prefect.flows.cloud_for_customers_to_databricks</code>","text":"<p>Flow for pulling data from CloudForCustomers to Databricks.</p>"},{"location":"references/orchestration/prefect/flows/#viadot.orchestration.prefect.flows.cloud_for_customers_to_databricks.cloud_for_customers_to_databricks","title":"<code>cloud_for_customers_to_databricks(cloud_for_customers_url, fields=None, dtype=None, endpoint=None, report_url=None, filter_params=None, databricks_table=None, databricks_schema=None, if_exists='fail', cloud_for_customers_credentials_secret=None, cloud_for_customers_config_key=None, databricks_credentials_secret=None, databricks_config_key=None, **kwargs)</code>","text":"<p>Download a file from SAP Cloud for Customers and upload it to Azure Data Lake.</p> <p>Parameters:</p> Name Type Description Default <code>cloud_for_customers_url</code> <code>str</code> <p>The URL to the C4C API. For example, 'https://myNNNNNN.crm.ondemand.com/c4c/v1/'.</p> required <code>fields</code> <code>list[str]</code> <p>List of fields to put in DataFrame.</p> <code>None</code> <code>dtype</code> <code>dict</code> <p>The dtypes to use in the DataFrame.</p> <code>None</code> <code>endpoint</code> <code>str</code> <p>The API endpoint.</p> <code>None</code> <code>report_url</code> <code>str</code> <p>The API url in case of prepared report.</p> <code>None</code> <code>filter_params</code> <code>dict[str, Any]</code> <p>Query parameters.</p> <code>None</code> <code>databricks_table</code> <code>str</code> <p>The name of the target table.</p> <code>None</code> <code>databricks_schema</code> <code>str</code> <p>The name of the target schema.</p> <code>None</code> <code>if_exists</code> <code>(str, Optional)</code> <p>What to do if the table already exists. One of 'replace', 'skip', and 'fail'.</p> <code>'fail'</code> <code>cloud_for_customers_credentials_secret</code> <code>str</code> <p>The name of the Azure Key Vault secret storing the C4C credentials. Defaults to None.</p> <code>None</code> <code>cloud_for_customers_config_key</code> <code>str</code> <p>The key in the viadot config holding relevant credentials. Defaults to None.</p> <code>None</code> <code>databricks_credentials_secret</code> <code>str</code> <p>The name of the Azure Key Vault secret storing relevant credentials. Defaults to None.</p> <code>None</code> <code>databricks_config_key</code> <code>str</code> <p>The key in the viadot config holding relevant credentials. Defaults to None.</p> <code>None</code> <code>kwargs</code> <code>dict[str, Any] | None</code> <p>The parameters to pass to the DataFrame constructor.</p> <code>{}</code> Source code in <code>src/viadot/orchestration/prefect/flows/cloud_for_customers_to_databricks.py</code> <pre><code>@flow\ndef cloud_for_customers_to_databricks(  # noqa: PLR0913\n    # C4C\n    cloud_for_customers_url: str,\n    fields: list[str] | None = None,\n    dtype: dict[str, Any] | None = None,\n    endpoint: str | None = None,\n    report_url: str | None = None,\n    filter_params: dict[str, Any] | None = None,\n    # Databricks\n    databricks_table: str | None = None,\n    databricks_schema: str | None = None,\n    if_exists: Literal[\"replace\", \"skip\", \"fail\"] = \"fail\",\n    # Auth\n    cloud_for_customers_credentials_secret: str | None = None,\n    cloud_for_customers_config_key: str | None = None,\n    databricks_credentials_secret: str | None = None,\n    databricks_config_key: str | None = None,\n    **kwargs: dict[str, Any] | None,\n) -&gt; None:\n    \"\"\"Download a file from SAP Cloud for Customers and upload it to Azure Data Lake.\n\n    Args:\n        cloud_for_customers_url (str): The URL to the C4C API. For example,\n            'https://myNNNNNN.crm.ondemand.com/c4c/v1/'.\n        fields (list[str], optional): List of fields to put in DataFrame.\n        dtype (dict, optional): The dtypes to use in the DataFrame.\n        endpoint (str, optional): The API endpoint.\n        report_url (str, optional): The API url in case of prepared report.\n        filter_params (dict[str, Any], optional): Query parameters.\n        databricks_table (str): The name of the target table.\n        databricks_schema (str, optional): The name of the target schema.\n        if_exists (str, Optional): What to do if the table already exists. One of\n            'replace', 'skip', and 'fail'.\n        cloud_for_customers_credentials_secret (str, optional): The name of the Azure\n            Key Vault secret storing the C4C credentials. Defaults to None.\n        cloud_for_customers_config_key (str, optional): The key in the viadot config\n            holding relevant credentials. Defaults to None.\n        databricks_credentials_secret (str, optional): The name of the Azure Key Vault\n            secret storing relevant credentials. Defaults to None.\n        databricks_config_key (str, optional): The key in the viadot config holding\n            relevant credentials. Defaults to None.\n        kwargs: The parameters to pass to the DataFrame constructor.\n    \"\"\"\n    df = cloud_for_customers_to_df(\n        url=cloud_for_customers_url,\n        fields=fields,\n        dtype=dtype,\n        endpoint=endpoint,\n        report_url=report_url,\n        credentials_secret=cloud_for_customers_credentials_secret,\n        config_key=cloud_for_customers_config_key,\n        filter_params=filter_params,\n        **kwargs,\n    )\n\n    return df_to_databricks(\n        df=df,\n        schema=databricks_schema,\n        table=databricks_table,\n        if_exists=if_exists,\n        credentials_secret=databricks_credentials_secret,\n        config_key=databricks_config_key,\n    )\n</code></pre>"},{"location":"references/orchestration/prefect/flows/#viadot.orchestration.prefect.flows.customer_gauge_to_adls","title":"<code>viadot.orchestration.prefect.flows.customer_gauge_to_adls</code>","text":"<p>'customer_gauge_to_adls.py'.</p>"},{"location":"references/orchestration/prefect/flows/#viadot.orchestration.prefect.flows.customer_gauge_to_adls.customer_gauge_to_adls","title":"<code>customer_gauge_to_adls(config_key=None, azure_key_vault_secret=None, endpoint='non-responses', cursor=None, pagesize=1000, date_field=None, start_date=None, end_date=None, total_load=True, unpack_by_field_reference_cols=None, unpack_by_nested_dict_transformer=None, validate_df_dict=None, anonymize=False, columns_to_anonymize=None, anonymize_method='mask', anonymize_value='***', date_column=None, days=None, adls_azure_key_vault_secret=None, adls_config_key=None, adls_path=None, adls_path_overwrite=False)</code>","text":"<p>Download data from the Customer Gauge API using Prefect.</p> <p>Parameters:</p> Name Type Description Default <code>config_key</code> <code>str</code> <p>The key in the viadot config holding relevant credentials. Defaults to None.</p> <code>None</code> <code>azure_key_vault_secret</code> <code>str</code> <p>The name of the Azure Key Vault secret containing a dictionary with ['client_id', 'client_secret']. Defaults to None.</p> <code>None</code> <code>endpoint</code> <code>Literal['responses', 'non-responses']</code> <p>Indicate which endpoint to connect. Defaults to \"non-responses.</p> <code>'non-responses'</code> <code>cursor</code> <code>int</code> <p>Cursor value to navigate to the page. Defaults to None.</p> <code>None</code> <code>pagesize</code> <code>int</code> <p>Number of responses (records) returned per page, max value = 1000. Defaults to 1000. Defaults to 1000.</p> <code>1000</code> <code>date_field</code> <code>str</code> <p>Specifies the date type which filter date range. Possible options: \"date_creation\", \"date_order\", \"date_sent\" or \"date_survey_response\". Defaults to None.</p> <code>None</code> <code>start_date</code> <code>datetime</code> <p>Defines the period start date in yyyy-mm-dd format. Defaults to None.</p> <code>None</code> <code>end_date</code> <code>datetime</code> <p>Defines the period end date in yyyy-mm-dd format. Defaults to None.</p> <code>None</code> <code>total_load</code> <code>bool</code> <p>Indicate whether to download the data to the latest. If 'False', only one API call is executed (up to 1000 records). Defaults to True.</p> <code>True</code> <code>unpack_by_field_reference_cols</code> <code>list[str]</code> <p>Columns to unpack and modify using <code>_field_reference_unpacker</code>. Defaults to None.</p> <code>None</code> <code>unpack_by_nested_dict_transformer</code> <code>list[str]</code> <p>Columns to unpack and modify using <code>_nested_dict_transformer</code>. Defaults to None.</p> <code>None</code> <code>validate_df_dict</code> <code>dict[str]</code> <p>A dictionary with optional list of tests to verify the output dataframe. If defined, triggers the <code>validate_df</code> task from task_utils. Defaults to None.</p> <code>None</code> <code>anonymize</code> <code>bool</code> <p>Indicates if anonymize selected columns. Defaults to False.</p> <code>False</code> <code>columns_to_anonymize</code> <code>list[str]</code> <p>List of columns to anonymize. Defaults to None.</p> <code>None</code> <code>anonymize_method</code> <code> (Literal[\"mask\", \"hash\"]</code> <p>Method of anonymizing data. \"mask\" -&gt; replace the data with \"value\" arg. \"hash\" -&gt; replace the data with the hash value of an object (using <code>hash()</code> method). Defaults to \"mask\".</p> <code>'mask'</code> <code>anonymize_value</code> <code>str</code> <p>Value to replace the data. Defaults to \"***\".</p> <code>'***'</code> <code>date_column</code> <code>str</code> <p>Name of the date column used to identify rows that are older than a specified number of days. Defaults to None.</p> <code>None</code> <code>days</code> <code>int</code> <p>The number of days beyond which we want to anonymize the data, e.g. older than 2 years can be: 2*365. Defaults to None.</p> <code>None</code> <code>adls_azure_key_vault_secret</code> <code>str</code> <p>The name of the Azure Key. Defaults to None.</p> <code>None</code> <code>adls_config_key</code> <code>str</code> <p>The key in the viadot config holding relevant credentials. Defaults to None.</p> <code>None</code> <code>adls_path</code> <code>str</code> <p>Azure Data Lake destination folder/catalog path. Defaults to None.</p> <code>None</code> <code>adls_path_overwrite</code> <code>bool</code> <p>Whether to overwrite the file in ADLS. Defaults to False.</p> <code>False</code> Source code in <code>src/viadot/orchestration/prefect/flows/customer_gauge_to_adls.py</code> <pre><code>@flow\ndef customer_gauge_to_adls(  # noqa: PLR0913\n    config_key: str | None = None,\n    azure_key_vault_secret: str | None = None,\n    endpoint: Literal[\"responses\", \"non-responses\"] = \"non-responses\",\n    cursor: int | None = None,\n    pagesize: int = 1000,\n    date_field: str | None = None,\n    start_date: datetime | None = None,\n    end_date: datetime | None = None,\n    total_load: bool = True,\n    unpack_by_field_reference_cols: list[str] | None = None,\n    unpack_by_nested_dict_transformer: list[str] | None = None,\n    validate_df_dict: dict[str, Any] | None = None,\n    anonymize: bool = False,\n    columns_to_anonymize: list[str] | None = None,\n    anonymize_method: Literal[\"mask\", \"hash\"] = \"mask\",\n    anonymize_value: str = \"***\",\n    date_column: str | None = None,\n    days: int | None = None,\n    adls_azure_key_vault_secret: str | None = None,\n    adls_config_key: str | None = None,\n    adls_path: str | None = None,\n    adls_path_overwrite: bool = False,\n) -&gt; None:\n    \"\"\"Download data from the Customer Gauge API using Prefect.\n\n    Args:\n        config_key (str, optional): The key in the viadot config holding relevant\n            credentials. Defaults to None.\n        azure_key_vault_secret (str, optional): The name of the Azure Key Vault secret\n            containing a dictionary with ['client_id', 'client_secret'].\n            Defaults to None.\n        endpoint (Literal[\"responses\", \"non-responses\"], optional): Indicate which\n            endpoint to connect. Defaults to \"non-responses.\n        cursor (int, optional): Cursor value to navigate to the page.\n            Defaults to None.\n        pagesize (int, optional): Number of responses (records) returned per page,\n            max value = 1000. Defaults to 1000. Defaults to 1000.\n        date_field (str, optional): Specifies the date type which filter date range.\n            Possible options: \"date_creation\", \"date_order\", \"date_sent\" or\n            \"date_survey_response\". Defaults to None.\n        start_date (datetime, optional): Defines the period start date in\n            yyyy-mm-dd format. Defaults to None.\n        end_date (datetime, optional): Defines the period end date in\n            yyyy-mm-dd format. Defaults to None.\n        total_load (bool, optional): Indicate whether to download the data to the\n            latest. If 'False', only one API call is executed (up to 1000 records).\n            Defaults to True.\n        unpack_by_field_reference_cols (list[str]): Columns to unpack and modify using\n            `_field_reference_unpacker`. Defaults to None.\n        unpack_by_nested_dict_transformer (list[str]): Columns to unpack and modify\n            using `_nested_dict_transformer`. Defaults to None.\n        validate_df_dict (dict[str], optional): A dictionary with optional list of\n            tests to verify the output dataframe. If defined, triggers the\n            `validate_df` task from task_utils. Defaults to None.\n        anonymize (bool, optional): Indicates if anonymize selected columns.\n            Defaults to False.\n        columns_to_anonymize (list[str], optional): List of columns to anonymize.\n            Defaults to None.\n        anonymize_method  (Literal[\"mask\", \"hash\"], optional): Method of\n            anonymizing data. \"mask\" -&gt; replace the data with \"value\" arg. \"hash\" -&gt;\n            replace the data with the hash value of an object (using `hash()`\n            method). Defaults to \"mask\".\n        anonymize_value (str, optional): Value to replace the data.\n            Defaults to \"***\".\n        date_column (str, optional): Name of the date column used to identify rows\n            that are older than a specified number of days. Defaults to None.\n        days (int, optional): The number of days beyond which we want to anonymize\n            the data, e.g. older than 2 years can be: 2*365. Defaults to None.\n        adls_azure_key_vault_secret (str, optional): The name of the Azure Key.\n            Defaults to None.\n        adls_config_key (str, optional): The key in the viadot config holding relevant\n            credentials. Defaults to None.\n        adls_path (str, optional): Azure Data Lake destination folder/catalog path.\n            Defaults to None.\n        adls_path_overwrite (bool, optional): Whether to overwrite the file in ADLS.\n            Defaults to False.\n    \"\"\"\n    data_frame = customer_gauge_to_df(\n        config_key=config_key,\n        azure_key_vault_secret=azure_key_vault_secret,\n        endpoint=endpoint,\n        cursor=cursor,\n        pagesize=pagesize,\n        date_field=date_field,\n        start_date=start_date,\n        end_date=end_date,\n        total_load=total_load,\n        unpack_by_field_reference_cols=unpack_by_field_reference_cols,\n        unpack_by_nested_dict_transformer=unpack_by_nested_dict_transformer,\n        validate_df_dict=validate_df_dict,\n        anonymize=anonymize,\n        columns_to_anonymize=columns_to_anonymize,\n        anonymize_method=anonymize_method,\n        anonymize_value=anonymize_value,\n        date_column=date_column,\n        days=days,\n    )\n\n    return df_to_adls(\n        df=data_frame,\n        path=adls_path,\n        credentials_secret=adls_azure_key_vault_secret,\n        config_key=adls_config_key,\n        overwrite=adls_path_overwrite,\n    )\n</code></pre>"},{"location":"references/orchestration/prefect/flows/#viadot.orchestration.prefect.flows.duckdb_to_parquet","title":"<code>viadot.orchestration.prefect.flows.duckdb_to_parquet</code>","text":"<p>Flow for extracting data from the DuckDB to a Parquet file.</p>"},{"location":"references/orchestration/prefect/flows/#viadot.orchestration.prefect.flows.duckdb_to_parquet.duckdb_to_parquet","title":"<code>duckdb_to_parquet(query, path, if_exists='replace', duckdb_credentials_secret=None, duckdb_credentials=None, duckdb_config_key=None)</code>","text":"<p>Download a table from DuckDB and save it to a Parquet file.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>(str, required)</code> <p>The query to execute on the DuckDB database. If the query doesn't start with \"SELECT\", returns an empty DataFrame.</p> required <code>path</code> <code>str</code> <p>Path where to save a Parquet file which will be created while executing flow.</p> required <code>if_exists</code> <code>Literal</code> <p>What to do if the file exists. Defaults to \"replace\".</p> <code>'replace'</code> <code>duckdb_credentials_secret</code> <code>str</code> <p>The name of the secret storing the credentials to the DuckDB. Defaults to None. More info on: https://docs.prefect.io/concepts/blocks/</p> <code>None</code> <code>duckdb_credentials</code> <code>dict[str, Any]</code> <p>Credentials to the DuckDB. Defaults to None.</p> <code>None</code> <code>duckdb_config_key</code> <code>str</code> <p>The key in the viadot config holding relevant credentials to the DuckDB. Defaults to None.</p> <code>None</code> Source code in <code>src/viadot/orchestration/prefect/flows/duckdb_to_parquet.py</code> <pre><code>@flow(\n    name=\"extract--duckdb--parquet\",\n    description=\"Extract data from DuckDB and save it to Parquet file.\",\n    retries=1,\n    retry_delay_seconds=60,\n    timeout_seconds=2 * 60 * 60,\n)\ndef duckdb_to_parquet(\n    query: str,\n    path: str,\n    if_exists: Literal[\"append\", \"replace\", \"skip\"] = \"replace\",\n    duckdb_credentials_secret: str | None = None,\n    # Specifying credentials in a dictionary is not recommended in the viadot flows,\n    # but in this case credentials can include only database name.\n    duckdb_credentials: dict[str, Any] | None = None,\n    duckdb_config_key: str | None = None,\n) -&gt; None:\n    \"\"\"Download a table from DuckDB and save it to a Parquet file.\n\n    Args:\n        query (str, required): The query to execute on the DuckDB database. If the query\n            doesn't start with \"SELECT\", returns an empty DataFrame.\n        path (str): Path where to save a Parquet file which will be created while\n            executing flow.\n        if_exists (Literal, optional): What to do if the file exists. Defaults to\n            \"replace\".\n        duckdb_credentials_secret (str, optional): The name of the secret storing\n            the credentials to the DuckDB. Defaults to None.\n            More info on: https://docs.prefect.io/concepts/blocks/\n        duckdb_credentials (dict[str, Any], optional): Credentials to the DuckDB.\n            Defaults to None.\n        duckdb_config_key (str, optional): The key in the viadot config holding relevant\n            credentials to the DuckDB. Defaults to None.\n\n    \"\"\"\n    df = duckdb_query(\n        query=query,\n        fetch_type=\"dataframe\",\n        credentials=duckdb_credentials,\n        config_key=duckdb_config_key,\n        credentials_secret=duckdb_credentials_secret,\n    )\n    return df_to_parquet(\n        df=df,\n        path=path,\n        if_exists=if_exists,\n    )\n</code></pre>"},{"location":"references/orchestration/prefect/flows/#viadot.orchestration.prefect.flows.duckdb_to_sql_server","title":"<code>viadot.orchestration.prefect.flows.duckdb_to_sql_server</code>","text":"<p>Flow for extracting data from the DuckDB into SQLServer.</p>"},{"location":"references/orchestration/prefect/flows/#viadot.orchestration.prefect.flows.duckdb_to_sql_server.cleanup_csv_task","title":"<code>cleanup_csv_task(path)</code>","text":"<p>Remove a CSV file from the local filesystem.</p> Source code in <code>src/viadot/orchestration/prefect/flows/duckdb_to_sql_server.py</code> <pre><code>@task(timeout_seconds=60 * 60)\ndef cleanup_csv_task(path: str) -&gt; None:\n    \"\"\"Remove a CSV file from the local filesystem.\"\"\"\n    logger = get_run_logger()\n\n    logger.info(f\"Removing file {path}...\")\n    try:\n        Path(path).unlink()\n        logger.info(f\"File {path} has been successfully removed.\")\n    except Exception:\n        logger.exception(f\"File {path} could not be removed.\")\n</code></pre>"},{"location":"references/orchestration/prefect/flows/#viadot.orchestration.prefect.flows.duckdb_to_sql_server.duckdb_to_sql_server","title":"<code>duckdb_to_sql_server(query, local_path, db_table, db_schema, if_exists='replace', dtypes=None, chunksize=5000, error_log_file_path='./log_file.log', on_error='skip', duckdb_credentials_secret=None, duckdb_credentials=None, duckdb_config_key=None, sql_server_credentials_secret=None, sql_server_config_key=None)</code>","text":"<p>Download a table from DuckDB and upload it to the SQLServer.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>(str, required)</code> <p>The query to execute on the SQL Server database. If the qery doesn't start with \"SELECT\" returns an empty DataFrame.</p> required <code>local_path</code> <code>str</code> <p>Where to store the CSV data dump used for bulk upload to SQL Server.</p> required <code>db_table</code> <code>str</code> <p>Destination table. Defaults to None.</p> required <code>db_schema</code> <code>str</code> <p>Destination schema. Defaults to None.</p> required <code>if_exists</code> <code>Literal</code> <p>What to do if the table exists. Defaults to \"replace\".</p> <code>'replace'</code> <code>dtypes</code> <code>dict</code> <p>The data types to be enforced for the resulting table. By default, inferred from the DataFrame. Defaults to None.</p> <code>None</code> <code>chunksize</code> <code>int</code> <p>Size of a chunk to use in the bcp function. Defaults to 5000.</p> <code>5000</code> <code>error_log_file_path</code> <code>string</code> <p>Full path of an error file. Defaults to \"./log_file.log\".</p> <code>'./log_file.log'</code> <code>on_error</code> <code>str</code> <p>What to do in case of a bcp error. Defaults to \"skip\".</p> <code>'skip'</code> <code>duckdb_credentials_secret</code> <code>str</code> <p>The name of the secret storing the credentials to the DuckDB. Defaults to None. More info on: https://docs.prefect.io/concepts/blocks/</p> <code>None</code> <code>duckdb_credentials</code> <code>dict[str, Any]</code> <p>Credentials to the DuckDB. Defaults to None.</p> <code>None</code> <code>duckdb_config_key</code> <code>str</code> <p>The key in the viadot config holding relevant credentials to the DuckDB. Defaults to None.</p> <code>None</code> <code>sql_server_credentials_secret</code> <code>str</code> <p>The name of the secret storing the credentials to the SQLServer. Defaults to None. More info on: https://docs.prefect.io/concepts/blocks/</p> <code>None</code> <code>sql_server_config_key</code> <code>str</code> <p>The key in the viadot config holding relevant credentials to the SQLServer. Defaults to None.</p> <code>None</code> Source code in <code>src/viadot/orchestration/prefect/flows/duckdb_to_sql_server.py</code> <pre><code>@flow(\n    name=\"extract--duckdb--sql_server\",\n    description=\"Extract data from DuckDB and save it in the SQLServer\",\n    retries=1,\n    retry_delay_seconds=60,\n    timeout_seconds=2 * 60 * 60,\n)\ndef duckdb_to_sql_server(  # noqa: PLR0913\n    query: str,\n    local_path: str,\n    db_table: str,\n    db_schema: str,\n    if_exists: Literal[\"fail\", \"replace\", \"skip\", \"delete\"] = \"replace\",\n    dtypes: dict[str, Any] | None = None,\n    chunksize: int = 5000,\n    error_log_file_path: str = \"./log_file.log\",\n    on_error: Literal[\"skip\", \"fail\"] = \"skip\",\n    duckdb_credentials_secret: str | None = None,\n    # Specifying credentials in a dictionary is not recommended in the viadot flows,\n    # but in this case credentials can include only database name.\n    duckdb_credentials: dict[str, Any] | None = None,\n    duckdb_config_key: str | None = None,\n    sql_server_credentials_secret: str | None = None,\n    sql_server_config_key: str | None = None,\n) -&gt; None:\n    \"\"\"Download a table from DuckDB and upload it to the SQLServer.\n\n    Args:\n        query (str, required): The query to execute on the SQL Server database.\n            If the qery doesn't start with \"SELECT\" returns an empty DataFrame.\n        local_path (str): Where to store the CSV data dump used for bulk upload to SQL\n            Server.\n        db_table (str, optional): Destination table. Defaults to None.\n        db_schema (str, optional): Destination schema. Defaults to None.\n        if_exists (Literal, optional): What to do if the table exists. Defaults to\n            \"replace\".\n        dtypes (dict, optional): The data types to be enforced for the resulting table.\n            By default, inferred from the DataFrame. Defaults to None.\n        chunksize (int, optional): Size of a chunk to use in the bcp function.\n            Defaults to 5000.\n        error_log_file_path (string, optional): Full path of an error file. Defaults\n            to \"./log_file.log\".\n        on_error (str, optional): What to do in case of a bcp error. Defaults to \"skip\".\n        duckdb_credentials_secret (str, optional): The name of the secret storing\n            the credentials to the DuckDB. Defaults to None.\n            More info on: https://docs.prefect.io/concepts/blocks/\n        duckdb_credentials (dict[str, Any], optional): Credentials to the DuckDB.\n            Defaults to None.\n        duckdb_config_key (str, optional): The key in the viadot config holding relevant\n            credentials to the DuckDB. Defaults to None.\n        sql_server_credentials_secret (str, optional): The name of the secret storing\n            the credentials to the SQLServer. Defaults to None.\n            More info on: https://docs.prefect.io/concepts/blocks/\n        sql_server_config_key (str, optional): The key in the viadot config holding\n            relevant credentials to the SQLServer. Defaults to None.\n\n    \"\"\"\n    df = duckdb_query(\n        query=query,\n        fetch_type=\"dataframe\",\n        credentials=duckdb_credentials,\n        config_key=duckdb_config_key,\n        credentials_secret=duckdb_credentials_secret,\n    )\n    if dtypes is None:\n        dtypes = get_sql_dtypes_from_df(df)\n\n    create_sql_server_table(\n        table=db_table,\n        schema=db_schema,\n        dtypes=dtypes,\n        if_exists=if_exists,\n        credentials_secret=sql_server_credentials_secret,\n        config_key=sql_server_config_key,\n    )\n    df_to_csv(df=df, path=local_path)\n\n    bcp(\n        path=local_path,\n        schema=db_schema,\n        table=db_table,\n        chunksize=chunksize,\n        error_log_file_path=error_log_file_path,\n        on_error=on_error,\n        credentials_secret=sql_server_credentials_secret,\n        config_key=sql_server_config_key,\n    )\n\n    cleanup_csv_task(path=local_path)\n</code></pre>"},{"location":"references/orchestration/prefect/flows/#viadot.orchestration.prefect.flows.duckdb_transform","title":"<code>viadot.orchestration.prefect.flows.duckdb_transform</code>","text":"<p>Flow for transforming data in DuckDB.</p>"},{"location":"references/orchestration/prefect/flows/#viadot.orchestration.prefect.flows.duckdb_transform.duckdb_transform","title":"<code>duckdb_transform(query, duckdb_credentials_secret=None, duckdb_credentials=None, duckdb_config_key=None)</code>","text":"<p>Transform data inside DuckDB database.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>(str, required)</code> <p>The query to execute on the DuckDB database.</p> required <code>duckdb_credentials_secret</code> <code>str</code> <p>The name of the secret storing the credentials. Defaults to None. More info on: https://docs.prefect.io/concepts/blocks/</p> <code>None</code> <code>duckdb_credentials</code> <code>dict[str, Any]</code> <p>Credentials to the database. Defaults to None.</p> <code>None</code> <code>duckdb_config_key</code> <code>str</code> <p>The key in the viadot config holding relevant credentials. Defaults to None.</p> <code>None</code> Source code in <code>src/viadot/orchestration/prefect/flows/duckdb_transform.py</code> <pre><code>@flow(\n    name=\"transform--duckdb\",\n    description=\"Transform data in the DuckDB.\",\n    retries=1,\n    retry_delay_seconds=60,\n    timeout_seconds=2 * 60 * 60,\n)\ndef duckdb_transform(\n    query: str,\n    duckdb_credentials_secret: str | None = None,\n    # Specifying credentials in a dictionary is not recommended in the viadot flows,\n    # but in this case credentials can include only database name.\n    duckdb_credentials: dict[str, Any] | None = None,\n    duckdb_config_key: str | None = None,\n) -&gt; None:\n    \"\"\"Transform data inside DuckDB database.\n\n    Args:\n        query (str, required): The query to execute on the DuckDB database.\n        duckdb_credentials_secret (str, optional): The name of the secret storing\n            the credentials. Defaults to None.\n            More info on: https://docs.prefect.io/concepts/blocks/\n        duckdb_credentials (dict[str, Any], optional): Credentials to the database.\n            Defaults to None.\n        duckdb_config_key (str, optional): The key in the viadot config holding relevant\n            credentials. Defaults to None.\n    \"\"\"\n    duckdb_query(\n        query=query,\n        credentials=duckdb_credentials,\n        config_key=duckdb_config_key,\n        credentials_secret=duckdb_credentials_secret,\n    )\n</code></pre>"},{"location":"references/orchestration/prefect/flows/#viadot.orchestration.prefect.flows.epicor_to_parquet","title":"<code>viadot.orchestration.prefect.flows.epicor_to_parquet</code>","text":"<p>Flows for downloading data from Epicor Prelude API to Parquet file.</p>"},{"location":"references/orchestration/prefect/flows/#viadot.orchestration.prefect.flows.epicor_to_parquet.epicor_to_parquet","title":"<code>epicor_to_parquet(path, base_url, filters_xml, if_exists='replace', validate_date_filter=True, start_date_field='BegInvoiceDate', end_date_field='EndInvoiceDate', epicor_credentials_secret=None, epicor_config_key=None)</code>","text":"<p>Download a pandas <code>DataFrame</code> from Epicor Prelude API load it into Parquet file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to Parquet file, where the data will be located. Defaults to None.</p> required <code>base_url</code> <code>(str, required)</code> <p>Base url to Epicor.</p> required <code>filters_xml</code> <code>(str, required)</code> <p>Filters in form of XML. The date filter is required.</p> required <code>if_exists</code> <code>Literal['append', 'replace', 'skip']</code> <p>Information what has to be done, if the file exists. Defaults to \"replace\"</p> <code>'replace'</code> <code>validate_date_filter</code> <code>bool</code> <p>Whether or not validate xml date filters.     Defaults to True.</p> <code>True</code> <code>epicor_credentials_secret</code> <code>str</code> <p>The name of the secret storing the credentials. Defaults to None. More info on: https://docs.prefect.io/concepts/blocks/</p> <code>None</code> <code>epicor_config_key</code> <code>str</code> <p>The key in the viadot config holding relevant credentials. Defaults to None.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; epicor_to_parquet(\n&gt;&gt;&gt;     path = \"my_parquet.parquet\",\n&gt;&gt;&gt;     base_url = \"/api/data/import/ORDER.QUERY\",\n&gt;&gt;&gt;     filters_xml = \"&lt;OrderQuery&gt;\n&gt;&gt;&gt;            &lt;QueryFields&gt;\n&gt;&gt;&gt;                &lt;CompanyNumber&gt;001&lt;/CompanyNumber&gt;\n&gt;&gt;&gt;                &lt;CustomerNumber&gt;&lt;/CustomerNumber&gt;\n&gt;&gt;&gt;                &lt;SellingWarehouse&gt;&lt;/SellingWarehouse&gt;\n&gt;&gt;&gt;                &lt;OrderNumber&gt;&lt;/OrderNumber&gt;\n&gt;&gt;&gt;                &lt;WrittenBy&gt;&lt;/WrittenBy&gt;\n&gt;&gt;&gt;                &lt;CustomerPurchaseOrderNumber&gt;&lt;/CustomerPurchaseOrderNumber&gt;\n&gt;&gt;&gt;                &lt;CustomerReleaseNumber&gt;&lt;/CustomerReleaseNumber&gt;\n&gt;&gt;&gt;                &lt;CustomerJobNumber&gt;&lt;/CustomerJobNumber&gt;\n&gt;&gt;&gt;                &lt;InvoiceNumber&gt;&lt;/InvoiceNumber&gt;\n&gt;&gt;&gt;                &lt;EcommerceId&gt;&lt;/EcommerceId&gt;\n&gt;&gt;&gt;                &lt;EcommerceOrderNumber&gt;&lt;/EcommerceOrderNumber&gt;\n&gt;&gt;&gt;                &lt;QuoteNumber&gt;&lt;/QuoteNumber&gt;\n&gt;&gt;&gt;                &lt;BegInvoiceDate&gt;{yesterday}&lt;/BegInvoiceDate&gt;\n&gt;&gt;&gt;                &lt;EndInvoiceDate&gt;{yesterday}&lt;/EndInvoiceDate&gt;\n&gt;&gt;&gt;                &lt;SortXMLTagName&gt;&lt;/SortXMLTagName&gt;\n&gt;&gt;&gt;                &lt;SortMethod&gt;&lt;/SortMethod&gt;\n&gt;&gt;&gt;                &lt;RecordCount&gt;&lt;/RecordCount&gt;\n&gt;&gt;&gt;                &lt;RecordCountPage&gt;&lt;/RecordCountPage&gt;\n&gt;&gt;&gt;            &lt;/QueryFields&gt;\n&gt;&gt;&gt;        &lt;/OrderQuery&gt;\",\n&gt;&gt;&gt;     epicor_config_key = \"epicor\"\n&gt;&gt;&gt; )\n</code></pre> Source code in <code>src/viadot/orchestration/prefect/flows/epicor_to_parquet.py</code> <pre><code>@flow(\n    name=\"extract--epicor--parquet\",\n    description=\"Extract data from Epicor Prelude API and load it into Parquet file\",\n    retries=1,\n    retry_delay_seconds=60,\n)\ndef epicor_to_parquet(\n    path: str,\n    base_url: str,\n    filters_xml: str,\n    if_exists: Literal[\"append\", \"replace\", \"skip\"] = \"replace\",\n    validate_date_filter: bool = True,\n    start_date_field: str = \"BegInvoiceDate\",\n    end_date_field: str = \"EndInvoiceDate\",\n    epicor_credentials_secret: str | None = None,\n    epicor_config_key: str | None = None,\n) -&gt; None:\n    \"\"\"Download a pandas `DataFrame` from Epicor Prelude API load it into Parquet file.\n\n    Args:\n        path (str): Path to Parquet file, where the data will be located.\n            Defaults to None.\n        base_url (str, required): Base url to Epicor.\n        filters_xml (str, required): Filters in form of XML. The date filter\n            is required.\n        if_exists (Literal[\"append\", \"replace\", \"skip\"], optional): Information what\n            has to be done, if the file exists. Defaults to \"replace\"\n        validate_date_filter (bool, optional): Whether or not validate xml date filters.\n                Defaults to True.\n        start_date_field (str, optional) The name of filters field containing\n            start date. Defaults to \"BegInvoiceDate\".\n        end_date_field (str, optional) The name of filters field containing end date.\n                Defaults to \"EndInvoiceDate\".\n        epicor_credentials_secret (str, optional): The name of the secret storing\n            the credentials. Defaults to None.\n            More info on: https://docs.prefect.io/concepts/blocks/\n        epicor_config_key (str, optional): The key in the viadot config holding relevant\n            credentials. Defaults to None.\n\n    Examples:\n        &gt;&gt;&gt; epicor_to_parquet(\n        &gt;&gt;&gt;     path = \"my_parquet.parquet\",\n        &gt;&gt;&gt;     base_url = \"/api/data/import/ORDER.QUERY\",\n        &gt;&gt;&gt;     filters_xml = \"&lt;OrderQuery&gt;\n        &gt;&gt;&gt;            &lt;QueryFields&gt;\n        &gt;&gt;&gt;                &lt;CompanyNumber&gt;001&lt;/CompanyNumber&gt;\n        &gt;&gt;&gt;                &lt;CustomerNumber&gt;&lt;/CustomerNumber&gt;\n        &gt;&gt;&gt;                &lt;SellingWarehouse&gt;&lt;/SellingWarehouse&gt;\n        &gt;&gt;&gt;                &lt;OrderNumber&gt;&lt;/OrderNumber&gt;\n        &gt;&gt;&gt;                &lt;WrittenBy&gt;&lt;/WrittenBy&gt;\n        &gt;&gt;&gt;                &lt;CustomerPurchaseOrderNumber&gt;&lt;/CustomerPurchaseOrderNumber&gt;\n        &gt;&gt;&gt;                &lt;CustomerReleaseNumber&gt;&lt;/CustomerReleaseNumber&gt;\n        &gt;&gt;&gt;                &lt;CustomerJobNumber&gt;&lt;/CustomerJobNumber&gt;\n        &gt;&gt;&gt;                &lt;InvoiceNumber&gt;&lt;/InvoiceNumber&gt;\n        &gt;&gt;&gt;                &lt;EcommerceId&gt;&lt;/EcommerceId&gt;\n        &gt;&gt;&gt;                &lt;EcommerceOrderNumber&gt;&lt;/EcommerceOrderNumber&gt;\n        &gt;&gt;&gt;                &lt;QuoteNumber&gt;&lt;/QuoteNumber&gt;\n        &gt;&gt;&gt;                &lt;BegInvoiceDate&gt;{yesterday}&lt;/BegInvoiceDate&gt;\n        &gt;&gt;&gt;                &lt;EndInvoiceDate&gt;{yesterday}&lt;/EndInvoiceDate&gt;\n        &gt;&gt;&gt;                &lt;SortXMLTagName&gt;&lt;/SortXMLTagName&gt;\n        &gt;&gt;&gt;                &lt;SortMethod&gt;&lt;/SortMethod&gt;\n        &gt;&gt;&gt;                &lt;RecordCount&gt;&lt;/RecordCount&gt;\n        &gt;&gt;&gt;                &lt;RecordCountPage&gt;&lt;/RecordCountPage&gt;\n        &gt;&gt;&gt;            &lt;/QueryFields&gt;\n        &gt;&gt;&gt;        &lt;/OrderQuery&gt;\",\n        &gt;&gt;&gt;     epicor_config_key = \"epicor\"\n        &gt;&gt;&gt; )\n    \"\"\"\n    df = epicor_to_df(\n        base_url=base_url,\n        filters_xml=filters_xml,\n        validate_date_filter=validate_date_filter,\n        start_date_field=start_date_field,\n        end_date_field=end_date_field,\n        credentials_secret=epicor_credentials_secret,\n        config_key=epicor_config_key,\n    )\n\n    return df_to_parquet(\n        df=df,\n        path=path,\n        if_exists=if_exists,\n    )\n</code></pre>"},{"location":"references/orchestration/prefect/flows/#viadot.orchestration.prefect.flows.eurostat_to_adls","title":"<code>viadot.orchestration.prefect.flows.eurostat_to_adls</code>","text":"<p>Download data from Eurostat and upload it to Azure Data Lake Storage.</p>"},{"location":"references/orchestration/prefect/flows/#viadot.orchestration.prefect.flows.eurostat_to_adls.eurostat_to_adls","title":"<code>eurostat_to_adls(dataset_code, adls_path, params=None, columns=None, tests=None, adls_credentials_secret=None, overwrite_adls=False, adls_config_key=None)</code>","text":"<p>Flow for downloading data from Eurostat to Azure Data Lake.</p> <p>This module provides a prefect flow function to use the Eurostat connector: - Call the prefect task wrapper to retrieve a DataFrame from the connector. - Upload the retrieved data to Azure Data Lake Storage.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_code</code> <code>str</code> <p>The code of the Eurostat dataset to be uploaded.</p> required <code>adls_path</code> <code>str | None</code> <p>The destination folder or path in Azure Data Lake.</p> required <code>params</code> <code>dict[str, str] | None</code> <p>A dictionary with optional URL parameters. Each key is a parameter ID, and the value is a specific parameter code, e.g., <code>params = {'unit': 'EUR'}</code> where \"unit\" is the parameter, and \"EUR\" is the code. You can pass one code per parameter. Defaults to None.</p> <code>None</code> <code>columns</code> <code>list[str] | None</code> <p>A list of columns to filter from the DataFrame after downloading. This acts as a filter to retrieve only the needed columns. Defaults to None.</p> <code>None</code> <code>tests</code> <code>dict | None</code> <p>A dictionary containing test cases for the data, such as: - <code>column_size</code>: dict{column: size} - <code>column_unique_values</code>: list[columns] - <code>column_list_to_match</code>: list[columns] - <code>dataset_row_count</code>: dict{'min': number, 'max': number} - <code>column_match_regex</code>: dict{column: 'regex'} - <code>column_sum</code>: dict{column: {'min': number, 'max': number}}. Defaults to None.</p> <code>None</code> <code>adls_credentials_secret</code> <code>str | None</code> <p>The Azure Key Vault secret containing Service Principal credentials (TENANT_ID, CLIENT_ID, CLIENT_SECRET) and ACCOUNT_NAME for Azure Data Lake access. Defaults to None.</p> <code>None</code> <code>overwrite_adls</code> <code>bool</code> <p>Whether to overwrite files in the lake if they exist. Defaults to False.</p> <code>False</code> <code>adls_config_key</code> <code>str | None</code> <p>The key in the viadot config that holds the credentials for Azure Data Lake. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>src/viadot/orchestration/prefect/flows/eurostat_to_adls.py</code> <pre><code>@flow(\n    name=\"extract--eurostat--adls\",\n    description=\"\"\"Flow for downloading data from the Eurostat platform via\n    HTTPS REST API (no credentials required) to a CSV or Parquet file.\n    Then upload it to Azure Data Lake.\"\"\",\n    retries=1,\n    retry_delay_seconds=60,\n)\ndef eurostat_to_adls(\n    dataset_code: str,\n    adls_path: str,\n    params: dict[str, str] | None = None,\n    columns: list[str] | None = None,\n    tests: dict | None = None,\n    adls_credentials_secret: str | None = None,\n    overwrite_adls: bool = False,\n    adls_config_key: str | None = None,\n) -&gt; None:\n    \"\"\"Flow for downloading data from Eurostat to Azure Data Lake.\n\n    This module provides a prefect flow function to use the Eurostat connector:\n    - Call the prefect task wrapper to retrieve a DataFrame from the connector.\n    - Upload the retrieved data to Azure Data Lake Storage.\n\n    Args:\n        dataset_code (str):\n            The code of the Eurostat dataset to be uploaded.\n        adls_path (str | None, optional): The destination folder or path\n            in Azure Data Lake.\n        params (dict[str, str] | None, optional):\n            A dictionary with optional URL parameters. Each key is a parameter ID,\n            and the value is a specific parameter code, e.g.,\n            `params = {'unit': 'EUR'}` where \"unit\" is the parameter, and \"EUR\"\n            is the code. You can pass one code per parameter. Defaults to None.\n        columns (list[str] | None, optional):\n            A list of columns to filter from the DataFrame after downloading.\n            This acts as a filter to retrieve only the needed columns. Defaults to None.\n        tests (dict | None, optional):\n            A dictionary containing test cases for the data, such as:\n            - `column_size`: dict{column: size}\n            - `column_unique_values`: list[columns]\n            - `column_list_to_match`: list[columns]\n            - `dataset_row_count`: dict{'min': number, 'max': number}\n            - `column_match_regex`: dict{column: 'regex'}\n            - `column_sum`: dict{column: {'min': number, 'max': number}}.\n            Defaults to None.\n\n        adls_credentials_secret (str | None, optional):\n            The Azure Key Vault secret containing Service Principal credentials\n            (TENANT_ID, CLIENT_ID, CLIENT_SECRET) and ACCOUNT_NAME for Azure Data\n            Lake access. Defaults to None.\n        overwrite_adls (bool, optional):\n            Whether to overwrite files in the lake if they exist. Defaults to False.\n        adls_config_key (str | None, optional):\n            The key in the viadot config that holds the credentials for Azure\n            Data Lake. Defaults to None.\n\n    Returns:\n        None\n    \"\"\"\n    df = eurostat_to_df(\n        dataset_code=dataset_code,\n        params=params,\n        columns=columns,\n        tests=tests,\n    )\n    df_to_adls(\n        df=df,\n        path=adls_path,\n        credentials_secret=adls_credentials_secret,\n        config_key=adls_config_key,\n        overwrite=overwrite_adls,\n    )\n</code></pre>"},{"location":"references/orchestration/prefect/flows/#viadot.orchestration.prefect.flows.exchange_rates_to_adls","title":"<code>viadot.orchestration.prefect.flows.exchange_rates_to_adls</code>","text":"<p>Flows for pulling data from Exchange rates API to Azure Data Lake.</p>"},{"location":"references/orchestration/prefect/flows/#viadot.orchestration.prefect.flows.exchange_rates_to_adls.exchange_rates_to_adls","title":"<code>exchange_rates_to_adls(adls_path, overwrite=False, currency='USD', start_date=datetime.today().strftime('%Y-%m-%d'), end_date=datetime.today().strftime('%Y-%m-%d'), symbols=None, exchange_rates_credentials_secret=None, exchange_rates_config_key=None, adls_credentials_secret=None, adls_config_key=None)</code>","text":"<p>Download a DataFrame from ExchangeRates API and upload it to Azure Data Lake.</p> <p>Parameters:</p> Name Type Description Default <code>adls_path</code> <code>str</code> <p>The destination path.</p> required <code>overwrite</code> <code>bool</code> <p>Whether to overwrite files in the lake. Defaults to False.</p> <code>False</code> <code>currency</code> <code>Currency</code> <p>Base currency to which prices of searched currencies are related. Defaults to \"USD\".</p> <code>'USD'</code> <code>start_date</code> <code>str</code> <p>Initial date for data search. Data range is start_date -&gt; end_date, supported format 'yyyy-mm-dd'. Defaults to datetime.today().strftime(\"%Y-%m-%d\").</p> <code>strftime('%Y-%m-%d')</code> <code>end_date</code> <code>str</code> <p>See above. Defaults to datetime.today().strftime(\"%Y-%m-%d\").</p> <code>strftime('%Y-%m-%d')</code> <code>symbols</code> <code>List[str]</code> <p>List of currencies for which exchange rates from base currency will be fetched. Defaults to [\"USD\",\"EUR\",\"GBP\",\"CHF\",\"PLN\",\"DKK\",\"COP\",\"CZK\",\"SEK\",\"NOK\",\"ISK\"].</p> <code>None</code> <code>exchange_rates_credentials_secret</code> <code>str</code> <p>The name of the Azure Key Vault secret storing the exchange rates credentials. Defaults to None.</p> <code>None</code> <code>exchange_rates_config_key</code> <code>str</code> <p>The key in the viadot config holding relevant credentials. Defaults to None.</p> <code>None</code> <code>adls_credentials_secret</code> <code>str</code> <p>The name of the Azure Key Vault secret storing the ADLS credentials. Defaults to None.</p> <code>None</code> <code>adls_config_key</code> <code>str</code> <p>The key in the viadot config holding relevant credentials. Defaults to None.</p> <code>None</code> Source code in <code>src/viadot/orchestration/prefect/flows/exchange_rates_to_adls.py</code> <pre><code>@flow(\n    name=\"extract--exchange-rates-api--adls\",\n    description=\"Extract data from Exchange Rates API and load it into Azure Data Lake.\",\n    retries=1,\n    retry_delay_seconds=60,\n)\ndef exchange_rates_to_adls(\n    adls_path: str,\n    overwrite: bool = False,\n    currency: Currency = \"USD\",\n    start_date: str = datetime.today().strftime(\"%Y-%m-%d\"),\n    end_date: str = datetime.today().strftime(\"%Y-%m-%d\"),\n    symbols: list[str] | None = None,\n    exchange_rates_credentials_secret: str | None = None,\n    exchange_rates_config_key: str | None = None,\n    adls_credentials_secret: str | None = None,\n    adls_config_key: str | None = None,\n) -&gt; None:\n    \"\"\"Download a DataFrame from ExchangeRates API and upload it to Azure Data Lake.\n\n    Args:\n        adls_path (str): The destination path.\n        overwrite (bool, optional): Whether to overwrite files in the lake.\n            Defaults to False.\n        currency (Currency, optional): Base currency to which prices of searched\n            currencies are related. Defaults to \"USD\".\n        start_date (str, optional): Initial date for data search.\n            Data range is start_date -&gt; end_date,\n            supported format 'yyyy-mm-dd'.\n            Defaults to datetime.today().strftime(\"%Y-%m-%d\").\n        end_date (str, optional): See above.\n            Defaults to datetime.today().strftime(\"%Y-%m-%d\").\n        symbols (List[str], optional): List of currencies for which\n            exchange rates from base currency will be fetched.\n            Defaults to\n            [\"USD\",\"EUR\",\"GBP\",\"CHF\",\"PLN\",\"DKK\",\"COP\",\"CZK\",\"SEK\",\"NOK\",\"ISK\"].\n        exchange_rates_credentials_secret (str, optional): The name of the\n            Azure Key Vault secret storing the exchange rates credentials.\n            Defaults to None.\n        exchange_rates_config_key (str, optional): The key in the viadot config holding\n            relevant credentials. Defaults to None.\n        adls_credentials_secret (str, optional): The name of the Azure Key Vault secret\n            storing the ADLS credentials. Defaults to None.\n        adls_config_key (str, optional): The key in the viadot config holding relevant\n            credentials. Defaults to None.\n    \"\"\"\n    df = exchange_rates_to_df(\n        currency=currency,\n        credentials_secret=exchange_rates_credentials_secret,\n        config_key=exchange_rates_config_key,\n        start_date=start_date,\n        end_date=end_date,\n        symbols=symbols,\n    )\n\n    return df_to_adls(\n        df=df,\n        path=adls_path,\n        credentials_secret=adls_credentials_secret,\n        config_key=adls_config_key,\n        overwrite=overwrite,\n    )\n</code></pre>"},{"location":"references/orchestration/prefect/flows/#viadot.orchestration.prefect.flows.exchange_rates_to_databricks","title":"<code>viadot.orchestration.prefect.flows.exchange_rates_to_databricks</code>","text":"<p>Flows for pulling data from Exchange rates API to Databricks.</p>"},{"location":"references/orchestration/prefect/flows/#viadot.orchestration.prefect.flows.exchange_rates_to_databricks.exchange_rates_to_databricks","title":"<code>exchange_rates_to_databricks(databricks_table, databricks_schema=None, if_exists='fail', currency='USD', start_date=datetime.today().strftime('%Y-%m-%d'), end_date=datetime.today().strftime('%Y-%m-%d'), symbols=None, exchange_rates_credentials_secret=None, exchange_rates_config_key=None, databricks_credentials_secret=None, databricks_config_key=None)</code>","text":"<p>Download a DataFrame from ExchangeRates API and upload it to Databricks.</p> <p>Parameters:</p> Name Type Description Default <code>databricks_table</code> <code>str</code> <p>The name of the target table.</p> required <code>databricks_schema</code> <code>str</code> <p>The name of the target schema. Defaults to None.</p> <code>None</code> <code>if_exists</code> <code>Literal['replace', 'skip', 'fail']</code> <p>What to do if the table already exists. One of \"replace\", \"skip\", and \"fail\". Defaults to \"fail\".</p> <code>'fail'</code> <code>currency</code> <code>Currency</code> <p>Base currency to which prices of searched currencies are related. Defaults to \"USD\".</p> <code>'USD'</code> <code>start_date</code> <code>str</code> <p>Initial date for data search. Data range is start_date -&gt; end_date, supported format 'yyyy-mm-dd'. Defaults to datetime.today().strftime(\"%Y-%m-%d\").</p> <code>strftime('%Y-%m-%d')</code> <code>end_date</code> <code>str</code> <p>See above. Defaults to datetime.today().strftime(\"%Y-%m-%d\").</p> <code>strftime('%Y-%m-%d')</code> <code>symbols</code> <code>List[str]</code> <p>List of currencies for which exchange rates from base currency will be fetched. Defaults to [\"USD\",\"EUR\",\"GBP\",\"CHF\",\"PLN\",\"DKK\",\"COP\",\"CZK\",\"SEK\",\"NOK\",\"ISK\"]. Only ISO codes.</p> <code>None</code> <code>exchange_rates_credentials_secret</code> <code>str</code> <p>The name of the Azure Key Vault secret storing the exchange rates credentials. Defaults to None.</p> <code>None</code> <code>exchange_rates_config_key</code> <code>str</code> <p>The key in the viadot config holding relevant credentials. Defaults to None.</p> <code>None</code> <code>databricks_credentials_secret</code> <code>str</code> <p>The name of the Azure Key Vault secret storing relevant credentials. Defaults to None.</p> <code>None</code> <code>databricks_config_key</code> <code>str</code> <p>The key in the viadot config holding relevant credentials. Defaults to None.</p> <code>None</code> Source code in <code>src/viadot/orchestration/prefect/flows/exchange_rates_to_databricks.py</code> <pre><code>@flow(\n    name=\"extract--exchange-rates-api--databricks\",\n    description=\"Extract data from Exchange Rates API and load it into Databricks.\",\n    retries=1,\n    retry_delay_seconds=60,\n)\ndef exchange_rates_to_databricks(  # noqa: PLR0913\n    databricks_table: str,\n    databricks_schema: str | None = None,\n    if_exists: Literal[\"replace\", \"skip\", \"fail\"] = \"fail\",\n    currency: Currency = \"USD\",\n    start_date: str = datetime.today().strftime(\"%Y-%m-%d\"),\n    end_date: str = datetime.today().strftime(\"%Y-%m-%d\"),\n    symbols: list[str] | None = None,\n    exchange_rates_credentials_secret: str | None = None,\n    exchange_rates_config_key: str | None = None,\n    databricks_credentials_secret: str | None = None,\n    databricks_config_key: str | None = None,\n) -&gt; None:\n    \"\"\"Download a DataFrame from ExchangeRates API and upload it to Databricks.\n\n    Args:\n        databricks_table (str): The name of the target table.\n        databricks_schema (str, optional): The name of the target schema.\n            Defaults to None.\n        if_exists (Literal[\"replace\", \"skip\", \"fail\"], optional):\n            What to do if the table already exists.\n            One of \"replace\", \"skip\", and \"fail\". Defaults to \"fail\".\n        currency (Currency, optional): Base currency to which prices of searched\n            currencies are related. Defaults to \"USD\".\n        start_date (str, optional): Initial date for data search.\n            Data range is start_date -&gt; end_date,\n            supported format 'yyyy-mm-dd'.\n            Defaults to datetime.today().strftime(\"%Y-%m-%d\").\n        end_date (str, optional): See above.\n            Defaults to datetime.today().strftime(\"%Y-%m-%d\").\n        symbols (List[str], optional): List of currencies for which\n            exchange rates from base currency will be fetched.\n            Defaults to\n            [\"USD\",\"EUR\",\"GBP\",\"CHF\",\"PLN\",\"DKK\",\"COP\",\"CZK\",\"SEK\",\"NOK\",\"ISK\"].\n            Only ISO codes.\n        exchange_rates_credentials_secret (str, optional): The name of the\n            Azure Key Vault secret storing the exchange rates credentials.\n            Defaults to None.\n        exchange_rates_config_key (str, optional): The key in the viadot config holding\n            relevant credentials. Defaults to None.\n        databricks_credentials_secret (str, optional): The name of the Azure Key Vault\n            secret storing relevant credentials. Defaults to None.\n        databricks_config_key (str, optional): The key in the viadot config holding\n            relevant credentials. Defaults to None.\n    \"\"\"\n    df = exchange_rates_to_df(\n        currency=currency,\n        credentials_secret=exchange_rates_credentials_secret,\n        config_key=exchange_rates_config_key,\n        start_date=start_date,\n        end_date=end_date,\n        symbols=symbols,\n    )\n\n    return df_to_databricks(\n        df=df,\n        schema=databricks_schema,\n        table=databricks_table,\n        if_exists=if_exists,\n        credentials_secret=databricks_credentials_secret,\n        config_key=databricks_config_key,\n    )\n</code></pre>"},{"location":"references/orchestration/prefect/flows/#viadot.orchestration.prefect.flows.exchange_rates_api_to_redshift_spectrum","title":"<code>viadot.orchestration.prefect.flows.exchange_rates_api_to_redshift_spectrum(to_path, schema_name, table, currency='USD', start_date=datetime.today().strftime('%Y-%m-%d'), end_date=datetime.today().strftime('%Y-%m-%d'), symbols=None, if_exists='overwrite', partition_cols=None, compression=None, table_description=None, aws_config_key=None, aws_credentials_secret=None, exchange_rates_api_credentials_secret=None, exchange_rates_api_config_key=None)</code>","text":"<p>Extract data from Exchange Rates API and load it into AWS Redshift Spectrum.</p> <p>Parameters:</p> Name Type Description Default <code>currency</code> <code>Currency</code> <p>Base currency to which prices of searched currencies are related. Defaults to \"USD\".</p> <code>'USD'</code> <code>start_date</code> <code>str</code> <p>Initial date for data search. Data range is start_date -&gt; end_date, supported format 'yyyy-mm-dd'. Defaults to datetime.today().strftime(\"%Y-%m-%d\").</p> <code>strftime('%Y-%m-%d')</code> <code>end_date</code> <code>str</code> <p>See above. Defaults to datetime.today().strftime(\"%Y-%m-%d\").</p> <code>strftime('%Y-%m-%d')</code> <code>symbols</code> <code>list[str]</code> <p>List of ISO codes of currencies for which exchange rates from base currency will be fetched. Defaults to [\"USD\",\"EUR\",\"GBP\",\"CHF\",\"PLN\",\"DKK\",\"COP\",\"CZK\",\"SEK\",\"NOK\",\"ISK\"].</p> <code>None</code> <code>to_path</code> <code>str</code> <p>Path to a S3 folder where the table will be located. Defaults to None.</p> required <code>schema_name</code> <code>str</code> <p>AWS Glue catalog database name.</p> required <code>table</code> <code>str</code> <p>AWS Glue catalog table name.</p> required <code>if_exists</code> <code>str</code> <p>'overwrite' to recreate any possible existing table or 'append' to keep any possible existing table. Defaults to overwrite.</p> <code>'overwrite'</code> <code>partition_cols</code> <code>list[str]</code> <p>List of column names that will be used to create partitions. Only takes effect if dataset=True. Defaults to None.</p> <code>None</code> <code>compression</code> <code>str</code> <p>Compression style (None, snappy, gzip, zstd).</p> <code>None</code> <code>sep</code> <code>str</code> <p>Field delimiter for the output file. Defaults to ','.</p> required <code>table_description</code> <code>str</code> <p>AWS Glue catalog table description. Defaults to None.</p> <code>None</code> <code>aws_config_key</code> <code>str</code> <p>The key in the viadot config holding relevant credentials. Defaults to None.</p> <code>None</code> <code>aws_credentials_secret</code> <code>str</code> <p>The name of a secret block in Prefect that stores AWS credentials. Defaults to None.</p> <code>None</code> <code>exchange_rates_api_credentials_secret</code> <code>str</code> <p>The name of the secret storing Exchange Rates API API key. Defaults to None. More info on: https://docs.prefect.io/concepts/blocks/</p> <code>None</code> <code>exchange_rates_api_config_key</code> <code>str</code> <p>The key in the viadot config holding relevant credentials. Defaults to None.</p> <code>None</code> Source code in <code>src/viadot/orchestration/prefect/flows/exchange_rates_to_redshift_spectrum.py</code> <pre><code>@flow(\n    name=\"extract--exchange-rates-api--redshift_spectrum\",\n    description=\"Extract data from Exchange Rates API and load it into AWS Redshift Spectrum.\",\n    retries=1,\n    retry_delay_seconds=60,\n)\ndef exchange_rates_api_to_redshift_spectrum(  # noqa: PLR0913\n    to_path: str,\n    schema_name: str,\n    table: str,\n    currency: Currency = \"USD\",\n    start_date: str = datetime.today().strftime(\"%Y-%m-%d\"),\n    end_date: str = datetime.today().strftime(\"%Y-%m-%d\"),\n    symbols: list[str] | None = None,\n    if_exists: Literal[\"overwrite\", \"append\"] = \"overwrite\",\n    partition_cols: list[str] | None = None,\n    compression: str | None = None,\n    table_description: str | None = None,\n    aws_config_key: str | None = None,\n    aws_credentials_secret: str | None = None,\n    exchange_rates_api_credentials_secret: str | None = None,\n    exchange_rates_api_config_key: str | None = None,\n) -&gt; None:\n    \"\"\"Extract data from Exchange Rates API and load it into AWS Redshift Spectrum.\n\n    Args:\n        currency (Currency, optional): Base currency to which prices of searched\n            currencies are related. Defaults to \"USD\".\n        start_date (str, optional): Initial date for data search.\n            Data range is start_date -&gt; end_date,\n            supported format 'yyyy-mm-dd'.\n            Defaults to datetime.today().strftime(\"%Y-%m-%d\").\n        end_date (str, optional): See above.\n            Defaults to datetime.today().strftime(\"%Y-%m-%d\").\n        symbols (list[str], optional): List of ISO codes of currencies for which\n            exchange rates from base currency will be fetched. Defaults to\n            [\"USD\",\"EUR\",\"GBP\",\"CHF\",\"PLN\",\"DKK\",\"COP\",\"CZK\",\"SEK\",\"NOK\",\"ISK\"].\n        to_path (str): Path to a S3 folder where the table will be located. Defaults to\n            None.\n        schema_name (str): AWS Glue catalog database name.\n        table (str): AWS Glue catalog table name.\n        if_exists (str, optional): 'overwrite' to recreate any possible existing table\n            or 'append' to keep any possible existing table. Defaults to overwrite.\n        partition_cols (list[str], optional): List of column names that will be used to\n            create partitions. Only takes effect if dataset=True. Defaults to None.\n        compression (str, optional): Compression style (None, snappy, gzip, zstd).\n        sep (str, optional): Field delimiter for the output file. Defaults to ','.\n        table_description (str, optional): AWS Glue catalog table description. Defaults\n            to None.\n        aws_config_key (str, optional): The key in the viadot config holding relevant\n            credentials. Defaults to None.\n        aws_credentials_secret (str, optional): The name of a secret block in Prefect\n            that stores AWS credentials. Defaults to None.\n        exchange_rates_api_credentials_secret (str, optional): The name of the secret\n            storing Exchange Rates API API key. Defaults to None.\n            More info on: https://docs.prefect.io/concepts/blocks/\n        exchange_rates_api_config_key (str, optional): The key in the viadot config\n            holding relevant credentials. Defaults to None.\n    \"\"\"\n    df = exchange_rates_to_df(\n        currency=currency,\n        start_date=start_date,\n        end_date=end_date,\n        symbols=symbols,\n        credentials_secret=exchange_rates_api_credentials_secret,\n        config_key=exchange_rates_api_config_key,\n    )\n    df_to_redshift_spectrum(\n        df=df,\n        to_path=to_path,\n        schema_name=schema_name,\n        table=table,\n        if_exists=if_exists,\n        partition_cols=partition_cols,\n        compression=compression,\n        description=table_description,\n        config_key=aws_config_key,\n        credentials_secret=aws_credentials_secret,\n    )\n</code></pre>"},{"location":"references/orchestration/prefect/flows/#viadot.orchestration.prefect.flows.genesys_to_adls","title":"<code>viadot.orchestration.prefect.flows.genesys_to_adls</code>","text":"<p>Download data from Genesys Cloud and upload it to Azure Data Lake Storage.</p>"},{"location":"references/orchestration/prefect/flows/#viadot.orchestration.prefect.flows.genesys_to_adls.genesys_to_adls","title":"<code>genesys_to_adls(config_key=None, azure_key_vault_secret=None, verbose=None, endpoint=None, environment='mypurecloud.de', queues_ids=None, view_type=None, view_type_time_sleep=None, post_data_list=None, time_between_api_call=0.5, normalization_sep='.', drop_duplicates=False, validate_df_dict=None, adls_config_key=None, adls_azure_key_vault_secret=None, adls_path=None, adls_path_overwrite=False)</code>","text":"<p>Flow for downloading data from mindful to Azure Data Lake.</p> <p>Parameters:</p> Name Type Description Default <code>config_key</code> <code>str</code> <p>The key in the viadot config holding relevant credentials. Defaults to None.</p> <code>None</code> <code>azure_key_vault_secret</code> <code>Optional[str]</code> <p>The name of the Azure Key Vault secret where credentials are stored. Defaults to None.</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>Increase the details of the logs printed on the     screen. Defaults to False.</p> <code>None</code> <code>endpoint</code> <code>Optional[str]</code> <p>Final end point to the API. Defaults to None.</p> <code>None</code> <code>environment</code> <code>str</code> <p>the domain that appears for Genesys Cloud Environment based on the location of your Genesys Cloud organization. Defaults to \"mypurecloud.de\".</p> <code>'mypurecloud.de'</code> <code>queues_ids</code> <code>Optional[List[str]]</code> <p>List of queues ids to consult the     members. Defaults to None.</p> <code>None</code> <code>view_type</code> <code>Optional[str]</code> <p>The type of view export job to be created. Defaults to None.</p> <code>None</code> <code>view_type_time_sleep</code> <code>Optional[int]</code> <p>Waiting time to retrieve data from Genesys Cloud API. Defaults to None.</p> <code>None</code> <code>post_data_list</code> <code>Optional[List[Dict[str, Any]]]</code> <p>List of string templates to generate json body in POST calls to the API. Defaults to None.</p> <code>None</code> <code>time_between_api_call</code> <code>int</code> <p>The time, in seconds, to sleep the call to the API. Defaults to 0.5.</p> <code>0.5</code> <code>normalization_sep</code> <code>str</code> <p>Nested records will generate names separated by sep. Defaults to \".\".</p> <code>'.'</code> <code>drop_duplicates</code> <code>bool</code> <p>Remove duplicates from the DataFrame. Defaults to False.</p> <code>False</code> <code>validate_df_dict</code> <code>Optional[Dict[str, Any]]</code> <p>A dictionary with optional list of tests to verify the output dataframe. Defaults to None.</p> <code>None</code> <code>adls_config_key</code> <code>Optional[str]</code> <p>The key in the viadot config holding relevant credentials. Defaults to None.</p> <code>None</code> <code>adls_azure_key_vault_secret</code> <code>Optional[str]</code> <p>The name of the Azure Key Vault secret containing a dictionary with ACCOUNT_NAME and Service Principal credentials (TENANT_ID, CLIENT_ID, CLIENT_SECRET) for the Azure Data Lake. Defaults to None.</p> <code>None</code> <code>adls_path</code> <code>Optional[str]</code> <p>Azure Data Lake destination file path (with file name). Defaults to None.</p> <code>None</code> <code>adls_path_overwrite</code> <code>bool</code> <p>Whether to overwrite the file in ADLS. Defaults to True.</p> <code>False</code> <p>Examples:</p> <p>genesys_to_adls(     config_key=config_key,     verbose=False,     endpoint=endpoint,     post_data_list=data_to_post,     adls_config_key=adls_config_key,     adls_path=adls_path,     adls_path_overwrite=True, )</p> Source code in <code>src/viadot/orchestration/prefect/flows/genesys_to_adls.py</code> <pre><code>@flow(\n    name=\"Genesys extraction to ADLS\",\n    description=\"Extract data from Genesys Cloud\"\n    + \" and load it into Azure Data Lake Storage.\",\n    retries=1,\n    retry_delay_seconds=60,\n    task_runner=ConcurrentTaskRunner,\n    log_prints=True,\n)\ndef genesys_to_adls(  # noqa: PLR0913\n    config_key: str | None = None,\n    azure_key_vault_secret: str | None = None,\n    verbose: bool | None = None,\n    endpoint: str | None = None,\n    environment: str = \"mypurecloud.de\",\n    queues_ids: list[str] | None = None,\n    view_type: str | None = None,\n    view_type_time_sleep: int | None = None,\n    post_data_list: list[dict[str, Any]] | None = None,\n    time_between_api_call: float = 0.5,\n    normalization_sep: str = \".\",\n    drop_duplicates: bool = False,\n    validate_df_dict: dict[str, Any] | None = None,\n    adls_config_key: str | None = None,\n    adls_azure_key_vault_secret: str | None = None,\n    adls_path: str | None = None,\n    adls_path_overwrite: bool = False,\n) -&gt; None:\n    \"\"\"Flow for downloading data from mindful to Azure Data Lake.\n\n    Args:\n        config_key (str, optional): The key in the viadot config holding relevant\n            credentials. Defaults to None.\n        azure_key_vault_secret (Optional[str], optional): The name of the Azure Key\n            Vault secret where credentials are stored. Defaults to None.\n        verbose (bool, optional): Increase the details of the logs printed on the\n                screen. Defaults to False.\n        endpoint (Optional[str], optional): Final end point to the API.\n            Defaults to None.\n        environment (str, optional): the domain that appears for Genesys Cloud\n            Environment based on the location of your Genesys Cloud organization.\n            Defaults to \"mypurecloud.de\".\n        queues_ids (Optional[List[str]], optional): List of queues ids to consult the\n                members. Defaults to None.\n        view_type (Optional[str], optional): The type of view export job to be created.\n            Defaults to None.\n        view_type_time_sleep (Optional[int], optional): Waiting time to retrieve data\n            from Genesys Cloud API. Defaults to None.\n        post_data_list (Optional[List[Dict[str, Any]]], optional): List of string\n            templates to generate json body in POST calls to the API. Defaults to None.\n        time_between_api_call (int, optional): The time, in seconds, to sleep the call\n            to the API. Defaults to 0.5.\n        normalization_sep (str, optional): Nested records will generate names separated\n            by sep. Defaults to \".\".\n        drop_duplicates (bool, optional): Remove duplicates from the DataFrame.\n            Defaults to False.\n        validate_df_dict (Optional[Dict[str, Any]], optional): A dictionary with\n            optional list of tests to verify the output dataframe. Defaults to None.\n        adls_config_key (Optional[str], optional): The key in the viadot config holding\n            relevant credentials. Defaults to None.\n        adls_azure_key_vault_secret (Optional[str], optional): The name of the Azure Key\n            Vault secret containing a dictionary with ACCOUNT_NAME and Service Principal\n            credentials (TENANT_ID, CLIENT_ID, CLIENT_SECRET) for the Azure Data Lake.\n            Defaults to None.\n        adls_path (Optional[str], optional): Azure Data Lake destination file path (with\n            file name). Defaults to None.\n        adls_path_overwrite (bool, optional): Whether to overwrite the file in ADLS.\n            Defaults to True.\n\n    Examples:\n        genesys_to_adls(\n            config_key=config_key,\n            verbose=False,\n            endpoint=endpoint,\n            post_data_list=data_to_post,\n            adls_config_key=adls_config_key,\n            adls_path=adls_path,\n            adls_path_overwrite=True,\n        )\n    \"\"\"\n    data_frame = genesys_to_df(\n        config_key=config_key,\n        azure_key_vault_secret=azure_key_vault_secret,\n        verbose=verbose,\n        endpoint=endpoint,\n        environment=environment,\n        queues_ids=queues_ids,\n        view_type=view_type,\n        view_type_time_sleep=view_type_time_sleep,\n        post_data_list=post_data_list,\n        time_between_api_call=time_between_api_call,\n        normalization_sep=normalization_sep,\n        drop_duplicates=drop_duplicates,\n        validate_df_dict=validate_df_dict,\n    )\n\n    return df_to_adls(\n        df=data_frame,\n        path=adls_path,\n        credentials_secret=adls_azure_key_vault_secret,\n        config_key=adls_config_key,\n        overwrite=adls_path_overwrite,\n    )\n</code></pre>"},{"location":"references/orchestration/prefect/flows/#viadot.orchestration.prefect.flows.hubspot_to_adls","title":"<code>viadot.orchestration.prefect.flows.hubspot_to_adls</code>","text":"<p>Download data from Hubspot API and load into Azure Data Lake Storage.</p>"},{"location":"references/orchestration/prefect/flows/#viadot.orchestration.prefect.flows.hubspot_to_adls.hubspot_to_adls","title":"<code>hubspot_to_adls(config_key=None, azure_key_vault_secret=None, endpoint=None, filters=None, properties=None, nrows=1000, adls_config_key=None, adls_azure_key_vault_secret=None, adls_path=None, adls_path_overwrite=False)</code>","text":"<p>Flow for downloading data from mindful to Azure Data Lake.</p> <p>Parameters:</p> Name Type Description Default <code>config_key</code> <code>str</code> <p>The key in the viadot config holding relevant credentials. Defaults to None.</p> <code>None</code> <code>azure_key_vault_secret</code> <code>Optional[str]</code> <p>The name of the Azure Key Vault secret where credentials are stored. Defaults to None.</p> <code>None</code> <code>endpoint</code> <code>Optional[str]</code> <p>API endpoint for an individual request. Defaults to None.</p> <code>None</code> <code>filters</code> <code>Optional[List[Dict[str, Any]]]</code> <p>Filters defined for the API body in specific order. Defaults to None.</p> <code>None</code> <code>properties</code> <code>Optional[List[Any]]</code> <p>List of user-defined columns to be pulled from the API. Defaults to None.</p> <code>None</code> <code>nrows</code> <code>int</code> <p>Max number of rows to pull during execution. Defaults to 1000.</p> <code>1000</code> <code>adls_config_key</code> <code>Optional[str]</code> <p>The key in the viadot config holding relevant credentials. Defaults to None.</p> <code>None</code> <code>adls_azure_key_vault_secret</code> <code>Optional[str]</code> <p>The name of the Azure Key Vault secret containing a dictionary with ACCOUNT_NAME and Service Principal credentials (TENANT_ID, CLIENT_ID, CLIENT_SECRET) for the Azure Data Lake. Defaults to None.</p> <code>None</code> <code>adls_path</code> <code>Optional[str]</code> <p>Azure Data Lake destination file path (with file name). Defaults to None.</p> <code>None</code> <code>adls_path_overwrite</code> <code>bool</code> <p>Whether to overwrite the file in ADLS. Defaults to True.</p> <code>False</code> <p>Examples:</p> <p>hubspot_to_adls(     config_key=config_key,     endpoint=endpoint,     nrows=nrows,     adls_config_key=adls_config_key,     adls_path=adls_path,     adls_path_overwrite=True, )</p> Source code in <code>src/viadot/orchestration/prefect/flows/hubspot_to_adls.py</code> <pre><code>@flow(\n    name=\"Hubspot extraction to ADLS\",\n    description=\"Extract data from Hubspot API and load into Azure Data Lake Storage.\",\n    retries=1,\n    retry_delay_seconds=60,\n    task_runner=ConcurrentTaskRunner,\n)\ndef hubspot_to_adls(\n    config_key: str | None = None,\n    azure_key_vault_secret: str | None = None,\n    endpoint: str | None = None,\n    filters: list[dict[str, Any]] | None = None,\n    properties: list[Any] | None = None,\n    nrows: int = 1000,\n    adls_config_key: str | None = None,\n    adls_azure_key_vault_secret: str | None = None,\n    adls_path: str | None = None,\n    adls_path_overwrite: bool = False,\n) -&gt; None:\n    \"\"\"Flow for downloading data from mindful to Azure Data Lake.\n\n    Args:\n        config_key (str, optional): The key in the viadot config holding relevant\n            credentials. Defaults to None.\n        azure_key_vault_secret (Optional[str], optional): The name of the Azure Key\n            Vault secret where credentials are stored. Defaults to None.\n        endpoint (Optional[str], optional): API endpoint for an individual request.\n            Defaults to None.\n        filters (Optional[List[Dict[str, Any]]], optional): Filters defined for the API\n            body in specific order. Defaults to None.\n        properties (Optional[List[Any]], optional): List of user-defined columns to be\n            pulled from the API. Defaults to None.\n        nrows (int, optional): Max number of rows to pull during execution.\n            Defaults to 1000.\n        adls_config_key (Optional[str], optional): The key in the viadot config holding\n            relevant credentials. Defaults to None.\n        adls_azure_key_vault_secret (Optional[str], optional): The name of the Azure Key\n            Vault secret containing a dictionary with ACCOUNT_NAME and Service Principal\n            credentials (TENANT_ID, CLIENT_ID, CLIENT_SECRET) for the Azure Data Lake.\n            Defaults to None.\n        adls_path (Optional[str], optional): Azure Data Lake destination file path\n            (with file name). Defaults to None.\n        adls_path_overwrite (bool, optional): Whether to overwrite the file in ADLS.\n            Defaults to True.\n\n    Examples:\n        hubspot_to_adls(\n            config_key=config_key,\n            endpoint=endpoint,\n            nrows=nrows,\n            adls_config_key=adls_config_key,\n            adls_path=adls_path,\n            adls_path_overwrite=True,\n        )\n    \"\"\"\n    data_frame = hubspot_to_df(\n        config_key=config_key,\n        azure_key_vault_secret=azure_key_vault_secret,\n        endpoint=endpoint,\n        filters=filters,\n        properties=properties,\n        nrows=nrows,\n    )\n\n    return df_to_adls(\n        df=data_frame,\n        path=adls_path,\n        credentials_secret=adls_azure_key_vault_secret,\n        config_key=adls_config_key,\n        overwrite=adls_path_overwrite,\n    )\n</code></pre>"},{"location":"references/orchestration/prefect/flows/#viadot.orchestration.prefect.flows.mediatool_to_adls","title":"<code>viadot.orchestration.prefect.flows.mediatool_to_adls</code>","text":"<p>'mediatool_to_adls.py'.</p>"},{"location":"references/orchestration/prefect/flows/#viadot.orchestration.prefect.flows.mediatool_to_adls.mediatool_to_adls","title":"<code>mediatool_to_adls(config_key=None, azure_key_vault_secret=None, organization_ids=None, media_entries_columns=None, adls_credentials=None, adls_config_key=None, adls_azure_key_vault_secret=None, adls_path=None, adls_path_overwrite=False)</code>","text":"<p>Download data from Mediatool to Azure Data Lake.</p> <p>Parameters:</p> Name Type Description Default <code>config_key</code> <code>str</code> <p>The key in the viadot config holding relevant credentials. Defaults to None.</p> <code>None</code> <code>azure_key_vault_secret</code> <code>str</code> <p>The name of the Azure Key Vault secret where credentials are stored. Defaults to None.</p> <code>None</code> <code>organization_ids</code> <code>list[str]</code> <p>List of organization IDs. Defaults to None.</p> <code>None</code> <code>media_entries_columns</code> <code>list[str]</code> <p>Columns to get from media entries. Defaults to None.</p> <code>None</code> <code>adls_credentials</code> <code>dict[str, Any]</code> <p>The credentials as a dictionary. Defaults to None.</p> <code>None</code> <code>adls_config_key</code> <code>str</code> <p>The key in the viadot config holding relevant credentials. Defaults to None.</p> <code>None</code> <code>adls_azure_key_vault_secret</code> <code>str</code> <p>The name of the Azure Key Vault secret containing a dictionary with ACCOUNT_NAME and Service Principal credentials (TENANT_ID, CLIENT_ID, CLIENT_SECRET) for the Azure Data Lake. Defaults to None.</p> <code>None</code> <code>adls_path</code> <code>str</code> <p>Azure Data Lake destination file path. Defaults to None.</p> <code>None</code> <code>adls_path_overwrite</code> <code>bool</code> <p>Whether to overwrite the file in ADLS. Defaults to True.</p> <code>False</code> Source code in <code>src/viadot/orchestration/prefect/flows/mediatool_to_adls.py</code> <pre><code>@flow(\n    name=\"Mediatool extraction to ADLS\",\n    description=\"Extract data from Mediatool and load it into Azure Data Lake Storage.\",\n    retries=1,\n    retry_delay_seconds=60,\n    task_runner=ConcurrentTaskRunner,\n)\ndef mediatool_to_adls(\n    config_key: str | None = None,\n    azure_key_vault_secret: str | None = None,\n    organization_ids: list[str] | None = None,\n    media_entries_columns: list[str] | None = None,\n    adls_credentials: dict[str, Any] | None = None,\n    adls_config_key: str | None = None,\n    adls_azure_key_vault_secret: str | None = None,\n    adls_path: str | None = None,\n    adls_path_overwrite: bool = False,\n) -&gt; None:\n    \"\"\"Download data from Mediatool to Azure Data Lake.\n\n    Args:\n        config_key (str, optional): The key in the viadot config holding relevant\n            credentials. Defaults to None.\n        azure_key_vault_secret (str, optional): The name of the Azure Key Vault secret\n            where credentials are stored. Defaults to None.\n        organization_ids (list[str], optional): List of organization IDs.\n            Defaults to None.\n        media_entries_columns (list[str], optional): Columns to get from media entries.\n            Defaults to None.\n        adls_credentials (dict[str, Any], optional): The credentials as a dictionary.\n            Defaults to None.\n        adls_config_key (str, optional): The key in the viadot config holding relevant\n            credentials. Defaults to None.\n        adls_azure_key_vault_secret (str, optional): The name of the Azure Key Vault\n            secret containing a dictionary with ACCOUNT_NAME and Service Principal\n            credentials (TENANT_ID, CLIENT_ID, CLIENT_SECRET) for the Azure Data Lake.\n            Defaults to None.\n        adls_path (str, optional): Azure Data Lake destination file path.\n            Defaults to None.\n        adls_path_overwrite (bool, optional): Whether to overwrite the file in ADLS.\n            Defaults to True.\n    \"\"\"\n    data_frame = mediatool_to_df(\n        config_key=config_key,\n        azure_key_vault_secret=azure_key_vault_secret,\n        organization_ids=organization_ids,\n        media_entries_columns=media_entries_columns,\n    )\n\n    return df_to_adls(\n        df=data_frame,\n        path=adls_path,\n        credentials=adls_credentials,\n        credentials_secret=adls_azure_key_vault_secret,\n        config_key=adls_config_key,\n        overwrite=adls_path_overwrite,\n    )\n</code></pre>"},{"location":"references/orchestration/prefect/flows/#viadot.orchestration.prefect.flows.mindful_to_adls","title":"<code>viadot.orchestration.prefect.flows.mindful_to_adls</code>","text":"<p>Download data from Mindful API and load it into Azure Data Lake Storage.</p>"},{"location":"references/orchestration/prefect/flows/#viadot.orchestration.prefect.flows.mindful_to_adls.mindful_to_adls","title":"<code>mindful_to_adls(config_key=None, azure_key_vault_secret=None, region='eu1', endpoint=None, date_interval=None, limit=1000, adls_config_key=None, adls_azure_key_vault_secret=None, adls_path=None, adls_path_overwrite=False)</code>","text":"<p>Flow to download data from Mindful to Azure Data Lake.</p> <p>Parameters:</p> Name Type Description Default <code>credentials</code> <code>Optional[Dict[str, Any]]</code> <p>Mindful credentials as a dictionary. Defaults to None.</p> required <code>config_key</code> <code>str</code> <p>The key in the viadot config holding relevant credentials. Defaults to None.</p> <code>None</code> <code>azure_key_vault_secret</code> <code>Optional[str]</code> <p>The name of the Azure Key Vault secret where credentials are stored. Defaults to None.</p> <code>None</code> <code>region</code> <code>Literal[us1, us2, us3, ca1, eu1, au1]</code> <p>Survey Dynamix region from where to interact with the mindful API. Defaults to \"eu1\" English (United Kingdom).</p> <code>'eu1'</code> <code>endpoint</code> <code>Optional[Union[List[str], str]]</code> <p>Endpoint name or list of them from where to download data. Defaults to None.</p> <code>None</code> <code>date_interval</code> <code>Optional[List[date]]</code> <p>Date time range detailing the starting date and the ending date. If no range is passed, one day of data since this moment will be retrieved. Defaults to None.</p> <code>None</code> <code>limit</code> <code>int</code> <p>The number of matching interactions to return. Defaults to 1000.</p> <code>1000</code> <code>adls_credentials</code> <code>Optional[Dict[str, Any]]</code> <p>The credentials as a dictionary. Defaults to None.</p> required <code>adls_config_key</code> <code>Optional[str]</code> <p>The key in the viadot config holding relevant credentials. Defaults to None.</p> <code>None</code> <code>adls_azure_key_vault_secret</code> <code>Optional[str]</code> <p>The name of the Azure Key Vault secret containing a dictionary with ACCOUNT_NAME and Service Principal credentials (TENANT_ID, CLIENT_ID, CLIENT_SECRET) for the Azure Data Lake. Defaults to None.</p> <code>None</code> <code>adls_path</code> <code>Optional[str]</code> <p>Azure Data Lake destination file path. Defaults to None.</p> <code>None</code> <code>adls_path_overwrite</code> <code>bool</code> <p>Whether to overwrite the file in ADLS. Defaults to True.</p> <code>False</code> <p>Examples:</p> <p>mindful_to_adls(     config_key=config_key,     endpoint=endpoint,     date_interval=date_interval,     adls_path=adls_path,     adls_config_key=adls_config_key,     adls_azure_key_vault_secret=adls_azure_key_vault_secret,     adls_path_overwrite=True, )</p> Source code in <code>src/viadot/orchestration/prefect/flows/mindful_to_adls.py</code> <pre><code>@flow(\n    name=\"Mindful extraction to ADLS\",\n    description=\"Extract data from mindful and load it into Azure Data Lake Storage.\",\n    retries=1,\n    retry_delay_seconds=60,\n    task_runner=ConcurrentTaskRunner,\n)\ndef mindful_to_adls(\n    config_key: str | None = None,\n    azure_key_vault_secret: str | None = None,\n    region: Literal[\"us1\", \"us2\", \"us3\", \"ca1\", \"eu1\", \"au1\"] = \"eu1\",\n    endpoint: list[str] | str | None = None,\n    date_interval: list[date] | None = None,\n    limit: int = 1000,\n    adls_config_key: str | None = None,\n    adls_azure_key_vault_secret: str | None = None,\n    adls_path: str | None = None,\n    adls_path_overwrite: bool = False,\n) -&gt; None:\n    \"\"\"Flow to download data from Mindful to Azure Data Lake.\n\n    Args:\n        credentials (Optional[Dict[str, Any]], optional): Mindful credentials as a\n            dictionary. Defaults to None.\n        config_key (str, optional): The key in the viadot config holding relevant\n            credentials. Defaults to None.\n        azure_key_vault_secret (Optional[str], optional): The name of the Azure Key\n            Vault secret where credentials are stored. Defaults to None.\n        region (Literal[us1, us2, us3, ca1, eu1, au1], optional): Survey Dynamix region\n            from where to interact with the mindful API. Defaults to \"eu1\" English\n            (United Kingdom).\n        endpoint (Optional[Union[List[str], str]], optional): Endpoint name or list of\n            them from where to download data. Defaults to None.\n        date_interval (Optional[List[date]], optional): Date time range detailing the\n            starting date and the ending date. If no range is passed, one day of data\n            since this moment will be retrieved. Defaults to None.\n        limit (int, optional): The number of matching interactions to return.\n            Defaults to 1000.\n        adls_credentials (Optional[Dict[str, Any]], optional): The credentials as a\n            dictionary. Defaults to None.\n        adls_config_key (Optional[str], optional): The key in the viadot config holding\n            relevant credentials. Defaults to None.\n        adls_azure_key_vault_secret (Optional[str], optional): The name of the Azure Key\n            Vault secret containing a dictionary with ACCOUNT_NAME and Service Principal\n            credentials (TENANT_ID, CLIENT_ID, CLIENT_SECRET) for the Azure Data Lake.\n            Defaults to None.\n        adls_path (Optional[str], optional): Azure Data Lake destination file path.\n            Defaults to None.\n        adls_path_overwrite (bool, optional): Whether to overwrite the file in ADLS.\n            Defaults to True.\n\n    Examples:\n        mindful_to_adls(\n            config_key=config_key,\n            endpoint=endpoint,\n            date_interval=date_interval,\n            adls_path=adls_path,\n            adls_config_key=adls_config_key,\n            adls_azure_key_vault_secret=adls_azure_key_vault_secret,\n            adls_path_overwrite=True,\n        )\n    \"\"\"\n    if isinstance(endpoint, str):\n        endpoint = [endpoint]\n\n    endpoints = endpoint\n\n    for endpoint in endpoints:\n        data_frame = mindful_to_df(\n            config_key=config_key,\n            azure_key_vault_secret=azure_key_vault_secret,\n            region=region,\n            endpoint=endpoint,\n            date_interval=date_interval,\n            limit=limit,\n        )\n\n        # ???\n        time.sleep(0.5)\n\n        df_to_adls(\n            df=data_frame,\n            path=adls_path.rstrip(\"/\") + \"/\" + f\"{endpoint}.csv\",\n            credentials_secret=adls_azure_key_vault_secret,\n            config_key=adls_config_key,\n            overwrite=adls_path_overwrite,\n        )\n</code></pre>"},{"location":"references/orchestration/prefect/flows/#viadot.orchestration.prefect.flows.outlook_to_adls","title":"<code>viadot.orchestration.prefect.flows.outlook_to_adls</code>","text":"<p>Download data from Outlook API to Azure Data Lake Storage.</p>"},{"location":"references/orchestration/prefect/flows/#viadot.orchestration.prefect.flows.outlook_to_adls.outlook_to_adls","title":"<code>outlook_to_adls(config_key=None, azure_key_vault_secret=None, mailbox_name=None, request_retries=10, start_date=None, end_date=None, limit=10000, address_limit=8000, outbox_list=None, adls_config_key=None, adls_azure_key_vault_secret=None, adls_path=None, adls_path_overwrite=False)</code>","text":"<p>Flow to download data from Outlook API to Azure Data Lake.</p> <p>Parameters:</p> Name Type Description Default <code>credentials</code> <code>Optional[Dict[str, Any]]</code> <p>Outlook credentials as a dictionary. Defaults to None.</p> required <code>config_key</code> <code>Optional[str]</code> <p>The key in the viadot config holding relevant credentials. Defaults to None.</p> <code>None</code> <code>azure_key_vault_secret</code> <code>Optional[str]</code> <p>The name of the Azure Key Vault secret where credentials are stored. Defaults to None.</p> <code>None</code> <code>mailbox_name</code> <code>Optional[str]</code> <p>Mailbox name. Defaults to None.</p> <code>None</code> <code>request_retries</code> <code>int</code> <p>How many times to retry the connection to Outlook. Defaults to 10.</p> <code>10</code> <code>start_date</code> <code>Optional[str]</code> <p>A filtering start date parameter e.g. \"2022-01-01\". Defaults to None.</p> <code>None</code> <code>end_date</code> <code>Optional[str]</code> <p>A filtering end date parameter e.g. \"2022-01-02\". Defaults to None.</p> <code>None</code> <code>limit</code> <code>int</code> <p>Number of fetched top messages. Defaults to 10000.</p> <code>10000</code> <code>address_limit</code> <code>int</code> <p>The maximum number of accepted characters in the sum of all email names. Defaults to 8000.</p> <code>8000</code> <code>outbox_list</code> <code>List[str]</code> <p>List of outbox folders to differentiate between Inboxes and Outboxes. Defaults to [\"Sent Items\"].</p> <code>None</code> <code>adls_credentials</code> <code>Optional[Dict[str, Any]]</code> <p>The credentials as a dictionary. Defaults to None.</p> required <code>adls_config_key</code> <code>Optional[str]</code> <p>The key in the viadot config holding relevant credentials. Defaults to None.</p> <code>None</code> <code>adls_azure_key_vault_secret</code> <code>Optional[str]</code> <p>The name of the Azure Key Vault secret containing a dictionary with ACCOUNT_NAME and Service Principal credentials (TENANT_ID, CLIENT_ID, CLIENT_SECRET) for the Azure Data Lake. Defaults to None.</p> <code>None</code> <code>adls_path</code> <code>Optional[str]</code> <p>Azure Data Lake destination file path (with file name). Defaults to None.</p> <code>None</code> <code>adls_path_overwrite</code> <code>bool</code> <p>Whether to overwrite the file in ADLS. Defaults to True.</p> <code>False</code> <p>Examples:</p> <p>outlook_to_adls(     config_key=config_key,     mailbox_name=mailbox_name,     start_date=start_date,     end_date=end_date,     adls_config_key=adls_config_key,     adls_path=adls_path,     adls_path_overwrite=True, )</p> Source code in <code>src/viadot/orchestration/prefect/flows/outlook_to_adls.py</code> <pre><code>@flow(\n    name=\"Outlook extraction to ADLS\",\n    description=\"Extract data from Outlook and load it into Azure Data Lake Storage.\",\n    retries=1,\n    retry_delay_seconds=60,\n    task_runner=ConcurrentTaskRunner,\n)\ndef outlook_to_adls(  # noqa: PLR0913\n    config_key: str | None = None,\n    azure_key_vault_secret: str | None = None,\n    mailbox_name: str | None = None,\n    request_retries: int = 10,\n    start_date: str | None = None,\n    end_date: str | None = None,\n    limit: int = 10000,\n    address_limit: int = 8000,\n    outbox_list: list[str] | None = None,\n    adls_config_key: str | None = None,\n    adls_azure_key_vault_secret: str | None = None,\n    adls_path: str | None = None,\n    adls_path_overwrite: bool = False,\n) -&gt; None:\n    \"\"\"Flow to download data from Outlook API to Azure Data Lake.\n\n    Args:\n        credentials (Optional[Dict[str, Any]], optional): Outlook credentials as a\n            dictionary. Defaults to None.\n        config_key (Optional[str], optional): The key in the viadot config holding\n            relevant credentials. Defaults to None.\n        azure_key_vault_secret (Optional[str], optional): The name of the Azure Key\n            Vault secret where credentials are stored. Defaults to None.\n        mailbox_name (Optional[str], optional): Mailbox name. Defaults to None.\n        request_retries (int, optional): How many times to retry the connection to\n            Outlook. Defaults to 10.\n        start_date (Optional[str], optional): A filtering start date parameter e.g.\n            \"2022-01-01\". Defaults to None.\n        end_date (Optional[str], optional): A filtering end date parameter e.g.\n            \"2022-01-02\". Defaults to None.\n        limit (int, optional): Number of fetched top messages. Defaults to 10000.\n        address_limit (int, optional): The maximum number of accepted characters in the\n            sum of all email names. Defaults to 8000.\n        outbox_list (List[str], optional): List of outbox folders to differentiate\n            between Inboxes and Outboxes. Defaults to [\"Sent Items\"].\n        adls_credentials (Optional[Dict[str, Any]], optional): The credentials as a\n            dictionary. Defaults to None.\n        adls_config_key (Optional[str], optional): The key in the viadot config holding\n            relevant credentials. Defaults to None.\n        adls_azure_key_vault_secret (Optional[str], optional): The name of the Azure Key\n            Vault secret containing a dictionary with ACCOUNT_NAME and Service Principal\n            credentials (TENANT_ID, CLIENT_ID, CLIENT_SECRET) for the Azure Data Lake.\n            Defaults to None.\n        adls_path (Optional[str], optional): Azure Data Lake destination file path\n            (with file name). Defaults to None.\n        adls_path_overwrite (bool, optional): Whether to overwrite the file in ADLS.\n            Defaults to True.\n\n    Examples:\n        outlook_to_adls(\n            config_key=config_key,\n            mailbox_name=mailbox_name,\n            start_date=start_date,\n            end_date=end_date,\n            adls_config_key=adls_config_key,\n            adls_path=adls_path,\n            adls_path_overwrite=True,\n        )\n    \"\"\"\n    if outbox_list is None:\n        outbox_list = [\"Sent Items\"]\n\n    data_frame = outlook_to_df(\n        config_key=config_key,\n        azure_key_vault_secret=azure_key_vault_secret,\n        mailbox_name=mailbox_name,\n        request_retries=request_retries,\n        start_date=start_date,\n        end_date=end_date,\n        limit=limit,\n        address_limit=address_limit,\n        outbox_list=outbox_list,\n    )\n\n    return df_to_adls(\n        df=data_frame,\n        path=adls_path,\n        credentials_secret=adls_azure_key_vault_secret,\n        config_key=adls_config_key,\n        overwrite=adls_path_overwrite,\n    )\n</code></pre>"},{"location":"references/orchestration/prefect/flows/#viadot.orchestration.prefect.flows.salesforce_to_adls","title":"<code>viadot.orchestration.prefect.flows.salesforce_to_adls</code>","text":"<p>Download data from Salesforce API to Azure Data Lake Storage.</p>"},{"location":"references/orchestration/prefect/flows/#viadot.orchestration.prefect.flows.salesforce_to_adls.salesforce_to_adls","title":"<code>salesforce_to_adls(config_key=None, azure_key_vault_secret=None, env=None, domain=None, client_id=None, query=None, table=None, columns=None, adls_config_key=None, adls_azure_key_vault_secret=None, adls_path=None, adls_path_overwrite=False)</code>","text":"<p>Flow to download data from Salesforce API to Azure Data Lake.</p> <p>Parameters:</p> Name Type Description Default <code>config_key</code> <code>str</code> <p>The key in the viadot config holding relevant credentials. Defaults to None.</p> <code>None</code> <code>azure_key_vault_secret</code> <code>str</code> <p>The name of the Azure Key Vault secret where credentials are stored. Defaults to None.</p> <code>None</code> <code>env</code> <code>str</code> <p>Environment information, provides information about credential and connection configuration. Defaults to 'DEV'.</p> <code>None</code> <code>domain</code> <code>str</code> <p>Domain of a connection. defaults to 'test' (sandbox). Can only be added if built-in username/password/security token is provided. Defaults to None.</p> <code>None</code> <code>client_id</code> <code>str</code> <p>Client id to keep the track of API calls. Defaults to None.</p> <code>None</code> <code>query</code> <code>str</code> <p>Query for download the data if specific download is needed. Defaults to None.</p> <code>None</code> <code>table</code> <code>str</code> <p>Table name. Can be used instead of query. Defaults to None.</p> <code>None</code> <code>columns</code> <code>list[str]</code> <p>List of columns which are needed - table argument is needed. Defaults to None.</p> <code>None</code> <code>adls_config_key</code> <code>str</code> <p>The key in the viadot config holding relevant credentials. Defaults to None.</p> <code>None</code> <code>adls_azure_key_vault_secret</code> <code>str</code> <p>The name of the Azure Key Vault secret containing a dictionary with ACCOUNT_NAME and Service Principal credentials (TENANT_ID, CLIENT_ID, CLIENT_SECRET) for the Azure Data Lake. Defaults to None.</p> <code>None</code> <code>adls_path</code> <code>str</code> <p>Azure Data Lake destination file path (with file name). Defaults to None.</p> <code>None</code> <code>adls_path_overwrite</code> <code>bool</code> <p>Whether to overwrite the file in ADLS. Defaults to True.</p> <code>False</code> Source code in <code>src/viadot/orchestration/prefect/flows/salesforce_to_adls.py</code> <pre><code>@flow(\n    name=\"Salesforce extraction to ADLS\",\n    description=\"Extract data from Salesforce and load \"\n    + \"it into Azure Data Lake Storage.\",\n    retries=1,\n    retry_delay_seconds=60,\n    task_runner=ConcurrentTaskRunner,\n)\ndef salesforce_to_adls(  # noqa: PLR0913\n    config_key: str | None = None,\n    azure_key_vault_secret: str | None = None,\n    env: str | None = None,\n    domain: str | None = None,\n    client_id: str | None = None,\n    query: str | None = None,\n    table: str | None = None,\n    columns: list[str] | None = None,\n    adls_config_key: str | None = None,\n    adls_azure_key_vault_secret: str | None = None,\n    adls_path: str | None = None,\n    adls_path_overwrite: bool = False,\n) -&gt; None:\n    \"\"\"Flow to download data from Salesforce API to Azure Data Lake.\n\n    Args:\n        config_key (str, optional): The key in the viadot config holding relevant\n            credentials. Defaults to None.\n        azure_key_vault_secret (str, optional): The name of the Azure Key Vault secret\n            where credentials are stored. Defaults to None.\n        env (str, optional): Environment information, provides information about\n            credential and connection configuration. Defaults to 'DEV'.\n        domain (str, optional): Domain of a connection. defaults to 'test' (sandbox).\n            Can only be added if built-in username/password/security token is provided.\n            Defaults to None.\n        client_id (str, optional): Client id to keep the track of API calls.\n            Defaults to None.\n        query (str, optional): Query for download the data if specific download is\n            needed. Defaults to None.\n        table (str, optional): Table name. Can be used instead of query.\n            Defaults to None.\n        columns (list[str], optional): List of columns which are needed - table\n            argument is needed. Defaults to None.\n        adls_config_key (str, optional): The key in the viadot config holding\n            relevant credentials. Defaults to None.\n        adls_azure_key_vault_secret (str, optional): The name of the Azure Key\n            Vault secret containing a dictionary with ACCOUNT_NAME and Service Principal\n            credentials (TENANT_ID, CLIENT_ID, CLIENT_SECRET) for the Azure Data Lake.\n            Defaults to None.\n        adls_path (str, optional): Azure Data Lake destination file path\n            (with file name). Defaults to None.\n        adls_path_overwrite (bool, optional): Whether to overwrite the file in ADLS.\n            Defaults to True.\n    \"\"\"\n    data_frame = salesforce_to_df(\n        config_key=config_key,\n        azure_key_vault_secret=azure_key_vault_secret,\n        env=env,\n        domain=domain,\n        client_id=client_id,\n        query=query,\n        table=table,\n        columns=columns,\n    )\n\n    return df_to_adls(\n        df=data_frame,\n        path=adls_path,\n        credentials_secret=adls_azure_key_vault_secret,\n        config_key=adls_config_key,\n        overwrite=adls_path_overwrite,\n    )\n</code></pre>"},{"location":"references/orchestration/prefect/flows/#viadot.orchestration.prefect.flows.sap_bw_to_adls","title":"<code>viadot.orchestration.prefect.flows.sap_bw_to_adls</code>","text":"<p>Task to download data from SAP BW API into a Pandas DataFrame.</p>"},{"location":"references/orchestration/prefect/flows/#viadot.orchestration.prefect.flows.sap_bw_to_adls.sap_bw_to_adls","title":"<code>sap_bw_to_adls(config_key=None, azure_key_vault_secret=None, mdx_query=None, mapping_dict=None, adls_azure_key_vault_secret=None, adls_config_key=None, adls_path=None, adls_path_overwrite=False)</code>","text":"<p>Flow for downloading data from SAP BW API to Azure Data Lake.</p> <p>Parameters:</p> Name Type Description Default <code>config_key</code> <code>Optional[str]</code> <p>The key in the viadot config holding relevant credentials. Defaults to None.</p> <code>None</code> <code>azure_key_vault_secret</code> <code>Optional[str]</code> <p>The name of the Azure Key Vault secret where credentials are stored. Defaults to None.</p> <code>None</code> <code>mdx_query</code> <code>str</code> <p>The MDX query to be passed to connection.</p> <code>None</code> <code>mapping_dict</code> <code>dict[str, Any]</code> <p>Dictionary with original and new column names. Defaults to None.</p> <code>None</code> <code>adls_azure_key_vault_secret</code> <code>str</code> <p>The name of the Azure Key. Defaults to None.</p> <code>None</code> <code>adls_config_key</code> <code>str</code> <p>The key in the viadot config holding relevant credentials. Defaults to None.</p> <code>None</code> <code>adls_path</code> <code>str</code> <p>Azure Data Lake destination folder/catalog path. Defaults to None.</p> <code>None</code> <code>adls_path_overwrite</code> <code>bool</code> <p>Whether to overwrite the file in ADLS. Defaults to False.</p> <code>False</code> Source code in <code>src/viadot/orchestration/prefect/flows/sap_bw_to_adls.py</code> <pre><code>@flow(\n    name=\"SAP BW extraction to ADLS\",\n    description=\"Extract data from SAP BW and load it into Azure Data Lake Storage.\",\n    retries=1,\n    retry_delay_seconds=60,\n    task_runner=ConcurrentTaskRunner,\n)\ndef sap_bw_to_adls(\n    config_key: str | None = None,\n    azure_key_vault_secret: str | None = None,\n    mdx_query: str | None = None,\n    mapping_dict: dict[str, Any] | None = None,\n    adls_azure_key_vault_secret: str | None = None,\n    adls_config_key: str | None = None,\n    adls_path: str | None = None,\n    adls_path_overwrite: bool = False,\n) -&gt; None:\n    \"\"\"Flow for downloading data from SAP BW API to Azure Data Lake.\n\n    Args:\n        config_key (Optional[str], optional): The key in the viadot config holding\n            relevant credentials. Defaults to None.\n        azure_key_vault_secret (Optional[str], optional): The name of the Azure Key\n            Vault secret where credentials are stored. Defaults to None.\n        mdx_query (str, optional): The MDX query to be passed to connection.\n        mapping_dict (dict[str, Any], optional): Dictionary with original and new\n            column names. Defaults to None.\n        adls_azure_key_vault_secret (str, optional): The name of the Azure Key.\n            Defaults to None.\n        adls_config_key (str, optional): The key in the viadot config holding relevant\n            credentials. Defaults to None.\n        adls_path (str, optional): Azure Data Lake destination folder/catalog path.\n            Defaults to None.\n        adls_path_overwrite (bool, optional): Whether to overwrite the file in ADLS.\n            Defaults to False.\n    \"\"\"\n    data_frame = sap_bw_to_df(\n        config_key=config_key,\n        azure_key_vault_secret=azure_key_vault_secret,\n        mdx_query=mdx_query,\n        mapping_dict=mapping_dict,\n    )\n\n    return df_to_adls(\n        df=data_frame,\n        path=adls_path,\n        credentials_secret=adls_azure_key_vault_secret,\n        config_key=adls_config_key,\n        overwrite=adls_path_overwrite,\n    )\n</code></pre>"},{"location":"references/orchestration/prefect/flows/#viadot.orchestration.prefect.flows.sap_to_parquet","title":"<code>viadot.orchestration.prefect.flows.sap_to_parquet</code>","text":"<p>Flows for downloading data from SAP to Parquet file.</p>"},{"location":"references/orchestration/prefect/flows/#viadot.orchestration.prefect.flows.sap_to_parquet.sap_to_parquet","title":"<code>sap_to_parquet(path, if_exists='replace', query=None, func=None, sap_sep=None, rfc_total_col_width_character_limit=400, rfc_unique_id=None, sap_credentials_secret=None, sap_config_key='SAP', replacement='-')</code>","text":"<p>Download a pandas <code>DataFrame</code> from SAP load it into Parquet file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to Parquet file, where the data will be located. Defaults to None.</p> required <code>if_exists</code> <code>Literal['append', 'replace', 'skip']</code> <p>What to do if the file exists. Defaults to \"replace\".</p> <code>'replace'</code> <code>query</code> <code>str</code> <p>The query to be executed with pyRFC.</p> <code>None</code> <code>sap_sep</code> <code>str</code> <p>The separator to use when reading query results. If not provided, multiple options are automatically tested. Defaults to None.</p> <code>None</code> <code>func</code> <code>str</code> <p>SAP RFC function to use. Defaults to None.</p> <code>None</code> <code>rfc_total_col_width_character_limit</code> <code>int</code> <p>Number of characters by which query will be split in chunks in case of too many columns for RFC function. According to SAP documentation, the limit is 512 characters. However, it was observed that SAP raising an exception even on a slightly lower number of characters, so safety margin was added. Defaults to 400.</p> <code>400</code> <code>rfc_unique_id</code> <code> (list[str]</code> <p>Reference columns to merge chunks Data Frames. These columns must to be unique. Otherwise, the table will be malformed. If no columns are provided, all data frame columns will be concatenated. Defaults to None.</p> <code>None</code> <code>sap_credentials_secret</code> <code>str</code> <p>The name of the Prefect Secret that stores SAP credentials. Defaults to None.</p> <code>None</code> <code>sap_config_key</code> <code>str</code> <p>The key in the viadot config holding relevant credentials. Defaults to \"SAP\".</p> <code>'SAP'</code> <code>replacement</code> <code>str</code> <p>In case of sep is on a columns, set up a new character to replace inside the string to avoid flow breakdowns. Defaults to \"-\".</p> <code>'-'</code> Source code in <code>src/viadot/orchestration/prefect/flows/sap_to_parquet.py</code> <pre><code>@flow(\n    name=\"extract--sap--parquet\",\n    description=\"Extract data from SAP and load it into Parquet file\",\n    retries=1,\n    retry_delay_seconds=60,\n)\ndef sap_to_parquet(\n    path: str,\n    if_exists: Literal[\"append\", \"replace\", \"skip\"] = \"replace\",\n    query: str | None = None,\n    func: str | None = None,\n    sap_sep: str | None = None,\n    rfc_total_col_width_character_limit: int = 400,\n    rfc_unique_id: list[str] | None = None,\n    sap_credentials_secret: str | None = None,\n    sap_config_key: str = \"SAP\",\n    replacement: str = \"-\",\n) -&gt; None:\n    \"\"\"Download a pandas `DataFrame` from SAP load it into Parquet file.\n\n    Args:\n        path (str): Path to Parquet file, where the data will be located.\n            Defaults to None.\n        if_exists (Literal[\"append\", \"replace\", \"skip\"], optional): What to do if the\n            file exists. Defaults to \"replace\".\n        query (str): The query to be executed with pyRFC.\n        sap_sep (str, optional): The separator to use when reading query results.\n            If not provided, multiple options are automatically tested.\n            Defaults to None.\n        func (str, optional): SAP RFC function to use. Defaults to None.\n        rfc_total_col_width_character_limit (int, optional): Number of characters by\n            which query will be split in chunks in case of too many columns for RFC\n            function. According to SAP documentation, the limit is 512 characters.\n            However, it was observed that SAP raising an exception even on a slightly\n            lower number of characters, so safety margin was added. Defaults to 400.\n        rfc_unique_id  (list[str], optional): Reference columns to merge chunks Data\n            Frames. These columns must to be unique. Otherwise, the table will be\n            malformed. If no columns are provided, all data frame columns will be\n            concatenated. Defaults to None.\n        sap_credentials_secret (str, optional): The name of the Prefect Secret that\n            stores SAP credentials. Defaults to None.\n        sap_config_key (str, optional): The key in the viadot config holding relevant\n            credentials. Defaults to \"SAP\".\n        replacement (str, optional): In case of sep is on a columns, set up a new\n            character to replace inside the string to avoid flow breakdowns.\n            Defaults to \"-\".\n    \"\"\"\n    df = sap_rfc_to_df(\n        query=query,\n        sep=sap_sep,\n        func=func,\n        replacement=replacement,\n        rfc_total_col_width_character_limit=rfc_total_col_width_character_limit,\n        rfc_unique_id=rfc_unique_id,\n        config_key=sap_config_key,\n        credentials_secret=sap_credentials_secret,\n    )\n\n    return df_to_parquet(\n        df=df,\n        path=path,\n        if_exists=if_exists,\n    )\n</code></pre>"},{"location":"references/orchestration/prefect/flows/#viadot.orchestration.prefect.flows.sap_to_redshift_spectrum","title":"<code>viadot.orchestration.prefect.flows.sap_to_redshift_spectrum</code>","text":"<p>Flows for downloading data from SAP and uploading it to AWS Redshift Spectrum.</p>"},{"location":"references/orchestration/prefect/flows/#viadot.orchestration.prefect.flows.sap_to_redshift_spectrum.sap_to_redshift_spectrum","title":"<code>sap_to_redshift_spectrum(to_path, schema_name, table, tests=None, extension='.parquet', if_exists='overwrite', partition_cols=None, index=False, compression=None, aws_sep=',', description='test', dynamic_date_symbols=['&lt;&lt;', '&gt;&gt;'], dynamic_date_format='%Y%m%d', dynamic_date_timezone='UTC', credentials_secret=None, aws_config_key=None, query=None, sap_sep=None, func=None, rfc_total_col_width_character_limit=400, rfc_unique_id=None, sap_credentials_secret=None, sap_config_key=None, replacement='-')</code>","text":"<p>Download a pandas <code>DataFrame</code> from SAP and upload it to AWS Redshift Spectrum.</p> <p>Parameters:</p> Name Type Description Default <code>to_path</code> <code>str</code> <p>Path to a S3 folder where the table will be located. Defaults to None.</p> required <code>schema_name</code> <code>str</code> <p>AWS Glue catalog database name.</p> required <code>table</code> <code>str</code> <p>AWS Glue catalog table name.</p> required <code>tests</code> <code>dict[str]</code> <p>A dictionary with optional list of tests to verify the output dataframe. If defined, triggers the <code>validate</code> function from viadot.utils. Defaults to None.</p> <code>None</code> <code>partition_cols</code> <code>list[str]</code> <p>List of column names that will be used to create partitions. Only takes effect if dataset=True.</p> <code>None</code> <code>extension</code> <code>str</code> <p>Required file type. Accepted file formats are 'csv' and 'parquet'.</p> <code>'.parquet'</code> <code>if_exists</code> <code>str</code> <p>'overwrite' to recreate any possible existing table or 'append' to keep any possible existing table. Defaults to overwrite.</p> <code>'overwrite'</code> <code>partition_cols</code> <code>list[str]</code> <p>List of column names that will be used to create partitions. Only takes effect if dataset=True. Defaults to None.</p> <code>None</code> <code>index</code> <code>bool</code> <p>Write row names (index). Defaults to False.</p> <code>False</code> <code>compression</code> <code>str</code> <p>Compression style (None, snappy, gzip, zstd).</p> <code>None</code> <code>aws_sep</code> <code>str</code> <p>Field delimiter for the output file. Defaults to ','.</p> <code>','</code> <code>description</code> <code>str</code> <p>AWS Glue catalog table description.</p> <code>'test'</code> <code>dynamic_date_symbols</code> <code>list[str]</code> <p>Symbols used for dynamic date handling. Defaults to [\"&lt;&lt;\", \"&gt;&gt;\"].</p> <code>['&lt;&lt;', '&gt;&gt;']</code> <code>dynamic_date_format</code> <code>str</code> <p>Format used for dynamic date parsing. Defaults to \"%Y%m%d\".</p> <code>'%Y%m%d'</code> <code>dynamic_date_timezone</code> <code>str</code> <p>Timezone used for dynamic date processing. Defaults to \"UTC\".</p> <code>'UTC'</code> <code>aws_config_key</code> <code>str</code> <p>The key in the viadot config holding relevant credentials. Defaults to None.</p> <code>None</code> <code>credentials_secret</code> <code>str</code> <p>The name of a secret block in Prefect that stores AWS credentials. Defaults to None.</p> <code>None</code> <code>query</code> <code>str</code> <p>The query to be executed with pyRFC.</p> <code>None</code> <code>sap_sep</code> <code>str</code> <p>The separator to use when reading query results. If not provided, multiple options are automatically tried. Defaults to None.</p> <code>None</code> <code>func</code> <code>str</code> <p>SAP RFC function to use. Defaults to None.</p> <code>None</code> <code>rfc_total_col_width_character_limit</code> <code>int</code> <p>Number of characters by which query will be split in chunks in case of too many columns for RFC function. According to SAP documentation, the limit is 512 characters. However, we observed SAP raising an exception even on a slightly lower number of characters, so we add a safety margin. Defaults to 400.</p> <code>400</code> <code>rfc_unique_id</code> <code> (list[str]</code> <p>Reference columns to merge chunks Data Frames. These columns must to be unique. If no columns are provided, all     data frame columns will by concatenated. Defaults to None.</p> <code>None</code> <code>sap_credentials_secret</code> <code>str</code> <p>The name of the AWS secret that stores SAP credentials. Defaults to None.</p> <code>None</code> <code>sap_config_key</code> <code>str</code> <p>The key in the viadot config holding relevant credentials. Defaults to None.</p> <code>None</code> <code>replacement</code> <code>str</code> <p>In case of sep is on a columns, set up a new character to replace inside the string to avoid flow breakdowns. Defaults to \"-\".</p> <code>'-'</code> <p>Examples:</p> <p>sap_to_redshift_spectrum(     ...     rfc_unique_id=[\"VBELN\", \"LPRIO\"],     ... )</p> Source code in <code>src/viadot/orchestration/prefect/flows/sap_to_redshift_spectrum.py</code> <pre><code>@flow(\n    name=\"extract--sap--redshift_spectrum\",\n    description=\"Extract data from SAP and load it into AWS Redshift Spectrum.\",\n    retries=1,\n    retry_delay_seconds=60,\n)\ndef sap_to_redshift_spectrum(  # noqa: PLR0913\n    to_path: str,\n    schema_name: str,\n    table: str,\n    tests: dict[str, Any] | None = None,\n    extension: str = \".parquet\",\n    if_exists: Literal[\"overwrite\", \"append\"] = \"overwrite\",\n    partition_cols: list[str] | None = None,\n    index: bool = False,\n    compression: str | None = None,\n    aws_sep: str = \",\",\n    description: str = \"test\",\n    dynamic_date_symbols: list[str] = [\"&lt;&lt;\", \"&gt;&gt;\"],  # noqa: B006\n    dynamic_date_format: str = \"%Y%m%d\",\n    dynamic_date_timezone: str = \"UTC\",\n    credentials_secret: str | None = None,\n    aws_config_key: str | None = None,\n    query: str | None = None,\n    sap_sep: str | None = None,\n    func: str | None = None,\n    rfc_total_col_width_character_limit: int = 400,\n    rfc_unique_id: list[str] | None = None,\n    sap_credentials_secret: str | None = None,\n    sap_config_key: str | None = None,\n    replacement: str = \"-\",\n) -&gt; None:\n    \"\"\"Download a pandas `DataFrame` from SAP and upload it to AWS Redshift Spectrum.\n\n    Args:\n        to_path (str): Path to a S3 folder where the table will be located.\n            Defaults to None.\n        schema_name (str): AWS Glue catalog database name.\n        table (str): AWS Glue catalog table name.\n        tests (dict[str], optional): A dictionary with optional list of tests\n            to verify the output dataframe. If defined, triggers the `validate`\n            function from viadot.utils. Defaults to None.\n        partition_cols (list[str]): List of column names that will be used to create\n            partitions. Only takes effect if dataset=True.\n        extension (str): Required file type. Accepted file formats are 'csv' and\n            'parquet'.\n        if_exists (str, optional): 'overwrite' to recreate any possible existing table\n            or 'append' to keep any possible existing table. Defaults to overwrite.\n        partition_cols (list[str], optional): List of column names that will be used to\n            create partitions. Only takes effect if dataset=True. Defaults to None.\n        index (bool, optional): Write row names (index). Defaults to False.\n        compression (str, optional): Compression style (None, snappy, gzip, zstd).\n        aws_sep (str, optional): Field delimiter for the output file. Defaults to ','.\n        description (str, optional): AWS Glue catalog table description.\n        dynamic_date_symbols (list[str], optional): Symbols used for dynamic date\n            handling. Defaults to [\"&lt;&lt;\", \"&gt;&gt;\"].\n        dynamic_date_format (str, optional): Format used for dynamic date parsing.\n            Defaults to \"%Y%m%d\".\n        dynamic_date_timezone (str, optional): Timezone used for dynamic date\n            processing. Defaults to \"UTC\".\n        aws_config_key (str, optional): The key in the viadot config holding relevant\n            credentials. Defaults to None.\n        credentials_secret (str, optional): The name of a secret block in Prefect\n            that stores AWS credentials. Defaults to None.\n        query (str): The query to be executed with pyRFC.\n        sap_sep (str, optional): The separator to use when reading query results.\n            If not provided, multiple options are automatically tried.\n            Defaults to None.\n        func (str, optional): SAP RFC function to use. Defaults to None.\n        rfc_total_col_width_character_limit (int, optional): Number of characters by\n            which query will be split in chunks in case of too many columns for RFC\n            function. According to SAP documentation, the limit is 512 characters.\n            However, we observed SAP raising an exception even on a slightly lower\n            number of characters, so we add a safety margin. Defaults to 400.\n        rfc_unique_id  (list[str], optional): Reference columns to merge chunks Data\n            Frames. These columns must to be unique. If no columns are provided, all\n                data frame columns will by concatenated. Defaults to None.\n        sap_credentials_secret (str, optional): The name of the AWS secret that stores\n            SAP credentials. Defaults to None.\n        sap_config_key (str, optional): The key in the viadot config holding relevant\n            credentials. Defaults to None.\n        replacement (str, optional): In case of sep is on a columns, set up a new\n            character to replace inside the string to avoid flow breakdowns.\n            Defaults to \"-\".\n\n    Examples:\n        sap_to_redshift_spectrum(\n            ...\n            rfc_unique_id=[\"VBELN\", \"LPRIO\"],\n            ...\n        )\n    \"\"\"\n    df = sap_rfc_to_df(\n        query=query,\n        sep=sap_sep,\n        tests=tests,\n        func=func,\n        rfc_unique_id=rfc_unique_id,\n        rfc_total_col_width_character_limit=rfc_total_col_width_character_limit,\n        credentials_secret=sap_credentials_secret,\n        config_key=sap_config_key,\n        replacement=replacement,\n        dynamic_date_symbols=dynamic_date_symbols,\n        dynamic_date_format=dynamic_date_format,\n        dynamic_date_timezone=dynamic_date_timezone,\n    )\n\n    return df_to_redshift_spectrum(\n        df=df,\n        to_path=to_path,\n        schema_name=schema_name,\n        table=table,\n        extension=extension,\n        if_exists=if_exists,\n        partition_cols=partition_cols,\n        index=index,\n        compression=compression,\n        sep=aws_sep,\n        description=description,\n        config_key=aws_config_key,\n        credentials_secret=credentials_secret,\n    )\n</code></pre>"},{"location":"references/orchestration/prefect/flows/#viadot.orchestration.prefect.flows.sftp_to_adls","title":"<code>viadot.orchestration.prefect.flows.sftp_to_adls</code>","text":"<p>Download data from a SFTP server to Azure Data Lake Storage.</p>"},{"location":"references/orchestration/prefect/flows/#viadot.orchestration.prefect.flows.sftp_to_adls.sftp_to_adls","title":"<code>sftp_to_adls(config_key=None, azure_key_vault_secret=None, file_name=None, sep='\\t', columns=None, adls_config_key=None, adls_azure_key_vault_secret=None, adls_path=None, adls_path_overwrite=False)</code>","text":"<p>Flow to download data from a SFTP server to Azure Data Lake.</p> <p>Parameters:</p> Name Type Description Default <code>config_key</code> <code>str</code> <p>The key in the viadot config holding relevant credentials. Defaults to None.</p> <code>None</code> <code>azure_key_vault_secret</code> <code>str</code> <p>The name of the Azure Key Vault secret where credentials are stored. Defaults to None.</p> <code>None</code> <code>file_name</code> <code>str</code> <p>Path to the file in SFTP server. Defaults to None.</p> <code>None</code> <code>sep</code> <code>str</code> <p>The separator to use to read the CSV file. Defaults to \"\\t\".</p> <code>'\\t'</code> <code>columns</code> <code>List[str]</code> <p>Columns to read from the file. Defaults to None.</p> <code>None</code> <code>adls_config_key</code> <code>str</code> <p>The key in the viadot config holding relevant credentials. Defaults to None.</p> <code>None</code> <code>adls_azure_key_vault_secret</code> <code>str</code> <p>The name of the Azure Key Vault secret containing a dictionary with ACCOUNT_NAME and Service Principal credentials (TENANT_ID, CLIENT_ID, CLIENT_SECRET) for the Azure Data Lake. Defaults to None.</p> <code>None</code> <code>adls_path</code> <code>str</code> <p>Azure Data Lake destination file path (with file name). Defaults to None.</p> <code>None</code> <code>adls_path_overwrite</code> <code>bool</code> <p>Whether to overwrite the file in ADLS. Defaults to True.</p> <code>False</code> Source code in <code>src/viadot/orchestration/prefect/flows/sftp_to_adls.py</code> <pre><code>@flow(\n    name=\"SFTP extraction to ADLS\",\n    description=\"Extract data from a SFTP server and \"\n    + \"load it into Azure Data Lake Storage.\",\n    retries=1,\n    retry_delay_seconds=60,\n    task_runner=ConcurrentTaskRunner,\n)\ndef sftp_to_adls(\n    config_key: str | None = None,\n    azure_key_vault_secret: str | None = None,\n    file_name: str | None = None,\n    sep: str = \"\\t\",\n    columns: list[str] | None = None,\n    adls_config_key: str | None = None,\n    adls_azure_key_vault_secret: str | None = None,\n    adls_path: str | None = None,\n    adls_path_overwrite: bool = False,\n) -&gt; None:\n    r\"\"\"Flow to download data from a SFTP server to Azure Data Lake.\n\n    Args:\n        config_key (str, optional): The key in the viadot config holding relevant\n            credentials. Defaults to None.\n        azure_key_vault_secret (str, optional): The name of the Azure Key Vault secret\n            where credentials are stored. Defaults to None.\n        file_name (str, optional): Path to the file in SFTP server. Defaults to None.\n        sep (str, optional): The separator to use to read the CSV file.\n            Defaults to \"\\t\".\n        columns (List[str], optional): Columns to read from the file. Defaults to None.\n        adls_config_key (str, optional): The key in the viadot config holding\n            relevant credentials. Defaults to None.\n        adls_azure_key_vault_secret (str, optional): The name of the Azure Key\n            Vault secret containing a dictionary with ACCOUNT_NAME and Service Principal\n            credentials (TENANT_ID, CLIENT_ID, CLIENT_SECRET) for the Azure Data Lake.\n            Defaults to None.\n        adls_path (str, optional): Azure Data Lake destination file path\n            (with file name). Defaults to None.\n        adls_path_overwrite (bool, optional): Whether to overwrite the file in ADLS.\n            Defaults to True.\n    \"\"\"\n    data_frame = sftp_to_df(\n        config_key=config_key,\n        azure_key_vault_secret=azure_key_vault_secret,\n        file_name=file_name,\n        sep=sep,\n        columns=columns,\n    )\n\n    return df_to_adls(\n        df=data_frame,\n        path=adls_path,\n        credentials_secret=adls_azure_key_vault_secret,\n        config_key=adls_config_key,\n        overwrite=adls_path_overwrite,\n    )\n</code></pre>"},{"location":"references/orchestration/prefect/flows/#viadot.orchestration.prefect.flows.sharepoint_to_adls","title":"<code>viadot.orchestration.prefect.flows.sharepoint_to_adls</code>","text":"<p>Flows for pulling data from/into Sharepoint.</p>"},{"location":"references/orchestration/prefect/flows/#viadot.orchestration.prefect.flows.sharepoint_to_adls.sharepoint_to_adls","title":"<code>sharepoint_to_adls(sharepoint_url, adls_path, sharepoint_credentials_secret=None, sharepoint_config_key=None, adls_credentials_secret=None, adls_config_key=None, sheet_name=None, columns=None, overwrite=False)</code>","text":"<p>Download a file from Sharepoint and upload it to Azure Data Lake.</p> <p>Parameters:</p> Name Type Description Default <code>sharepoint_url</code> <code>str</code> <p>The URL to the file.</p> required <code>adls_path</code> <code>str</code> <p>The destination path.</p> required <code>sharepoint_credentials_secret</code> <code>str</code> <p>The name of the Azure Key Vault secret storing the Sharepoint credentials. Defaults to None.</p> <code>None</code> <code>sharepoint_config_key</code> <code>str</code> <p>The key in the viadot config holding relevant credentials. Defaults to None.</p> <code>None</code> <code>adls_credentials_secret</code> <code>str</code> <p>The name of the Azure Key Vault secret storing the ADLS credentials. Defaults to None.</p> <code>None</code> <code>adls_config_key</code> <code>str</code> <p>The key in the viadot config holding relevant credentials. Defaults to None.</p> <code>None</code> <code>sheet_name</code> <code>str | list | int</code> <p>Strings are used for sheet names. Integers are used in zero-indexed sheet positions (chart sheets do not count as a sheet position). Lists of strings/integers are used to request multiple sheets. Specify None to get all worksheets. Defaults to None.</p> <code>None</code> <code>columns</code> <code>str | list[str] | list[int]</code> <p>Which columns to ingest. Defaults to None.</p> <code>None</code> <code>overwrite</code> <code>bool</code> <p>Whether to overwrite files in the lake. Defaults to False.</p> <code>False</code> Source code in <code>src/viadot/orchestration/prefect/flows/sharepoint_to_adls.py</code> <pre><code>@flow(\n    name=\"extract--sharepoint--adls\",\n    description=\"Extract data from Exchange Rates API and load it into Azure Data Lake.\",\n    retries=1,\n    retry_delay_seconds=60,\n)\ndef sharepoint_to_adls(\n    sharepoint_url: str,\n    adls_path: str,\n    sharepoint_credentials_secret: str | None = None,\n    sharepoint_config_key: str | None = None,\n    adls_credentials_secret: str | None = None,\n    adls_config_key: str | None = None,\n    sheet_name: str | list[str | int] | int | None = None,\n    columns: str | list[str] | list[int] | None = None,\n    overwrite: bool = False,\n) -&gt; None:\n    \"\"\"Download a file from Sharepoint and upload it to Azure Data Lake.\n\n    Args:\n        sharepoint_url (str): The URL to the file.\n        adls_path (str): The destination path.\n        sharepoint_credentials_secret (str, optional): The name of the Azure Key Vault\n            secret storing the Sharepoint credentials. Defaults to None.\n        sharepoint_config_key (str, optional): The key in the viadot config holding\n            relevant credentials. Defaults to None.\n        adls_credentials_secret (str, optional): The name of the Azure Key Vault secret\n            storing the ADLS credentials. Defaults to None.\n        adls_config_key (str, optional): The key in the viadot config holding relevant\n            credentials. Defaults to None.\n        sheet_name (str | list | int, optional): Strings are used\n            for sheet names. Integers are used in zero-indexed sheet positions\n            (chart sheets do not count as a sheet position). Lists of strings/integers\n            are used to request multiple sheets. Specify None to get all worksheets.\n            Defaults to None.\n        columns (str | list[str] | list[int], optional): Which columns to ingest.\n            Defaults to None.\n        overwrite (bool, optional): Whether to overwrite files in the lake. Defaults\n            to False.\n    \"\"\"\n    df = sharepoint_to_df(\n        url=sharepoint_url,\n        credentials_secret=sharepoint_credentials_secret,\n        config_key=sharepoint_config_key,\n        sheet_name=sheet_name,\n        columns=columns,\n    )\n    return df_to_adls(\n        df=df,\n        path=adls_path,\n        credentials_secret=adls_credentials_secret,\n        config_key=adls_config_key,\n        overwrite=overwrite,\n    )\n</code></pre>"},{"location":"references/orchestration/prefect/flows/#viadot.orchestration.prefect.flows.sharepoint_to_databricks","title":"<code>viadot.orchestration.prefect.flows.sharepoint_to_databricks</code>","text":"<p>Flows for pulling data from/into Sharepoint.</p>"},{"location":"references/orchestration/prefect/flows/#viadot.orchestration.prefect.flows.sharepoint_to_databricks.sharepoint_to_databricks","title":"<code>sharepoint_to_databricks(sharepoint_url, databricks_table, databricks_schema=None, if_exists='fail', sheet_name=None, columns=None, sharepoint_credentials_secret=None, sharepoint_config_key=None, databricks_credentials_secret=None, databricks_config_key=None)</code>","text":"<p>Download a file from Sharepoint and upload it to Azure Data Lake.</p> <p>Parameters:</p> Name Type Description Default <code>sharepoint_url</code> <code>str</code> <p>The URL to the file.</p> required <code>databricks_table</code> <code>str</code> <p>The name of the target table.</p> required <code>databricks_schema</code> <code>str</code> <p>The name of the target schema.</p> <code>None</code> <code>if_exists</code> <code>(str, Optional)</code> <p>What to do if the table already exists. One of 'replace', 'skip', and 'fail'.</p> <code>'fail'</code> <code>columns</code> <code>str | list[str] | list[int]</code> <p>Which columns to ingest. Defaults to None.</p> <code>None</code> <code>sheet_name</code> <code>str | list | int</code> <p>Strings are used for sheet names. Integers are used in zero-indexed sheet positions (chart sheets do not count as a sheet position). Lists of strings/integers are used to request multiple sheets. Specify None to get all worksheets. Defaults to None.</p> <code>None</code> <code>sharepoint_credentials_secret</code> <code>str</code> <p>The name of the Azure Key Vault secret storing relevant credentials. Defaults to None.</p> <code>None</code> <code>sharepoint_config_key</code> <code>str</code> <p>The key in the viadot config holding relevant credentials. Defaults to None.</p> <code>None</code> <code>databricks_credentials_secret</code> <code>str</code> <p>The name of the Azure Key Vault secret storing relevant credentials. Defaults to None.</p> <code>None</code> <code>databricks_config_key</code> <code>str</code> <p>The key in the viadot config holding relevant credentials. Defaults to None.</p> <code>None</code> Source code in <code>src/viadot/orchestration/prefect/flows/sharepoint_to_databricks.py</code> <pre><code>@flow(\n    name=\"extract--sharepoint--databricks\",\n    description=\"Extract data from Sharepoint and load it into Databricks.\",\n    retries=1,\n    retry_delay_seconds=60,\n)\ndef sharepoint_to_databricks(\n    sharepoint_url: str,\n    databricks_table: str,\n    databricks_schema: str | None = None,\n    if_exists: Literal[\"replace\", \"skip\", \"fail\"] = \"fail\",\n    sheet_name: str | list | int | None = None,\n    columns: str | list[str] | list[int] | None = None,\n    sharepoint_credentials_secret: str | None = None,\n    sharepoint_config_key: str | None = None,\n    databricks_credentials_secret: str | None = None,\n    databricks_config_key: str | None = None,\n) -&gt; None:\n    \"\"\"Download a file from Sharepoint and upload it to Azure Data Lake.\n\n    Args:\n        sharepoint_url (str): The URL to the file.\n        databricks_table (str): The name of the target table.\n        databricks_schema (str, optional): The name of the target schema.\n        if_exists (str, Optional): What to do if the table already exists.\n            One of 'replace', 'skip', and 'fail'.\n        columns (str | list[str] | list[int], optional): Which columns to ingest.\n            Defaults to None.\n        sheet_name (str | list | int, optional): Strings are used for sheet names.\n            Integers are used in zero-indexed sheet positions\n            (chart sheets do not count as a sheet position). Lists of strings/integers\n            are used to request multiple sheets. Specify None to get all worksheets.\n            Defaults to None.\n        sharepoint_credentials_secret (str, optional): The name of the Azure Key Vault\n            secret storing relevant credentials. Defaults to None.\n        sharepoint_config_key (str, optional): The key in the viadot config holding\n            relevant credentials. Defaults to None.\n        databricks_credentials_secret (str, optional): The name of the Azure Key Vault\n            secret storing relevant credentials. Defaults to None.\n        databricks_config_key (str, optional): The key in the viadot config holding\n            relevant credentials. Defaults to None.\n    \"\"\"\n    # Workaround Prefect converting this parameter to string due to multiple\n    # supported input types -- pandas relies on the data type to choose relevant\n    # implementation.\n    if sheet_name is not None:\n        with contextlib.suppress(ValueError):\n            sheet_name = int(sheet_name)\n\n    df = sharepoint_to_df(\n        url=sharepoint_url,\n        credentials_secret=sharepoint_credentials_secret,\n        config_key=sharepoint_config_key,\n        sheet_name=sheet_name,\n        columns=columns,\n    )\n    return df_to_databricks(\n        df=df,\n        schema=databricks_schema,\n        table=databricks_table,\n        if_exists=if_exists,\n        credentials_secret=databricks_credentials_secret,\n        config_key=databricks_config_key,\n    )\n</code></pre>"},{"location":"references/orchestration/prefect/flows/#viadot.orchestration.prefect.flows.sharepoint_to_redshift_spectrum","title":"<code>viadot.orchestration.prefect.flows.sharepoint_to_redshift_spectrum</code>","text":"<p>Flows for downloading data from Sharepoint and uploading it to AWS Redshift Spectrum.</p>"},{"location":"references/orchestration/prefect/flows/#viadot.orchestration.prefect.flows.sharepoint_to_redshift_spectrum.sharepoint_to_redshift_spectrum","title":"<code>sharepoint_to_redshift_spectrum(sharepoint_url, to_path, schema_name, table, tests=None, extension='.parquet', if_exists='overwrite', partition_cols=None, index=False, compression=None, sep=',', description=None, aws_config_key=None, credentials_secret=None, sheet_name=None, columns=None, na_values=None, sharepoint_credentials_secret=None, sharepoint_config_key=None, file_sheet_mapping=None)</code>","text":"<p>Extract data from SharePoint and load it into AWS Redshift Spectrum.</p> <p>This function downloads data either from SharePoint file or the whole directory and uploads it to AWS Redshift Spectrum.</p> <p>Modes: If the <code>URL</code> ends with the file (e.g ../file.xlsx) it downloads only the file and creates a table from it. If the <code>URL</code> ends with the folder (e.g ../folder_name/): it downloads multiple files and creates a table from them:     - If <code>file_sheet_mapping</code> is provided, it downloads and processes only         the specified files and sheets.     - If <code>file_sheet_mapping</code> is NOT provided, it downloads and processes all of         the files from the chosen folder.</p> <p>Parameters:</p> Name Type Description Default <code>sharepoint_url</code> <code>str</code> <p>The URL to the file.</p> required <code>to_path</code> <code>str</code> <p>Path to a S3 folder where the table will be located. Defaults to None.</p> required <code>schema_name</code> <code>str</code> <p>AWS Glue catalog database name.</p> required <code>table</code> <code>str</code> <p>AWS Glue catalog table name.</p> required <code>tests</code> <code>dict[str]</code> <p>A dictionary with optional list of tests to verify the output dataframe. If defined, triggers the <code>validate</code> function from viadot.utils. Defaults to None.</p> <code>None</code> <code>partition_cols</code> <code>list[str]</code> <p>List of column names that will be used to create partitions. Only takes effect if dataset=True.</p> <code>None</code> <code>extension</code> <code>str</code> <p>Required file type. Accepted file formats are 'csv' and 'parquet'.</p> <code>'.parquet'</code> <code>if_exists</code> <code>str</code> <p>'overwrite' to recreate any possible existing table or 'append' to keep any possible existing table. Defaults to overwrite.</p> <code>'overwrite'</code> <code>partition_cols</code> <code>list[str]</code> <p>List of column names that will be used to create partitions. Only takes effect if dataset=True. Defaults to None.</p> <code>None</code> <code>index</code> <code>bool</code> <p>Write row names (index). Defaults to False.</p> <code>False</code> <code>compression</code> <code>str</code> <p>Compression style (None, snappy, gzip, zstd).</p> <code>None</code> <code>sep</code> <code>str</code> <p>Field delimiter for the output file. Defaults to ','.</p> <code>','</code> <code>description</code> <code>str</code> <p>AWS Glue catalog table description. Defaults to None.</p> <code>None</code> <code>aws_config_key</code> <code>str</code> <p>The key in the viadot config holding relevant credentials. Defaults to None.</p> <code>None</code> <code>credentials_secret</code> <code>str</code> <p>The name of a secret block in Prefect that stores AWS credentials. Defaults to None.</p> <code>None</code> <code>sheet_name</code> <code>str | list | int</code> <p>Strings are used for sheet names. Integers are used in zero-indexed sheet positions (chart sheets do not count as a sheet position). Lists of strings/integers are used to request multiple sheets. Specify None to get all worksheets. Defaults to None.</p> <code>None</code> <code>columns</code> <code>str | list[str] | list[int]</code> <p>Which columns to ingest. Defaults to None.</p> <code>None</code> <code>na_values</code> <code>list[str] | None</code> <p>Additional strings to recognize as NA/NaN. If list passed, the specific NA values for each column will be recognized. Defaults to None.</p> <code>None</code> <code>sharepoint_credentials_secret</code> <code>str</code> <p>The name of the secret storing Sharepoint credentials. Defaults to None. More info on: https://docs.prefect.io/concepts/blocks/</p> <code>None</code> <code>sharepoint_config_key</code> <code>str</code> <p>The key in the viadot config holding relevant credentials. Defaults to None.</p> <code>None</code> <code>file_sheet_mapping</code> <code>dict</code> <p>A dictionary where keys are filenames and values are the sheet names to be loaded from each file. If provided, only these files and sheets will be downloaded. Defaults to None.</p> <code>None</code> Source code in <code>src/viadot/orchestration/prefect/flows/sharepoint_to_redshift_spectrum.py</code> <pre><code>@flow(\n    name=\"extract--sharepoint--redshift_spectrum\",\n    description=\"Extract data from Sharepoint and load it into AWS Redshift Spectrum.\",\n    retries=1,\n    retry_delay_seconds=60,\n)\ndef sharepoint_to_redshift_spectrum(  # noqa: PLR0913\n    sharepoint_url: str,\n    to_path: str,\n    schema_name: str,\n    table: str,\n    tests: dict[str, Any] | None = None,\n    extension: str = \".parquet\",\n    if_exists: Literal[\"overwrite\", \"append\"] = \"overwrite\",\n    partition_cols: list[str] | None = None,\n    index: bool = False,\n    compression: str | None = None,\n    sep: str = \",\",\n    description: str | None = None,\n    aws_config_key: str | None = None,\n    credentials_secret: str | None = None,\n    sheet_name: str | list[str | int] | int | None = None,\n    columns: str | list[str] | list[int] | None = None,\n    na_values: list[str] | None = None,\n    sharepoint_credentials_secret: str | None = None,\n    sharepoint_config_key: str | None = None,\n    file_sheet_mapping: dict | None = None,\n) -&gt; None:\n    \"\"\"Extract data from SharePoint and load it into AWS Redshift Spectrum.\n\n    This function downloads data either from SharePoint file or the whole directory and\n    uploads it to AWS Redshift Spectrum.\n\n    Modes:\n    If the `URL` ends with the file (e.g ../file.xlsx) it downloads only the file and\n    creates a table from it.\n    If the `URL` ends with the folder (e.g ../folder_name/): it downloads multiple files\n    and creates a table from them:\n        - If `file_sheet_mapping` is provided, it downloads and processes only\n            the specified files and sheets.\n        - If `file_sheet_mapping` is NOT provided, it downloads and processes all of\n            the files from the chosen folder.\n\n\n    Args:\n        sharepoint_url (str): The URL to the file.\n        to_path (str): Path to a S3 folder where the table will be located. Defaults to\n            None.\n        schema_name (str): AWS Glue catalog database name.\n        table (str): AWS Glue catalog table name.\n        tests (dict[str], optional): A dictionary with optional list of tests\n            to verify the output dataframe. If defined, triggers the `validate`\n            function from viadot.utils. Defaults to None.\n        partition_cols (list[str]): List of column names that will be used to create\n            partitions. Only takes effect if dataset=True.\n        extension (str): Required file type. Accepted file formats are 'csv' and\n            'parquet'.\n        if_exists (str, optional): 'overwrite' to recreate any possible existing table\n            or 'append' to keep any possible existing table. Defaults to overwrite.\n        partition_cols (list[str], optional): List of column names that will be used to\n            create partitions. Only takes effect if dataset=True. Defaults to None.\n        index (bool, optional): Write row names (index). Defaults to False.\n        compression (str, optional): Compression style (None, snappy, gzip, zstd).\n        sep (str, optional): Field delimiter for the output file. Defaults to ','.\n        description (str, optional): AWS Glue catalog table description. Defaults to\n            None.\n        aws_config_key (str, optional): The key in the viadot config holding relevant\n            credentials. Defaults to None.\n        credentials_secret (str, optional): The name of a secret block in Prefect\n            that stores AWS credentials. Defaults to None.\n        sheet_name (str | list | int, optional): Strings are used for sheet names.\n            Integers are used in zero-indexed sheet positions (chart sheets do not count\n            as a sheet position). Lists of strings/integers are used to request multiple\n            sheets. Specify None to get all worksheets. Defaults to None.\n        columns (str | list[str] | list[int], optional): Which columns to ingest.\n            Defaults to None.\n        na_values (list[str] | None): Additional strings to recognize as NA/NaN.\n            If list passed, the specific NA values for each column will be recognized.\n            Defaults to None.\n        sharepoint_credentials_secret (str, optional): The name of the secret storing\n            Sharepoint credentials. Defaults to None.\n            More info on: https://docs.prefect.io/concepts/blocks/\n        sharepoint_config_key (str, optional): The key in the viadot config holding\n            relevant credentials. Defaults to None.\n        file_sheet_mapping (dict): A dictionary where keys are filenames and values are\n            the sheet names to be loaded from each file. If provided, only these files\n            and sheets will be downloaded. Defaults to None.\n    \"\"\"\n    df = sharepoint_to_df(\n        url=sharepoint_url,\n        sheet_name=sheet_name,\n        tests=tests,\n        columns=columns,\n        na_values=na_values,\n        file_sheet_mapping=file_sheet_mapping,\n        credentials_secret=sharepoint_credentials_secret,\n        config_key=sharepoint_config_key,\n    )\n    df_to_redshift_spectrum(\n        df=df,\n        to_path=to_path,\n        schema_name=schema_name,\n        table=table,\n        extension=extension,\n        if_exists=if_exists,\n        partition_cols=partition_cols,\n        index=index,\n        compression=compression,\n        sep=sep,\n        description=description,\n        config_key=aws_config_key,\n        credentials_secret=credentials_secret,\n    )\n</code></pre>"},{"location":"references/orchestration/prefect/flows/#viadot.orchestration.prefect.flows.sharepoint_to_s3","title":"<code>viadot.orchestration.prefect.flows.sharepoint_to_s3</code>","text":"<p>Flows for downloading data from Sharepoint and uploading it to Amazon S3.</p>"},{"location":"references/orchestration/prefect/flows/#viadot.orchestration.prefect.flows.sharepoint_to_s3.sharepoint_to_s3","title":"<code>sharepoint_to_s3(url, local_path, to_path, sharepoint_credentials_secret=None, sharepoint_config_key=None, aws_config_key=None)</code>","text":"<p>Download a file from Sharepoint and upload it to S3.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL of the file to be downloaded.</p> required <code>local_path</code> <code>str</code> <p>Local file directory. Defaults to None.</p> required <code>to_path</code> <code>str</code> <p>Where to download the file.</p> required <code>sharepoint_credentials_secret</code> <code>str</code> <p>The name of the secret that stores Sharepoint credentials. Defaults to None. More info on: https://docs.prefect.io/concepts/blocks/</p> <code>None</code> <code>sharepoint_config_key</code> <code>str</code> <p>The key in the viadot config holding relevant credentials.</p> <code>None</code> <code>aws_config_key</code> <code>str</code> <p>The key in the viadot config holding relevant credentials. Defaults to None.</p> <code>None</code> Source code in <code>src/viadot/orchestration/prefect/flows/sharepoint_to_s3.py</code> <pre><code>@flow(\n    name=\"extract--sharepoint--s3\",\n    description=\"Flows for downloading data from Sharepoint and uploading it to Amazon S3.\",\n    retries=1,\n    retry_delay_seconds=60,\n)\ndef sharepoint_to_s3(\n    url: str,\n    local_path: str,\n    to_path: str,\n    sharepoint_credentials_secret: str | None = None,\n    sharepoint_config_key: str | None = None,\n    aws_config_key: str | None = None,\n) -&gt; None:\n    \"\"\"Download a file from Sharepoint and upload it to S3.\n\n    Args:\n        url (str): The URL of the file to be downloaded.\n        local_path (str): Local file directory. Defaults to None.\n        to_path (str): Where to download the file.\n        sharepoint_credentials_secret (str, optional): The name of the secret that\n            stores Sharepoint credentials. Defaults to None. More info on:\n            https://docs.prefect.io/concepts/blocks/\n        sharepoint_config_key (str, optional): The key in the viadot config holding\n            relevant credentials.\n        aws_config_key (str, optional): The key in the viadot config holding relevant\n            credentials. Defaults to None.\n    \"\"\"\n    sharepoint_download_file(\n        url=url,\n        to_path=local_path,\n        credentials_secret=sharepoint_credentials_secret,\n        config_key=sharepoint_config_key,\n    )\n\n    s3_upload_file(\n        from_path=local_path,\n        to_path=to_path,\n        config_key=aws_config_key,\n    )\n</code></pre>"},{"location":"references/orchestration/prefect/flows/#viadot.orchestration.prefect.flows.sql_server_to_minio","title":"<code>viadot.orchestration.prefect.flows.sql_server_to_minio</code>","text":"<p>Flows for downloading data from SQLServer and uploading it to MinIO.</p>"},{"location":"references/orchestration/prefect/flows/#viadot.orchestration.prefect.flows.sql_server_to_minio.sql_server_to_minio","title":"<code>sql_server_to_minio(query, path, if_exists='error', basename_template=None, sql_server_credentials_secret=None, sql_server_config_key=None, minio_credentials_secret=None, minio_config_key=None)</code>","text":"<p>Download a file from SQLServer and upload it to MinIO.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>(str, required)</code> <p>The query to execute on the SQL Server database. If the query doesn't start with \"SELECT\" returns an empty DataFrame.</p> required <code>path</code> <code>str</code> <p>Path to the MinIO file/folder.</p> required <code>basename_template</code> <code>str</code> <p>A template string used to generate base names of written data files. The token '{i}' will be replaced with an automatically incremented integer. Defaults to None.</p> <code>None</code> <code>sql_server_credentials_secret</code> <code>str</code> <p>The name of the secret storing the credentials to the SQLServer. Defaults to None. More info on: https://docs.prefect.io/concepts/blocks/</p> <code>None</code> <code>sql_server_config_key</code> <code>str</code> <p>The key in the viadot config holding relevant credentials to the SQLServer. Defaults to None.</p> <code>None</code> <code>minio_credentials_secret</code> <code>str</code> <p>The name of the secret storing the credentials to the MinIO. Defaults to None. More info on: https://docs.prefect.io/concepts/blocks/</p> <code>None</code> <code>minio_config_key</code> <code>str</code> <p>The key in the viadot config holding relevant credentials to the MinIO. Defaults to None.</p> <code>None</code> Source code in <code>src/viadot/orchestration/prefect/flows/sql_server_to_minio.py</code> <pre><code>@flow(\n    name=\"extract--sql_server--minio\",\n    description=\"Extract data from SQLServer and load it into MinIO.\",\n    retries=1,\n    retry_delay_seconds=60,\n)\ndef sql_server_to_minio(\n    query: str,\n    path: str,\n    if_exists: Literal[\"error\", \"delete_matching\", \"overwrite_or_ignore\"] = \"error\",\n    basename_template: str | None = None,\n    sql_server_credentials_secret: str | None = None,\n    sql_server_config_key: str | None = None,\n    minio_credentials_secret: str | None = None,\n    minio_config_key: str | None = None,\n) -&gt; None:\n    \"\"\"Download a file from SQLServer and upload it to MinIO.\n\n    Args:\n        query (str, required): The query to execute on the SQL Server database.\n            If the query doesn't start with \"SELECT\" returns an empty DataFrame.\n        path (str): Path to the MinIO file/folder.\n        basename_template (str, optional): A template string used to generate\n            base names of written data files. The token '{i}' will be replaced with\n            an automatically incremented integer. Defaults to None.\n        if_exists (Literal[\"error\", \"delete_matching\", \"overwrite_or_ignore\"],\n            optional). What to do if the dataset already exists. Defaults to \"error\".\n        sql_server_credentials_secret (str, optional): The name of the secret storing\n            the credentials to the SQLServer. Defaults to None.\n            More info on: https://docs.prefect.io/concepts/blocks/\n        sql_server_config_key (str, optional): The key in the viadot config holding\n            relevant credentials to the SQLServer. Defaults to None.\n        minio_credentials_secret (str, optional): The name of the secret storing\n            the credentials to the MinIO. Defaults to None.\n            More info on: https://docs.prefect.io/concepts/blocks/\n        minio_config_key (str, optional): The key in the viadot config holding relevant\n            credentials to the MinIO. Defaults to None.\n    \"\"\"\n    df = sql_server_to_df(\n        query=query,\n        config_key=sql_server_config_key,\n        credentials_secret=sql_server_credentials_secret,\n    )\n\n    return df_to_minio(\n        df=df,\n        path=path,\n        if_exists=if_exists,\n        basename_template=basename_template,\n        config_key=minio_config_key,\n        credentials_secret=minio_credentials_secret,\n    )\n</code></pre>"},{"location":"references/orchestration/prefect/flows/#viadot.orchestration.prefect.flows.sql_server_to_parquet","title":"<code>viadot.orchestration.prefect.flows.sql_server_to_parquet</code>","text":"<p>Flow for downloading data from SQLServer and saveing it to a Parquet file.</p>"},{"location":"references/orchestration/prefect/flows/#viadot.orchestration.prefect.flows.sql_server_to_parquet.sql_server_to_parquet","title":"<code>sql_server_to_parquet(query, path, if_exists='replace', sql_server_credentials_secret=None, sql_server_config_key=None)</code>","text":"<p>Download a file from SQLServer and save it to a Parquet file.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>(str, required)</code> <p>The query to execute on the SQL Server database. If the qery doesn't start with \"SELECT\" returns an empty DataFrame.</p> required <code>path</code> <code>str</code> <p>Path where to save a Parquet file which will be created while executing flow.</p> required <code>if_exists</code> <code>Literal</code> <p>What to do if Parquet exists. Defaults to \"replace\".</p> <code>'replace'</code> <code>sql_server_credentials_secret</code> <code>str</code> <p>The name of the secret storing the credentialsto the SQLServer. Defaults to None. More info on: https://docs.prefect.io/concepts/blocks/</p> <code>None</code> <code>sql_server_config_key</code> <code>str</code> <p>The key in the viadot config holding relevant credentials to the SQLServer. Defaults to None.</p> <code>None</code> Source code in <code>src/viadot/orchestration/prefect/flows/sql_server_to_parquet.py</code> <pre><code>@flow(\n    name=\"extract--sql_server--parquet\",\n    description=\"Extract data from SQLServer and save it to a Parquet file.\",\n    retries=1,\n    retry_delay_seconds=60,\n)\ndef sql_server_to_parquet(\n    query: str,\n    path: str,\n    if_exists: Literal[\"append\", \"replace\", \"skip\"] = \"replace\",\n    sql_server_credentials_secret: str | None = None,\n    sql_server_config_key: str | None = None,\n) -&gt; None:\n    \"\"\"Download a file from SQLServer and save it to a Parquet file.\n\n    Args:\n        query (str, required): The query to execute on the SQL Server database.\n            If the qery doesn't start with \"SELECT\" returns an empty DataFrame.\n        path (str): Path where to save a Parquet file which will be created while\n            executing flow.\n        if_exists (Literal, optional): What to do if Parquet exists.\n            Defaults to \"replace\".\n        sql_server_credentials_secret (str, optional): The name of the secret storing\n            the credentialsto the SQLServer. Defaults to None.\n            More info on: https://docs.prefect.io/concepts/blocks/\n        sql_server_config_key (str, optional): The key in the viadot config\n            holding relevant credentials to the SQLServer. Defaults to None.\n    \"\"\"\n    df = sql_server_to_df(\n        query=query,\n        config_key=sql_server_config_key,\n        credentials_secret=sql_server_credentials_secret,\n    )\n\n    return df_to_parquet(\n        df=df,\n        path=path,\n        if_exists=if_exists,\n    )\n</code></pre>"},{"location":"references/orchestration/prefect/flows/#viadot.orchestration.prefect.flows.sql_server_transform","title":"<code>viadot.orchestration.prefect.flows.sql_server_transform</code>","text":"<p>Flow for transforming data inside the SQLServer.</p>"},{"location":"references/orchestration/prefect/flows/#viadot.orchestration.prefect.flows.sql_server_transform.sql_server_transform","title":"<code>sql_server_transform(query, sql_server_credentials_secret=None, sql_server_config_key=None)</code>","text":"<p>Run query inside the SQLServer.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>(str, required)</code> <p>The query to execute on the SQL Server database. If the qery doesn't start with \"SELECT\" returns an empty DataFrame.</p> required <code>sql_server_credentials_secret</code> <code>str</code> <p>The name of the secret storing the credentialsto the SQLServer. Defaults to None. More info on: https://docs.prefect.io/concepts/blocks/</p> <code>None</code> <code>sql_server_config_key</code> <code>str</code> <p>The key in the viadot config holding relevant credentials to the SQLServer. Defaults to None.</p> <code>None</code> Source code in <code>src/viadot/orchestration/prefect/flows/sql_server_transform.py</code> <pre><code>@flow(\n    name=\"transform--sql_server\",\n    description=\"Transform data inside the SQLServer.\",\n    retries=1,\n    retry_delay_seconds=60,\n)\ndef sql_server_transform(\n    query: str,\n    sql_server_credentials_secret: str | None = None,\n    sql_server_config_key: str | None = None,\n) -&gt; None:\n    \"\"\"Run query inside the SQLServer.\n\n    Args:\n        query (str, required): The query to execute on the SQL Server database.\n            If the qery doesn't start with \"SELECT\" returns an empty DataFrame.\n        sql_server_credentials_secret (str, optional): The name of the secret storing\n            the credentialsto the SQLServer. Defaults to None.\n            More info on: https://docs.prefect.io/concepts/blocks/\n        sql_server_config_key (str, optional): The key in the viadot config\n            holding relevant credentials to the SQLServer. Defaults to None.\n    \"\"\"\n    sql_server_query(\n        query=query,\n        config_key=sql_server_config_key,\n        credentials_secret=sql_server_credentials_secret,\n    )\n</code></pre>"},{"location":"references/orchestration/prefect/flows/#viadot.orchestration.prefect.flows.supermetrics_to_adls","title":"<code>viadot.orchestration.prefect.flows.supermetrics_to_adls</code>","text":"<p>Flow for downloading the data from Superpetrics and uploading it to ADLS.</p>"},{"location":"references/orchestration/prefect/flows/#viadot.orchestration.prefect.flows.supermetrics_to_adls.supermetrics_to_adls","title":"<code>supermetrics_to_adls(query_params=None, adls_path=None, overwrite=False, supermetrics_credentials_secret=None, supermetrics_config_key=None, adls_credentials_secret=None, adls_config_key=None, **kwargs)</code>","text":"<p>Flow to extract data from the Supermetrics API and save it to ADLS.</p> <p>This function queries data from the Supermetrics API using the provided query parameters and saves the resulting DataFrame to Azure Data Lake Storage (ADLS) as a file.</p> <pre><code>query_params (dict[str, Any], optional):\n    A dictionary of query parameters for the Supermetrics API. These parameters\n    specify the data to retrieve from Supermetrics. If not provided, the default\n    parameters from the Supermetrics configuration will be used.\nadls_path (str, optional):\n    The destination path in ADLS where the DataFrame will be saved. This should\n    include the file name and extension (e.g., 'myfolder/myfile.csv'). If not\n    provided, the function will use a default path from the configuration\n    or raise an error.\noverwrite (bool, optional):\n    A flag indicating whether to overwrite the existing file in ADLS. If set\n    to Falseand the file exists, an error will be raised. Default is False.\nsupermetrics_credentials_secret (str, optional):\n    The name of the secret in the secret management system containing\n    the Supermetrics API credentials. If not provided, the function will use\n    credentials specified in the configuration.\nsupermetrics_config_key (str, optional):\n    The key in the viadot configuration holding relevant credentials.\n    Defaults to None.\nadls_credentials_secret (str, optional):\n    The name of the secret in the secret management system containing\n    the ADLS credentials. If not provided, the function will use credentials\n    specified in the configuration.\nadls_config_key (str, optional):\n    The key in the viadot configuration holding relevant credentials.\n    Defaults to None.\n**kwargs (dict[str, Any], optional):\n    Additional keyword arguments to pass to the `supermetrics_to_df` function\n    for further customization of the Supermetrics query.\n</code></pre> <pre><code>ValueError:\n    If `adls_path` is not provided and cannot be determined from\n    the configuration.\n</code></pre> Source code in <code>src/viadot/orchestration/prefect/flows/supermetrics_to_adls.py</code> <pre><code>@flow(\n    name=\"Supermetrics extraction to ADLS\",\n    description=\"Extract data from Supermetrics and load it into ADLS.\",\n    retries=1,\n    retry_delay_seconds=60,\n    task_runner=ConcurrentTaskRunner,\n)\ndef supermetrics_to_adls(\n    # supermetrics\n    query_params: dict[str, Any] | None = None,\n    # ADLS\n    adls_path: str | None = None,\n    overwrite: bool = False,\n    # Auth\n    supermetrics_credentials_secret: str | None = None,\n    supermetrics_config_key: str | None = None,\n    adls_credentials_secret: str | None = None,\n    adls_config_key: str | None = None,\n    **kwargs: dict[str, Any] | None,\n) -&gt; None:\n    \"\"\"Flow to extract data from the Supermetrics API and save it to ADLS.\n\n    This function queries data from the Supermetrics API using the provided query\n    parameters and saves the resulting DataFrame to Azure Data Lake Storage (ADLS)\n    as a file.\n\n    Args:\n    ----\n        query_params (dict[str, Any], optional):\n            A dictionary of query parameters for the Supermetrics API. These parameters\n            specify the data to retrieve from Supermetrics. If not provided, the default\n            parameters from the Supermetrics configuration will be used.\n        adls_path (str, optional):\n            The destination path in ADLS where the DataFrame will be saved. This should\n            include the file name and extension (e.g., 'myfolder/myfile.csv'). If not\n            provided, the function will use a default path from the configuration\n            or raise an error.\n        overwrite (bool, optional):\n            A flag indicating whether to overwrite the existing file in ADLS. If set\n            to Falseand the file exists, an error will be raised. Default is False.\n        supermetrics_credentials_secret (str, optional):\n            The name of the secret in the secret management system containing\n            the Supermetrics API credentials. If not provided, the function will use\n            credentials specified in the configuration.\n        supermetrics_config_key (str, optional):\n            The key in the viadot configuration holding relevant credentials.\n            Defaults to None.\n        adls_credentials_secret (str, optional):\n            The name of the secret in the secret management system containing\n            the ADLS credentials. If not provided, the function will use credentials\n            specified in the configuration.\n        adls_config_key (str, optional):\n            The key in the viadot configuration holding relevant credentials.\n            Defaults to None.\n        **kwargs (dict[str, Any], optional):\n            Additional keyword arguments to pass to the `supermetrics_to_df` function\n            for further customization of the Supermetrics query.\n\n    Raises:\n    ------\n        ValueError:\n            If `adls_path` is not provided and cannot be determined from\n            the configuration.\n\n    \"\"\"\n    df = supermetrics_to_df(\n        query_params=query_params,\n        credentials_secret=supermetrics_credentials_secret,\n        config_key=supermetrics_config_key,\n        **kwargs,\n    )\n\n    return df_to_adls(\n        df=df,\n        path=adls_path,\n        credentials_secret=adls_credentials_secret,\n        config_key=adls_config_key,\n        overwrite=overwrite,\n    )\n</code></pre>"},{"location":"references/orchestration/prefect/flows/#viadot.orchestration.prefect.flows.transform","title":"<code>viadot.orchestration.prefect.flows.transform</code>","text":"<p>Build specified dbt model(s).</p>"},{"location":"references/orchestration/prefect/flows/#viadot.orchestration.prefect.flows.transform.transform","title":"<code>transform(dbt_project_path, dbt_repo_url=None, dbt_repo_url_secret=None, dbt_repo_branch=None, token_secret=None, local_dbt_repo_path=None, dbt_selects=None, dbt_target=None)</code>","text":"<p>Build specified dbt model(s).</p> <p>This flow implements a simplified version of the <code>transform_and_catalog()</code> flow, excluding metadata handling, source freshness checks, and stateful operations.</p> <p>Parameters:</p> Name Type Description Default <code>dbt_project_path</code> <code>str</code> <p>The path to the dbt project (the directory containing the <code>dbt_project.yml</code> file).</p> required <code>dbt_repo_url</code> <code>str</code> <p>The URL for cloning the dbt repo with relevant dbt project.</p> <code>None</code> <code>dbt_repo_url_secret</code> <code>str</code> <p>Alternatively to above, the secret containing <code>dbt_repo_url</code>.</p> <code>None</code> <code>dbt_repo_branch</code> <code>str</code> <p>The branch of the dbt repo to use.</p> <code>None</code> <code>token_secret</code> <code>str</code> <p>The name of the secret storing the git token. Defaults to None.</p> <code>None</code> <code>local_dbt_repo_path</code> <code>str</code> <p>The path where to clone the repo to.</p> <code>None</code> <code>dbt_selects</code> <code>dict</code> <p>Valid dbt node selection expressions. Valid keys are <code>run</code>, <code>test</code>, and <code>source_freshness</code>. The test     select expression is taken from run's, as long as run select is     provided. Defaults to None.</p> <code>None</code> <code>dbt_target</code> <code>str</code> <p>The dbt target to use. If not specified, the default dbt target (as specified in <code>profiles.yaml</code>) will be used. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>list[str]: Lines from stdout of the <code>upload_metadata</code> task as a list.</p> <p>Examples:</p>"},{"location":"references/orchestration/prefect/flows/#viadot.orchestration.prefect.flows.transform.transform--build-a-single-model","title":"Build a single model","text":"<pre><code>import os\nfrom prefect_viadot.flows import transform_and_catalog\n\nmy_dbt_project_path = os.path.expanduser(\"~/dbt/my_dbt_project\")\n\ntransform_and_catalog(\n    dbt_project_path=my_dbt_project_path,\n    dbt_selects={\"run\": \"my_model\"}\n)\n</code></pre> <p>Some common <code>dbt_select</code> patterns: - build a model and all its downstream dependencies: <code>dbt_select=\"my_model+\"</code> - build all models in a directory: <code>dbt_select=\"models/my_project\"</code> ```</p> Source code in <code>src/viadot/orchestration/prefect/flows/transform.py</code> <pre><code>@flow(\n    name=\"Transform\",\n    description=\"Build specified dbt model(s).\",\n    timeout_seconds=2 * 60 * 60,\n)\ndef transform(\n    dbt_project_path: str,\n    dbt_repo_url: str | None = None,\n    dbt_repo_url_secret: str | None = None,\n    dbt_repo_branch: str | None = None,\n    token_secret: str | None = None,\n    local_dbt_repo_path: str | None = None,\n    dbt_selects: dict[str, str] | None = None,\n    dbt_target: str | None = None,\n) -&gt; None:\n    \"\"\"Build specified dbt model(s).\n\n    This flow implements a simplified version of the `transform_and_catalog()` flow,\n    excluding metadata handling, source freshness checks, and stateful operations.\n\n    Args:\n        dbt_project_path (str): The path to the dbt project (the directory containing\n            the `dbt_project.yml` file).\n        dbt_repo_url (str, optional): The URL for cloning the dbt repo with relevant\n            dbt project.\n        dbt_repo_url_secret (str, optional): Alternatively to above, the secret\n            containing `dbt_repo_url`.\n        dbt_repo_branch (str, optional): The branch of the dbt repo to use.\n        token_secret (str, optional): The name of the secret storing the git token.\n            Defaults to None.\n        local_dbt_repo_path (str, optional): The path where to clone the repo to.\n        dbt_selects (dict, optional): Valid\n            [dbt node selection](https://docs.getdbt.com/reference/node-selection/syntax)\n            expressions. Valid keys are `run`, `test`, and `source_freshness`. The test\n                select expression is taken from run's, as long as run select is\n                provided. Defaults to None.\n        dbt_target (str): The dbt target to use. If not specified, the default dbt\n            target (as specified in `profiles.yaml`) will be used. Defaults to None.\n\n    Returns:\n        list[str]: Lines from stdout of the `upload_metadata` task as a list.\n\n    Examples:\n        # Build a single model\n        ```python\n        import os\n        from prefect_viadot.flows import transform_and_catalog\n\n        my_dbt_project_path = os.path.expanduser(\"~/dbt/my_dbt_project\")\n\n        transform_and_catalog(\n            dbt_project_path=my_dbt_project_path,\n            dbt_selects={\"run\": \"my_model\"}\n        )\n        ```\n\n        Some common `dbt_select` patterns:\n        - build a model and all its downstream dependencies: `dbt_select=\"my_model+\"`\n        - build all models in a directory: `dbt_select=\"models/my_project\"`\n        ```\n    \"\"\"\n    dbt_repo_url = dbt_repo_url or get_credentials(dbt_repo_url_secret)\n    local_dbt_repo_path = (\n        os.path.expandvars(local_dbt_repo_path)\n        if local_dbt_repo_path is not None\n        else \"tmp_dbt_repo_dir\"\n    )\n\n    clone = clone_repo(\n        url=dbt_repo_url,\n        checkout_branch=dbt_repo_branch,\n        token_secret=token_secret,\n        path=local_dbt_repo_path,\n    )\n\n    # dbt CLI does not handle passing --target=None\n    dbt_target_option = f\"-t {dbt_target}\" if dbt_target is not None else \"\"\n\n    # Clean up artifacts from previous runs (`target/` dir and packages)\n    dbt_clean_task = dbt_task.with_options(name=\"dbt_task_clean\")\n    dbt_clean_up = dbt_clean_task(\n        project_path=dbt_project_path, command=\"clean\", wait_for=[clone]\n    )\n    dbt_pull_deps_task = dbt_task.with_options(name=\"dbt_task_deps\")\n    pull_dbt_deps = dbt_pull_deps_task(\n        project_path=dbt_project_path,\n        command=\"deps\",\n        wait_for=[dbt_clean_up],\n    )\n\n    run_select = dbt_selects.get(\"run\")\n    run_select_safe = f\"-s {run_select}\" if run_select is not None else \"\"\n    run = dbt_task(\n        project_path=dbt_project_path,\n        command=f\"run {run_select_safe} {dbt_target_option}\",\n        wait_for=[pull_dbt_deps],\n    )\n\n    test_select = dbt_selects.get(\"test\", run_select)\n    test_select_safe = f\"-s {test_select}\" if test_select is not None else \"\"\n    test = dbt_task(\n        project_path=dbt_project_path,\n        command=f\"test {test_select_safe} {dbt_target_option}\",\n        wait_for=[run],\n    )\n\n    _cleanup_repo(\n        local_dbt_repo_path,\n        wait_for=[test],\n    )\n</code></pre>"},{"location":"references/orchestration/prefect/flows/#viadot.orchestration.prefect.flows.transform_and_catalog","title":"<code>viadot.orchestration.prefect.flows.transform_and_catalog</code>","text":"<p>Build specified dbt model(s) and upload the generated metadata to Luma.</p>"},{"location":"references/orchestration/prefect/flows/#viadot.orchestration.prefect.flows.transform_and_catalog.remove_dbt_repo_dir","title":"<code>remove_dbt_repo_dir(dbt_repo_dir_name)</code>","text":"<p>Remove the repo directory.</p> <p>Parameters:</p> Name Type Description Default <code>dbt_repo_dir_name</code> <code>str</code> <p>The name of the dbt repo directory.</p> required Source code in <code>src/viadot/orchestration/prefect/flows/transform_and_catalog.py</code> <pre><code>@task\ndef remove_dbt_repo_dir(dbt_repo_dir_name: str) -&gt; None:\n    \"\"\"Remove the repo directory.\n\n    Args:\n        dbt_repo_dir_name (str): The name of the dbt repo directory.\n    \"\"\"\n    shutil.rmtree(dbt_repo_dir_name, ignore_errors=True)\n</code></pre>"},{"location":"references/orchestration/prefect/flows/#viadot.orchestration.prefect.flows.transform_and_catalog.transform_and_catalog","title":"<code>transform_and_catalog(dbt_repo_url=None, dbt_repo_url_secret=None, dbt_project_path='dbt', dbt_repo_branch=None, dbt_repo_token_secret=None, dbt_selects=None, dbt_target=None, dbt_target_dir_path=None, luma_url='http://localhost:8000', luma_follow=False, metadata_kind='model_run', run_results_storage_path=None, run_results_storage_config_key=None, run_results_storage_credentials_secret=None)</code>","text":"<p>Build specified dbt model(s) and upload the generated metadata to Luma.</p> <p>Supports ingesting both model and model run metadata (controlled by the <code>metadata_kind</code> parameter).</p> <p>Note that metadata is still ingested even if the preceding <code>dbt test</code> task fails. This is done in order to capture test failure metadata in the data catalog.</p> <p>Parameters:</p> Name Type Description Default <code>dbt_repo_url</code> <code>str</code> <p>The URL for cloning the dbt repo with relevant dbt project. Defaults to None.</p> <code>None</code> <code>dbt_repo_url_secret</code> <code>str</code> <p>Alternatively to above, the secret containing <code>dbt_repo_url</code>. Defaults to None.</p> <code>None</code> <code>dbt_project_path</code> <code>str</code> <p>Path to the dbt project directory, relative to the dbt repository's root. For example, \"dbt/my_dbt_project\". Defaults to \"dbt\".</p> <code>'dbt'</code> <code>dbt_repo_branch</code> <code>str</code> <p>The branch of the dbt repo to use. Defaults to None (default repo branch).</p> <code>None</code> <code>dbt_repo_token_secret</code> <code>str</code> <p>The secret containing the personal access token used to clone the dbt repository, in case it's private. Not required if token is already included in <code>dbt_repo_url</code> (which is NOT recommended). Defaults to None.</p> <code>None</code> <code>dbt_selects</code> <code>dict</code> <p>Valid dbt node selection expressions. Valid keys are <code>run</code>, <code>test</code>,<code>build</code>, and <code>source_freshness</code>.     The test select expression is taken from run's, as long as run select is     provided. Defaults to None.</p> <code>None</code> <code>dbt_target</code> <code>str</code> <p>The dbt target to use. If not specified, the default dbt target (as specified in <code>profiles.yaml</code>) will be used. Defaults to None.</p> <code>None</code> <code>dbt_target_dir_path</code> <code>str</code> <p>The path to your dbt project's target directory, which contains dbt artifact JSON files, relative to dbt project's root directory. By default, <code>&lt;repo_name&gt;/&lt;dbt_project_path&gt;/target</code>, since \"target\" is the default name of the directory generated by dbt.</p> <code>None</code> <code>luma_url</code> <code>str</code> <p>The URL of the Luma instance to ingest into. Defaults to \"http://localhost:8000\".</p> <code>'http://localhost:8000'</code> <code>luma_follow</code> <code>bool</code> <p>Whether to follow the ingestion process until it's completed (by default, ingestion request is sent without awaiting for the response). By default, <code>False</code>.</p> <code>False</code> <code>metadata_kind</code> <code>Literal['model', 'model_run']</code> <p>The kind of metadata to ingest. Defaults to \"model_run\".</p> <code>'model_run'</code> <code>run_results_storage_path</code> <code>str</code> <p>The directory to upload the <code>run_results.json</code> file to. Note that a timestamp will be appended to the end of the file. Currently, only S3 is supported. Defaults to None.</p> <code>None</code> <code>run_results_storage_config_key</code> <code>str</code> <p>The key in the viadot config holding AWS credentials. Defaults to None.</p> <code>None</code> <code>run_results_storage_credentials_secret</code> <code>str</code> <p>The name of the secret block in Prefect holding AWS credentials. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: Lines from stdout of the <code>upload_metadata</code> task as a list.</p> <p>Examples:</p>"},{"location":"references/orchestration/prefect/flows/#viadot.orchestration.prefect.flows.transform_and_catalog.transform_and_catalog--build-staging-models","title":"Build staging models.","text":"<pre><code>import os\nfrom prefect_viadot.flows import transform_and_catalog\n\nmy_dbt_repo_url = \"https://github.com/dbt-labs/jaffle_shop\"\nmy_luma_url = \"http://localhost:8000\"\n\ntransform_and_catalog(\n    dbt_repo_url=my_dbt_repo_url\n    dbt_selects={\"run\": \"staging\"}\n    luma_url=my_luma_url,\n    run_results_storage_path=\"s3://my-bucket/dbt/run_results\",\n    run_results_storage_credentials_secret=\"my-aws-credentials-block\",\n)\n</code></pre> <p>Some common <code>dbt_selects</code> patterns: - runs a specific model and all its downstream dependencies:     <code>dbt_select={\"run\": \"my_model+\"}</code> - runs all models in a directory:     <code>dbt_select={\"run: \"models/staging\"}</code> - runs a specific model in a folder:     <code>dbt_select={\"run\": \"marts.domain.some_model\"}</code> - runs tests for a specific model:     <code>dbt_select={\"test\": \"my_model\"}</code> - build a specific model:     <code>dbt_select={\"build\": \"my_model\"}</code> - build all models in a folder:     <code>dbt_select={\"build\": \"models.intermediate\"}</code></p> Source code in <code>src/viadot/orchestration/prefect/flows/transform_and_catalog.py</code> <pre><code>@flow(\n    name=\"Transform and Catalog\",\n    description=\"Build specified dbt model(s) and upload generated metadata to Luma.\",\n    timeout_seconds=2 * 60 * 60,\n)\ndef transform_and_catalog(  # noqa: PLR0913\n    dbt_repo_url: str | None = None,\n    dbt_repo_url_secret: str | None = None,\n    dbt_project_path: str = \"dbt\",\n    dbt_repo_branch: str | None = None,\n    dbt_repo_token_secret: str | None = None,\n    dbt_selects: dict[str, str] | None = None,\n    dbt_target: str | None = None,\n    dbt_target_dir_path: str | None = None,\n    luma_url: str = \"http://localhost:8000\",\n    luma_follow: bool = False,\n    metadata_kind: Literal[\"model\", \"model_run\"] = \"model_run\",\n    run_results_storage_path: str | None = None,\n    run_results_storage_config_key: str | None = None,\n    run_results_storage_credentials_secret: str | None = None,\n) -&gt; list[str]:\n    \"\"\"Build specified dbt model(s) and upload the generated metadata to Luma.\n\n    Supports ingesting both model and model run metadata (controlled by the\n    `metadata_kind` parameter).\n\n    Note that metadata is still ingested even if the preceding `dbt test` task fails.\n    This is done in order to capture test failure metadata in the data catalog.\n\n    Args:\n        dbt_repo_url (str, optional): The URL for cloning the dbt repo with relevant\n            dbt project. Defaults to None.\n        dbt_repo_url_secret (str, optional): Alternatively to above, the secret\n            containing `dbt_repo_url`. Defaults to None.\n        dbt_project_path (str): Path to the dbt project directory, relative to the\n            dbt repository's root. For example, \"dbt/my_dbt_project\". Defaults to \"dbt\".\n        dbt_repo_branch (str, optional): The branch of the dbt repo to use. Defaults to\n            None (default repo branch).\n        dbt_repo_token_secret (str, optional): The secret containing the personal access\n            token used to clone the dbt repository, in case it's private. Not required\n            if token is already included in `dbt_repo_url` (which is NOT recommended).\n            Defaults to None.\n        dbt_selects (dict, optional): Valid\n            [dbt node selection](https://docs.getdbt.com/reference/node-selection/syntax)\n            expressions. Valid keys are `run`, `test`,`build`, and `source_freshness`.\n                The test select expression is taken from run's, as long as run select is\n                provided. Defaults to None.\n        dbt_target (str): The dbt target to use. If not specified, the default dbt\n            target (as specified in `profiles.yaml`) will be used. Defaults to None.\n        dbt_target_dir_path (str): The path to your dbt project's target\n            directory, which contains dbt artifact JSON files, relative\n            to dbt project's root directory. By default,\n            `&lt;repo_name&gt;/&lt;dbt_project_path&gt;/target`, since \"target\" is the default\n            name of the directory generated by dbt.\n        luma_url (str, optional): The URL of the Luma instance to ingest into.\n            Defaults to \"http://localhost:8000\".\n        luma_follow (bool, optional): Whether to follow the ingestion process until it's\n            completed (by default, ingestion request is sent without awaiting for the\n            response). By default, `False`.\n        metadata_kind (Literal[\"model\", \"model_run\"], optional): The kind of metadata\n            to ingest. Defaults to \"model_run\".\n        run_results_storage_path (str, optional): The directory to upload the\n            `run_results.json` file to. Note that a timestamp will be appended to the\n            end of the file. Currently, only S3 is supported. Defaults to None.\n        run_results_storage_config_key (str, optional): The key in the viadot config\n            holding AWS credentials. Defaults to None.\n        run_results_storage_credentials_secret (str, optional): The name of the secret\n            block in Prefect holding AWS credentials. Defaults to None.\n\n    Returns:\n        list[str]: Lines from stdout of the `upload_metadata` task as a list.\n\n    Examples:\n        # Build staging models.\n\n        ```python\n        import os\n        from prefect_viadot.flows import transform_and_catalog\n\n        my_dbt_repo_url = \"https://github.com/dbt-labs/jaffle_shop\"\n        my_luma_url = \"http://localhost:8000\"\n\n        transform_and_catalog(\n            dbt_repo_url=my_dbt_repo_url\n            dbt_selects={\"run\": \"staging\"}\n            luma_url=my_luma_url,\n            run_results_storage_path=\"s3://my-bucket/dbt/run_results\",\n            run_results_storage_credentials_secret=\"my-aws-credentials-block\",\n        )\n        ```\n\n        Some common `dbt_selects` patterns:\n        - runs a specific model and all its downstream dependencies:\n            `dbt_select={\"run\": \"my_model+\"}`\n        - runs all models in a directory:\n            `dbt_select={\"run: \"models/staging\"}`\n        - runs a specific model in a folder:\n            `dbt_select={\"run\": \"marts.domain.some_model\"}`\n        - runs tests for a specific model:\n            `dbt_select={\"test\": \"my_model\"}`\n        - build a specific model:\n            `dbt_select={\"build\": \"my_model\"}`\n        - build all models in a folder:\n            `dbt_select={\"build\": \"models.intermediate\"}`\n    \"\"\"\n    logger = get_run_logger()\n    # Clone the dbt project.\n    dbt_repo_url = dbt_repo_url or get_credentials(dbt_repo_url_secret)\n    clone = clone_repo(\n        url=dbt_repo_url,\n        checkout_branch=dbt_repo_branch,\n        token_secret=dbt_repo_token_secret,\n    )\n\n    # Prepare the environment.\n    dbt_repo_name = dbt_repo_url.split(\"/\")[-1].replace(\".git\", \"\")\n    dbt_project_path_full = Path(dbt_repo_name) / dbt_project_path\n    dbt_pull_deps_task = dbt_task.with_options(\n        name=\"dbt_deps\", retries=3, retry_delay_seconds=60\n    )\n    pull_dbt_deps = dbt_pull_deps_task(\n        project_path=dbt_project_path_full,\n        command=\"deps\",\n        wait_for=[clone],\n    )\n\n    # Run dbt commands.\n    dbt_target_option = f\"-t {dbt_target}\" if dbt_target is not None else \"\"\n\n    if metadata_kind == \"model_run\":\n        # Produce `run-results.json` artifact for Luma ingestion.\n        if dbt_selects:\n            build_select = dbt_selects.get(\"build\")\n            run_select = dbt_selects.get(\"run\")\n            test_select = dbt_selects.get(\"test\", run_select)\n\n            build_select_safe = f\"-s {build_select}\" if build_select is not None else \"\"\n            run_select_safe = f\"-s {run_select}\" if run_select is not None else \"\"\n            test_select_safe = f\"-s {test_select}\" if test_select is not None else \"\"\n        else:\n            run_select_safe = \"\"\n            test_select_safe = \"\"\n        if build_select:\n            # If build task is used, run and test tasks are not needed.\n            # Build task executes run and tests commands internally.\n            build_task = dbt_task.with_options(name=\"dbt_build\")\n            build = build_task(\n                project_path=dbt_project_path_full,\n                command=f\"build {build_select_safe} {dbt_target_option}\",\n                wait_for=[pull_dbt_deps],\n            )\n            upload_metadata_upstream_task = build\n        else:\n            run_task = dbt_task.with_options(name=\"dbt_run\")\n            run = run_task(\n                project_path=dbt_project_path_full,\n                command=f\"run {run_select_safe} {dbt_target_option}\",\n                wait_for=[pull_dbt_deps],\n            )\n\n            test_task = dbt_task.with_options(name=\"dbt_test\")\n            test = test_task(\n                project_path=dbt_project_path_full,\n                command=f\"test {test_select_safe} {dbt_target_option}\",\n                raise_on_failure=False,\n                wait_for=[run],\n            )\n            upload_metadata_upstream_task = test\n\n    else:\n        # Produce `catalog.json` and `manifest.json` artifacts for Luma ingestion.\n        docs_generate_task = dbt_task.with_options(name=\"dbt_docs_generate\")\n        docs = docs_generate_task(\n            project_path=dbt_project_path_full,\n            command=\"docs generate\",\n            wait_for=[pull_dbt_deps],\n        )\n        upload_metadata_upstream_task = docs\n\n    # Upload metadata to Luma.\n    if dbt_target_dir_path is None:\n        dbt_target_dir_path = dbt_project_path_full / \"target\"\n\n    upload_metadata = luma_ingest_task(\n        metadata_kind=metadata_kind,\n        metadata_dir_path=dbt_target_dir_path,\n        luma_url=luma_url,\n        follow=luma_follow,\n        wait_for=[upload_metadata_upstream_task],\n    )\n\n    if run_results_storage_path:\n        # Set the file path to include date info.\n        file_name = \"run_results.json\"\n        now = datetime.now(timezone.utc)\n        run_results_storage_path = run_results_storage_path.rstrip(\"/\") + \"/\"\n\n        # Add partitioning.\n        date_str = now.strftime(\"%Y%m%d\")\n        run_results_storage_path += date_str + \"/\"\n\n        # Add timestamp suffix, eg. run_results_1737556947.934292.json.\n        timestamp = now.timestamp()\n        run_results_storage_path += (\n            Path(file_name).stem + \"_\" + str(timestamp) + \".json\"\n        )\n        logger.info(f\"Uploading run results to {run_results_storage_path}\")\n        # Upload the file to s3.\n        dump_test_results_to_s3 = s3_upload_file(\n            from_path=str(dbt_target_dir_path / file_name),\n            to_path=run_results_storage_path,\n            wait_for=[upload_metadata_upstream_task],\n            config_key=run_results_storage_config_key,\n            credentials_secret=run_results_storage_credentials_secret,\n        )\n\n    # Cleanup.\n    wait_for = (\n        [upload_metadata, dump_test_results_to_s3]\n        if run_results_storage_path\n        else [upload_metadata]\n    )\n    remove_dbt_repo_dir(dbt_repo_name, wait_for=wait_for)\n\n    return remove_dbt_repo_dir\n</code></pre>"},{"location":"references/orchestration/prefect/flows/#viadot.orchestration.prefect.flows.vid_club_to_adls","title":"<code>viadot.orchestration.prefect.flows.vid_club_to_adls</code>","text":"<p>Download data from Vid CLub API and load it into Azure Data Lake Storage.</p>"},{"location":"references/orchestration/prefect/flows/#viadot.orchestration.prefect.flows.vid_club_to_adls.vid_club_to_adls","title":"<code>vid_club_to_adls(*args, endpoint=None, from_date='2022-03-22', to_date=None, items_per_page=100, region=None, days_interval=30, cols_to_drop=None, config_key=None, azure_key_vault_secret=None, adls_config_key=None, adls_azure_key_vault_secret=None, adls_path=None, adls_path_overwrite=False, validate_df_dict=None, timeout=3600, **kwargs)</code>","text":"<p>Flow for downloading data from the Vid Club via API to a CSV or Parquet file.</p> <p>Then upload it to Azure Data Lake.</p> <p>Parameters:</p> Name Type Description Default <code>endpoint</code> <code>Literal['jobs', 'product', 'company', 'survey']</code> <p>The</p> <code>None</code> <code>from_date</code> <code>str</code> <p>Start date for the query, by default is the oldest date in the data 2022-03-22.</p> <code>'2022-03-22'</code> <code>to_date</code> <code>str</code> <p>End date for the query. By default None, which will be executed as datetime.today().strftime(\"%Y-%m-%d\") in code.</p> <code>None</code> <code>items_per_page</code> <code>int</code> <p>Number of entries per page. Defaults to 100.</p> <code>100</code> <code>region</code> <code>Literal['bg', 'hu', 'hr', 'pl', 'ro', 'si', 'all']</code> <p>Region filter for the query. Defaults to None (parameter is not used in url). [December 2023 status: value 'all' does not work for company and jobs]</p> <code>None</code> <code>days_interval</code> <code>int</code> <p>Days specified in date range per API call (test showed that 30-40 is optimal for performance). Defaults to 30.</p> <code>30</code> <code>cols_to_drop</code> <code>List[str]</code> <p>List of columns to drop. Defaults to None.</p> <code>None</code> <code>config_key</code> <code>str</code> <p>The key in the viadot config holding relevant credentials. Defaults to None.</p> <code>None</code> <code>azure_key_vault_secret</code> <code>Optional[str]</code> <p>The name of the Azure Key Vault secret where credentials are stored. Defaults to None.</p> <code>None</code> <code>adls_config_key</code> <code>Optional[str]</code> <p>The key in the viadot config holding relevant credentials. Defaults to None.</p> <code>None</code> <code>adls_azure_key_vault_secret</code> <code>Optional[str]</code> <p>The name of the Azure Key Vault secret containing a dictionary with ACCOUNT_NAME and Service Principal credentials (TENANT_ID, CLIENT_ID, CLIENT_SECRET) for the Azure Data Lake. Defaults to None.</p> <code>None</code> <code>adls_path</code> <code>Optional[str]</code> <p>Azure Data Lake destination file path. Defaults to None.</p> <code>None</code> <code>adls_path_overwrite</code> <code>bool</code> <p>Whether to overwrite the file in ADLS. Defaults to True.</p> <code>False</code> <code>validate_df_dict</code> <code>dict</code> <p>A dictionary with optional list of tests to verify the output dataframe. If defined, triggers the <code>validate_df</code> task from task_utils. Defaults to None.</p> <code>None</code> <code>timeout</code> <code>int</code> <p>The time (in seconds) to wait while running this task before a timeout occurs. Defaults to 3600.</p> <code>3600</code> Source code in <code>src/viadot/orchestration/prefect/flows/vid_club_to_adls.py</code> <pre><code>@flow(\n    name=\"Vid CLub extraction to ADLS\",\n    description=\"Extract data from Vid CLub and load it into Azure Data Lake Storage.\",\n    retries=1,\n    retry_delay_seconds=60,\n    task_runner=ConcurrentTaskRunner,\n)\ndef vid_club_to_adls(  # noqa: PLR0913\n    *args: list[Any],\n    endpoint: Literal[\"jobs\", \"product\", \"company\", \"survey\"] | None = None,\n    from_date: str = \"2022-03-22\",\n    to_date: str | None = None,\n    items_per_page: int = 100,\n    region: Literal[\"bg\", \"hu\", \"hr\", \"pl\", \"ro\", \"si\", \"all\"] | None = None,\n    days_interval: int = 30,\n    cols_to_drop: list[str] | None = None,\n    config_key: str | None = None,\n    azure_key_vault_secret: str | None = None,\n    adls_config_key: str | None = None,\n    adls_azure_key_vault_secret: str | None = None,\n    adls_path: str | None = None,\n    adls_path_overwrite: bool = False,\n    validate_df_dict: dict | None = None,\n    timeout: int = 3600,\n    **kwargs: dict[str, Any],\n) -&gt; None:\n    \"\"\"Flow for downloading data from the Vid Club via API to a CSV or Parquet file.\n\n    Then upload it to Azure Data Lake.\n\n    Args:\n        endpoint (Literal[\"jobs\", \"product\", \"company\", \"survey\"], optional): The\n        endpoint source to be accessed. Defaults to None.\n        from_date (str, optional): Start date for the query, by default is the oldest\n            date in the data 2022-03-22.\n        to_date (str, optional): End date for the query. By default None,\n            which will be executed as datetime.today().strftime(\"%Y-%m-%d\") in code.\n        items_per_page (int, optional): Number of entries per page. Defaults to 100.\n        region (Literal[\"bg\", \"hu\", \"hr\", \"pl\", \"ro\", \"si\", \"all\"], optional): Region\n            filter for the query. Defaults to None (parameter is not used in url).\n            [December 2023 status: value 'all' does not work for company and jobs]\n        days_interval (int, optional): Days specified in date range per API call\n            (test showed that 30-40 is optimal for performance). Defaults to 30.\n        cols_to_drop (List[str], optional): List of columns to drop. Defaults to None.\n        config_key (str, optional): The key in the viadot config holding relevant\n            credentials. Defaults to None.\n        azure_key_vault_secret (Optional[str], optional): The name of the Azure Key\n            Vault secret where credentials are stored. Defaults to None.\n        adls_config_key (Optional[str], optional): The key in the viadot config holding\n            relevant credentials. Defaults to None.\n        adls_azure_key_vault_secret (Optional[str], optional): The name of the Azure Key\n            Vault secret containing a dictionary with ACCOUNT_NAME and Service Principal\n            credentials (TENANT_ID, CLIENT_ID, CLIENT_SECRET) for the Azure Data Lake.\n            Defaults to None.\n        adls_path (Optional[str], optional): Azure Data Lake destination file path.\n            Defaults to None.\n        adls_path_overwrite (bool, optional): Whether to overwrite the file in ADLS.\n            Defaults to True.\n        validate_df_dict (dict, optional): A dictionary with optional list of tests\n            to verify the output\n            dataframe. If defined, triggers the `validate_df` task from task_utils.\n            Defaults to None.\n        timeout (int, optional): The time (in seconds) to wait while running this task\n            before a timeout occurs. Defaults to 3600.\n    \"\"\"\n    data_frame = vid_club_to_df(\n        args=args,\n        endpoint=endpoint,\n        from_date=from_date,\n        to_date=to_date,\n        items_per_page=items_per_page,\n        region=region,\n        days_interval=days_interval,\n        cols_to_drop=cols_to_drop,\n        config_key=config_key,\n        azure_key_vault_secret=azure_key_vault_secret,\n        validate_df_dict=validate_df_dict,\n        timeout=timeout,\n        kawrgs=kwargs,\n    )\n\n    return df_to_adls(\n        df=data_frame,\n        path=adls_path,\n        credentials_secret=adls_azure_key_vault_secret,\n        config_key=adls_config_key,\n        overwrite=adls_path_overwrite,\n    )\n</code></pre>"},{"location":"references/orchestration/prefect/tasks/","title":"Tasks","text":""},{"location":"references/orchestration/prefect/tasks/#viadot.orchestration.prefect.tasks.azure_sql_to_df","title":"<code>viadot.orchestration.prefect.tasks.azure_sql_to_df(query=None, credentials_secret=None, validate_df_dict=None, convert_bytes=False, remove_special_characters=None, columns_to_clean=None, if_empty='warn')</code>","text":"<p>Task to download data from Azure SQL.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>Query to perform on a database. Defaults to None.</p> <code>None</code> <code>credentials_secret</code> <code>str</code> <p>The name of the Azure Key Vault secret containing a dictionary with database credentials. Defaults to None.</p> <code>None</code> <code>validate_df_dict</code> <code>Dict[str]</code> <p>A dictionary with optional list of tests to verify the output dataframe. If defined, triggers the <code>validate_df</code> task from task_utils. Defaults to None.</p> <code>None</code> <code>remove_special_characters</code> <code>str</code> <p>Call a function that remove special characters like escape symbols. Defaults to None.</p> <code>None</code> <code>columns_to_clean</code> <code>List(str)</code> <p>Select columns to clean, used with remove_special_characters. If None whole data frame will be processed. Defaults to None.</p> <code>None</code> <code>if_empty</code> <code>Literal['warn', 'skip', 'fail']</code> <p>What to do if the query returns no data. Defaults to None.</p> <code>'warn'</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>Raising ValueError if credentials_secret is not provided</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: The response data as a pandas DataFrame.</p> Source code in <code>src/viadot/orchestration/prefect/tasks/azure_sql.py</code> <pre><code>@task(retries=3, retry_delay_seconds=10, timeout_seconds=60 * 60)\ndef azure_sql_to_df(\n    query: str | None = None,\n    credentials_secret: str | None = None,\n    validate_df_dict: dict[str, Any] | None = None,\n    convert_bytes: bool = False,\n    remove_special_characters: bool | None = None,\n    columns_to_clean: list[str] | None = None,\n    if_empty: Literal[\"warn\", \"skip\", \"fail\"] = \"warn\",\n) -&gt; pd.DataFrame:\n    r\"\"\"Task to download data from Azure SQL.\n\n    Args:\n        query (str): Query to perform on a database. Defaults to None.\n        credentials_secret (str, optional): The name of the Azure Key Vault\n            secret containing a dictionary with database credentials.\n            Defaults to None.\n        validate_df_dict (Dict[str], optional): A dictionary with optional list of\n            tests to verify the output dataframe. If defined, triggers the `validate_df`\n            task from task_utils. Defaults to None.\n        convert_bytes (bool). A boolean value to trigger method df_converts_bytes_to_int\n            It is used to convert bytes data type into int, as pulling data with bytes\n            can lead to malformed data in data frame.\n            Defaults to False.\n        remove_special_characters (str, optional): Call a function that remove\n            special characters like escape symbols. Defaults to None.\n        columns_to_clean (List(str), optional): Select columns to clean, used with\n            remove_special_characters. If None whole data frame will be processed.\n            Defaults to None.\n        if_empty (Literal[\"warn\", \"skip\", \"fail\"], optional): What to do if the\n            query returns no data. Defaults to None.\n\n    Raises:\n        ValueError: Raising ValueError if credentials_secret is not provided\n\n    Returns:\n        pd.DataFrame: The response data as a pandas DataFrame.\n    \"\"\"\n    if not credentials_secret:\n        msg = \"`credentials_secret` has to be specified and not empty.\"\n        raise ValueError(msg)\n\n    credentials = get_credentials(credentials_secret)\n\n    azure_sql = AzureSQL(credentials=credentials)\n\n    df = azure_sql.to_df(\n        query=query,\n        if_empty=if_empty,\n        convert_bytes=convert_bytes,\n        remove_special_characters=remove_special_characters,\n        columns_to_clean=columns_to_clean,\n    )\n\n    if validate_df_dict is not None:\n        validate(df=df, tests=validate_df_dict)\n\n    return df\n</code></pre>"},{"location":"references/orchestration/prefect/tasks/#viadot.orchestration.prefect.tasks.adls_upload","title":"<code>viadot.orchestration.prefect.tasks.adls_upload(to_path, from_path=None, recursive=False, overwrite=False, credentials_secret=None, config_key=None)</code>","text":"<p>Upload file(s) to Azure Data Lake.</p> <p>Credentials can be specified as either a key inside viadot config file, or the name of the Prefect <code>AzureKeyVaultSecretReference</code> block document storing the reference to an Azure Key Vault secret.</p> <p>Parameters:</p> Name Type Description Default <code>to_path</code> <code>str</code> <p>The destination path.</p> required <code>recursive</code> <code>bool</code> <p>Set this to true if uploading entire directories. Defaults to False.</p> <code>False</code> <code>from_path</code> <code>str</code> <p>The local path from which to upload the file(s). Defaults to None.</p> <code>None</code> <code>overwrite</code> <code>bool</code> <p>Whether to overwrite files in the lake. Defaults to False.</p> <code>False</code> <code>credentials_secret</code> <code>str</code> <p>The name of the Azure Key Vault secret storing the credentials.</p> <code>None</code> <code>config_key</code> <code>str</code> <p>The key in the viadot config holding relevant credentials.</p> <code>None</code> Source code in <code>src/viadot/orchestration/prefect/tasks/adls.py</code> <pre><code>@task(retries=3, retry_delay_seconds=10, timeout_seconds=60 * 60)\ndef adls_upload(\n    to_path: str,\n    from_path: str | None = None,\n    recursive: bool = False,\n    overwrite: bool = False,\n    credentials_secret: str | None = None,\n    config_key: str | None = None,\n) -&gt; None:\n    \"\"\"Upload file(s) to Azure Data Lake.\n\n    Credentials can be specified as either a key inside viadot config file,\n    or the name of the Prefect `AzureKeyVaultSecretReference` block document\n    storing the reference to an Azure Key Vault secret.\n\n    Args:\n        to_path (str, optional): The destination path.\n        recursive (bool, optional): Set this to true if uploading entire directories.\n            Defaults to False.\n        from_path (str, optional): The local path from which to upload the file(s).\n            Defaults to None.\n        overwrite (bool, optional): Whether to overwrite files in the lake. Defaults\n            to False.\n        credentials_secret (str, optional): The name of the Azure Key Vault secret\n            storing the credentials.\n        config_key (str, optional): The key in the viadot config holding relevant\n            credentials.\n    \"\"\"\n    if not (credentials_secret or config_key):\n        raise MissingSourceCredentialsError\n\n    credentials = get_credentials(credentials_secret)\n    lake = AzureDataLake(credentials=credentials, config_key=config_key)\n\n    lake.upload(\n        from_path=from_path,\n        to_path=to_path,\n        recursive=recursive,\n        overwrite=overwrite,\n    )\n</code></pre>"},{"location":"references/orchestration/prefect/tasks/#viadot.orchestration.prefect.tasks.bcp","title":"<code>viadot.orchestration.prefect.tasks.bcp</code>","text":"<p>Task for running BCP shell command.</p>"},{"location":"references/orchestration/prefect/tasks/#viadot.orchestration.prefect.tasks.bcp.bcp","title":"<code>bcp(path, schema, table, chunksize=5000, error_log_file_path='./log_file.log', on_error='skip', credentials_secret=None, config_key=None)</code>","text":"<p>Upload data from a CSV file into an SQLServer table using BCP.</p> <p>For more information on bcp (bulk copy program), see https://learn.microsoft.com/en-us/sql/tools/bcp-utility.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Where to store the CSV data dump used for bulk upload to</p> required <code>schema</code> <code>str</code> <p>Destination schema. Defaults to None.</p> required <code>table</code> <code>str</code> <p>Destination table. Defaults to None.</p> required <code>chunksize</code> <code>int</code> <p>Size of a chunk to use in the bcp function. Defaults to 5000.</p> <code>5000</code> <code>error_log_file_path</code> <code>string</code> <p>Full path of an error file. Defaults to \"./log_file.log\".</p> <code>'./log_file.log'</code> <code>on_error</code> <code>str</code> <p>What to do in case of a bcp error. Defaults to \"skip\".</p> <code>'skip'</code> <code>credentials_secret</code> <code>str</code> <p>The name of the secret storing the credentials to the SQLServer. Defaults to None. More info on: https://docs.prefect.io/concepts/blocks/</p> <code>None</code> <code>config_key</code> <code>str</code> <p>The key in the viadot config holding relevant credentials to the SQLServer. Defaults to None.</p> <code>None</code> Source code in <code>src/viadot/orchestration/prefect/tasks/bcp.py</code> <pre><code>@task\ndef bcp(\n    path: str,\n    schema: str,\n    table: str,\n    chunksize: int = 5000,\n    error_log_file_path: str = \"./log_file.log\",\n    on_error: Literal[\"skip\", \"fail\"] = \"skip\",\n    credentials_secret: str | None = None,\n    config_key: str | None = None,\n) -&gt; None:\n    \"\"\"Upload data from a CSV file into an SQLServer table using BCP.\n\n    For more information on bcp (bulk copy program), see\n    https://learn.microsoft.com/en-us/sql/tools/bcp-utility.\n\n    Args:\n        path (str):  Where to store the CSV data dump used for bulk upload to\n        a database.\n        schema (str): Destination schema. Defaults to None.\n        table (str): Destination table. Defaults to None.\n        chunksize (int, optional): Size of a chunk to use in the bcp function.\n            Defaults to 5000.\n        error_log_file_path (string, optional): Full path of an error file. Defaults\n            to \"./log_file.log\".\n        on_error (str, optional): What to do in case of a bcp error. Defaults to \"skip\".\n        credentials_secret (str, optional): The name of the secret storing\n            the credentials to the SQLServer. Defaults to None.\n            More info on: https://docs.prefect.io/concepts/blocks/\n        config_key (str, optional): The key in the viadot config holding relevant\n            credentials to the SQLServer. Defaults to None.\n\n    \"\"\"\n    if not (credentials_secret or config_key):\n        raise MissingSourceCredentialsError\n\n    credentials = get_source_credentials(config_key) or get_credentials(\n        credentials_secret\n    )\n    fqn = f\"{schema}.{table}\" if schema else table\n    server = credentials[\"server\"]\n    db_name = credentials[\"db_name\"]\n    uid = credentials[\"user\"]\n    pwd = credentials[\"password\"]\n\n    if \",\" in server:\n        # A space after the comma is allowed in the ODBC connection string\n        # but not in BCP's 'server' argument.\n        server = server.replace(\" \", \"\")\n\n    if on_error == \"skip\":\n        max_error = 0\n    elif on_error == \"fail\":\n        max_error = 1\n    else:\n        msg = \"Please provide correct 'on_error' parameter value - 'skip' or 'fail'. \"\n        raise ValueError(msg)\n    bcp_command = [\n        \"/opt/mssql-tools/bin/bcp\",\n        fqn,\n        \"in\",\n        path,\n        \"-S\",\n        server,\n        \"-d\",\n        db_name,\n        \"-U\",\n        uid,\n        \"-P\",\n        pwd,\n        \"-b\",\n        str(chunksize),\n        \"-m\",\n        str(max_error),\n        \"-c\",\n        \"-v\",\n        \"-e\",\n        error_log_file_path,\n        \"-h\",\n        \"TABLOCK\",\n        \"-F\",\n        \"2\",\n    ]\n\n    return subprocess.run(bcp_command, capture_output=True, text=True, check=False)  # noqa: S603\n</code></pre>"},{"location":"references/orchestration/prefect/tasks/#viadot.orchestration.prefect.tasks.clone_repo","title":"<code>viadot.orchestration.prefect.tasks.clone_repo(url, token=None, token_secret=None, logger=None, **kwargs)</code>","text":"<p>Clone Azure DevOps or GitHub repository.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>Alternatively to URL + token or URL + token_secret, you can also provide the full URL (including token, if the repo is private) here.</p> required <code>token</code> <code>str</code> <p>The token to use to clone the repo.</p> <code>None</code> <code>token_secret</code> <code>str</code> <p>The secret holding the token.</p> <code>None</code> <code>logger</code> <code>Logger</code> <p>The logger to use. By default, Prefect's task run logger is used.</p> <code>None</code> <code>token</code> <code>str</code> <p>The personal access token.</p> <code>None</code> <code>token_secret</code> <code>str</code> <p>The name of the secret storing the token. Defaults to None.</p> <code>None</code> <code>logger</code> <code>Logger</code> <p>The logger to use for logging the task's output. By default, Prefect's task run logger.</p> <code>None</code> <code>kwargs</code> <code>dict</code> <p>Keyword arguments to be passed to <code>pygit2.clone_repository()</code>.</p> <code>{}</code> <code>**kwargs</code> <code>dict</code> <p>Keyword arguments to pass to <code>pygit2.clone_repository()</code>.</p> <code>{}</code> <p>Examples:</p> <p>Azure DevOps (public repo): https://dev.azure.com/{organization_name}/{project_name}/_git/{repo_name} Azure DevOps (private repo): https://{token}@dev.azure.com/{organization_name}/{project_name}/_git/{repo_name} GitHub (public repo): https://github.com/{organization_name}/{repo_name}.git GitHub (private repo): https://{token}@github.com/{organization_name}/{repo_name}.git</p> Source code in <code>src/viadot/orchestration/prefect/tasks/git.py</code> <pre><code>@task(retries=3, retry_delay_seconds=10, timeout_seconds=60 * 10)\ndef clone_repo(\n    url: str,\n    token: str | None = None,\n    token_secret: str | None = None,\n    logger: logging.Logger | None = None,\n    **kwargs: dict[str, Any] | None,\n) -&gt; None:\n    \"\"\"Clone Azure DevOps or GitHub repository.\n\n    Args:\n        url (str): Alternatively to URL + token or URL + token_secret, you can also\n            provide the full URL (including token, if the repo is private) here.\n        token (str, optional): The token to use to clone the repo.\n        token_secret (str, optional): The secret holding the token.\n        logger (logging.Logger): The logger to use. By default, Prefect's task run\n            logger is used.\n        token (str, optional): The personal access token.\n        token_secret (str, optional): The name of the secret storing the token. Defaults\n            to None.\n        logger (logging.Logger, optional): The logger to use for logging the task's\n            output. By default, Prefect's task run logger.\n        kwargs (dict): Keyword arguments to be passed to `pygit2.clone_repository()`.\n        **kwargs (dict, optional): Keyword arguments to pass to\n            `pygit2.clone_repository()`.\n\n    Examples:\n        Azure DevOps (public repo):\n        https://dev.azure.com/{organization_name}/{project_name}/_git/{repo_name}\n        Azure DevOps (private repo):\n        https://{token}@dev.azure.com/{organization_name}/{project_name}/_git/{repo_name}\n        GitHub (public repo): https://github.com/{organization_name}/{repo_name}.git\n        GitHub (private repo): https://{token}@github.com/{organization_name}/{repo_name}.git\n    \"\"\"\n    if not logger:\n        logger = get_run_logger()\n\n    url = url.strip(\"/\")\n\n    if token_secret:\n        token = get_credentials(token_secret)\n\n    if token:\n        url = url.replace(\"https://dev.azure.com\", f\"https://{token}@dev.azure.com\")\n        url = url.replace(\"https://github.com\", f\"https://{token}@github.com\")\n        url = url.replace(\"gitlab\", f\"oauth2:{token}@gitlab\")\n\n    repo_name = url.split(\"/\")[-1].replace(\".git\", \"\")\n    path = kwargs.get(\"path\") or repo_name\n    kwargs[\"path\"] = path\n\n    logger.info(f\"Removing {path}...\")\n    shutil.rmtree(path, ignore_errors=True)  # Delete folder on run\n\n    logger.info(f\"Cloning repo '{repo_name}' into {path}...\")\n    pygit2.clone_repository(url, **kwargs)\n    logger.info(f\"Repo '{repo_name}' has been successfully cloned into {path}.\")\n</code></pre>"},{"location":"references/orchestration/prefect/tasks/#viadot.orchestration.prefect.tasks.bigquery_to_df","title":"<code>viadot.orchestration.prefect.tasks.bigquery_to_df(config_key=None, azure_key_vault_secret=None, query=None, dataset_name=None, table_name=None, date_column_name=None, start_date=None, end_date=None, columns=None)</code>","text":"<p>Task to download data from BigQuery API.</p> <p>Parameters:</p> Name Type Description Default <code>config_key</code> <code>str</code> <p>The key in the viadot config holding relevant credentials. Defaults to None.</p> <code>None</code> <code>azure_key_vault_secret</code> <code>str</code> <p>The name of the Azure Key Vault secret where credentials are stored. Defaults to None.</p> <code>None</code> <code>query</code> <code>str</code> <p>SQL query to querying data in BigQuery. Format of basic query: (SELECT * FROM <code>{project}.{dataset_name}.{table_name}</code>). Defaults to None.</p> <code>None</code> <code>dataset_name</code> <code>str</code> <p>Dataset name. Defaults to None.</p> <code>None</code> <code>table_name</code> <code>str</code> <p>Table name. Defaults to None.</p> <code>None</code> <code>date_column_name</code> <code>str</code> <p>The user can provide the name of the date. If the user-specified column does not exist, all data will be retrieved from the table. Defaults to None.</p> <code>None</code> <code>start_date</code> <code>str</code> <p>Parameter to pass start date e.g. \"2022-01-01\". Defaults to None.</p> <code>None</code> <code>end_date</code> <code>str</code> <p>Parameter to pass end date e.g. \"2022-01-01\". Defaults to None.</p> <code>None</code> <code>columns</code> <code>list[str]</code> <p>List of columns from given table name.     Defaults to None.</p> <code>None</code> <p>Raises:</p> Type Description <code>MissingSourceCredentialsError</code> <p>Credentials were not loaded.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: The response data as a Pandas Data Frame.</p> Source code in <code>src/viadot/orchestration/prefect/tasks/bigquery.py</code> <pre><code>@task(retries=3, log_prints=True, retry_delay_seconds=10, timeout_seconds=60 * 60)\ndef bigquery_to_df(\n    config_key: str | None = None,\n    azure_key_vault_secret: str | None = None,\n    query: str | None = None,\n    dataset_name: str | None = None,\n    table_name: str | None = None,\n    date_column_name: str | None = None,\n    start_date: str | None = None,\n    end_date: str | None = None,\n    columns: list[str] | None = None,\n) -&gt; pd.DataFrame:\n    \"\"\"Task to download data from BigQuery API.\n\n    Args:\n        config_key (str, optional): The key in the viadot config holding relevant\n            credentials. Defaults to None.\n        azure_key_vault_secret (str, optional): The name of the Azure Key Vault secret\n            where credentials are stored. Defaults to None.\n        query (str): SQL query to querying data in BigQuery. Format of basic query:\n            (SELECT * FROM `{project}.{dataset_name}.{table_name}`). Defaults to None.\n        dataset_name (str, optional): Dataset name. Defaults to None.\n        table_name (str, optional): Table name. Defaults to None.\n        date_column_name (str, optional): The user can provide the name of the date. If\n            the user-specified column does not exist, all data will be retrieved from\n            the table. Defaults to None.\n        start_date (str, optional): Parameter to pass start date e.g.\n            \"2022-01-01\". Defaults to None.\n        end_date (str, optional): Parameter to pass end date e.g.\n            \"2022-01-01\". Defaults to None.\n        columns (list[str], optional): List of columns from given table name.\n                Defaults to None.\n\n    Raises:\n        MissingSourceCredentialsError: Credentials were not loaded.\n\n    Returns:\n        pd.DataFrame: The response data as a Pandas Data Frame.\n    \"\"\"\n    if not (azure_key_vault_secret or config_key):\n        raise MissingSourceCredentialsError\n\n    if not config_key:\n        credentials = get_credentials(azure_key_vault_secret)\n\n    bigquery = BigQuery(credentials=credentials, config_key=config_key)\n\n    return bigquery.to_df(\n        query=query,\n        dataset_name=dataset_name,\n        table_name=table_name,\n        date_column_name=date_column_name,\n        start_date=start_date,\n        end_date=end_date,\n        columns=columns,\n    )\n</code></pre>"},{"location":"references/orchestration/prefect/tasks/#viadot.orchestration.prefect.tasks.business_core.business_core_to_df","title":"<code>viadot.orchestration.prefect.tasks.business_core.business_core_to_df(url=None, filters=None, credentials_secret=None, config_key=None, if_empty='skip', verify=True)</code>","text":"<p>Download data from Business Core API to a pandas DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>(str, required)</code> <p>Base url to the view in Business Core API. Defaults to None.</p> <code>None</code> <code>filters</code> <code>dict[str, Any]</code> <p>Filters in form of dictionary. Available filters: 'BucketCount','BucketNo', 'FromDate', 'ToDate'. Defaults to None.</p> <code>None</code> <code>credentials_secret</code> <code>str</code> <p>The name of the secret that stores Business Core credentials. More info on: https://docs.prefect.io/concepts/blocks/. Defaults to None.</p> <code>None</code> <code>config_key</code> <code>str</code> <p>The key in the viadot config holding relevant credentials. Defaults to None.</p> <code>None</code> <code>if_empty</code> <code>str</code> <p>What to do if output DataFrame is empty. Defaults to \"skip\".</p> <code>'skip'</code> <code>verify</code> <code>bool</code> <p>Whether or not verify certificates while connecting to an API. Defaults to True.</p> <code>True</code> Source code in <code>src/viadot/orchestration/prefect/tasks/business_core.py</code> <pre><code>@task(retries=3, retry_delay_seconds=10, timeout_seconds=60 * 60 * 3)\ndef business_core_to_df(\n    url: str | None = None,\n    filters: dict[str, Any] | None = None,\n    credentials_secret: str | None = None,\n    config_key: str | None = None,\n    if_empty: str = \"skip\",\n    verify: bool = True,\n) -&gt; DataFrame:\n    \"\"\"Download data from Business Core API to a pandas DataFrame.\n\n    Args:\n        url (str, required): Base url to the view in Business Core API. Defaults to\n            None.\n        filters (dict[str, Any], optional): Filters in form of dictionary. Available\n            filters: 'BucketCount','BucketNo', 'FromDate', 'ToDate'. Defaults to None.\n        credentials_secret (str, optional): The name of the secret that stores Business\n            Core credentials. More info on: https://docs.prefect.io/concepts/blocks/.\n            Defaults to None.\n        config_key (str, optional): The key in the viadot config holding relevant\n            credentials. Defaults to None.\n        if_empty (str, optional): What to do if output DataFrame is empty. Defaults to\n            \"skip\".\n        verify (bool, optional): Whether or not verify certificates while connecting\n            to an API. Defaults to True.\n    \"\"\"\n    if not (credentials_secret or config_key):\n        raise MissingSourceCredentialsError\n\n    logger = get_run_logger()\n\n    credentials = get_source_credentials(config_key) or get_credentials(\n        credentials_secret\n    )\n\n    bc = BusinessCore(\n        url=url,\n        credentials=credentials,\n        config_key=config_key,\n        filters=filters,\n        verify=verify,\n    )\n\n    df = bc.to_df(if_empty=if_empty)\n\n    nrows = df.shape[0]\n    ncols = df.shape[1]\n\n    logger.info(\n        f\"Successfully downloaded {nrows} rows and {ncols} columns of data to a DataFrame.\"\n    )\n\n    return df\n</code></pre>"},{"location":"references/orchestration/prefect/tasks/#viadot.orchestration.prefect.tasks.cloud_for_customers_to_df","title":"<code>viadot.orchestration.prefect.tasks.cloud_for_customers_to_df(url=None, endpoint=None, report_url=None, filter_params=None, credentials_secret=None, config_key=None, **kwargs)</code>","text":"<p>Extracts Cloud for Customers records as pd.DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The API url.</p> <code>None</code> <code>endpoint</code> <code>str</code> <p>The API endpoint.</p> <code>None</code> <code>report_url</code> <code>str</code> <p>The API url in case of prepared report.</p> <code>None</code> <code>filter_params</code> <code>dict[str, Any]</code> <p>Query parameters.</p> <code>None</code> <code>credentials_secret</code> <code>str</code> <p>The name of the secret storing the credentials. More info on: https://docs.prefect.io/concepts/blocks/</p> <code>None</code> <code>config_key</code> <code>str</code> <p>The key in the viadot config holding relevant credentials.</p> <code>None</code> <code>credentials</code> <code>dict</code> <p>Cloud for Customers credentials.</p> required <code>kwargs</code> <code>dict[str, Any] | None</code> <p>The parameters to pass to DataFrame constructor.</p> <code>{}</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.Dataframe: The pandas <code>DataFrame</code> containing data from the file.</p> Source code in <code>src/viadot/orchestration/prefect/tasks/cloud_for_customers.py</code> <pre><code>@task(retries=3, retry_delay_seconds=10, timeout_seconds=60 * 60)\ndef cloud_for_customers_to_df(\n    url: str | None = None,\n    endpoint: str | None = None,\n    report_url: str | None = None,\n    filter_params: dict[str, Any] | None = None,\n    credentials_secret: str | None = None,\n    config_key: str | None = None,\n    **kwargs: dict[str, Any] | None,\n) -&gt; pd.DataFrame:\n    \"\"\"Extracts Cloud for Customers records as pd.DataFrame.\n\n    Args:\n        url (str, optional): The API url.\n        endpoint (str, optional): The API endpoint.\n        report_url (str, optional): The API url in case of prepared report.\n        filter_params (dict[str, Any], optional): Query parameters.\n        credentials_secret (str, optional): The name of the secret storing the\n            credentials.\n            More info on: https://docs.prefect.io/concepts/blocks/\n        config_key (str, optional): The key in the viadot config holding relevant\n            credentials.\n        credentials (dict, optional): Cloud for Customers credentials.\n        kwargs: The parameters to pass to DataFrame constructor.\n\n    Returns:\n        pd.Dataframe: The pandas `DataFrame` containing data from the file.\n    \"\"\"\n    if not (credentials_secret or config_key):\n        msg = \"Either `credentials_secret` or `config_key` has to be specified and not empty.\"\n        raise ValueError(msg)\n\n    credentials = get_credentials(credentials_secret)\n    c4c = CloudForCustomers(\n        url=url,\n        endpoint=endpoint,\n        report_url=report_url,\n        filter_params=filter_params,\n        credentials=credentials,\n        config_key=config_key,\n    )\n    return c4c.to_df(**kwargs)\n</code></pre>"},{"location":"references/orchestration/prefect/tasks/#viadot.orchestration.prefect.tasks.create_sql_server_table","title":"<code>viadot.orchestration.prefect.tasks.create_sql_server_table(schema, table, if_exists='fail', dtypes=None, credentials_secret=None, config_key=None)</code>","text":"<p>A task for creating table in SQL Server.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>str</code> <p>Destination schema.</p> required <code>table</code> <code>str</code> <p>Destination table.</p> required <code>if_exists</code> <code>Literal</code> <p>What to do if the table already exists.</p> <code>'fail'</code> <code>dtypes</code> <code>dict[str, Any]</code> <p>Data types to enforce.</p> <code>None</code> <code>credentials_secret</code> <code>str</code> <p>The name of the secret storing the credentials. Defaults to None. More info on: https://docs.prefect.io/concepts/blocks/</p> <code>None</code> <code>config_key</code> <code>str</code> <p>The key in the viadot config holding relevant credentials. Defaults to None.</p> <code>None</code> Source code in <code>src/viadot/orchestration/prefect/tasks/sql_server.py</code> <pre><code>@task(retries=3, retry_delay_seconds=10, timeout_seconds=60 * 60 * 3)\ndef create_sql_server_table(\n    schema: str,\n    table: str,\n    if_exists: Literal[\"fail\", \"replace\", \"skip\", \"delete\"] = \"fail\",\n    dtypes: dict[str, Any] | None = None,\n    credentials_secret: str | None = None,\n    config_key: str | None = None,\n) -&gt; None:\n    \"\"\"A task for creating table in SQL Server.\n\n    Args:\n        schema (str): Destination schema.\n        table (str): Destination table.\n        if_exists (Literal, optional): What to do if the table already exists.\n        dtypes (dict[str, Any], optional): Data types to enforce.\n        credentials_secret (str, optional): The name of the secret storing\n            the credentials. Defaults to None.\n            More info on: https://docs.prefect.io/concepts/blocks/\n        config_key (str, optional): The key in the viadot config holding relevant\n            credentials. Defaults to None.\n    \"\"\"\n    if not (credentials_secret or config_key):\n        raise MissingSourceCredentialsError\n\n    logger = get_run_logger()\n\n    credentials = get_credentials(credentials_secret) or get_source_credentials(\n        config_key\n    )\n    sql_server = SQLServer(credentials=credentials)\n\n    fqn = f\"{schema}.{table}\" if schema is not None else table\n    created = sql_server.create_table(\n        schema=schema, table=table, dtypes=dtypes, if_exists=if_exists\n    )\n    if created:\n        logger.info(f\"Successfully created table {fqn}.\")\n    else:\n        logger.info(\n            f\"Table {fqn} has not been created as if_exists is set to {if_exists}.\"\n        )\n</code></pre>"},{"location":"references/orchestration/prefect/tasks/#viadot.orchestration.prefect.tasks.customer_gauge_to_df","title":"<code>viadot.orchestration.prefect.tasks.customer_gauge_to_df</code>","text":"<p>'customer_gauge_to_df.py'.</p>"},{"location":"references/orchestration/prefect/tasks/#viadot.orchestration.prefect.tasks.customer_gauge_to_df.customer_gauge_to_df","title":"<code>customer_gauge_to_df(config_key=None, azure_key_vault_secret=None, endpoint='non-responses', cursor=None, pagesize=1000, date_field=None, start_date=None, end_date=None, total_load=True, unpack_by_field_reference_cols=None, unpack_by_nested_dict_transformer=None, validate_df_dict=None, anonymize=False, columns_to_anonymize=None, anonymize_method='mask', anonymize_value='***', date_column=None, days=None)</code>","text":"<p>Download the selected range of data from Customer Gauge API.</p> <p>Parameters:</p> Name Type Description Default <code>config_key</code> <code>str</code> <p>The key in the viadot config holding relevant credentials. Defaults to None.</p> <code>None</code> <code>azure_key_vault_secret</code> <code>str</code> <p>The name of the Azure Key Vault secret containing a dictionary with ['client_id', 'client_secret']. Defaults to None.</p> <code>None</code> <code>endpoint</code> <code>Literal['responses', 'non-responses']</code> <p>Indicate which endpoint to connect. Defaults to \"non-responses.</p> <code>'non-responses'</code> <code>cursor</code> <code>int</code> <p>Cursor value to navigate to the page. Defaults to None.</p> <code>None</code> <code>pagesize</code> <code>int</code> <p>Number of responses (records) returned per page, max value = 1000. Defaults to 1000. Defaults to 1000.</p> <code>1000</code> <code>date_field</code> <code>str</code> <p>Specifies the date type which filter date range. Possible options: \"date_creation\", \"date_order\", \"date_sent\" or \"date_survey_response\". Defaults to None.</p> <code>None</code> <code>start_date</code> <code>datetime</code> <p>Defines the period start date in yyyy-mm-dd format. Defaults to None.</p> <code>None</code> <code>end_date</code> <code>datetime</code> <p>Defines the period end date in yyyy-mm-dd format. Defaults to None.</p> <code>None</code> <code>total_load</code> <code>bool</code> <p>Indicate whether to download the data to the latest. If 'False', only one API call is executed (up to 1000 records). Defaults to True.</p> <code>True</code> <code>unpack_by_field_reference_cols</code> <code>list[str]</code> <p>Columns to unpack and modify using <code>_field_reference_unpacker</code>. Defaults to None.</p> <code>None</code> <code>unpack_by_nested_dict_transformer</code> <code>list[str]</code> <p>Columns to unpack and modify using <code>_nested_dict_transformer</code>. Defaults to None.</p> <code>None</code> <code>validate_df_dict</code> <code>dict[str, Any]</code> <p>A dictionary with optional list of tests to verify the output dataframe. If defined, triggers the <code>validate_df</code> task from task_utils. Defaults to None.</p> <code>None</code> <code>anonymize</code> <code>bool</code> <p>Indicates if anonymize selected columns. Defaults to False.</p> <code>False</code> <code>columns_to_anonymize</code> <code>list[str]</code> <p>List of columns to anonymize. Defaults to None.</p> <code>None</code> <code>anonymize_method</code> <code> (Literal[\"mask\", \"hash\"]</code> <p>Method of anonymizing data. \"mask\" -&gt; replace the data with \"value\" arg. \"hash\" -&gt; replace the data with the hash value of an object (using <code>hash()</code> method). Defaults to \"mask\".</p> <code>'mask'</code> <code>anonymize_value</code> <code>str</code> <p>Value to replace the data. Defaults to \"***\".</p> <code>'***'</code> <code>date_column</code> <code>str</code> <p>Name of the date column used to identify rows that are older than a specified number of days. Defaults to None.</p> <code>None</code> <code>days</code> <code>int</code> <p>The number of days beyond which we want to anonymize the data, e.g. older than 2 years can be: 2*365. Defaults to None.</p> <code>None</code> <p>Raises:</p> Type Description <code>MissingSourceCredentialsError</code> <p>If none credentials have been provided.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: The response data as a Pandas Data Frame.</p> Source code in <code>src/viadot/orchestration/prefect/tasks/customer_gauge_to_df.py</code> <pre><code>@task(retries=3, log_prints=True, retry_delay_seconds=10, timeout_seconds=2 * 60 * 60)\ndef customer_gauge_to_df(  # noqa: PLR0913\n    config_key: str | None = None,\n    azure_key_vault_secret: str | None = None,\n    endpoint: Literal[\"responses\", \"non-responses\"] = \"non-responses\",\n    cursor: int | None = None,\n    pagesize: int = 1000,\n    date_field: str | None = None,\n    start_date: datetime | None = None,\n    end_date: datetime | None = None,\n    total_load: bool = True,\n    unpack_by_field_reference_cols: list[str] | None = None,\n    unpack_by_nested_dict_transformer: list[str] | None = None,\n    validate_df_dict: dict[str, Any] | None = None,\n    anonymize: bool = False,\n    columns_to_anonymize: list[str] | None = None,\n    anonymize_method: Literal[\"mask\", \"hash\"] = \"mask\",\n    anonymize_value: str = \"***\",\n    date_column: str | None = None,\n    days: int | None = None,\n) -&gt; pd.DataFrame:\n    \"\"\"Download the selected range of data from Customer Gauge API.\n\n    Args:\n        config_key (str, optional): The key in the viadot config holding relevant\n            credentials. Defaults to None.\n        azure_key_vault_secret (str, optional): The name of the Azure Key Vault secret\n            containing a dictionary with ['client_id', 'client_secret'].\n            Defaults to None.\n        endpoint (Literal[\"responses\", \"non-responses\"], optional): Indicate which\n            endpoint to connect. Defaults to \"non-responses.\n        cursor (int, optional): Cursor value to navigate to the page.\n            Defaults to None.\n        pagesize (int, optional): Number of responses (records) returned per page,\n            max value = 1000. Defaults to 1000. Defaults to 1000.\n        date_field (str, optional): Specifies the date type which filter date range.\n            Possible options: \"date_creation\", \"date_order\", \"date_sent\" or\n            \"date_survey_response\". Defaults to None.\n        start_date (datetime, optional): Defines the period start date in\n            yyyy-mm-dd format. Defaults to None.\n        end_date (datetime, optional): Defines the period end date in\n            yyyy-mm-dd format. Defaults to None.\n        total_load (bool, optional): Indicate whether to download the data to the\n            latest. If 'False', only one API call is executed (up to 1000 records).\n            Defaults to True.\n        unpack_by_field_reference_cols (list[str]): Columns to unpack and modify using\n            `_field_reference_unpacker`. Defaults to None.\n        unpack_by_nested_dict_transformer (list[str]): Columns to unpack and modify\n            using `_nested_dict_transformer`. Defaults to None.\n        validate_df_dict (dict[str, Any], optional): A dictionary with optional list of\n            tests to verify the output dataframe. If defined, triggers the\n            `validate_df` task from task_utils. Defaults to None.\n        anonymize (bool, optional): Indicates if anonymize selected columns.\n            Defaults to False.\n        columns_to_anonymize (list[str], optional): List of columns to anonymize.\n            Defaults to None.\n        anonymize_method  (Literal[\"mask\", \"hash\"], optional): Method of\n            anonymizing data. \"mask\" -&gt; replace the data with \"value\" arg. \"hash\" -&gt;\n            replace the data with the hash value of an object (using `hash()`\n            method). Defaults to \"mask\".\n        anonymize_value (str, optional): Value to replace the data.\n            Defaults to \"***\".\n        date_column (str, optional): Name of the date column used to identify rows\n            that are older than a specified number of days. Defaults to None.\n        days (int, optional): The number of days beyond which we want to anonymize\n            the data, e.g. older than 2 years can be: 2*365. Defaults to None.\n\n    Raises:\n        MissingSourceCredentialsError: If none credentials have been provided.\n\n    Returns:\n        pd.DataFrame: The response data as a Pandas Data Frame.\n    \"\"\"\n    if not (azure_key_vault_secret or config_key):\n        raise MissingSourceCredentialsError\n\n    if not config_key:\n        credentials = get_credentials(azure_key_vault_secret)\n\n    customer_gauge = CustomerGauge(credentials=credentials, config_key=config_key)\n    customer_gauge.api_connection(\n        endpoint=endpoint,\n        cursor=cursor,\n        pagesize=pagesize,\n        date_field=date_field,\n        start_date=start_date,\n        end_date=end_date,\n        total_load=total_load,\n        unpack_by_field_reference_cols=unpack_by_field_reference_cols,\n        unpack_by_nested_dict_transformer=unpack_by_nested_dict_transformer,\n    )\n\n    return customer_gauge.to_df(\n        validate_df_dict=validate_df_dict,\n        anonymize=anonymize,\n        columns_to_anonymize=columns_to_anonymize,\n        anonymize_method=anonymize_method,\n        anonymize_value=anonymize_value,\n        date_column=date_column,\n        days=days,\n    )\n</code></pre>"},{"location":"references/orchestration/prefect/tasks/#viadot.orchestration.prefect.tasks.dbt_task","title":"<code>viadot.orchestration.prefect.tasks.dbt_task(command='run', project_path=None, env=None, shell='bash', return_all=False, stream_level=logging.INFO, raise_on_failure=True)</code>  <code>async</code>","text":"<p>Runs dbt commands within a shell.</p> <p>Parameters:</p> Name Type Description Default <code>command</code> <code>str</code> <p>dbt command to be executed; can also be provided post-initialization by calling this task instance.</p> <code>'run'</code> <code>project_path</code> <code>str | None</code> <p>The path to the dbt project.</p> <code>None</code> <code>env</code> <code>dict[str, Any] | None</code> <p>Dictionary of environment variables to use for the subprocess; can also be provided at runtime.</p> <code>None</code> <code>shell</code> <code>str</code> <p>Shell to run the command with.</p> <code>'bash'</code> <code>return_all</code> <code>bool</code> <p>Whether this task should return all lines of stdout as a list, or just the last line as a string.</p> <code>False</code> <code>stream_level</code> <code>int</code> <p>The logging level of the stream; defaults to 20, equivalent to <code>logging.INFO</code>.</p> <code>INFO</code> <code>raise_on_failure</code> <code>bool</code> <p>Whether to fail the task if the command fails.</p> <code>True</code> <p>Returns:</p> Type Description <code>list[str] | str</code> <p>If return all, returns all lines as a list; else the last line as a string.</p> Example <p>Executes <code>dbt run</code> on a specified dbt project. <pre><code>from prefect import flow\nfrom viadot.tasks import dbt_task\n\nPROJECT_PATH = \"/home/viadot/dbt/my_dbt_project\"\n\n@flow\ndef example_dbt_task_flow():\n    return dbt_task(\n        command=\"run\", project_path=PROJECT_PATH, return_all=True\n    )\n\nexample_dbt_task_flow()\n</code></pre></p> Source code in <code>src/viadot/orchestration/prefect/tasks/dbt.py</code> <pre><code>@task(retries=0, timeout_seconds=2 * 60 * 60)\nasync def dbt_task(\n    command: str = \"run\",\n    project_path: str | None = None,\n    env: dict[str, Any] | None = None,\n    shell: str = \"bash\",\n    return_all: bool = False,\n    stream_level: int = logging.INFO,\n    raise_on_failure: bool = True,\n) -&gt; list[str] | str:\n    \"\"\"Runs dbt commands within a shell.\n\n    Args:\n        command: dbt command to be executed; can also be provided post-initialization\n            by calling this task instance.\n        project_path: The path to the dbt project.\n        env: Dictionary of environment variables to use for the subprocess; can also be\n            provided at runtime.\n        shell: Shell to run the command with.\n        return_all: Whether this task should return all lines of stdout as a list, or\n            just the last line as a string.\n        stream_level: The logging level of the stream; defaults to 20, equivalent to\n            `logging.INFO`.\n        raise_on_failure: Whether to fail the task if the command fails.\n\n    Returns:\n        If return all, returns all lines as a list; else the last line as a string.\n\n    Example:\n        Executes `dbt run` on a specified dbt project.\n        ```python\n        from prefect import flow\n        from viadot.tasks import dbt_task\n\n        PROJECT_PATH = \"/home/viadot/dbt/my_dbt_project\"\n\n        @flow\n        def example_dbt_task_flow():\n            return dbt_task(\n                command=\"run\", project_path=PROJECT_PATH, return_all=True\n            )\n\n        example_dbt_task_flow()\n        ```\n    \"\"\"\n    logger = get_run_logger()\n\n    project_path = os.path.expandvars(project_path) if project_path is not None else \".\"\n\n    return await shell_run_command(\n        command=f\"dbt {command}\",\n        env=env,\n        helper_command=f\"cd {project_path}\",\n        shell=shell,\n        return_all=return_all,\n        stream_level=stream_level,\n        raise_on_failure=raise_on_failure,\n        logger=logger,\n    )\n</code></pre>"},{"location":"references/orchestration/prefect/tasks/#viadot.orchestration.prefect.tasks.df_to_adls","title":"<code>viadot.orchestration.prefect.tasks.df_to_adls(df, path, sep='\\t', credentials_secret=None, config_key=None, overwrite=False)</code>","text":"<p>Upload a pandas <code>DataFrame</code> to a file on Azure Data Lake.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The pandas DataFrame to upload.</p> required <code>path</code> <code>str</code> <p>The destination path. Defaults to None.</p> required <code>sep</code> <code>str</code> <p>The separator to use in the <code>to_csv</code> function. Defaults to \"\\t\".</p> <code>'\\t'</code> <code>overwrite</code> <code>bool</code> <p>Whether to overwrite files in the lake. Defaults to False.</p> <code>False</code> <code>credentials_secret</code> <code>str</code> <p>The name of the Azure Key Vault secret storing the credentials.</p> <code>None</code> <code>config_key</code> <code>str</code> <p>The key in the viadot config holding relevant credentials. Defaults to None.</p> <code>None</code> Source code in <code>src/viadot/orchestration/prefect/tasks/adls.py</code> <pre><code>@task(retries=3, retry_delay_seconds=10)\ndef df_to_adls(\n    df: pd.DataFrame,\n    path: str,\n    sep: str = \"\\t\",\n    credentials_secret: str | None = None,\n    config_key: str | None = None,\n    overwrite: bool = False,\n) -&gt; None:\n    r\"\"\"Upload a pandas `DataFrame` to a file on Azure Data Lake.\n\n    Args:\n        df (pd.DataFrame): The pandas DataFrame to upload.\n        path (str): The destination path. Defaults to None.\n        sep (str, optional): The separator to use in the `to_csv` function. Defaults to\n            \"\\t\".\n        overwrite (bool, optional): Whether to overwrite files in the lake. Defaults\n            to False.\n        credentials_secret (str, optional): The name of the Azure Key Vault secret\n            storing the credentials.\n        config_key (str, optional): The key in the viadot config holding relevant\n            credentials. Defaults to None.\n    \"\"\"\n    if not (credentials_secret or config_key):\n        raise MissingSourceCredentialsError\n\n    credentials = get_credentials(credentials_secret)\n    lake = AzureDataLake(credentials=credentials, config_key=config_key)\n\n    lake.from_df(\n        df=df,\n        path=path,\n        sep=sep,\n        overwrite=overwrite,\n    )\n</code></pre>"},{"location":"references/orchestration/prefect/tasks/#viadot.orchestration.prefect.tasks.df_to_databricks","title":"<code>viadot.orchestration.prefect.tasks.df_to_databricks(df, table, schema=None, if_exists='fail', if_empty='warn', credentials_secret=None, config_key=None)</code>","text":"<p>Insert a pandas <code>DataFrame</code> into a Delta table.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>A pandas <code>DataFrame</code> with the data to be inserted into the table.</p> required <code>table</code> <code>str</code> <p>The name of the target table.</p> required <code>schema</code> <code>str</code> <p>The name of the target schema.</p> <code>None</code> <code>if_exists</code> <code>(str, Optional)</code> <p>What to do if the table already exists. One of 'replace', 'skip', and 'fail'.</p> <code>'fail'</code> <code>if_empty</code> <code>str</code> <p>What to do if the input <code>DataFrame</code> is empty. Defaults to 'warn'.</p> <code>'warn'</code> <code>credentials_secret</code> <code>str</code> <p>The name of the secret storing the credentials. Defaults to None. More info on: https://docs.prefect.io/concepts/blocks/</p> <code>None</code> <code>config_key</code> <code>str</code> <p>The key in the viadot config holding relevant credentials. Defaults to None.</p> <code>None</code> Example <pre><code>from prefect_viadot.tasks df_to_databricks\nfrom prefect import flow\nimport pandas as pd\n\n@flow\ndef insert_df_into_databricks():\n    list = [{\"id\":\"1\", \"name\":\"Joe\"}]\n    df = pd.DataFrame(list)\n    insert = df_to_databricks(\n        df=df,\n        schema=\"prefect_viadot_test\"\n        table=\"test\",\n        if_exists=\"replace\"\n    )\n    return insert\n\ninsert_df_into_databricks()\n</code></pre> Source code in <code>src/viadot/orchestration/prefect/tasks/databricks.py</code> <pre><code>@task(retries=3, retry_delay_seconds=10, timeout_seconds=60 * 60)\ndef df_to_databricks(\n    df: pd.DataFrame,\n    table: str,\n    schema: str | None = None,\n    if_exists: Literal[\"replace\", \"skip\", \"fail\"] = \"fail\",\n    if_empty: Literal[\"warn\", \"skip\", \"fail\"] = \"warn\",\n    credentials_secret: str | None = None,\n    config_key: str | None = None,\n) -&gt; None:\n    \"\"\"Insert a pandas `DataFrame` into a Delta table.\n\n    Args:\n        df (pd.DataFrame): A pandas `DataFrame` with the data\n            to be inserted into the table.\n        table (str): The name of the target table.\n        schema (str, optional): The name of the target schema.\n        if_exists (str, Optional): What to do if the table already exists.\n            One of 'replace', 'skip', and 'fail'.\n        if_empty (str, optional): What to do if the input `DataFrame` is empty.\n            Defaults to 'warn'.\n        credentials_secret (str, optional): The name of the secret storing\n            the credentials. Defaults to None.\n            More info on: https://docs.prefect.io/concepts/blocks/\n        config_key (str, optional): The key in the viadot config holding relevant\n            credentials. Defaults to None.\n\n    Example:\n        ```python\n        from prefect_viadot.tasks df_to_databricks\n        from prefect import flow\n        import pandas as pd\n\n        @flow\n        def insert_df_into_databricks():\n            list = [{\"id\":\"1\", \"name\":\"Joe\"}]\n            df = pd.DataFrame(list)\n            insert = df_to_databricks(\n                df=df,\n                schema=\"prefect_viadot_test\"\n                table=\"test\",\n                if_exists=\"replace\"\n            )\n            return insert\n\n        insert_df_into_databricks()\n        ```\n    \"\"\"\n    if not (credentials_secret or config_key):\n        raise MissingSourceCredentialsError\n\n    credentials = get_credentials(credentials_secret)\n    databricks = Databricks(\n        credentials=credentials,\n        config_key=config_key,\n    )\n    if schema and not databricks._check_if_schema_exists(schema):\n        databricks.create_schema(schema)\n    databricks.create_table_from_pandas(\n        df=df, schema=schema, table=table, if_exists=if_exists, if_empty=if_empty\n    )\n</code></pre>"},{"location":"references/orchestration/prefect/tasks/#viadot.orchestration.prefect.tasks.df_to_minio","title":"<code>viadot.orchestration.prefect.tasks.df_to_minio(df, path, credentials_secret=None, config_key=None, basename_template=None, if_exists='error')</code>","text":"<p>Task for uploading the contents of a pandas DataFrame to MinIO.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Pandas dataframe to be uploaded.</p> required <code>path</code> <code>str</code> <p>Path to the MinIO file/folder.</p> required <code>credentials_secret</code> <code>str</code> <p>The name of the secret storing the credentials. Defaults to None. More info on: https://docs.prefect.io/concepts/blocks/</p> <code>None</code> <code>config_key</code> <code>str</code> <p>The key in the viadot config holding relevant credentials. Defaults to None.</p> <code>None</code> <code>basename_template</code> <code>str</code> <p>A template string used to generate basenames of written data files. The token '{i}' will be replaced with an automatically incremented integer. Defaults to None.</p> <code>None</code> Source code in <code>src/viadot/orchestration/prefect/tasks/minio.py</code> <pre><code>@task(retries=3, retry_delay_seconds=10, timeout_seconds=60 * 60)\ndef df_to_minio(\n    df: pd.DataFrame,\n    path: str,\n    credentials_secret: str | None = None,\n    config_key: str | None = None,\n    basename_template: str | None = None,\n    if_exists: Literal[\"error\", \"delete_matching\", \"overwrite_or_ignore\"] = \"error\",\n) -&gt; None:\n    \"\"\"Task for uploading the contents of a pandas DataFrame to MinIO.\n\n    Args:\n        df (pd.DataFrame): Pandas dataframe to be uploaded.\n        path (str): Path to the MinIO file/folder.\n        credentials_secret (str, optional): The name of the secret storing\n            the credentials. Defaults to None. More info on:\n            https://docs.prefect.io/concepts/blocks/\n        config_key (str, optional): The key in the viadot config holding relevant\n            credentials. Defaults to None.\n        basename_template (str, optional): A template string used to generate\n            basenames of written data files. The token '{i}' will be replaced with\n            an automatically incremented integer. Defaults to None.\n        if_exists (Literal[\"error\", \"delete_matching\", \"overwrite_or_ignore\"],\n            optional). What to do if the dataset already exists. Defaults to \"error\".\n    \"\"\"\n    if not (credentials_secret or config_key):\n        raise MissingSourceCredentialsError\n\n    logger = get_run_logger()\n\n    credentials = get_source_credentials(config_key) or get_credentials(\n        credentials_secret\n    )\n    minio = MinIO(credentials=credentials)\n\n    minio.from_df(\n        df=df, path=path, if_exists=if_exists, basename_template=basename_template\n    )\n\n    logger.info(\"Data has been uploaded successfully.\")\n</code></pre>"},{"location":"references/orchestration/prefect/tasks/#viadot.orchestration.prefect.tasks.df_to_redshift_spectrum","title":"<code>viadot.orchestration.prefect.tasks.df_to_redshift_spectrum(df, to_path, schema_name, table, extension='.parquet', if_exists='overwrite', partition_cols=None, index=False, compression=None, sep=',', description=None, config_key=None, credentials_secret=None, **kwargs)</code>","text":"<p>Task to upload a pandas <code>DataFrame</code> to a csv or parquet file.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The Pandas DataFrame to ingest into Redshift Spectrum.</p> required <code>to_path</code> <code>str</code> <p>Path to a S3 folder where the table will be located. If needed, a bottom-level directory named f\"{table}\" is automatically created, so that files are always located in a folder named the same as the table.</p> required <code>schema_name</code> <code>str</code> <p>AWS Glue catalog database name.</p> required <code>table</code> <code>str</code> <p>AWS Glue catalog table name.</p> required <code>partition_cols</code> <code>list[str]</code> <p>List of column names that will be used to create partitions. Only takes effect if dataset=True.</p> <code>None</code> <code>extension</code> <code>str</code> <p>Required file type. Accepted file formats are 'csv' and 'parquet'.</p> <code>'.parquet'</code> <code>if_exists</code> <code>str</code> <p>'overwrite' to recreate any possible existing table or 'append' to keep any possible existing table. Defaults to overwrite.</p> <code>'overwrite'</code> <code>partition_cols</code> <code>list[str]</code> <p>List of column names that will be used to create partitions. Only takes effect if dataset=True. Defaults to None.</p> <code>None</code> <code>index</code> <code>bool</code> <p>Write row names (index). Defaults to False.</p> <code>False</code> <code>compression</code> <code>str</code> <p>Compression style (None, snappy, gzip, zstd).</p> <code>None</code> <code>sep</code> <code>str</code> <p>Field delimiter for the output file. Applies only to '.csv' extension. Defaults to ','.</p> <code>','</code> <code>description</code> <code>str</code> <p>AWS Glue catalog table description.</p> <code>None</code> <code>config_key</code> <code>str</code> <p>The key in the viadot config holding relevant credentials. Defaults to None.</p> <code>None</code> <code>credentials_secret</code> <code>str</code> <p>The name of a secret block in Prefect that stores AWS credentials. Defaults to None.</p> <code>None</code> <code>kwargs</code> <code>dict[str, Any] | None</code> <p>The parameters to pass in awswrangler to_parquet/to_csv function.</p> <code>{}</code> Source code in <code>src/viadot/orchestration/prefect/tasks/redshift_spectrum.py</code> <pre><code>@task(retries=3, retry_delay_seconds=10, timeout_seconds=60 * 60)\ndef df_to_redshift_spectrum(  # noqa: PLR0913\n    df: pd.DataFrame,\n    to_path: str,\n    schema_name: str,\n    table: str,\n    extension: str = \".parquet\",\n    if_exists: Literal[\"overwrite\", \"append\"] = \"overwrite\",\n    partition_cols: list[str] | None = None,\n    index: bool = False,\n    compression: str | None = None,\n    sep: str = \",\",\n    description: str | None = None,\n    config_key: str | None = None,\n    credentials_secret: str | None = None,\n    **kwargs: dict[str, Any] | None,\n) -&gt; None:\n    \"\"\"Task to upload a pandas `DataFrame` to a csv or parquet file.\n\n    Args:\n        df (pd.DataFrame): The Pandas DataFrame to ingest into Redshift Spectrum.\n        to_path (str): Path to a S3 folder where the table will be located. If needed,\n            a bottom-level directory named f\"{table}\" is automatically created, so\n            that files are always located in a folder named the same as the table.\n        schema_name (str): AWS Glue catalog database name.\n        table (str): AWS Glue catalog table name.\n        partition_cols (list[str]): List of column names that will be used to create\n            partitions. Only takes effect if dataset=True.\n        extension (str): Required file type. Accepted file formats are 'csv' and\n            'parquet'.\n        if_exists (str, optional): 'overwrite' to recreate any possible existing table\n            or 'append' to keep any possible existing table. Defaults to overwrite.\n        partition_cols (list[str], optional): List of column names that will be used to\n            create partitions. Only takes effect if dataset=True. Defaults to None.\n        index (bool, optional): Write row names (index). Defaults to False.\n        compression (str, optional): Compression style (None, snappy, gzip, zstd).\n        sep (str, optional): Field delimiter for the output file. Applies only to '.csv'\n            extension. Defaults to ','.\n        description (str, optional): AWS Glue catalog table description.\n        config_key (str, optional): The key in the viadot config holding relevant\n            credentials. Defaults to None.\n        credentials_secret (str, optional): The name of a secret block in Prefect\n            that stores AWS credentials. Defaults to None.\n        kwargs: The parameters to pass in awswrangler to_parquet/to_csv function.\n    \"\"\"\n    if not (credentials_secret or config_key):\n        raise MissingSourceCredentialsError\n\n    credentials = get_source_credentials(config_key) or get_credentials(\n        credentials_secret\n    )\n\n    # Convert columns containing only null values to string to avoid errors\n    # during new table creation\n    null_columns = df.isna().all()\n    null_cols_list = null_columns[null_columns].index.tolist()\n    df[null_cols_list] = df[null_cols_list].astype(\"string\")\n\n    rs = RedshiftSpectrum(credentials=credentials, config_key=config_key)\n\n    rs.from_df(\n        df=df,\n        to_path=to_path,\n        schema=schema_name,\n        table=table,\n        extension=extension,\n        if_exists=if_exists,\n        partition_cols=partition_cols,\n        index=index,\n        compression=compression,\n        sep=sep,\n        description=description,\n        **kwargs,\n    )\n\n    logger = get_run_logger()\n    logger.info(\"Data has been uploaded successfully.\")\n</code></pre>"},{"location":"references/orchestration/prefect/tasks/#viadot.orchestration.prefect.tasks.duckdb_query","title":"<code>viadot.orchestration.prefect.tasks.duckdb_query(query, fetch_type='record', credentials=None, credentials_secret=None, config_key=None)</code>","text":"<p>Run query on a DuckDB database.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>(str, required)</code> <p>The query to execute on the DuckDB database.</p> required <code>fetch_type</code> <code>str</code> <p>In which form the data should be returned. Defaults to \"record\".</p> <code>'record'</code> <code>credentials</code> <code>dict[str, Any]</code> <p>Credentials to the Database. Defaults to None.</p> <code>None</code> <code>credentials_secret</code> <code>str</code> <p>The name of the secret storing credentials to the database. Defaults to None. More info on: https://docs.prefect.io/concepts/blocks/</p> <code>None</code> <code>config_key</code> <code>str</code> <p>The key in the viadot config holding relevant credentials to the database. Defaults to None.</p> <code>None</code> Source code in <code>src/viadot/orchestration/prefect/tasks/duckdb.py</code> <pre><code>@task(retries=3, retry_delay_seconds=10, timeout_seconds=60 * 60)\ndef duckdb_query(\n    query: str,\n    fetch_type: Literal[\"record\", \"dataframe\"] = \"record\",\n    # Specifying credentials in a dictionary is not recommended in viadot tasks,\n    # but in this case credentials can include only database name.\n    credentials: dict[str, Any] | None = None,\n    credentials_secret: str | None = None,\n    config_key: str | None = None,\n) -&gt; list[Record] | bool:\n    \"\"\"Run query on a DuckDB database.\n\n    Args:\n        query (str, required): The query to execute on the DuckDB database.\n        fetch_type (str, optional): In which form the data should be returned.\n            Defaults to \"record\".\n        credentials (dict[str, Any], optional): Credentials to the Database. Defaults to\n            None.\n        credentials_secret (str, optional): The name of the secret storing credentials\n            to the database. Defaults to None. More info on:\n            https://docs.prefect.io/concepts/blocks/\n        config_key (str, optional): The key in the viadot config holding relevant\n            credentials to the database. Defaults to None.\n    \"\"\"\n    if not (credentials or credentials_secret or config_key):\n        raise MissingSourceCredentialsError\n\n    logger = get_run_logger()\n\n    credentials = (\n        credentials\n        or get_source_credentials(config_key)\n        or get_credentials(credentials_secret)\n    )\n    duckdb = DuckDB(credentials=credentials)\n    result = duckdb.run_query(query=query, fetch_type=fetch_type)\n    logger.info(\"Query has been executed successfully.\")\n    return result\n</code></pre>"},{"location":"references/orchestration/prefect/tasks/#viadot.orchestration.prefect.tasks.epicor_to_df","title":"<code>viadot.orchestration.prefect.tasks.epicor_to_df(base_url, filters_xml, validate_date_filter=True, start_date_field='BegInvoiceDate', end_date_field='EndInvoiceDate', credentials_secret=None, config_key=None)</code>","text":"<p>Load the data from Epicor Prelude API into a pandas DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>base_url</code> <code>(str, required)</code> <p>Base url to Epicor.</p> required <code>filters_xml</code> <code>(str, required)</code> <p>Filters in form of XML. The date filter  is required.</p> required <code>validate_date_filter</code> <code>bool</code> <p>Whether or not validate xml date filters.     Defaults to True.</p> <code>True</code> <code>credentials_secret</code> <code>str</code> <p>The name of the secret storing the credentials. Defaults to None. More info on: https://docs.prefect.io/concepts/blocks/</p> <code>None</code> <code>config_key</code> <code>str</code> <p>The key in the viadot config holding relevant credentials. Defaults to None.</p> <code>None</code> Source code in <code>src/viadot/orchestration/prefect/tasks/epicor.py</code> <pre><code>@task(retries=3, retry_delay_seconds=10, timeout_seconds=60 * 60 * 3)\ndef epicor_to_df(\n    base_url: str,\n    filters_xml: str,\n    validate_date_filter: bool = True,\n    start_date_field: str = \"BegInvoiceDate\",\n    end_date_field: str = \"EndInvoiceDate\",\n    credentials_secret: str | None = None,\n    config_key: str | None = None,\n) -&gt; pd.DataFrame:\n    \"\"\"Load the data from Epicor Prelude API into a pandas DataFrame.\n\n    Args:\n        base_url (str, required): Base url to Epicor.\n        filters_xml (str, required): Filters in form of XML. The date filter\n             is required.\n        validate_date_filter (bool, optional): Whether or not validate xml date filters.\n                Defaults to True.\n        start_date_field (str, optional) The name of filters field containing\n            start date. Defaults to \"BegInvoiceDate\".\n        end_date_field (str, optional) The name of filters field containing end date.\n                Defaults to \"EndInvoiceDate\".\n        credentials_secret (str, optional): The name of the secret storing\n            the credentials. Defaults to None.\n            More info on: https://docs.prefect.io/concepts/blocks/\n        config_key (str, optional): The key in the viadot config holding relevant\n            credentials. Defaults to None.\n\n    \"\"\"\n    if not (credentials_secret or config_key):\n        raise MissingSourceCredentialsError\n\n    logger = get_run_logger()\n\n    credentials = get_source_credentials(config_key) or get_credentials(\n        credentials_secret\n    )\n    epicor = Epicor(\n        credentials=credentials,\n        base_url=base_url,\n        validate_date_filter=validate_date_filter,\n        start_date_field=start_date_field,\n        end_date_field=end_date_field,\n    )\n    df = epicor.to_df(filters_xml=filters_xml)\n    nrows = df.shape[0]\n    ncols = df.shape[1]\n\n    logger.info(\n        f\"Successfully downloaded {nrows} rows and {ncols} columns of data to a DataFrame.\"\n    )\n    return df\n</code></pre>"},{"location":"references/orchestration/prefect/tasks/#viadot.orchestration.prefect.tasks.eurostat_to_df","title":"<code>viadot.orchestration.prefect.tasks.eurostat_to_df(dataset_code, params=None, columns=None, tests=None)</code>","text":"<p>Task for creating a pandas DataFrame from Eurostat HTTPS REST API.</p> <p>This function serves as an intermediate wrapper between the prefect flow and the Eurostat connector: - Instantiates an Eurostat Cloud API connector. - Creates and returns a pandas DataFrame with the response from the API.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_code</code> <code>str</code> <p>The code of the Eurostat dataset that you would like to download.</p> required <code>params</code> <code>dict[str, str] | None</code> <p>A dictionary with optional URL parameters. Each key is a parameter ID, and the value is a specific parameter code, e.g., <code>params = {'unit': 'EUR'}</code> where \"unit\" is the parameter, and \"EUR\" is the code. Only one code per parameter is allowed. Defaults to None.</p> <code>None</code> <code>columns</code> <code>list[str] | None</code> <p>A list of column names (as strings) that are required from the dataset. Filters the DataFrame to only include the specified columns. Defaults to None.</p> <code>None</code> <code>tests</code> <code>dict | None</code> <p>A dictionary containing test cases for the data, including: - <code>column_size</code>: dict{column: size} - <code>column_unique_values</code>: list[columns] - <code>column_list_to_match</code>: list[columns] - <code>dataset_row_count</code>: dict{'min': number, 'max': number} - <code>column_match_regex</code>: dict{column: 'regex'} - <code>column_sum</code>: dict{column: {'min': number, 'max': number}}. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A pandas DataFrame containing the data retrieved from the Eurostat API.</p> Source code in <code>src/viadot/orchestration/prefect/tasks/eurostat.py</code> <pre><code>@task(retries=3, retry_delay_seconds=10, timeout_seconds=60 * 60)\ndef eurostat_to_df(\n    dataset_code: str,\n    params: dict[str, str] | None = None,\n    columns: list[str] | None = None,\n    tests: dict | None = None,\n) -&gt; pd.DataFrame:\n    \"\"\"Task for creating a pandas DataFrame from Eurostat HTTPS REST API.\n\n    This function serves as an intermediate wrapper between the prefect flow\n    and the Eurostat connector:\n    - Instantiates an Eurostat Cloud API connector.\n    - Creates and returns a pandas DataFrame with the response from the API.\n\n    Args:\n        dataset_code (str): The code of the Eurostat dataset that you would like\n            to download.\n        params (dict[str, str] | None, optional):\n            A dictionary with optional URL parameters. Each key is a parameter ID,\n            and the value is a specific parameter code, e.g.,\n            `params = {'unit': 'EUR'}` where \"unit\" is the parameter, and \"EUR\" is\n            the code. Only one code per parameter is allowed. Defaults to None.\n        columns (list[str] | None, optional):\n            A list of column names (as strings) that are required from the dataset.\n            Filters the DataFrame to only include the specified columns.\n            Defaults to None.\n        tests (dict | None, optional):\n            A dictionary containing test cases for the data, including:\n            - `column_size`: dict{column: size}\n            - `column_unique_values`: list[columns]\n            - `column_list_to_match`: list[columns]\n            - `dataset_row_count`: dict{'min': number, 'max': number}\n            - `column_match_regex`: dict{column: 'regex'}\n            - `column_sum`: dict{column: {'min': number, 'max': number}}.\n            Defaults to None.\n\n    Returns:\n        pd.DataFrame:\n            A pandas DataFrame containing the data retrieved from the Eurostat API.\n    \"\"\"\n    return Eurostat(\n        dataset_code=dataset_code, params=params, columns=columns, tests=tests\n    ).to_df()\n</code></pre>"},{"location":"references/orchestration/prefect/tasks/#viadot.orchestration.prefect.tasks.exchange_rates_to_df","title":"<code>viadot.orchestration.prefect.tasks.exchange_rates_to_df(currency='USD', credentials_secret=None, config_key=None, start_date=datetime.today().strftime('%Y-%m-%d'), end_date=datetime.today().strftime('%Y-%m-%d'), symbols=None, tests=None)</code>","text":"<p>Loads exchange rates from the Exchange Rates API into a pandas DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>currency</code> <code>Currency</code> <p>Base currency to which prices of searched currencies are related. Defaults to \"USD\".</p> <code>'USD'</code> <code>credentials_secret</code> <code>str</code> <p>The name of the secret storing the credentials. Defaults to None. More info on: https://docs.prefect.io/concepts/blocks/</p> <code>None</code> <code>config_key</code> <code>str</code> <p>The key in the viadot config holding relevant credentials. Defaults to None.</p> <code>None</code> <code>start_date</code> <code>str</code> <p>Initial date for data search. Data range is start_date -&gt; end_date, supported format 'yyyy-mm-dd'. Defaults to datetime.today().strftime(\"%Y-%m-%d\").</p> <code>strftime('%Y-%m-%d')</code> <code>end_date</code> <code>str</code> <p>See above. Defaults to datetime.today().strftime(\"%Y-%m-%d\").</p> <code>strftime('%Y-%m-%d')</code> <code>symbols</code> <code>list[str]</code> <p>List of ISO codes of currencies for which exchange rates from base currency will be fetched. Defaults to [\"USD\",\"EUR\",\"GBP\",\"CHF\",\"PLN\",\"DKK\",\"COP\",\"CZK\",\"SEK\",\"NOK\",\"ISK\"].</p> <code>None</code> <code>tests</code> <code>dict[str]</code> <p>A dictionary with optional list of tests to verify the output dataframe. If defined, triggers the <code>validate</code> function from viadot.utils. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: The pandas <code>DataFrame</code> containing data from the file.</p> Source code in <code>src/viadot/orchestration/prefect/tasks/exchange_rates.py</code> <pre><code>@task(retries=3, retry_delay_seconds=10, timeout_seconds=60 * 60)\ndef exchange_rates_to_df(\n    currency: Currency = \"USD\",\n    credentials_secret: str | None = None,\n    config_key: str | None = None,\n    start_date: str = datetime.today().strftime(\"%Y-%m-%d\"),\n    end_date: str = datetime.today().strftime(\"%Y-%m-%d\"),\n    symbols: list[str] | None = None,\n    tests: dict | None = None,\n) -&gt; pd.DataFrame:\n    \"\"\"Loads exchange rates from the Exchange Rates API into a pandas DataFrame.\n\n    Args:\n        currency (Currency, optional): Base currency to which prices of searched\n            currencies are related. Defaults to \"USD\".\n        credentials_secret (str, optional): The name of the secret storing\n            the credentials. Defaults to None.\n            More info on: https://docs.prefect.io/concepts/blocks/\n        config_key (str, optional): The key in the viadot config holding relevant\n            credentials.\n            Defaults to None.\n        start_date (str, optional): Initial date for data search.\n            Data range is start_date -&gt; end_date,\n            supported format 'yyyy-mm-dd'.\n            Defaults to datetime.today().strftime(\"%Y-%m-%d\").\n        end_date (str, optional): See above.\n            Defaults to datetime.today().strftime(\"%Y-%m-%d\").\n        symbols (list[str], optional): List of ISO codes of currencies for which\n            exchange rates from base currency will be fetched. Defaults to\n            [\"USD\",\"EUR\",\"GBP\",\"CHF\",\"PLN\",\"DKK\",\"COP\",\"CZK\",\"SEK\",\"NOK\",\"ISK\"].\n        tests (dict[str], optional): A dictionary with optional list of tests\n            to verify the output dataframe. If defined, triggers the `validate` function\n            from viadot.utils. Defaults to None.\n\n    Returns:\n        pd.DataFrame: The pandas `DataFrame` containing data from the file.\n    \"\"\"\n    if not (credentials_secret or config_key):\n        raise MissingSourceCredentialsError\n\n    if not symbols:\n        symbols = [\n            \"USD\",\n            \"EUR\",\n            \"GBP\",\n            \"CHF\",\n            \"PLN\",\n            \"DKK\",\n            \"COP\",\n            \"CZK\",\n            \"SEK\",\n            \"NOK\",\n            \"ISK\",\n        ]\n\n    if not config_key:\n        credentials = get_credentials(credentials_secret)\n\n    e = ExchangeRates(\n        currency=currency,\n        start_date=start_date,\n        end_date=end_date,\n        symbols=symbols,\n        credentials=credentials,\n        config_key=config_key,\n    )\n    return e.to_df(tests=tests)\n</code></pre>"},{"location":"references/orchestration/prefect/tasks/#viadot.orchestration.prefect.tasks.genesys_to_df","title":"<code>viadot.orchestration.prefect.tasks.genesys_to_df(config_key=None, azure_key_vault_secret=None, verbose=None, endpoint=None, environment='mypurecloud.de', queues_ids=None, view_type=None, view_type_time_sleep=None, post_data_list=None, time_between_api_call=0.5, normalization_sep='.', drop_duplicates=False, validate_df_dict=None)</code>","text":"<p>Task to download data from Genesys Cloud API.</p> <p>Parameters:</p> Name Type Description Default <code>config_key</code> <code>str</code> <p>The key in the viadot config holding relevant credentials. Defaults to None.</p> <code>None</code> <code>azure_key_vault_secret</code> <code>Optional[str]</code> <p>The name of the Azure Key Vault secret where credentials are stored. Defaults to None.</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>Increase the details of the logs printed on the     screen. Defaults to False.</p> <code>None</code> <code>endpoint</code> <code>Optional[str]</code> <p>Final end point to the API. Defaults to None.</p> <code>None</code> <code>environment</code> <code>str</code> <p>the domain that appears for Genesys Cloud Environment based on the location of your Genesys Cloud organization. Defaults to \"mypurecloud.de\".</p> <code>'mypurecloud.de'</code> <code>queues_ids</code> <code>Optional[List[str]]</code> <p>List of queues ids to consult the     members. Defaults to None.</p> <code>None</code> <code>view_type</code> <code>Optional[str]</code> <p>The type of view export job to be created. Defaults to None.</p> <code>None</code> <code>view_type_time_sleep</code> <code>Optional[int]</code> <p>Waiting time to retrieve data from Genesys Cloud API. Defaults to None.</p> <code>None</code> <code>post_data_list</code> <code>Optional[List[Dict[str, Any]]]</code> <p>List of string templates to generate json body in POST calls to the API. Defaults to None.</p> <code>None</code> <code>time_between_api_call</code> <code>int</code> <p>The time, in seconds, to sleep the call to the API. Defaults to 0.5.</p> <code>0.5</code> <code>normalization_sep</code> <code>str</code> <p>Nested records will generate names separated by sep. Defaults to \".\".</p> <code>'.'</code> <code>drop_duplicates</code> <code>bool</code> <p>Remove duplicates from the DataFrame. Defaults to False.</p> <code>False</code> <code>validate_df_dict</code> <code>Optional[Dict[str, Any]]</code> <p>A dictionary with optional list of tests to verify the output dataframe. Defaults to None.</p> <code>None</code> <p>Examples:</p> <p>data_frame = genesys_to_df(     config_key=config_key,     azure_key_vault_secret=azure_key_vault_secret,     verbose=verbose,     endpoint=endpoint,     environment=environment,     queues_ids=queues_ids,     view_type=view_type,     view_type_time_sleep=view_type_time_sleep,     post_data_list=post_data_list,     normalization_sep=normalization_sep,     validate_df_dict=validate_df_dict, )</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: The response data as a pandas DataFrame.</p> Source code in <code>src/viadot/orchestration/prefect/tasks/genesys.py</code> <pre><code>@task(retries=3, log_prints=True, retry_delay_seconds=10, timeout_seconds=2 * 60 * 60)\ndef genesys_to_df(  # noqa: PLR0913\n    config_key: str | None = None,\n    azure_key_vault_secret: str | None = None,\n    verbose: bool | None = None,\n    endpoint: str | None = None,\n    environment: str = \"mypurecloud.de\",\n    queues_ids: list[str] | None = None,\n    view_type: str | None = None,\n    view_type_time_sleep: int | None = None,\n    post_data_list: list[dict[str, Any]] | None = None,\n    time_between_api_call: float = 0.5,\n    normalization_sep: str = \".\",\n    drop_duplicates: bool = False,\n    validate_df_dict: dict[str, Any] | None = None,\n) -&gt; pd.DataFrame:\n    \"\"\"Task to download data from Genesys Cloud API.\n\n    Args:\n        config_key (str, optional): The key in the viadot config holding relevant\n            credentials. Defaults to None.\n        azure_key_vault_secret (Optional[str], optional): The name of the Azure Key\n            Vault secret where credentials are stored. Defaults to None.\n        verbose (bool, optional): Increase the details of the logs printed on the\n                screen. Defaults to False.\n        endpoint (Optional[str], optional): Final end point to the API.\n            Defaults to None.\n        environment (str, optional): the domain that appears for Genesys Cloud\n            Environment based on the location of your Genesys Cloud organization.\n            Defaults to \"mypurecloud.de\".\n        queues_ids (Optional[List[str]], optional): List of queues ids to consult the\n                members. Defaults to None.\n        view_type (Optional[str], optional): The type of view export job to be created.\n            Defaults to None.\n        view_type_time_sleep (Optional[int], optional): Waiting time to retrieve data\n            from Genesys Cloud API. Defaults to None.\n        post_data_list (Optional[List[Dict[str, Any]]], optional): List of string\n            templates to generate json body in POST calls to the API. Defaults to None.\n        time_between_api_call (int, optional): The time, in seconds, to sleep the call\n            to the API. Defaults to 0.5.\n        normalization_sep (str, optional): Nested records will generate names separated\n            by sep. Defaults to \".\".\n        drop_duplicates (bool, optional): Remove duplicates from the DataFrame.\n            Defaults to False.\n        validate_df_dict (Optional[Dict[str, Any]], optional): A dictionary with\n            optional list of tests to verify the output dataframe. Defaults to None.\n\n    Examples:\n        data_frame = genesys_to_df(\n            config_key=config_key,\n            azure_key_vault_secret=azure_key_vault_secret,\n            verbose=verbose,\n            endpoint=endpoint,\n            environment=environment,\n            queues_ids=queues_ids,\n            view_type=view_type,\n            view_type_time_sleep=view_type_time_sleep,\n            post_data_list=post_data_list,\n            normalization_sep=normalization_sep,\n            validate_df_dict=validate_df_dict,\n        )\n\n    Returns:\n        pd.DataFrame: The response data as a pandas DataFrame.\n    \"\"\"\n    logger = get_run_logger()\n\n    if not (azure_key_vault_secret or config_key):\n        raise MissingSourceCredentialsError\n\n    if not config_key:\n        credentials = get_credentials(azure_key_vault_secret)\n\n    if endpoint is None:\n        msg = \"The API endpoint parameter was not defined.\"\n        raise APIError(msg)\n\n    genesys = Genesys(\n        credentials=credentials,\n        config_key=config_key,\n        verbose=verbose,\n        environment=environment,\n    )\n    logger.info(\"running `api_connection` method:\\n\")\n    genesys.api_connection(\n        endpoint=endpoint,\n        queues_ids=queues_ids,\n        view_type=view_type,\n        view_type_time_sleep=view_type_time_sleep,\n        post_data_list=post_data_list,\n        time_between_api_call=time_between_api_call,\n        normalization_sep=normalization_sep,\n    )\n    logger.info(\"running `to_df` method:\\n\")\n\n    return genesys.to_df(\n        drop_duplicates=drop_duplicates,\n        validate_df_dict=validate_df_dict,\n    )\n</code></pre>"},{"location":"references/orchestration/prefect/tasks/#viadot.orchestration.prefect.tasks.hubspot_to_df","title":"<code>viadot.orchestration.prefect.tasks.hubspot_to_df(endpoint, config_key=None, azure_key_vault_secret=None, filters=None, properties=None, nrows=1000)</code>","text":"<p>Task to download data from Hubspot API to a pandas DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>endpoint</code> <code>str</code> <p>API endpoint for an individual request.</p> required <code>config_key</code> <code>Optional[str]</code> <p>The key in the viadot config holding relevant credentials. Defaults to None.</p> <code>None</code> <code>azure_key_vault_secret</code> <code>Optional[str]</code> <p>The name of the Azure Key Vault secret where credentials are stored. Defaults to None.</p> <code>None</code> <code>filters</code> <code>Optional[List[Dict[str, Any]]]</code> <p>Filters defined for the API body in specific order. Defaults to None.</p> <code>None</code> <code>properties</code> <code>Optional[List[Any]]</code> <p>List of user-defined columns to be pulled from the API. Defaults to None.</p> <code>None</code> <code>nrows</code> <code>int</code> <p>Max number of rows to pull during execution. Defaults to 1000.</p> <code>1000</code> <p>Examples:</p> <p>data_frame = hubspot_to_df(     config_key=config_key,     azure_key_vault_secret=azure_key_vault_secret,     endpoint=endpoint,     filters=filters,     properties=properties,     nrows=nrows, )</p> <p>Raises:</p> Type Description <code>MissingSourceCredentialsError</code> <p>If no credentials have been provided.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: The response data as a pandas DataFrame.</p> Source code in <code>src/viadot/orchestration/prefect/tasks/hubspot.py</code> <pre><code>@task(retries=3, log_prints=True, retry_delay_seconds=10, timeout_seconds=60 * 60)\ndef hubspot_to_df(\n    endpoint: str,\n    config_key: str | None = None,\n    azure_key_vault_secret: str | None = None,\n    filters: list[dict[str, Any]] | None = None,\n    properties: list[Any] | None = None,\n    nrows: int = 1000,\n) -&gt; pd.DataFrame:\n    \"\"\"Task to download data from Hubspot API to a pandas DataFrame.\n\n    Args:\n        endpoint (str): API endpoint for an individual request.\n        config_key (Optional[str], optional): The key in the viadot config holding\n            relevant credentials. Defaults to None.\n        azure_key_vault_secret (Optional[str], optional): The name of the Azure Key\n            Vault secret where credentials are stored. Defaults to None.\n        filters (Optional[List[Dict[str, Any]]], optional): Filters defined for the API\n            body in specific order. Defaults to None.\n        properties (Optional[List[Any]], optional): List of user-defined columns to be\n            pulled from the API. Defaults to None.\n        nrows (int, optional): Max number of rows to pull during execution.\n            Defaults to 1000.\n\n    Examples:\n        data_frame = hubspot_to_df(\n            config_key=config_key,\n            azure_key_vault_secret=azure_key_vault_secret,\n            endpoint=endpoint,\n            filters=filters,\n            properties=properties,\n            nrows=nrows,\n        )\n\n    Raises:\n        MissingSourceCredentialsError: If no credentials have been provided.\n\n    Returns:\n        pd.DataFrame: The response data as a pandas DataFrame.\n    \"\"\"\n    if not (azure_key_vault_secret or config_key):\n        raise MissingSourceCredentialsError\n\n    if not config_key:\n        credentials = get_credentials(azure_key_vault_secret)\n\n    hubspot = Hubspot(\n        credentials=credentials,\n        config_key=config_key,\n    )\n    hubspot.api_connection(\n        endpoint=endpoint,\n        filters=filters,\n        properties=properties,\n        nrows=nrows,\n    )\n\n    return hubspot.to_df()\n</code></pre>"},{"location":"references/orchestration/prefect/tasks/#viadot.orchestration.prefect.tasks.luma_ingest_task","title":"<code>viadot.orchestration.prefect.tasks.luma_ingest_task(metadata_dir_path, luma_url='http://localhost:8000', metadata_kind='model', follow=False, env=None, shell='bash', return_all=True, stream_level=logging.INFO, max_retries=3, logger=None, raise_on_failure=True)</code>  <code>async</code>","text":"<p>Runs Luma ingestion by sending dbt artifacts to Luma ingestion API.</p> <p>Parameters:</p> Name Type Description Default <code>metadata_dir_path</code> <code>str | Path</code> <p>The path to the directory containing metadata files. In the case of dbt, it's dbt project's <code>target</code> directory, which contains dbt artifacts (<code>sources.json</code>, <code>catalog.json</code>, <code>manifest.json</code>, and <code>run_results.json</code>).</p> required <code>luma_url</code> <code>str</code> <p>The URL of the Luma instance to ingest into.</p> <code>'http://localhost:8000'</code> <code>metadata_kind</code> <code>Literal['model', 'model_run']</code> <p>The kind of metadata to ingest. Either <code>model</code> or <code>model_run</code>.</p> <code>'model'</code> <code>follow</code> <code>bool</code> <p>Whether to follow the ingestion process until it's completed (by default, ingestion request is sent without awaiting for the response). By default, <code>False</code>.</p> <code>False</code> <code>env</code> <code>dict[str, Any] | None</code> <p>Dictionary of environment variables to use for the subprocess; can also be provided at runtime.</p> <code>None</code> <code>shell</code> <code>str</code> <p>Shell to run the command with.</p> <code>'bash'</code> <code>return_all</code> <code>bool</code> <p>Whether this task should return all lines of stdout as a list, or just the last line as a string.</p> <code>True</code> <code>stream_level</code> <code>int</code> <p>The logging level of the stream; defaults to 20; equivalent to <code>logging.INFO</code>.</p> <code>INFO</code> <code>max_retries</code> <code>int</code> <p>The maximum number of times to retry the task. Defaults to 3.</p> <code>3</code> <code>logger</code> <code>Logger | None</code> <p>The logger to use for logging the task's output. By default, the Prefect's task run logger.</p> <code>None</code> <code>raise_on_failure</code> <code>bool</code> <p>Whether to raise an exception if the command fails.</p> <code>True</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: Lines from stdout as a list.</p> Example <pre><code>from prefect import flow\nfrom viadot.tasks import luma_ingest_task\n\nmetadata_dir_path = \"${HOME}/dbt/my_dbt_project/target\"\n\n@flow\ndef example_luma_ingest_flow():\n    return luma_ingest_task(metadata_dir_path=metadata_dir_path)\n\nexample_luma_ingest_flow()\n</code></pre> Source code in <code>src/viadot/orchestration/prefect/tasks/luma.py</code> <pre><code>@task(retries=2, retry_delay_seconds=5, timeout_seconds=60 * 10)\nasync def luma_ingest_task(  # noqa: PLR0913\n    metadata_dir_path: str | Path,\n    luma_url: str = \"http://localhost:8000\",\n    metadata_kind: Literal[\"model\", \"model_run\"] = \"model\",\n    follow: bool = False,\n    env: dict[str, Any] | None = None,\n    shell: str = \"bash\",\n    return_all: bool = True,\n    stream_level: int = logging.INFO,\n    max_retries: int = 3,  # noqa: ARG001\n    logger: logging.Logger | None = None,\n    raise_on_failure: bool = True,\n) -&gt; list[str]:\n    \"\"\"Runs Luma ingestion by sending dbt artifacts to Luma ingestion API.\n\n    Args:\n        metadata_dir_path: The path to the directory containing metadata files.\n            In the case of dbt, it's dbt project's `target` directory,\n            which contains dbt artifacts (`sources.json`, `catalog.json`,\n            `manifest.json`, and `run_results.json`).\n        luma_url: The URL of the Luma instance to ingest into.\n        metadata_kind: The kind of metadata to ingest. Either `model` or `model_run`.\n        follow: Whether to follow the ingestion process until it's completed (by\n            default, ingestion request is sent without awaiting for the response). By\n            default, `False`.\n        env: Dictionary of environment variables to use for\n            the subprocess; can also be provided at runtime.\n        shell: Shell to run the command with.\n        return_all: Whether this task should return all lines of stdout as a list,\n            or just the last line as a string.\n        stream_level: The logging level of the stream;\n            defaults to 20; equivalent to `logging.INFO`.\n        max_retries: The maximum number of times to retry the task. Defaults to 3.\n        logger: The logger to use for logging the task's output. By default, the\n            Prefect's task run logger.\n        raise_on_failure: Whether to raise an exception if the command fails.\n\n    Returns:\n        list[str]: Lines from stdout as a list.\n\n    Example:\n        ```python\n        from prefect import flow\n        from viadot.tasks import luma_ingest_task\n\n        metadata_dir_path = \"${HOME}/dbt/my_dbt_project/target\"\n\n        @flow\n        def example_luma_ingest_flow():\n            return luma_ingest_task(metadata_dir_path=metadata_dir_path)\n\n        example_luma_ingest_flow()\n        ```\n    \"\"\"\n    if not logger:\n        logger = get_run_logger()\n\n    if isinstance(metadata_dir_path, str):\n        path_expanded = os.path.expandvars(metadata_dir_path)\n        metadata_dir_path = Path(path_expanded)\n\n    luma_command = \"ingest\" if metadata_kind == \"model\" else \"send-test-results\"\n    follow_flag = \"--follow\" if follow else \"\"\n    command = (\n        f\"luma dbt {luma_command} -m {metadata_dir_path} -l {luma_url} {follow_flag}\"\n    )\n\n    return await shell_run_command(\n        command=command,\n        env=env,\n        shell=shell,\n        return_all=return_all,\n        stream_level=stream_level,\n        logger=logger,\n        raise_on_failure=raise_on_failure,\n    )\n</code></pre>"},{"location":"references/orchestration/prefect/tasks/#viadot.orchestration.prefect.tasks.mediatool_to_df","title":"<code>viadot.orchestration.prefect.tasks.mediatool_to_df(config_key=None, azure_key_vault_secret=None, organization_ids=None, media_entries_columns=None)</code>","text":"<p>Task to download data from Mediatool API.</p> <p>Parameters:</p> Name Type Description Default <code>config_key</code> <code>str</code> <p>The key in the viadot config holding relevant credentials. Defaults to None.</p> <code>None</code> <code>azure_key_vault_secret</code> <code>str</code> <p>The name of the Azure Key Vault secret where credentials are stored. Defaults to None.</p> <code>None</code> <code>organization_ids</code> <code>list[str]</code> <p>List of organization IDs. Defaults to None.</p> <code>None</code> <code>media_entries_columns</code> <code>list[str]</code> <p>Columns to get from media entries. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: The response data as a Pandas Data Frame.</p> Source code in <code>src/viadot/orchestration/prefect/tasks/mediatool.py</code> <pre><code>@task(retries=3, log_prints=True, retry_delay_seconds=10, timeout_seconds=60 * 60)\ndef mediatool_to_df(\n    config_key: str | None = None,\n    azure_key_vault_secret: str | None = None,\n    organization_ids: list[str] | None = None,\n    media_entries_columns: list[str] | None = None,\n) -&gt; pd.DataFrame:\n    \"\"\"Task to download data from Mediatool API.\n\n    Args:\n        config_key (str, optional): The key in the viadot config holding relevant\n            credentials. Defaults to None.\n        azure_key_vault_secret (str, optional): The name of the Azure Key Vault secret\n            where credentials are stored. Defaults to None.\n        organization_ids (list[str], optional): List of organization IDs.\n            Defaults to None.\n        media_entries_columns (list[str], optional): Columns to get from media entries.\n            Defaults to None.\n\n    Returns:\n        pd.DataFrame: The response data as a Pandas Data Frame.\n    \"\"\"\n    if not (azure_key_vault_secret or config_key):\n        raise MissingSourceCredentialsError\n\n    if not config_key:\n        credentials = get_credentials(azure_key_vault_secret)\n\n    mediatool = Mediatool(\n        credentials=credentials,\n        config_key=config_key,\n    )\n\n    return mediatool.to_df(organization_ids, media_entries_columns)\n</code></pre>"},{"location":"references/orchestration/prefect/tasks/#viadot.orchestration.prefect.tasks.mindful_to_df","title":"<code>viadot.orchestration.prefect.tasks.mindful_to_df(config_key=None, azure_key_vault_secret=None, region='eu1', endpoint=None, date_interval=None, limit=1000)</code>","text":"<p>Task to download data from Mindful API.</p> <p>Parameters:</p> Name Type Description Default <code>config_key</code> <code>str</code> <p>The key in the viadot config holding relevant credentials. Defaults to None.</p> <code>None</code> <code>azure_key_vault_secret</code> <code>Optional[str]</code> <p>The name of the Azure Key Vault secret where credentials are stored. Defaults to None.</p> <code>None</code> <code>region</code> <code>Literal[us1, us2, us3, ca1, eu1, au1]</code> <p>Survey Dynamix region from where to interact with the mindful API. Defaults to \"eu1\" English (United Kingdom).</p> <code>'eu1'</code> <code>endpoint</code> <code>Optional[str]</code> <p>API endpoint for an individual request. Defaults to None.</p> <code>None</code> <code>date_interval</code> <code>Optional[List[date]]</code> <p>Date time range detailing the starting date and the ending date. If no range is passed, one day of data since this moment will be retrieved. Defaults to None.</p> <code>None</code> <code>limit</code> <code>int</code> <p>The number of matching interactions to return. Defaults to 1000.</p> <code>1000</code> <p>Examples:</p> <p>data_frame = mindful_to_df(     config_key=config_key,     azure_key_vault_secret=azure_key_vault_secret,     region=region,     endpoint=end,     date_interval=date_interval,     limit=limit, )</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: The response data as a pandas DataFrame.</p> Source code in <code>src/viadot/orchestration/prefect/tasks/mindful.py</code> <pre><code>@task(retries=3, log_prints=True, retry_delay_seconds=10, timeout_seconds=60 * 60)\ndef mindful_to_df(\n    config_key: str | None = None,\n    azure_key_vault_secret: str | None = None,\n    region: Literal[\"us1\", \"us2\", \"us3\", \"ca1\", \"eu1\", \"au1\"] = \"eu1\",\n    endpoint: str | None = None,\n    date_interval: list[date] | None = None,\n    limit: int = 1000,\n) -&gt; pd.DataFrame:\n    \"\"\"Task to download data from Mindful API.\n\n    Args:\n        config_key (str, optional): The key in the viadot config holding relevant\n            credentials. Defaults to None.\n        azure_key_vault_secret (Optional[str], optional): The name of the Azure Key\n            Vault secret where credentials are stored. Defaults to None.\n        region (Literal[us1, us2, us3, ca1, eu1, au1], optional): Survey Dynamix region\n            from where to interact with the mindful API. Defaults to \"eu1\" English\n            (United Kingdom).\n        endpoint (Optional[str], optional): API endpoint for an individual request.\n            Defaults to None.\n        date_interval (Optional[List[date]], optional): Date time range detailing the\n            starting date and the ending date. If no range is passed, one day of data\n            since this moment will be retrieved. Defaults to None.\n        limit (int, optional): The number of matching interactions to return.\n            Defaults to 1000.\n\n    Examples:\n        data_frame = mindful_to_df(\n            config_key=config_key,\n            azure_key_vault_secret=azure_key_vault_secret,\n            region=region,\n            endpoint=end,\n            date_interval=date_interval,\n            limit=limit,\n        )\n\n    Returns:\n        pd.DataFrame: The response data as a pandas DataFrame.\n    \"\"\"\n    logger = get_run_logger()\n\n    if not (azure_key_vault_secret or config_key):\n        raise MissingSourceCredentialsError\n\n    if not config_key:\n        credentials = get_credentials(azure_key_vault_secret)\n\n    if endpoint is None:\n        logger.warning(\n            \"The API endpoint parameter was not defined. The default value is 'surveys'.\"\n        )\n        endpoint = \"surveys\"\n\n    mindful = Mindful(\n        credentials=credentials,\n        config_key=config_key,\n        region=region,\n    )\n    mindful.api_connection(\n        endpoint=endpoint,\n        date_interval=date_interval,\n        limit=limit,\n    )\n\n    return mindful.to_df()\n</code></pre>"},{"location":"references/orchestration/prefect/tasks/#viadot.orchestration.prefect.tasks.outlook_to_df","title":"<code>viadot.orchestration.prefect.tasks.outlook_to_df(mailbox_name, config_key=None, azure_key_vault_secret=None, request_retries=10, start_date=None, end_date=None, limit=10000, address_limit=8000, outbox_list=None)</code>","text":"<p>Task for downloading data from Outlook API to a pandas DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>mailbox_name</code> <code>str</code> <p>Mailbox name.</p> required <code>config_key</code> <code>Optional[str]</code> <p>The key in the viadot config holding relevant credentials. Defaults to None.</p> <code>None</code> <code>azure_key_vault_secret</code> <code>Optional[str]</code> <p>The name of the Azure Key Vault secret where credentials are stored. Defaults to None.</p> <code>None</code> <code>request_retries</code> <code>int</code> <p>How many times retries to authorizate. Defaults to 10.</p> <code>10</code> <code>start_date</code> <code>Optional[str]</code> <p>A filtering start date parameter e.g. \"2022-01-01\". Defaults to None.</p> <code>None</code> <code>end_date</code> <code>Optional[str]</code> <p>A filtering end date parameter e.g. \"2022-01-02\". Defaults to None.</p> <code>None</code> <code>limit</code> <code>int</code> <p>Number of fetched top messages. Defaults to 10000.</p> <code>10000</code> <code>address_limit</code> <code>int</code> <p>The maximum number of accepted characters in the sum of all email names. Defaults to 8000.</p> <code>8000</code> <code>outbox_list</code> <code>List[str]</code> <p>List of outbox folders to differentiate between Inboxes and Outboxes. Defaults to [\"Sent Items\"].</p> <code>None</code> <p>Examples:</p> <p>data_frame = mindful_to_df(     config_key=config_key,     azure_key_vault_secret=azure_key_vault_secret,     region=region,     endpoint=end,     date_interval=date_interval,     limit=limit, )</p> <p>Raises:</p> Type Description <code>MissingSourceCredentialsError</code> <p>If none credentials have been provided.</p> <code>APIError</code> <p>The mailbox name is a \"must\" requirement.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: The response data as a pandas DataFrame.</p> Source code in <code>src/viadot/orchestration/prefect/tasks/outlook.py</code> <pre><code>@task(retries=3, log_prints=True, retry_delay_seconds=10, timeout_seconds=60 * 60)\ndef outlook_to_df(\n    mailbox_name: str,\n    config_key: str | None = None,\n    azure_key_vault_secret: str | None = None,\n    request_retries: int = 10,\n    start_date: str | None = None,\n    end_date: str | None = None,\n    limit: int = 10000,\n    address_limit: int = 8000,\n    outbox_list: list[str] | None = None,\n) -&gt; pd.DataFrame:\n    \"\"\"Task for downloading data from Outlook API to a pandas DataFrame.\n\n    Args:\n        mailbox_name (str): Mailbox name.\n        config_key (Optional[str], optional): The key in the viadot config holding\n            relevant credentials. Defaults to None.\n        azure_key_vault_secret (Optional[str], optional): The name of the Azure Key\n            Vault secret where credentials are stored. Defaults to None.\n        request_retries (int, optional): How many times retries to authorizate.\n            Defaults to 10.\n        start_date (Optional[str], optional): A filtering start date parameter e.g.\n            \"2022-01-01\". Defaults to None.\n        end_date (Optional[str], optional): A filtering end date parameter e.g.\n            \"2022-01-02\". Defaults to None.\n        limit (int, optional): Number of fetched top messages. Defaults to 10000.\n        address_limit (int, optional): The maximum number of accepted characters in the\n            sum of all email names. Defaults to 8000.\n        outbox_list (List[str], optional): List of outbox folders to differentiate\n            between Inboxes and Outboxes. Defaults to [\"Sent Items\"].\n\n    Examples:\n        data_frame = mindful_to_df(\n            config_key=config_key,\n            azure_key_vault_secret=azure_key_vault_secret,\n            region=region,\n            endpoint=end,\n            date_interval=date_interval,\n            limit=limit,\n        )\n\n    Raises:\n        MissingSourceCredentialsError: If none credentials have been provided.\n        APIError: The mailbox name is a \"must\" requirement.\n\n    Returns:\n        pd.DataFrame: The response data as a pandas DataFrame.\n    \"\"\"\n    if not (azure_key_vault_secret or config_key):\n        raise MissingSourceCredentialsError\n\n    if not config_key:\n        credentials = get_credentials(azure_key_vault_secret)\n\n    if not outbox_list:\n        outbox_list = [\"Sent Items\"]\n\n    outlook = Outlook(\n        credentials=credentials,\n        config_key=config_key,\n    )\n    outlook.api_connection(\n        mailbox_name=mailbox_name,\n        request_retries=request_retries,\n        start_date=start_date,\n        end_date=end_date,\n        limit=limit,\n        address_limit=address_limit,\n        outbox_list=outbox_list,\n    )\n\n    return outlook.to_df()\n</code></pre>"},{"location":"references/orchestration/prefect/tasks/#viadot.orchestration.prefect.tasks.s3_upload_file","title":"<code>viadot.orchestration.prefect.tasks.s3_upload_file(from_path, to_path, credentials=None, config_key=None, credentials_secret=None)</code>","text":"<p>Task to upload a file to Amazon S3.</p> <p>Parameters:</p> Name Type Description Default <code>from_path</code> <code>str</code> <p>Path to local file(s) to be uploaded.</p> required <code>to_path</code> <code>str</code> <p>Path to the Amazon S3 file/folder.</p> required <code>credentials</code> <code>dict[str, Any]</code> <p>Credentials to the Amazon S3. Defaults to None.</p> <code>None</code> <code>config_key</code> <code>str</code> <p>The key in the viadot config holding relevant credentials. Defaults to None.</p> <code>None</code> <code>credentials_secret</code> <code>str</code> <p>The name of a secret block in Prefect that stores AWS credentials. Defaults to None.</p> <code>None</code> Example <pre><code>from prefect_viadot.tasks import s3_upload_file\nfrom prefect import flow\n\n@flow\ndef test_flow():\n    s3_upload_file(\n        from_path='test.parquet',\n        to_path=\"s3://bucket_name/test.parquet\",\n        credentials= {\n            'profile_name': 'your_profile'\n            'region_name': 'your_region'\n            'aws_access_key_id': 'your_access_key_id'\n            'aws_secret_access_key': 'your_secret_access_key'\n        }\n    )\n\ntest_flow()\n</code></pre> Source code in <code>src/viadot/orchestration/prefect/tasks/s3.py</code> <pre><code>@task(retries=3, retry_delay_seconds=10, timeout_seconds=60 * 60)\ndef s3_upload_file(\n    from_path: str,\n    to_path: str,\n    credentials: dict[str, Any] | None = None,\n    config_key: str | None = None,\n    credentials_secret: str | None = None,\n) -&gt; None:\n    \"\"\"Task to upload a file to Amazon S3.\n\n    Args:\n        from_path (str): Path to local file(s) to be uploaded.\n        to_path (str): Path to the Amazon S3 file/folder.\n        credentials (dict[str, Any], optional): Credentials to the Amazon S3.\n            Defaults to None.\n        config_key (str, optional): The key in the viadot config holding relevant\n            credentials. Defaults to None.\n        credentials_secret (str, optional): The name of a secret block in Prefect\n            that stores AWS credentials. Defaults to None.\n\n    Example:\n        ```python\n        from prefect_viadot.tasks import s3_upload_file\n        from prefect import flow\n\n        @flow\n        def test_flow():\n            s3_upload_file(\n                from_path='test.parquet',\n                to_path=\"s3://bucket_name/test.parquet\",\n                credentials= {\n                    'profile_name': 'your_profile'\n                    'region_name': 'your_region'\n                    'aws_access_key_id': 'your_access_key_id'\n                    'aws_secret_access_key': 'your_secret_access_key'\n                }\n            )\n\n        test_flow()\n        ```\n    \"\"\"\n    if not (credentials_secret or config_key):\n        raise MissingSourceCredentialsError\n\n    credentials = get_source_credentials(config_key) or get_credentials(\n        credentials_secret\n    )\n\n    s3 = S3(credentials=credentials, config_key=config_key)\n\n    s3.upload(from_path=from_path, to_path=to_path)\n\n    logger = get_run_logger()\n    logger.info(\"Data has been uploaded successfully.\")\n</code></pre>"},{"location":"references/orchestration/prefect/tasks/#viadot.orchestration.prefect.tasks.salesforce_to_df","title":"<code>viadot.orchestration.prefect.tasks.salesforce_to_df(config_key=None, azure_key_vault_secret=None, env=None, domain=None, client_id=None, query=None, table=None, columns=None)</code>","text":"<p>Querying Salesforce and saving data as the data frame.</p> <p>Parameters:</p> Name Type Description Default <code>config_key</code> <code>str</code> <p>The key in the viadot config holding relevant credentials. Defaults to None.</p> <code>None</code> <code>azure_key_vault_secret</code> <code>str</code> <p>The name of the Azure Key Vault secret where credentials are stored. Defaults to None.</p> <code>None</code> <code>env</code> <code>str</code> <p>Environment information, provides information about credential and connection configuration. Defaults to 'DEV'.</p> <code>None</code> <code>domain</code> <code>str</code> <p>Domain of a connection. defaults to 'test' (sandbox). Can only be added if built-in username/password/security token is provided. Defaults to None.</p> <code>None</code> <code>client_id</code> <code>str</code> <p>Client id to keep the track of API calls. Defaults to None.</p> <code>None</code> <code>query</code> <code>str</code> <p>Query for download the data if specific download is needed. Defaults to None.</p> <code>None</code> <code>table</code> <code>str</code> <p>Table name. Can be used instead of query. Defaults to None.</p> <code>None</code> <code>columns</code> <code>list[str]</code> <p>List of columns which are needed - table argument is needed. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: The response data as a pandas DataFrame.</p> Source code in <code>src/viadot/orchestration/prefect/tasks/salesforce.py</code> <pre><code>@task(retries=3, log_prints=True, retry_delay_seconds=10, timeout_seconds=60 * 60)\ndef salesforce_to_df(\n    config_key: str | None = None,\n    azure_key_vault_secret: str | None = None,\n    env: str | None = None,\n    domain: str | None = None,\n    client_id: str | None = None,\n    query: str | None = None,\n    table: str | None = None,\n    columns: list[str] | None = None,\n) -&gt; pd.DataFrame:\n    \"\"\"Querying Salesforce and saving data as the data frame.\n\n    Args:\n        config_key (str, optional): The key in the viadot config holding relevant\n            credentials. Defaults to None.\n        azure_key_vault_secret (str, optional): The name of the Azure Key Vault secret\n            where credentials are stored. Defaults to None.\n        env (str, optional): Environment information, provides information about\n            credential and connection configuration. Defaults to 'DEV'.\n        domain (str, optional): Domain of a connection. defaults to 'test' (sandbox).\n            Can only be added if built-in username/password/security token is provided.\n            Defaults to None.\n        client_id (str, optional): Client id to keep the track of API calls.\n            Defaults to None.\n        query (str, optional): Query for download the data if specific download is\n            needed. Defaults to None.\n        table (str, optional): Table name. Can be used instead of query.\n            Defaults to None.\n        columns (list[str], optional): List of columns which are needed - table\n            argument is needed. Defaults to None.\n\n    Returns:\n        pd.DataFrame: The response data as a pandas DataFrame.\n    \"\"\"\n    if not (azure_key_vault_secret or config_key):\n        raise MissingSourceCredentialsError\n\n    if not config_key:\n        credentials = get_credentials(azure_key_vault_secret)\n\n    salesforce = Salesforce(\n        credentials=credentials,\n        config_key=config_key,\n        env=env,\n        domain=domain,\n        client_id=client_id,\n    )\n\n    return salesforce.to_df(query=query, table=table, columns=columns)\n</code></pre>"},{"location":"references/orchestration/prefect/tasks/#viadot.orchestration.prefect.tasks.sap_bw_to_df","title":"<code>viadot.orchestration.prefect.tasks.sap_bw_to_df(mdx_query, config_key=None, azure_key_vault_secret=None, mapping_dict=None)</code>","text":"<p>Task to download data from SAP BW to DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>mdx_query</code> <code>(str, required)</code> <p>The MDX query to be passed to connection.</p> required <code>config_key</code> <code>Optional[str]</code> <p>The key in the viadot config holding relevant credentials. Defaults to None.</p> <code>None</code> <code>azure_key_vault_secret</code> <code>Optional[str]</code> <p>The name of the Azure Key Vault secret where credentials are stored. Defaults to None.</p> <code>None</code> <code>mapping_dict</code> <code>dict[str, Any]</code> <p>Dictionary with original and new column names. Defaults to None.</p> <code>None</code> <p>Raises:</p> Type Description <code>MissingSourceCredentialsError</code> <p>If none credentials have been provided.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: The response data as a Pandas Data Frame.</p> Source code in <code>src/viadot/orchestration/prefect/tasks/sap_bw.py</code> <pre><code>@task(retries=3, log_prints=True, retry_delay_seconds=10, timeout_seconds=60 * 60)\ndef sap_bw_to_df(\n    mdx_query: str,\n    config_key: str | None = None,\n    azure_key_vault_secret: str | None = None,\n    mapping_dict: dict[str, Any] | None = None,\n) -&gt; pd.DataFrame:\n    \"\"\"Task to download data from SAP BW to DataFrame.\n\n    Args:\n        mdx_query (str, required): The MDX query to be passed to connection.\n        config_key (Optional[str], optional): The key in the viadot config holding\n            relevant credentials. Defaults to None.\n        azure_key_vault_secret (Optional[str], optional): The name of the Azure Key\n            Vault secret where credentials are stored. Defaults to None.\n        mapping_dict (dict[str, Any], optional): Dictionary with original and new\n            column names. Defaults to None.\n\n    Raises:\n        MissingSourceCredentialsError: If none credentials have been provided.\n\n\n    Returns:\n        pd.DataFrame: The response data as a Pandas Data Frame.\n    \"\"\"\n    if not (azure_key_vault_secret or config_key):\n        raise MissingSourceCredentialsError\n\n    if not config_key:\n        credentials = get_credentials(azure_key_vault_secret)\n\n    sap_bw = SAPBW(\n        credentials=credentials,\n        config_key=config_key,\n    )\n    sap_bw.api_connection(mdx_query=mdx_query)\n\n    return sap_bw.to_df(mapping_dict=mapping_dict)\n</code></pre>"},{"location":"references/orchestration/prefect/tasks/#viadot.orchestration.prefect.tasks.sap_rfc_to_df","title":"<code>viadot.orchestration.prefect.tasks.sap_rfc_to_df(rfc_unique_id, query=None, sep=None, func=None, replacement='-', rfc_total_col_width_character_limit=400, tests=None, credentials_secret=None, credentials=None, config_key=None, dynamic_date_symbols=['&lt;&lt;', '&gt;&gt;'], dynamic_date_format='%Y%m%d', dynamic_date_timezone='UTC')</code>","text":"<p>A task for querying SAP with SQL using the RFC protocol.</p> <p>Note that only a very limited subset of SQL is supported: - aliases - where clauses combined using the AND operator - limit &amp; offset</p> <p>Unsupported: - aggregations - joins - subqueries - etc.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>The query to be executed with pyRFC.</p> <code>None</code> <code>sep</code> <code>str</code> <p>The separator to use when reading query results. If not provided, multiple options are automatically tried. Defaults to None.</p> <code>None</code> <code>func</code> <code>str</code> <p>SAP RFC function to use. Defaults to None.</p> <code>None</code> <code>replacement</code> <code>str</code> <p>In case of sep is on a columns, set up a new character to replace inside the string to avoid flow breakdowns. Defaults to \"-\".</p> <code>'-'</code> <code>rfc_total_col_width_character_limit</code> <code>int</code> <p>Number of characters by which query will be split in chunks in case of too many columns for RFC function. According to SAP documentation, the limit is 512 characters. However, we observed SAP raising an exception even on a slightly lower number of characters, so we add a safety margin. Defaults to 400.</p> <code>400</code> <code>rfc_unique_id</code> <code>list[str]</code> <p>Reference columns to merge chunks DataFrames. These columns must to be unique.</p> required <code>tests</code> <code>dict[str]</code> <p>A dictionary with optional list of tests     to verify the output dataframe. If defined, triggers the <code>validate</code>     function from viadot.utils. Defaults to None.</p> <code>None</code> <code>credentials_secret</code> <code>str</code> <p>The name of the secret that stores SAP credentials. Defaults to None. More info on: https://docs.prefect.io/concepts/blocks/</p> <code>None</code> <code>credentials</code> <code>dict[str, Any]</code> <p>Credentials to SAP. Defaults to None.</p> <code>None</code> <code>config_key</code> <code>str</code> <p>The key in the viadot config holding relevant credentials. Defaults to None.</p> <code>None</code> <code>dynamic_date_symbols</code> <code>list[str]</code> <p>Symbols used for dynamic date handling. Defaults to [\"&lt;&lt;\", \"&gt;&gt;\"].</p> <code>['&lt;&lt;', '&gt;&gt;']</code> <code>dynamic_date_format</code> <code>str</code> <p>Format used for dynamic date parsing. Defaults to \"%Y%m%d\".</p> <code>'%Y%m%d'</code> <code>dynamic_date_timezone</code> <code>str</code> <p>Timezone used for dynamic date processing. Defaults to \"UTC\".</p> <code>'UTC'</code> <p>Examples:</p> <p>sap_rfc_to_df(     ...     rfc_unique_id=[\"VBELN\", \"LPRIO\"],     ... )</p> Source code in <code>src/viadot/orchestration/prefect/tasks/sap_rfc.py</code> <pre><code>@task(retries=3, retry_delay_seconds=10, timeout_seconds=60 * 60 * 3)\ndef sap_rfc_to_df(  # noqa: PLR0913\n    rfc_unique_id: list[str],\n    query: str | None = None,\n    sep: str | None = None,\n    func: str | None = None,\n    replacement: str = \"-\",\n    rfc_total_col_width_character_limit: int = 400,\n    tests: dict[str, Any] | None = None,\n    credentials_secret: str | None = None,\n    credentials: dict[str, Any] | None = None,\n    config_key: str | None = None,\n    dynamic_date_symbols: list[str] = [\"&lt;&lt;\", \"&gt;&gt;\"],  # noqa: B006\n    dynamic_date_format: str = \"%Y%m%d\",\n    dynamic_date_timezone: str = \"UTC\",\n) -&gt; pd.DataFrame:\n    \"\"\"A task for querying SAP with SQL using the RFC protocol.\n\n    Note that only a very limited subset of SQL is supported:\n    - aliases\n    - where clauses combined using the AND operator\n    - limit &amp; offset\n\n    Unsupported:\n    - aggregations\n    - joins\n    - subqueries\n    - etc.\n\n    Args:\n        query (str): The query to be executed with pyRFC.\n        sep (str, optional): The separator to use when reading query results. If not\n            provided, multiple options are automatically tried. Defaults to None.\n        func (str, optional): SAP RFC function to use. Defaults to None.\n        replacement (str, optional): In case of sep is on a columns, set up a new\n            character to replace inside the string to avoid flow breakdowns. Defaults to\n            \"-\".\n        rfc_total_col_width_character_limit (int, optional): Number of characters by\n            which query will be split in chunks in case of too many columns for RFC\n            function. According to SAP documentation, the limit is 512 characters.\n            However, we observed SAP raising an exception even on a slightly lower\n            number of characters, so we add a safety margin. Defaults to 400.\n        rfc_unique_id (list[str]):\n            Reference columns to merge chunks DataFrames. These columns must to be\n            unique.\n        tests (dict[str], optional): A dictionary with optional list of tests\n                to verify the output dataframe. If defined, triggers the `validate`\n                function from viadot.utils. Defaults to None.\n        credentials_secret (str, optional): The name of the secret that stores SAP\n            credentials. Defaults to None.\n            More info on: https://docs.prefect.io/concepts/blocks/\n        credentials (dict[str, Any], optional): Credentials to SAP.\n            Defaults to None.\n        config_key (str, optional): The key in the viadot config holding relevant\n            credentials. Defaults to None.\n        dynamic_date_symbols (list[str], optional): Symbols used for dynamic date\n            handling. Defaults to [\"&lt;&lt;\", \"&gt;&gt;\"].\n        dynamic_date_format (str, optional): Format used for dynamic date parsing.\n            Defaults to \"%Y%m%d\".\n        dynamic_date_timezone (str, optional): Timezone used for dynamic date\n            processing. Defaults to \"UTC\".\n\n    Examples:\n        sap_rfc_to_df(\n            ...\n            rfc_unique_id=[\"VBELN\", \"LPRIO\"],\n            ...\n        )\n    \"\"\"\n    if not (credentials_secret or credentials or config_key):\n        raise MissingSourceCredentialsError\n\n    if query is None:\n        msg = \"Please provide the query.\"\n        raise ValueError(msg)\n    logger = get_run_logger()\n    logger.warning(\"If the column/set are not unique the table will be malformed.\")\n\n    credentials = credentials or get_credentials(credentials_secret)\n\n    sap = SAPRFC(\n        sep=sep,\n        replacement=replacement,\n        credentials=credentials,\n        func=func,\n        rfc_total_col_width_character_limit=rfc_total_col_width_character_limit,\n        rfc_unique_id=rfc_unique_id,\n        config_key=config_key,\n    )\n\n    query = sap._parse_dates(\n        query=query,\n        dynamic_date_symbols=dynamic_date_symbols,\n        dynamic_date_format=dynamic_date_format,\n        dynamic_date_timezone=dynamic_date_timezone,\n    )\n\n    sap.query(query)\n    logger.info(\"Downloading data from SAP to a DataFrame...\")\n    logger.debug(f\"Running query: \\n{sap.sql}.\")\n\n    df = sap.to_df(tests=tests)\n\n    if not df.empty:\n        logger.info(\"Data has been downloaded successfully.\")\n    elif df.empty:\n        logger.warn(\"Task finished but NO data was downloaded.\")\n    return df\n</code></pre>"},{"location":"references/orchestration/prefect/tasks/#viadot.orchestration.prefect.tasks.sftp_list","title":"<code>viadot.orchestration.prefect.tasks.sftp_list(config_key=None, azure_key_vault_secret=None, path=None, recursive=False, matching_path=None)</code>","text":"<p>Listing files in the SFTP server.</p> <p>Parameters:</p> Name Type Description Default <code>config_key</code> <code>str</code> <p>The key in the viadot config holding relevant credentials. Defaults to None.</p> <code>None</code> <code>azure_key_vault_secret</code> <code>str</code> <p>The name of the Azure Key Vault secret where credentials are stored. Defaults to None.</p> <code>None</code> <code>path</code> <code>str</code> <p>Destination path from where to get the structure. Defaults to None.</p> <code>None</code> <code>recursive</code> <code>bool</code> <p>Get the structure in deeper folders. Defaults to False.</p> <code>False</code> <code>matching_path</code> <code>str</code> <p>Filtering folders to return by a regex pattern. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>files_list</code> <code>list[str]</code> <p>List of files in the SFTP server.</p> Source code in <code>src/viadot/orchestration/prefect/tasks/sftp.py</code> <pre><code>@task(retries=3, log_prints=True, retry_delay_seconds=10, timeout_seconds=60 * 60)\ndef sftp_list(\n    config_key: str | None = None,\n    azure_key_vault_secret: str | None = None,\n    path: str | None = None,\n    recursive: bool = False,\n    matching_path: str | None = None,\n) -&gt; list[str]:\n    \"\"\"Listing files in the SFTP server.\n\n    Args:\n        config_key (str, optional): The key in the viadot config holding relevant\n            credentials. Defaults to None.\n        azure_key_vault_secret (str, optional): The name of the Azure Key Vault secret\n            where credentials are stored. Defaults to None.\n        path (str, optional): Destination path from where to get the structure.\n            Defaults to None.\n        recursive (bool, optional): Get the structure in deeper folders.\n            Defaults to False.\n        matching_path (str, optional): Filtering folders to return by a regex pattern.\n            Defaults to None.\n\n    Returns:\n        files_list (list[str]): List of files in the SFTP server.\n    \"\"\"\n    if not (azure_key_vault_secret or config_key):\n        raise MissingSourceCredentialsError\n\n    if not config_key:\n        credentials = get_credentials(azure_key_vault_secret)\n\n    sftp = Sftp(\n        credentials=credentials,\n        config_key=config_key,\n    )\n    sftp.get_connection()\n\n    return sftp.get_files_list(\n        path=path, recursive=recursive, matching_path=matching_path\n    )\n</code></pre>"},{"location":"references/orchestration/prefect/tasks/#viadot.orchestration.prefect.tasks.sftp_to_df","title":"<code>viadot.orchestration.prefect.tasks.sftp_to_df(config_key=None, azure_key_vault_secret=None, file_name=None, sep='\\t', columns=None)</code>","text":"<p>Querying SFTP server and saving data as the data frame.</p> <p>Parameters:</p> Name Type Description Default <code>config_key</code> <code>str</code> <p>The key in the viadot config holding relevant credentials. Defaults to None.</p> <code>None</code> <code>azure_key_vault_secret</code> <code>str</code> <p>The name of the Azure Key Vault secret where credentials are stored. Defaults to None.</p> <code>None</code> <code>file_name</code> <code>str</code> <p>Path to the file in SFTP server. Defaults to None.</p> <code>None</code> <code>sep</code> <code>str</code> <p>The separator to use to read the CSV file. Defaults to \"\\t\".</p> <code>'\\t'</code> <code>columns</code> <code>List[str]</code> <p>Columns to read from the file. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: The response data as a pandas DataFrame.</p> Source code in <code>src/viadot/orchestration/prefect/tasks/sftp.py</code> <pre><code>@task(retries=3, log_prints=True, retry_delay_seconds=10, timeout_seconds=60 * 60)\ndef sftp_to_df(\n    config_key: str | None = None,\n    azure_key_vault_secret: str | None = None,\n    file_name: str | None = None,\n    sep: str = \"\\t\",\n    columns: list[str] | None = None,\n) -&gt; pd.DataFrame:\n    r\"\"\"Querying SFTP server and saving data as the data frame.\n\n    Args:\n        config_key (str, optional): The key in the viadot config holding relevant\n            credentials. Defaults to None.\n        azure_key_vault_secret (str, optional): The name of the Azure Key Vault secret\n            where credentials are stored. Defaults to None.\n        file_name (str, optional): Path to the file in SFTP server. Defaults to None.\n        sep (str, optional): The separator to use to read the CSV file.\n            Defaults to \"\\t\".\n        columns (List[str], optional): Columns to read from the file. Defaults to None.\n\n    Returns:\n        pd.DataFrame: The response data as a pandas DataFrame.\n    \"\"\"\n    if not (azure_key_vault_secret or config_key):\n        raise MissingSourceCredentialsError\n\n    if not config_key:\n        credentials = get_credentials(azure_key_vault_secret)\n\n    sftp = Sftp(\n        credentials=credentials,\n        config_key=config_key,\n    )\n    sftp.get_connection()\n\n    return sftp.to_df(file_name=file_name, sep=sep, columns=columns)\n</code></pre>"},{"location":"references/orchestration/prefect/tasks/#viadot.orchestration.prefect.tasks.sharepoint_download_file","title":"<code>viadot.orchestration.prefect.tasks.sharepoint_download_file(url, to_path, credentials_secret=None, config_key=None)</code>","text":"<p>Download a file from Sharepoint.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL of the file to be downloaded.</p> required <code>to_path</code> <code>str</code> <p>Where to download the file.</p> required <code>credentials_secret</code> <code>str</code> <p>The name of the secret that stores Sharepoint credentials. Defaults to None.</p> <code>None</code> <code>credentials</code> <code>SharepointCredentials</code> <p>Sharepoint credentials.</p> required <code>config_key</code> <code>str</code> <p>The key in the viadot config holding relevant credentials.</p> <code>None</code> Source code in <code>src/viadot/orchestration/prefect/tasks/sharepoint.py</code> <pre><code>@task(retries=3, retry_delay_seconds=10, timeout_seconds=60 * 60)\ndef sharepoint_download_file(\n    url: str,\n    to_path: str,\n    credentials_secret: str | None = None,\n    config_key: str | None = None,\n) -&gt; None:\n    \"\"\"Download a file from Sharepoint.\n\n    Args:\n        url (str): The URL of the file to be downloaded.\n        to_path (str): Where to download the file.\n        credentials_secret (str, optional): The name of the secret that stores\n            Sharepoint credentials. Defaults to None.\n        credentials (SharepointCredentials, optional): Sharepoint credentials.\n        config_key (str, optional): The key in the viadot config holding relevant\n            credentials.\n    \"\"\"\n    if not (credentials_secret or config_key):\n        raise MissingSourceCredentialsError\n\n    logger = get_run_logger()\n\n    credentials = get_credentials(secret_name=credentials_secret)\n    s = Sharepoint(credentials=credentials, config_key=config_key)\n\n    logger.info(f\"Downloading data from {url}...\")\n    s.download_file(url=url, to_path=to_path)\n    logger.info(f\"Successfully downloaded data from {url}.\")\n</code></pre>"},{"location":"references/orchestration/prefect/tasks/#viadot.orchestration.prefect.tasks.sharepoint_to_df","title":"<code>viadot.orchestration.prefect.tasks.sharepoint_to_df(url, sheet_name=None, columns=None, tests=None, file_sheet_mapping=None, na_values=None, credentials_secret=None, config_key=None)</code>","text":"<p>Load an Excel file stored on Microsoft Sharepoint into a pandas <code>DataFrame</code>.</p> <p>Modes: If the <code>URL</code> ends with the file (e.g ../file.xlsx) it downloads only the file and creates a DataFrame from it. If the <code>URL</code> ends with the folder (e.g ../folder_name/): it downloads multiple files and creates a DataFrame from them:     - If <code>file_sheet_mapping</code> is provided, it downloads and processes only         the specified files and sheets.     - If <code>file_sheet_mapping</code> is NOT provided, it downloads and processes all of         the files from the chosen folder.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL to the file.</p> required <code>sheet_name</code> <code>str | list | int</code> <p>Strings are used for sheet names. Integers are used in zero-indexed sheet positions (chart sheets do not count as a sheet position). Lists of strings/integers are used to request multiple sheets. Specify None to get all worksheets. Defaults to None.</p> <code>None</code> <code>columns</code> <code>str | list[str] | list[int]</code> <p>Which columns to ingest. Defaults to None.</p> <code>None</code> <code>credentials_secret</code> <code>str</code> <p>The name of the secret storing the credentials. Defaults to None. More info on: https://docs.prefect.io/concepts/blocks/</p> <code>None</code> <code>file_sheet_mapping</code> <code>dict</code> <p>A dictionary where keys are filenames and values are the sheet names to be loaded from each file. If provided, only these files and sheets will be downloaded. Defaults to None.</p> <code>None</code> <code>na_values</code> <code>list[str] | None</code> <p>Additional strings to recognize as NA/NaN. If list passed, the specific NA values for each column will be recognized. Defaults to None.</p> <code>None</code> <code>tests</code> <code>dict[str]</code> <p>A dictionary with optional list of tests     to verify the output dataframe. If defined, triggers the <code>validate</code>     function from viadot.utils. Defaults to None.</p> <code>None</code> <code>config_key</code> <code>str</code> <p>The key in the viadot config holding relevant credentials. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.Dataframe: The pandas <code>DataFrame</code> containing data from the file.</p> Source code in <code>src/viadot/orchestration/prefect/tasks/sharepoint.py</code> <pre><code>@task(retries=3, retry_delay_seconds=10, timeout_seconds=60 * 60)\ndef sharepoint_to_df(\n    url: str,\n    sheet_name: str | list[str | int] | int | None = None,\n    columns: str | list[str] | list[int] | None = None,\n    tests: dict[str, Any] | None = None,\n    file_sheet_mapping: dict | None = None,\n    na_values: list[str] | None = None,\n    credentials_secret: str | None = None,\n    config_key: str | None = None,\n) -&gt; pd.DataFrame:\n    \"\"\"Load an Excel file stored on Microsoft Sharepoint into a pandas `DataFrame`.\n\n    Modes:\n    If the `URL` ends with the file (e.g ../file.xlsx) it downloads only the file and\n    creates a DataFrame from it.\n    If the `URL` ends with the folder (e.g ../folder_name/): it downloads multiple files\n    and creates a DataFrame from them:\n        - If `file_sheet_mapping` is provided, it downloads and processes only\n            the specified files and sheets.\n        - If `file_sheet_mapping` is NOT provided, it downloads and processes all of\n            the files from the chosen folder.\n\n    Args:\n        url (str): The URL to the file.\n        sheet_name (str | list | int, optional): Strings are used\n            for sheet names. Integers are used in zero-indexed sheet positions\n            (chart sheets do not count as a sheet position). Lists of strings/integers\n            are used to request multiple sheets. Specify None to get all worksheets.\n            Defaults to None.\n        columns (str | list[str] | list[int], optional): Which columns to ingest.\n            Defaults to None.\n        credentials_secret (str, optional): The name of the secret storing\n            the credentials. Defaults to None.\n            More info on: https://docs.prefect.io/concepts/blocks/\n        file_sheet_mapping (dict): A dictionary where keys are filenames and values are\n            the sheet names to be loaded from each file. If provided, only these files\n            and sheets will be downloaded. Defaults to None.\n        na_values (list[str] | None): Additional strings to recognize as NA/NaN.\n            If list passed, the specific NA values for each column will be recognized.\n            Defaults to None.\n        tests (dict[str], optional): A dictionary with optional list of tests\n                to verify the output dataframe. If defined, triggers the `validate`\n                function from viadot.utils. Defaults to None.\n        config_key (str, optional): The key in the viadot config holding relevant\n            credentials. Defaults to None.\n\n    Returns:\n        pd.Dataframe: The pandas `DataFrame` containing data from the file.\n    \"\"\"\n    if not (credentials_secret or config_key):\n        raise MissingSourceCredentialsError\n\n    logger = get_run_logger()\n\n    credentials = get_credentials(secret_name=credentials_secret)\n    s = Sharepoint(credentials=credentials, config_key=config_key)\n\n    logger.info(f\"Downloading data from {url}...\")\n    df = s.to_df(\n        url,\n        sheet_name=sheet_name,\n        tests=tests,\n        usecols=columns,\n        na_values=na_values,\n        file_sheet_mapping=file_sheet_mapping,\n    )\n    logger.info(f\"Successfully downloaded data from {url}.\")\n\n    return df\n</code></pre>"},{"location":"references/orchestration/prefect/tasks/#viadot.orchestration.prefect.tasks.sql_server_query","title":"<code>viadot.orchestration.prefect.tasks.sql_server_query(query, credentials_secret=None, config_key=None)</code>","text":"<p>Execute a query on SQL Server.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>(str, required)</code> <p>The query to execute on the SQL Server database.</p> required <code>credentials_secret</code> <code>str</code> <p>The name of the secret storing the credentials. Defaults to None. More info on: https://docs.prefect.io/concepts/blocks/</p> <code>None</code> <code>config_key</code> <code>str</code> <p>The key in the viadot config holding relevant credentials. Defaults to None.</p> <code>None</code> Source code in <code>src/viadot/orchestration/prefect/tasks/sql_server.py</code> <pre><code>@task(retries=3, retry_delay_seconds=10, timeout_seconds=60 * 60 * 3)\ndef sql_server_query(\n    query: str,\n    credentials_secret: str | None = None,\n    config_key: str | None = None,\n) -&gt; list[Record] | bool:\n    \"\"\"Execute a query on SQL Server.\n\n    Args:\n        query (str, required): The query to execute on the SQL Server database.\n        credentials_secret (str, optional): The name of the secret storing\n            the credentials. Defaults to None.\n            More info on: https://docs.prefect.io/concepts/blocks/\n        config_key (str, optional): The key in the viadot config holding relevant\n            credentials. Defaults to None.\n    \"\"\"\n    if not (credentials_secret or config_key):\n        raise MissingSourceCredentialsError\n\n    logger = get_run_logger()\n\n    credentials = get_source_credentials(config_key) or get_credentials(\n        credentials_secret\n    )\n    sql_server = SQLServer(credentials=credentials)\n    result = sql_server.run(query)\n\n    logger.info(\"Successfully ran the query.\")\n    return result\n</code></pre>"},{"location":"references/orchestration/prefect/tasks/#viadot.orchestration.prefect.tasks.sql_server_to_df","title":"<code>viadot.orchestration.prefect.tasks.sql_server_to_df(query, credentials_secret=None, config_key=None)</code>","text":"<p>Load the result of a SQL Server Database query into a pandas DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>(str, required)</code> <p>The query to execute on the SQL Server database. If the query doesn't start with \"SELECT\" returns an empty DataFrame.</p> required <code>credentials_secret</code> <code>str</code> <p>The name of the secret storing the credentials. Defaults to None. More info on: https://docs.prefect.io/concepts/blocks/</p> <code>None</code> <code>config_key</code> <code>str</code> <p>The key in the viadot config holding relevant credentials. Defaults to None.</p> <code>None</code> <p>Returns: pd.Dataframe</p> Source code in <code>src/viadot/orchestration/prefect/tasks/sql_server.py</code> <pre><code>@task(retries=3, retry_delay_seconds=10, timeout_seconds=60 * 60 * 3)\ndef sql_server_to_df(\n    query: str,\n    credentials_secret: str | None = None,\n    config_key: str | None = None,\n) -&gt; pd.DataFrame:\n    \"\"\"Load the result of a SQL Server Database query into a pandas DataFrame.\n\n    Args:\n        query (str, required): The query to execute on the SQL Server database.\n            If the query doesn't start with \"SELECT\" returns an empty DataFrame.\n        credentials_secret (str, optional): The name of the secret storing\n            the credentials. Defaults to None.\n            More info on: https://docs.prefect.io/concepts/blocks/\n        config_key (str, optional): The key in the viadot config holding relevant\n            credentials. Defaults to None.\n\n    Returns: pd.Dataframe\n    \"\"\"\n    if not (credentials_secret or config_key):\n        raise MissingSourceCredentialsError\n\n    logger = get_run_logger()\n\n    credentials = get_source_credentials(config_key) or get_credentials(\n        credentials_secret\n    )\n    sql_server = SQLServer(credentials=credentials)\n    df = sql_server.to_df(query=query)\n    nrows = df.shape[0]\n    ncols = df.shape[1]\n\n    logger.info(\n        f\"Successfully downloaded {nrows} rows and {ncols} columns of data to a DataFrame.\"\n    )\n    return df\n</code></pre>"},{"location":"references/orchestration/prefect/tasks/#viadot.orchestration.prefect.tasks.vid_club_to_df","title":"<code>viadot.orchestration.prefect.tasks.vid_club_to_df(*args, endpoint=None, from_date='2022-03-22', to_date=None, items_per_page=100, region=None, days_interval=30, cols_to_drop=None, azure_key_vault_secret=None, adls_config_key=None, validate_df_dict=None, timeout=3600, **kwargs)</code>","text":"<p>Task to downloading data from Vid Club APIs to Pandas DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>endpoint</code> <code>Literal['jobs', 'product', 'company', 'survey']</code> <p>The endpoint source to be accessed. Defaults to None.</p> <code>None</code> <code>from_date</code> <code>str</code> <p>Start date for the query, by default is the oldest date in the data 2022-03-22.</p> <code>'2022-03-22'</code> <code>to_date</code> <code>str</code> <p>End date for the query. By default None, which will be executed as datetime.today().strftime(\"%Y-%m-%d\") in code.</p> <code>None</code> <code>items_per_page</code> <code>int</code> <p>Number of entries per page. Defaults to 100.</p> <code>100</code> <code>region</code> <code>Literal['bg', 'hu', 'hr', 'pl', 'ro', 'si', 'all']</code> <p>Region filter for the query. Defaults to None (parameter is not used in url). [December 2023 status: value 'all' does not work for company and jobs]</p> <code>None</code> <code>days_interval</code> <code>int</code> <p>Days specified in date range per API call (test showed that 30-40 is optimal for performance). Defaults to 30.</p> <code>30</code> <code>cols_to_drop</code> <code>List[str]</code> <p>List of columns to drop. Defaults to None.</p> <code>None</code> <code>config_key</code> <code>str</code> <p>The key in the viadot config holding relevant credentials. Defaults to None.</p> required <code>azure_key_vault_secret</code> <code>Optional[str]</code> <p>The name of the Azure Key Vault secret where credentials are stored. Defaults to None.</p> <code>None</code> <code>validate_df_dict</code> <code>dict</code> <p>A dictionary with optional list of tests to verify the output dataframe. If defined, triggers the <code>validate_df</code> task from task_utils. Defaults to None.</p> <code>None</code> <code>timeout</code> <code>int</code> <p>The time (in seconds) to wait while running this task before a timeout occurs. Defaults to 3600.</p> <code>3600</code> <p>Returns: Pandas DataFrame</p> Source code in <code>src/viadot/orchestration/prefect/tasks/vid_club.py</code> <pre><code>@task(retries=3, log_prints=True, retry_delay_seconds=10, timeout_seconds=2 * 60 * 60)\ndef vid_club_to_df(  # noqa: PLR0913\n    *args: list[Any],\n    endpoint: Literal[\"jobs\", \"product\", \"company\", \"survey\"] | None = None,\n    from_date: str = \"2022-03-22\",\n    to_date: str | None = None,\n    items_per_page: int = 100,\n    region: Literal[\"bg\", \"hu\", \"hr\", \"pl\", \"ro\", \"si\", \"all\"] | None = None,\n    days_interval: int = 30,\n    cols_to_drop: list[str] | None = None,\n    azure_key_vault_secret: str | None = None,\n    adls_config_key: str | None = None,\n    validate_df_dict: dict | None = None,\n    timeout: int = 3600,\n    **kwargs: dict[str, Any],\n) -&gt; pd.DataFrame:\n    \"\"\"Task to downloading data from Vid Club APIs to Pandas DataFrame.\n\n    Args:\n        endpoint (Literal[\"jobs\", \"product\", \"company\", \"survey\"], optional):\n            The endpoint source to be accessed. Defaults to None.\n        from_date (str, optional): Start date for the query, by default is the oldest\n            date in the data 2022-03-22.\n        to_date (str, optional): End date for the query. By default None,\n            which will be executed as datetime.today().strftime(\"%Y-%m-%d\") in code.\n        items_per_page (int, optional): Number of entries per page. Defaults to 100.\n        region (Literal[\"bg\", \"hu\", \"hr\", \"pl\", \"ro\", \"si\", \"all\"], optional): Region\n            filter for the query. Defaults to None (parameter is not used in url).\n            [December 2023 status: value 'all' does not work for company and jobs]\n        days_interval (int, optional): Days specified in date range per API call\n            (test showed that 30-40 is optimal for performance). Defaults to 30.\n        cols_to_drop (List[str], optional): List of columns to drop. Defaults to None.\n        config_key (str, optional): The key in the viadot config holding relevant\n            credentials. Defaults to None.\n        azure_key_vault_secret (Optional[str], optional): The name of the Azure Key\n            Vault secret where credentials are stored. Defaults to None.\n        validate_df_dict (dict, optional): A dictionary with optional list of tests\n            to verify the output\n            dataframe. If defined, triggers the `validate_df` task from task_utils.\n            Defaults to None.\n        timeout (int, optional): The time (in seconds) to wait while running this task\n            before a timeout occurs. Defaults to 3600.\n\n    Returns: Pandas DataFrame\n    \"\"\"\n    if not (azure_key_vault_secret or adls_config_key):\n        raise MissingSourceCredentialsError\n\n    if not adls_config_key:\n        credentials = get_credentials(azure_key_vault_secret)\n\n    vc_obj = VidClub(\n        args=args,\n        endpoint=endpoint,\n        from_date=from_date,\n        to_date=to_date,\n        items_per_page=items_per_page,\n        region=region,\n        days_interval=days_interval,\n        cols_to_drop=cols_to_drop,\n        vid_club_credentials=credentials,\n        validate_df_dict=validate_df_dict,\n        timeout=timeout,\n        kwargs=kwargs,\n    )\n\n    return vc_obj.to_df()\n</code></pre>"},{"location":"references/orchestration/prefect/tasks/#viadot.orchestration.prefect.tasks.supermetrics_to_df","title":"<code>viadot.orchestration.prefect.tasks.supermetrics_to_df(query_params, config_key=None, credentials_secret=None)</code>","text":"<p>Task to retrive data from Supermetrics and returns it as a pandas DataFrame.</p> <p>This function queries the Supermetrics API using the provided query parameters and returns the data as a pandas DataFrame. The function supports both configuration-based and secret-based credentials.</p> <p>The function is decorated with a Prefect task, allowing it to handle retries, logging, and timeout behavior.</p> <pre><code>query_params (dict):\n    A dictionary containing the parameters for querying the Supermetrics API.\n    These parameters define what data to retrieve and how the query should\n    be constructed.\nconfig_key (str, optional): The key in the viadot config holding relevant\n    credentials. Defaults to None.\ncredentials_secret (str, optional):\n    The name of the secret in your secret management system that contains\n    the Supermetrics API credentials. If `config_key` is not provided,\n    this secret is used to authenticate with the Supermetrics API.\n</code></pre> <pre><code>pd.DataFrame:\n    A pandas DataFrame containing the data retrieved from Supermetrics based\n    on the provided query parameters.\n</code></pre> <pre><code>MissingSourceCredentialsError:\n    Raised if neither `credentials_secret` nor `config_key` is provided,\n    indicating that no valid credentials were supplied to access\n    the Supermetrics API.\n</code></pre> Source code in <code>src/viadot/orchestration/prefect/tasks/supermetrics.py</code> <pre><code>@task(retries=3, log_prints=True, retry_delay_seconds=10, timeout_seconds=60 * 60)\ndef supermetrics_to_df(\n    query_params: dict,\n    config_key: str | None = None,\n    credentials_secret: str | None = None,\n) -&gt; pd.DataFrame:\n    \"\"\"Task to retrive data from Supermetrics and returns it as a pandas DataFrame.\n\n    This function queries the Supermetrics API using the provided query parameters and\n    returns the data as a pandas DataFrame. The function supports both\n    configuration-based and secret-based credentials.\n\n    The function is decorated with a Prefect task, allowing it to handle retries,\n    logging, and timeout behavior.\n\n    Args:\n    ----\n        query_params (dict):\n            A dictionary containing the parameters for querying the Supermetrics API.\n            These parameters define what data to retrieve and how the query should\n            be constructed.\n        config_key (str, optional): The key in the viadot config holding relevant\n            credentials. Defaults to None.\n        credentials_secret (str, optional):\n            The name of the secret in your secret management system that contains\n            the Supermetrics API credentials. If `config_key` is not provided,\n            this secret is used to authenticate with the Supermetrics API.\n\n    Returns:\n    -------\n        pd.DataFrame:\n            A pandas DataFrame containing the data retrieved from Supermetrics based\n            on the provided query parameters.\n\n    Raises:\n    ------\n        MissingSourceCredentialsError:\n            Raised if neither `credentials_secret` nor `config_key` is provided,\n            indicating that no valid credentials were supplied to access\n            the Supermetrics API.\n\n    \"\"\"\n    if not (credentials_secret or config_key):\n        raise MissingSourceCredentialsError\n\n    credentials = get_credentials(credentials_secret) if not config_key else None\n\n    supermetrics = Supermetrics(\n        credentials=credentials,\n        config_key=config_key,\n    )\n    return supermetrics.to_df(query_params=query_params)\n</code></pre>"},{"location":"references/sources/api/","title":"API sources","text":""},{"location":"references/sources/api/#viadot.sources.bigquery.BigQuery","title":"<code>viadot.sources.bigquery.BigQuery</code>","text":"<p>               Bases: <code>Source</code></p> <p>Class to connect with Bigquery project and SQL tables.</p> <p>Documentation for this API is located at: https://cloud.google.com/bigquery/docs.</p> Source code in <code>src/viadot/sources/bigquery.py</code> <pre><code>class BigQuery(Source):\n    \"\"\"Class to connect with Bigquery project and SQL tables.\n\n    Documentation for this API is located at: https://cloud.google.com/bigquery/docs.\n    \"\"\"\n\n    def __init__(\n        self,\n        *args,\n        credentials: BigQueryCredentials | None = None,\n        config_key: str | None = None,\n        **kwargs,\n    ):\n        \"\"\"Create an instance of the BigQuery class.\n\n        Args:\n            credentials (BigQueryCredentials, optional): BigQuery credentials.\n                Defaults to None.\n            config_key (str, optional): The key in the viadot config holding\n                relevant credentials. Defaults to None.\n        \"\"\"\n        credentials = credentials or get_source_credentials(config_key) or None\n        if credentials is None:\n            message = \"Missing credentials.\"\n            raise CredentialError(message)\n\n        validated_creds = dict(BigQueryCredentials(**credentials))\n\n        super().__init__(*args, credentials=validated_creds, **kwargs)\n\n        credentials_service_account = (\n            service_account.Credentials.from_service_account_info(credentials)\n        )\n\n        self.project_id = credentials[\"project_id\"]\n\n        pandas_gbq.context.credentials = credentials_service_account\n        pandas_gbq.context.project = self.project_id\n\n    def _get_list_datasets_query(self) -&gt; str:\n        \"\"\"Get datasets from BigQuery project.\n\n        Returns:\n            str: Custom query.\n        \"\"\"\n        return f\"\"\"SELECT schema_name\n                FROM {self.project_id}.INFORMATION_SCHEMA.SCHEMATA\n                \"\"\"  # noqa: S608\n\n    def _get_list_tables_query(self, dataset_name: str) -&gt; str:\n        \"\"\"Get tables from BigQuery dataset. Dataset is required.\n\n        Args:\n            dataset_name (str): Dataset from Bigquery project.\n\n        Returns:\n            str: Custom query.\n        \"\"\"\n        return f\"\"\"SELECT table_name\n                FROM {self.project_id}.{dataset_name}.INFORMATION_SCHEMA.TABLES\n                \"\"\"  # noqa: S608\n\n    def _list_columns(self, dataset_name: str, table_name: str) -&gt; np.ndarray:\n        \"\"\"Get columns from BigQuery table. Dataset name and Table name are required.\n\n        Args:\n            dataset_name (str): Dataset from Bigquery project.\n            table_name (str): Table name from given dataset.\n\n        Returns:\n            np.ndarray: List of table names from the BigQuery dataset.\n        \"\"\"\n        query = f\"\"\"SELECT column_name\n                FROM {self.project_id}.{dataset_name}.INFORMATION_SCHEMA.COLUMNS\n                WHERE table_name=\"{table_name}\"\n                \"\"\"  # noqa: S608\n        df_columns = self._get_google_bigquery_data(query)\n\n        return df_columns[\"column_name\"].values\n\n    def _get_google_bigquery_data(self, query: str) -&gt; pd.DataFrame:\n        \"\"\"Connect to BigQuery API.\n\n        Args:\n            query (str): SQL query to querying data in BigQuery. Defaults to None.\n\n        Raises:\n            APIError: Error with BigQuery API connection.\n\n        Returns:\n            pd.DataFrame: BigQuery response data.\n        \"\"\"\n        try:\n            data = pandas_gbq.read_gbq(query)\n        except GenericGBQException as message:\n            raise APIError(message) from message\n\n        return data\n\n    @add_viadot_metadata_columns\n    def to_df(\n        self,\n        query: str | None = None,\n        dataset_name: str | None = None,\n        table_name: str | None = None,\n        date_column_name: str | None = None,\n        start_date: str | None = None,\n        end_date: str | None = None,\n        columns: list[str] | None = None,\n        if_empty: Literal[\"warn\", \"skip\", \"fail\"] = \"warn\",\n    ) -&gt; pd.DataFrame:\n        \"\"\"Generate a DataFrame from the API response of a queried BigQuery table.\n\n        Args:\n            query (str): SQL query to querying data in BigQuery.\n                Defaults to None.\n                Espetial queries:\n                -----------------\n                If the words \"tables\" or \"datasets\" are passed in this parameter all\n                tables and data sets will be returned as special internal queries.\n            dataset_name (str, optional): Dataset name. Defaults to None.\n            table_name (str, optional): Table name. Defaults to None.\n            date_column_name (str, optional): The user can provide the name of the date.\n                If the user-specified column does not exist, all data will be retrieved\n                from the table. Defaults to None.\n            start_date (str, optional): Parameter to pass start date e.g.\n                \"2022-01-01\". Defaults to None.\n            end_date (str, optional): Parameter to pass end date e.g.\n                \"2022-01-01\". Defaults to None.\n            columns (list[str], optional): List of columns from given table name.\n                Defaults to None.\n            if_empty (Literal[warn, skip, fail], optional): What to do if there is no\n                data. Defaults to \"warn\".\n\n        Returns:\n            pd.DataFrame: DataFrame with the data.\n        \"\"\"\n        if query == \"tables\":\n            query = self._get_list_tables_query(dataset_name=dataset_name)\n\n        elif query == \"datasets\":\n            query = self._get_list_datasets_query()\n\n        elif (\n            query is not None and \"select\" in query.lower() and \"from\" in query.lower()\n        ):\n            self.logger.info(\"query has been provided!\")\n        else:\n            if date_column_name or columns:\n                table_columns = self._list_columns(\n                    dataset_name=dataset_name, table_name=table_name\n                )\n\n            if columns is None or not set(columns).issubset(set(table_columns)):\n                self.logger.warning(\n                    \"Some of the columns provided are either, not in the table or \"\n                    + \"the list is empty. Downloading all the data instead.\"\n                )\n                columns = \"*\"\n            else:\n                columns = \", \".join(columns)\n\n            if date_column_name:\n                if (\n                    not set([date_column_name]).issubset(set(table_columns))  # noqa: C405\n                    or start_date is None\n                    or end_date is None\n                ):\n                    self.logger.warning(\n                        f\"'{date_column_name}' column is not recognized, \"\n                        + f\"or either `start_date`: {start_date} or `end_date`: \"\n                        + f\"{end_date} is None.\\n\"\n                        + \"Downloading all the data instead.\"\n                    )\n                    query = None\n                else:\n                    self.logger.info(\n                        f\"Filtering data from date {start_date} to {end_date}\"\n                    )\n                    query = f\"\"\"\n                        SELECT {columns}\n                            FROM `{self.project_id}.{dataset_name}.{table_name}`\n                            WHERE {date_column_name} BETWEEN\n                                PARSE_DATE(\"%Y-%m-%d\", \"{start_date}\") AND\n                                PARSE_DATE(\"%Y-%m-%d\", \"{end_date}\")\n                            ORDER BY {date_column_name} DESC\n                    \"\"\"  # noqa: S608\n\n            if query is None:\n                query = (\n                    f\"SELECT {columns} \"\n                    + f\"FROM `{self.project_id}.{dataset_name}.{table_name}`\"\n                )\n\n        df = self._get_google_bigquery_data(query)\n\n        if df.empty:\n            self._handle_if_empty(if_empty)\n\n        self.logger.info(f\"Downloaded the data from the table name: '{table_name}'.\")\n\n        return df\n</code></pre>"},{"location":"references/sources/api/#viadot.sources.bigquery.BigQuery.__init__","title":"<code>__init__(*args, credentials=None, config_key=None, **kwargs)</code>","text":"<p>Create an instance of the BigQuery class.</p> <p>Parameters:</p> Name Type Description Default <code>credentials</code> <code>BigQueryCredentials</code> <p>BigQuery credentials. Defaults to None.</p> <code>None</code> <code>config_key</code> <code>str</code> <p>The key in the viadot config holding relevant credentials. Defaults to None.</p> <code>None</code> Source code in <code>src/viadot/sources/bigquery.py</code> <pre><code>def __init__(\n    self,\n    *args,\n    credentials: BigQueryCredentials | None = None,\n    config_key: str | None = None,\n    **kwargs,\n):\n    \"\"\"Create an instance of the BigQuery class.\n\n    Args:\n        credentials (BigQueryCredentials, optional): BigQuery credentials.\n            Defaults to None.\n        config_key (str, optional): The key in the viadot config holding\n            relevant credentials. Defaults to None.\n    \"\"\"\n    credentials = credentials or get_source_credentials(config_key) or None\n    if credentials is None:\n        message = \"Missing credentials.\"\n        raise CredentialError(message)\n\n    validated_creds = dict(BigQueryCredentials(**credentials))\n\n    super().__init__(*args, credentials=validated_creds, **kwargs)\n\n    credentials_service_account = (\n        service_account.Credentials.from_service_account_info(credentials)\n    )\n\n    self.project_id = credentials[\"project_id\"]\n\n    pandas_gbq.context.credentials = credentials_service_account\n    pandas_gbq.context.project = self.project_id\n</code></pre>"},{"location":"references/sources/api/#viadot.sources.bigquery.BigQuery.to_df","title":"<code>to_df(query=None, dataset_name=None, table_name=None, date_column_name=None, start_date=None, end_date=None, columns=None, if_empty='warn')</code>","text":"<p>Generate a DataFrame from the API response of a queried BigQuery table.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>SQL query to querying data in BigQuery. Defaults to None. Espetial queries:</p> <p>If the words \"tables\" or \"datasets\" are passed in this parameter all tables and data sets will be returned as special internal queries.</p> <code>None</code> <code>dataset_name</code> <code>str</code> <p>Dataset name. Defaults to None.</p> <code>None</code> <code>table_name</code> <code>str</code> <p>Table name. Defaults to None.</p> <code>None</code> <code>date_column_name</code> <code>str</code> <p>The user can provide the name of the date. If the user-specified column does not exist, all data will be retrieved from the table. Defaults to None.</p> <code>None</code> <code>start_date</code> <code>str</code> <p>Parameter to pass start date e.g. \"2022-01-01\". Defaults to None.</p> <code>None</code> <code>end_date</code> <code>str</code> <p>Parameter to pass end date e.g. \"2022-01-01\". Defaults to None.</p> <code>None</code> <code>columns</code> <code>list[str]</code> <p>List of columns from given table name. Defaults to None.</p> <code>None</code> <code>if_empty</code> <code>Literal[warn, skip, fail]</code> <p>What to do if there is no data. Defaults to \"warn\".</p> <code>'warn'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: DataFrame with the data.</p> Source code in <code>src/viadot/sources/bigquery.py</code> <pre><code>@add_viadot_metadata_columns\ndef to_df(\n    self,\n    query: str | None = None,\n    dataset_name: str | None = None,\n    table_name: str | None = None,\n    date_column_name: str | None = None,\n    start_date: str | None = None,\n    end_date: str | None = None,\n    columns: list[str] | None = None,\n    if_empty: Literal[\"warn\", \"skip\", \"fail\"] = \"warn\",\n) -&gt; pd.DataFrame:\n    \"\"\"Generate a DataFrame from the API response of a queried BigQuery table.\n\n    Args:\n        query (str): SQL query to querying data in BigQuery.\n            Defaults to None.\n            Espetial queries:\n            -----------------\n            If the words \"tables\" or \"datasets\" are passed in this parameter all\n            tables and data sets will be returned as special internal queries.\n        dataset_name (str, optional): Dataset name. Defaults to None.\n        table_name (str, optional): Table name. Defaults to None.\n        date_column_name (str, optional): The user can provide the name of the date.\n            If the user-specified column does not exist, all data will be retrieved\n            from the table. Defaults to None.\n        start_date (str, optional): Parameter to pass start date e.g.\n            \"2022-01-01\". Defaults to None.\n        end_date (str, optional): Parameter to pass end date e.g.\n            \"2022-01-01\". Defaults to None.\n        columns (list[str], optional): List of columns from given table name.\n            Defaults to None.\n        if_empty (Literal[warn, skip, fail], optional): What to do if there is no\n            data. Defaults to \"warn\".\n\n    Returns:\n        pd.DataFrame: DataFrame with the data.\n    \"\"\"\n    if query == \"tables\":\n        query = self._get_list_tables_query(dataset_name=dataset_name)\n\n    elif query == \"datasets\":\n        query = self._get_list_datasets_query()\n\n    elif (\n        query is not None and \"select\" in query.lower() and \"from\" in query.lower()\n    ):\n        self.logger.info(\"query has been provided!\")\n    else:\n        if date_column_name or columns:\n            table_columns = self._list_columns(\n                dataset_name=dataset_name, table_name=table_name\n            )\n\n        if columns is None or not set(columns).issubset(set(table_columns)):\n            self.logger.warning(\n                \"Some of the columns provided are either, not in the table or \"\n                + \"the list is empty. Downloading all the data instead.\"\n            )\n            columns = \"*\"\n        else:\n            columns = \", \".join(columns)\n\n        if date_column_name:\n            if (\n                not set([date_column_name]).issubset(set(table_columns))  # noqa: C405\n                or start_date is None\n                or end_date is None\n            ):\n                self.logger.warning(\n                    f\"'{date_column_name}' column is not recognized, \"\n                    + f\"or either `start_date`: {start_date} or `end_date`: \"\n                    + f\"{end_date} is None.\\n\"\n                    + \"Downloading all the data instead.\"\n                )\n                query = None\n            else:\n                self.logger.info(\n                    f\"Filtering data from date {start_date} to {end_date}\"\n                )\n                query = f\"\"\"\n                    SELECT {columns}\n                        FROM `{self.project_id}.{dataset_name}.{table_name}`\n                        WHERE {date_column_name} BETWEEN\n                            PARSE_DATE(\"%Y-%m-%d\", \"{start_date}\") AND\n                            PARSE_DATE(\"%Y-%m-%d\", \"{end_date}\")\n                        ORDER BY {date_column_name} DESC\n                \"\"\"  # noqa: S608\n\n        if query is None:\n            query = (\n                f\"SELECT {columns} \"\n                + f\"FROM `{self.project_id}.{dataset_name}.{table_name}`\"\n            )\n\n    df = self._get_google_bigquery_data(query)\n\n    if df.empty:\n        self._handle_if_empty(if_empty)\n\n    self.logger.info(f\"Downloaded the data from the table name: '{table_name}'.\")\n\n    return df\n</code></pre>"},{"location":"references/sources/api/#viadot.sources.business_core.BusinessCore","title":"<code>viadot.sources.business_core.BusinessCore</code>","text":"<p>               Bases: <code>Source</code></p> <p>Business Core ERP API connector.</p> Source code in <code>src/viadot/sources/business_core.py</code> <pre><code>class BusinessCore(Source):\n    \"\"\"Business Core ERP API connector.\"\"\"\n\n    def __init__(\n        self,\n        url: str | None = None,\n        filters: dict[str, Any] | None = None,\n        credentials: dict[str, Any] | None = None,\n        config_key: str = \"BusinessCore\",\n        verify: bool = True,\n        *args,\n        **kwargs,\n    ):\n        \"\"\"Create a BusinessCore connector instance.\n\n        Args:\n            url (str, optional): Base url to a view in Business Core API.\n                Defaults to None.\n            filters (dict[str, Any], optional): Filters in form of dictionary. Available\n                filters: 'BucketCount', 'BucketNo', 'FromDate', 'ToDate'. Defaults to\n                None.\n            credentials (dict[str, Any], optional): Credentials stored in a dictionary.\n                Required credentials: username, password. Defaults to None.\n            config_key (str, optional): The key in the viadot config holding relevant\n                credentials. Defaults to \"BusinessCore\".\n            verify (bool, optional): Whether or not verify certificates while connecting\n                to an API. Defaults to True.\n        \"\"\"\n        raw_creds = credentials or get_source_credentials(config_key)\n        validated_creds = dict(BusinessCoreCredentials(**raw_creds))\n\n        self.url = url\n        # API requires that filters are always specified\n        if filters is None:\n            self.filters = {\n                \"BucketCount\": \"&amp;\",\n                \"BucketNo\": \"&amp;\",\n                \"FromDate\": \"&amp;\",\n                \"ToDate\": \"&amp;\",\n            }\n        else:\n            self.filters = self._clean_filters(filters)\n        self.verify = verify\n\n        super().__init__(*args, credentials=validated_creds, **kwargs)\n\n    def generate_token(self) -&gt; str:\n        \"\"\"Generate a token for the user.\n\n        Returns:\n            string: The token.\n        \"\"\"\n        url = \"https://api.businesscore.ae/api/user/Login\"\n\n        username = self.credentials.get(\"username\")\n        password = self.credentials.get(\"password\").get_secret_value()\n        payload = f\"grant_type=password&amp;username={username}&amp;password={password}&amp;scope=\"\n        headers = {\"Content-Type\": \"application/x-www-form-urlencoded\"}\n        response = handle_api_response(\n            url=url,\n            headers=headers,\n            method=\"GET\",\n            data=payload,\n            verify=self.verify,\n        )\n\n        return json.loads(response.text).get(\"access_token\")\n\n    @staticmethod\n    def _clean_filters(filters: dict[str, str | None]) -&gt; dict[str, str]:\n        \"\"\"Replace 'None' with '&amp;' in a dictionary.\n\n        Required for payload in 'x-www-form-urlencoded' from.\n\n        Returns:\n            dict[str, str]: Dictionary with filters prepared for further use.\n        \"\"\"\n        return {key: (\"&amp;\" if val is None else val) for key, val in filters.items()}\n\n    def get_data(self) -&gt; dict[str, Any]:\n        \"\"\"Obtain data from Business Core API.\n\n        Returns:\n            dict: Dictionary with data downloaded from Business Core API.\n        \"\"\"\n        view = self.url.split(\"/\")[-1]\n\n        if view not in [\n            \"GetCustomerData\",\n            \"GetItemMaster\",\n            \"GetPendingSalesOrderData\",\n            \"GetSalesInvoiceData\",\n            \"GetSalesReturnDetailData\",\n            \"GetSalesOrderData\",\n            \"GetSalesQuotationData\",\n        ]:\n            error_message = f\"View {view} currently not available.\"\n            raise APIError(error_message)\n\n        payload = (\n            \"BucketCount=\"\n            + str(self.filters.get(\"BucketCount\"))\n            + \"BucketNo=\"\n            + str(self.filters.get(\"BucketNo\"))\n            + \"FromDate=\"\n            + str(self.filters.get(\"FromDate\"))\n            + \"ToDate\"\n            + str(self.filters.get(\"ToDate\"))\n        )\n        headers = {\n            \"Content-Type\": \"application/x-www-form-urlencoded\",\n            \"Authorization\": \"Bearer \" + self.generate_token(),\n        }\n        self.logger.info(\"Downloading the data...\")\n        response = handle_api_response(\n            url=self.url,\n            headers=headers,\n            method=\"GET\",\n            data=payload,\n            verify=self.verify,\n        )\n        self.logger.info(\"Data was downloaded successfully.\")\n        return json.loads(response.text).get(\"MasterDataList\")\n\n    @add_viadot_metadata_columns\n    def to_df(self, if_empty: Literal[\"warn\", \"fail\", \"skip\"] = \"skip\") -&gt; pd.DataFrame:\n        \"\"\"Download data into a pandas DataFrame.\n\n        Args:\n            if_empty (Literal[\"warn\", \"fail\", \"skip\"], optional): What to do if output\n                DataFrame is empty. Defaults to \"skip\".\n\n        Returns:\n            pd.DataFrame: DataFrame with the data.\n\n        Raises:\n            APIError: When selected API view is not available.\n        \"\"\"\n        data = self.get_data()\n        df = pd.DataFrame.from_dict(data)\n        self.logger.info(\n            f\"Data was successfully transformed into DataFrame: {len(df.columns)} columns and {len(df)} rows.\"\n        )\n        if df.empty:\n            self._handle_if_empty(if_empty)\n\n        return df\n</code></pre>"},{"location":"references/sources/api/#viadot.sources.business_core.BusinessCore.__init__","title":"<code>__init__(url=None, filters=None, credentials=None, config_key='BusinessCore', verify=True, *args, **kwargs)</code>","text":"<p>Create a BusinessCore connector instance.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>Base url to a view in Business Core API. Defaults to None.</p> <code>None</code> <code>filters</code> <code>dict[str, Any]</code> <p>Filters in form of dictionary. Available filters: 'BucketCount', 'BucketNo', 'FromDate', 'ToDate'. Defaults to None.</p> <code>None</code> <code>credentials</code> <code>dict[str, Any]</code> <p>Credentials stored in a dictionary. Required credentials: username, password. Defaults to None.</p> <code>None</code> <code>config_key</code> <code>str</code> <p>The key in the viadot config holding relevant credentials. Defaults to \"BusinessCore\".</p> <code>'BusinessCore'</code> <code>verify</code> <code>bool</code> <p>Whether or not verify certificates while connecting to an API. Defaults to True.</p> <code>True</code> Source code in <code>src/viadot/sources/business_core.py</code> <pre><code>def __init__(\n    self,\n    url: str | None = None,\n    filters: dict[str, Any] | None = None,\n    credentials: dict[str, Any] | None = None,\n    config_key: str = \"BusinessCore\",\n    verify: bool = True,\n    *args,\n    **kwargs,\n):\n    \"\"\"Create a BusinessCore connector instance.\n\n    Args:\n        url (str, optional): Base url to a view in Business Core API.\n            Defaults to None.\n        filters (dict[str, Any], optional): Filters in form of dictionary. Available\n            filters: 'BucketCount', 'BucketNo', 'FromDate', 'ToDate'. Defaults to\n            None.\n        credentials (dict[str, Any], optional): Credentials stored in a dictionary.\n            Required credentials: username, password. Defaults to None.\n        config_key (str, optional): The key in the viadot config holding relevant\n            credentials. Defaults to \"BusinessCore\".\n        verify (bool, optional): Whether or not verify certificates while connecting\n            to an API. Defaults to True.\n    \"\"\"\n    raw_creds = credentials or get_source_credentials(config_key)\n    validated_creds = dict(BusinessCoreCredentials(**raw_creds))\n\n    self.url = url\n    # API requires that filters are always specified\n    if filters is None:\n        self.filters = {\n            \"BucketCount\": \"&amp;\",\n            \"BucketNo\": \"&amp;\",\n            \"FromDate\": \"&amp;\",\n            \"ToDate\": \"&amp;\",\n        }\n    else:\n        self.filters = self._clean_filters(filters)\n    self.verify = verify\n\n    super().__init__(*args, credentials=validated_creds, **kwargs)\n</code></pre>"},{"location":"references/sources/api/#viadot.sources.business_core.BusinessCore.generate_token","title":"<code>generate_token()</code>","text":"<p>Generate a token for the user.</p> <p>Returns:</p> Name Type Description <code>string</code> <code>str</code> <p>The token.</p> Source code in <code>src/viadot/sources/business_core.py</code> <pre><code>def generate_token(self) -&gt; str:\n    \"\"\"Generate a token for the user.\n\n    Returns:\n        string: The token.\n    \"\"\"\n    url = \"https://api.businesscore.ae/api/user/Login\"\n\n    username = self.credentials.get(\"username\")\n    password = self.credentials.get(\"password\").get_secret_value()\n    payload = f\"grant_type=password&amp;username={username}&amp;password={password}&amp;scope=\"\n    headers = {\"Content-Type\": \"application/x-www-form-urlencoded\"}\n    response = handle_api_response(\n        url=url,\n        headers=headers,\n        method=\"GET\",\n        data=payload,\n        verify=self.verify,\n    )\n\n    return json.loads(response.text).get(\"access_token\")\n</code></pre>"},{"location":"references/sources/api/#viadot.sources.business_core.BusinessCore.get_data","title":"<code>get_data()</code>","text":"<p>Obtain data from Business Core API.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict[str, Any]</code> <p>Dictionary with data downloaded from Business Core API.</p> Source code in <code>src/viadot/sources/business_core.py</code> <pre><code>def get_data(self) -&gt; dict[str, Any]:\n    \"\"\"Obtain data from Business Core API.\n\n    Returns:\n        dict: Dictionary with data downloaded from Business Core API.\n    \"\"\"\n    view = self.url.split(\"/\")[-1]\n\n    if view not in [\n        \"GetCustomerData\",\n        \"GetItemMaster\",\n        \"GetPendingSalesOrderData\",\n        \"GetSalesInvoiceData\",\n        \"GetSalesReturnDetailData\",\n        \"GetSalesOrderData\",\n        \"GetSalesQuotationData\",\n    ]:\n        error_message = f\"View {view} currently not available.\"\n        raise APIError(error_message)\n\n    payload = (\n        \"BucketCount=\"\n        + str(self.filters.get(\"BucketCount\"))\n        + \"BucketNo=\"\n        + str(self.filters.get(\"BucketNo\"))\n        + \"FromDate=\"\n        + str(self.filters.get(\"FromDate\"))\n        + \"ToDate\"\n        + str(self.filters.get(\"ToDate\"))\n    )\n    headers = {\n        \"Content-Type\": \"application/x-www-form-urlencoded\",\n        \"Authorization\": \"Bearer \" + self.generate_token(),\n    }\n    self.logger.info(\"Downloading the data...\")\n    response = handle_api_response(\n        url=self.url,\n        headers=headers,\n        method=\"GET\",\n        data=payload,\n        verify=self.verify,\n    )\n    self.logger.info(\"Data was downloaded successfully.\")\n    return json.loads(response.text).get(\"MasterDataList\")\n</code></pre>"},{"location":"references/sources/api/#viadot.sources.business_core.BusinessCore.to_df","title":"<code>to_df(if_empty='skip')</code>","text":"<p>Download data into a pandas DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>if_empty</code> <code>Literal['warn', 'fail', 'skip']</code> <p>What to do if output DataFrame is empty. Defaults to \"skip\".</p> <code>'skip'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: DataFrame with the data.</p> <p>Raises:</p> Type Description <code>APIError</code> <p>When selected API view is not available.</p> Source code in <code>src/viadot/sources/business_core.py</code> <pre><code>@add_viadot_metadata_columns\ndef to_df(self, if_empty: Literal[\"warn\", \"fail\", \"skip\"] = \"skip\") -&gt; pd.DataFrame:\n    \"\"\"Download data into a pandas DataFrame.\n\n    Args:\n        if_empty (Literal[\"warn\", \"fail\", \"skip\"], optional): What to do if output\n            DataFrame is empty. Defaults to \"skip\".\n\n    Returns:\n        pd.DataFrame: DataFrame with the data.\n\n    Raises:\n        APIError: When selected API view is not available.\n    \"\"\"\n    data = self.get_data()\n    df = pd.DataFrame.from_dict(data)\n    self.logger.info(\n        f\"Data was successfully transformed into DataFrame: {len(df.columns)} columns and {len(df)} rows.\"\n    )\n    if df.empty:\n        self._handle_if_empty(if_empty)\n\n    return df\n</code></pre>"},{"location":"references/sources/api/#viadot.sources.cloud_for_customers.CloudForCustomers","title":"<code>viadot.sources.cloud_for_customers.CloudForCustomers</code>","text":"<p>               Bases: <code>Source</code></p> <p>Cloud for Customers connector to fetch OData source.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL to the C4C API. For example, 'https://myNNNNNN.crm.ondemand.com/c4c/v1/'.</p> <code>None</code> <code>endpoint</code> <code>str</code> <p>The API endpoint.</p> <code>None</code> <code>report_url</code> <code>str</code> <p>The URL of a prepared report.</p> <code>None</code> <code>filter_params</code> <code>Dict[str, Any]</code> <p>Filtering parameters passed to the request. E.g {\"$filter\": \"AccountID eq '1234'\"}. More info on: https://userapps.support.sap.com/sap/support/knowledge/en/2330688</p> <code>None</code> <code>credentials</code> <code>CloudForCustomersCredentials</code> <p>Cloud for Customers credentials.</p> <code>None</code> <code>config_key</code> <code>str</code> <p>The key in the viadot config holding relevant credentials.</p> <code>None</code> Source code in <code>src/viadot/sources/cloud_for_customers.py</code> <pre><code>class CloudForCustomers(Source):\n    \"\"\"Cloud for Customers connector to fetch OData source.\n\n    Args:\n        url (str, optional): The URL to the C4C API. For example,\n            'https://myNNNNNN.crm.ondemand.com/c4c/v1/'.\n        endpoint (str, optional): The API endpoint.\n        report_url (str, optional): The URL of a prepared report.\n        filter_params (Dict[str, Any], optional): Filtering parameters passed to the\n            request. E.g {\"$filter\": \"AccountID eq '1234'\"}. More info on:\n            https://userapps.support.sap.com/sap/support/knowledge/en/2330688\n        credentials (CloudForCustomersCredentials, optional): Cloud for Customers\n            credentials.\n        config_key (str, optional): The key in the viadot config holding relevant\n            credentials.\n    \"\"\"\n\n    DEFAULT_PARAMS = {\"$format\": \"json\"}  # noqa: RUF012\n\n    def __init__(\n        self,\n        *args,\n        url: str | None = None,\n        endpoint: str | None = None,\n        report_url: str | None = None,\n        filter_params: dict[str, Any] | None = None,\n        credentials: CloudForCustomersCredentials | None = None,\n        config_key: str | None = None,\n        **kwargs,\n    ):\n        \"\"\"Initialize the class with the provided parameters.\n\n        Args:\n            *args: Variable length argument list.\n            url (str, optional): The base URL for the service.\n            endpoint (str, optional): The specific endpoint for the service.\n            report_url (str, optional): The URL for the report.\n            filter_params (Dict[str, Any], optional): Parameters to filter the report\n                data.\n            credentials (CloudForCustomersCredentials, optional): Credentials required\n                for authentication.\n            config_key (Optional[str], optional): A key to retrieve specific\n                configuration settings.\n            **kwargs: Arbitrary keyword arguments.\n        \"\"\"\n        # Credentials logic\n        raw_creds = credentials or get_source_credentials(config_key) or {}\n        validated_creds = dict(\n            CloudForCustomersCredentials(**raw_creds)\n        )  # validate the credentials\n        super().__init__(*args, credentials=validated_creds, **kwargs)\n\n        self.url = url or self.credentials.get(\"url\")\n        self.report_url = report_url or self.credentials.get(\"report_url\")\n\n        self.is_report = bool(self.report_url)\n        self.endpoint = endpoint\n\n        if self.url:\n            self.full_url = urljoin(self.url, self.endpoint)\n        if filter_params:\n            filter_params_merged = self.DEFAULT_PARAMS.copy()\n            filter_params_merged.update(filter_params)\n\n            self.filter_params = filter_params_merged\n        else:\n            self.filter_params = self.DEFAULT_PARAMS\n\n    @staticmethod\n    def create_metadata_url(url: str) -&gt; str:\n        \"\"\"Create URL to fetch metadata from.\n\n        Args:\n            url (str): The URL to transform to metadata URL.\n\n        Returns:\n            meta_url (str): The URL to fetch metadata from.\n        \"\"\"\n        start = url.split(\".svc\")[0]\n        url_raw = url.split(\"?\")[0]\n        end = url_raw.split(\"/\")[-1]\n        return start + \".svc/$metadata?entityset=\" + end\n\n    def _extract_records_from_report_url(self, report_url: str) -&gt; list[dict[str, Any]]:\n        \"\"\"Fetch report_url to extract records.\n\n        Args:\n            report_url (str): The url to extract records from.\n\n        Returns:\n            records (List[Dict[str, Any]]): The records extracted from report_url.\n        \"\"\"\n        records = []\n        while report_url:\n            response = self.get_response(report_url, filter_params=self.filter_params)\n            response_json = response.json()\n            new_records = self.get_entities(response_json, report_url)\n            records.extend(new_records)\n\n            report_url = response_json[\"d\"].get(\"__next\")\n\n        return records\n\n    def _extract_records_from_url(self, url: str) -&gt; list[dict[str, Any]]:\n        \"\"\"Fetch URL to extract records.\n\n        Args:\n            url (str): The URL to extract records from.\n\n        Returns:\n            records (List[Dict[str, Any]]): The records extracted from URL.\n        \"\"\"\n        tmp_full_url = deepcopy(url)\n        tmp_filter_params = deepcopy(self.filter_params)\n        records = []\n        while url:\n            response = self.get_response(tmp_full_url, filter_params=tmp_filter_params)\n            response_json = response.json()\n            if isinstance(response_json[\"d\"], dict):\n                # ODATA v2+ API\n                new_records = response_json[\"d\"].get(\"results\")\n                url = response_json[\"d\"].get(\"__next\", None)\n            else:\n                # ODATA v1\n                new_records = response_json[\"d\"]\n                url = response_json.get(\"__next\", None)\n\n            # prevents concatenation of previous urls with filter_params with the same\n            # filter_params\n            tmp_filter_params = None\n            tmp_full_url = url\n\n            if hasattr(new_records, \"__iter__\"):\n                records.extend(new_records)\n        return records\n\n    def extract_records(\n        self, url: str | None = None, report_url: str | None = None\n    ) -&gt; list[dict[str, Any]]:\n        \"\"\"Download records from `url` or `report_url` if present.\n\n        Returns:\n            records (List[Dict[str, Any]]): The records extracted from URL.\n        \"\"\"\n        if self.is_report:\n            return self._extract_records_from_report_url(report_url=report_url)\n        full_url = urljoin(url, self.endpoint) if url else self.full_url\n        return self._extract_records_from_url(url=full_url)\n\n    def get_entities(\n        self, dirty_json: dict[str, Any], url: str\n    ) -&gt; list[dict[str, Any]]:\n        \"\"\"Extract entities from request.json().\n\n        Entities represent objects that store information. More info on:\n        https://help.sap.com/docs/EAD_HANA/0e60f05842fd41078917822867220c78/\n        0bd1db568fa546d6823d4c19a6b609ab.html\n\n        Args:\n            dirty_json (Dict[str, Any]): request.json() dict from response to API.\n            url (str): The URL to fetch metadata from.\n\n        Returns:\n            entities (List[Dict[str, Any]]): list filled with entities.\n        \"\"\"\n        metadata_url = self.create_metadata_url(url)\n        column_maper_dict = self.get_property_to_sap_label_dict(metadata_url)\n        entities = []\n        for element in dirty_json[\"d\"][\"results\"]:\n            new_entity = {}\n            for key, object_of_interest in element.items():\n                if key not in [\"__metadata\", \"Photo\", \"\", \"Picture\"] and \"{\" not in str(\n                    object_of_interest\n                ):\n                    new_key = column_maper_dict.get(key)\n                    if new_key:\n                        new_entity[new_key] = object_of_interest\n                    else:\n                        new_entity[key] = object_of_interest\n            entities.append(new_entity)\n        return entities\n\n    def get_property_to_sap_label_dict(self, url: str | None = None) -&gt; dict[str, str]:\n        \"\"\"Create Dict that maps Property Name to value of SAP label.\n\n           Property: Properties define the characteristics of the data.\n           SAP label: Labels are used for identification and for provision of content\n           information.\n\n        Args:\n            url (str, optional): The URL to fetch metadata from.\n\n        Returns:\n            Dict[str, str]: Property Name to value of SAP label.\n        \"\"\"\n        column_mapping = {}\n        if url:\n            username = self.credentials.get(\"username\")\n            password = self.credentials.get(\"password\")\n            response = requests.get(\n                url,\n                auth=(username, password.get_secret_value()),\n                timeout=(3.05, 60 * 5),\n            )\n            for sentence in response.text.split(\"/&gt;\"):\n                result = re.search(\n                    r'(?&lt;=Name=\")([^\"]+).+(sap:label=\")([^\"]+)+', sentence\n                )\n                if result:\n                    key = result.groups(0)[0]\n                    val = result.groups(0)[2]\n                    column_mapping[key] = val\n        return column_mapping\n\n    def get_response(\n        self,\n        url: str,\n        filter_params: dict[str, Any] | None = None,\n        timeout: tuple = (3.05, 60 * 30),\n    ) -&gt; requests.models.Response:\n        \"\"\"Handle requests.\n\n        Args:\n            url (str): The url to request to.\n            filter_params (Dict[str, Any], optional): Additional parameters like filter,\n                used in case of normal url.\n            timeout (tuple, optional): The request time-out. Default is (3.05, 60 * 30).\n\n        Returns:\n            requests.models.Response.\n        \"\"\"\n        username = self.credentials.get(\"username\")\n        password = self.credentials.get(\"password\")\n        return handle_api_response(\n            url=url,\n            params=filter_params,\n            auth=(username, password.get_secret_value()),\n            timeout=timeout,\n        )\n\n    def to_df(\n        self,\n        if_empty: Literal[\"warn\", \"skip\", \"fail\"] = \"warn\",\n        **kwargs,\n    ) -&gt; pd.DataFrame:\n        \"\"\"Download a table or report into a pandas DataFrame.\n\n        Args:\n            kwargs: The parameters to pass to DataFrame constructor.\n\n        Returns:\n            df (pandas.DataFrame): DataFrame containing the records.\n        \"\"\"\n        # Your implementation here\n        if if_empty == \"warn\":\n            self.logger.info(\"Warning: DataFrame is empty.\")\n        elif if_empty == \"skip\":\n            self.logger.info(\"Skipping due to empty DataFrame.\")\n        elif if_empty == \"fail\":\n            self.logger.info(\"Failing due to empty DataFrame.\")\n        else:\n            msg = \"Invalid value for if_empty parameter.\"\n            raise ValueError(msg)\n\n        url: str = kwargs.get(\"url\", \"\")\n        fields: list[str] = kwargs.get(\"fields\", [])\n        dtype: dict[str, Any] = kwargs.get(\"dtype\", {})\n        tests: dict[str, Any] = kwargs.get(\"tests\", {})\n\n        url = url or self.url\n        records = self.extract_records(url=url, report_url=self.report_url)\n        df = pd.DataFrame(data=records, **kwargs)\n\n        if dtype:\n            df = df.astype(dtype)\n\n        if fields:\n            return df[fields]\n\n        if tests:\n            validate(df=df, tests=tests)\n\n        return df\n</code></pre>"},{"location":"references/sources/api/#viadot.sources.cloud_for_customers.CloudForCustomers.__init__","title":"<code>__init__(*args, url=None, endpoint=None, report_url=None, filter_params=None, credentials=None, config_key=None, **kwargs)</code>","text":"<p>Initialize the class with the provided parameters.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <p>Variable length argument list.</p> <code>()</code> <code>url</code> <code>str</code> <p>The base URL for the service.</p> <code>None</code> <code>endpoint</code> <code>str</code> <p>The specific endpoint for the service.</p> <code>None</code> <code>report_url</code> <code>str</code> <p>The URL for the report.</p> <code>None</code> <code>filter_params</code> <code>Dict[str, Any]</code> <p>Parameters to filter the report data.</p> <code>None</code> <code>credentials</code> <code>CloudForCustomersCredentials</code> <p>Credentials required for authentication.</p> <code>None</code> <code>config_key</code> <code>Optional[str]</code> <p>A key to retrieve specific configuration settings.</p> <code>None</code> <code>**kwargs</code> <p>Arbitrary keyword arguments.</p> <code>{}</code> Source code in <code>src/viadot/sources/cloud_for_customers.py</code> <pre><code>def __init__(\n    self,\n    *args,\n    url: str | None = None,\n    endpoint: str | None = None,\n    report_url: str | None = None,\n    filter_params: dict[str, Any] | None = None,\n    credentials: CloudForCustomersCredentials | None = None,\n    config_key: str | None = None,\n    **kwargs,\n):\n    \"\"\"Initialize the class with the provided parameters.\n\n    Args:\n        *args: Variable length argument list.\n        url (str, optional): The base URL for the service.\n        endpoint (str, optional): The specific endpoint for the service.\n        report_url (str, optional): The URL for the report.\n        filter_params (Dict[str, Any], optional): Parameters to filter the report\n            data.\n        credentials (CloudForCustomersCredentials, optional): Credentials required\n            for authentication.\n        config_key (Optional[str], optional): A key to retrieve specific\n            configuration settings.\n        **kwargs: Arbitrary keyword arguments.\n    \"\"\"\n    # Credentials logic\n    raw_creds = credentials or get_source_credentials(config_key) or {}\n    validated_creds = dict(\n        CloudForCustomersCredentials(**raw_creds)\n    )  # validate the credentials\n    super().__init__(*args, credentials=validated_creds, **kwargs)\n\n    self.url = url or self.credentials.get(\"url\")\n    self.report_url = report_url or self.credentials.get(\"report_url\")\n\n    self.is_report = bool(self.report_url)\n    self.endpoint = endpoint\n\n    if self.url:\n        self.full_url = urljoin(self.url, self.endpoint)\n    if filter_params:\n        filter_params_merged = self.DEFAULT_PARAMS.copy()\n        filter_params_merged.update(filter_params)\n\n        self.filter_params = filter_params_merged\n    else:\n        self.filter_params = self.DEFAULT_PARAMS\n</code></pre>"},{"location":"references/sources/api/#viadot.sources.cloud_for_customers.CloudForCustomers.create_metadata_url","title":"<code>create_metadata_url(url)</code>  <code>staticmethod</code>","text":"<p>Create URL to fetch metadata from.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL to transform to metadata URL.</p> required <p>Returns:</p> Name Type Description <code>meta_url</code> <code>str</code> <p>The URL to fetch metadata from.</p> Source code in <code>src/viadot/sources/cloud_for_customers.py</code> <pre><code>@staticmethod\ndef create_metadata_url(url: str) -&gt; str:\n    \"\"\"Create URL to fetch metadata from.\n\n    Args:\n        url (str): The URL to transform to metadata URL.\n\n    Returns:\n        meta_url (str): The URL to fetch metadata from.\n    \"\"\"\n    start = url.split(\".svc\")[0]\n    url_raw = url.split(\"?\")[0]\n    end = url_raw.split(\"/\")[-1]\n    return start + \".svc/$metadata?entityset=\" + end\n</code></pre>"},{"location":"references/sources/api/#viadot.sources.cloud_for_customers.CloudForCustomers.extract_records","title":"<code>extract_records(url=None, report_url=None)</code>","text":"<p>Download records from <code>url</code> or <code>report_url</code> if present.</p> <p>Returns:</p> Name Type Description <code>records</code> <code>List[Dict[str, Any]]</code> <p>The records extracted from URL.</p> Source code in <code>src/viadot/sources/cloud_for_customers.py</code> <pre><code>def extract_records(\n    self, url: str | None = None, report_url: str | None = None\n) -&gt; list[dict[str, Any]]:\n    \"\"\"Download records from `url` or `report_url` if present.\n\n    Returns:\n        records (List[Dict[str, Any]]): The records extracted from URL.\n    \"\"\"\n    if self.is_report:\n        return self._extract_records_from_report_url(report_url=report_url)\n    full_url = urljoin(url, self.endpoint) if url else self.full_url\n    return self._extract_records_from_url(url=full_url)\n</code></pre>"},{"location":"references/sources/api/#viadot.sources.cloud_for_customers.CloudForCustomers.get_entities","title":"<code>get_entities(dirty_json, url)</code>","text":"<p>Extract entities from request.json().</p> <p>Entities represent objects that store information. More info on: https://help.sap.com/docs/EAD_HANA/0e60f05842fd41078917822867220c78/ 0bd1db568fa546d6823d4c19a6b609ab.html</p> <p>Parameters:</p> Name Type Description Default <code>dirty_json</code> <code>Dict[str, Any]</code> <p>request.json() dict from response to API.</p> required <code>url</code> <code>str</code> <p>The URL to fetch metadata from.</p> required <p>Returns:</p> Name Type Description <code>entities</code> <code>List[Dict[str, Any]]</code> <p>list filled with entities.</p> Source code in <code>src/viadot/sources/cloud_for_customers.py</code> <pre><code>def get_entities(\n    self, dirty_json: dict[str, Any], url: str\n) -&gt; list[dict[str, Any]]:\n    \"\"\"Extract entities from request.json().\n\n    Entities represent objects that store information. More info on:\n    https://help.sap.com/docs/EAD_HANA/0e60f05842fd41078917822867220c78/\n    0bd1db568fa546d6823d4c19a6b609ab.html\n\n    Args:\n        dirty_json (Dict[str, Any]): request.json() dict from response to API.\n        url (str): The URL to fetch metadata from.\n\n    Returns:\n        entities (List[Dict[str, Any]]): list filled with entities.\n    \"\"\"\n    metadata_url = self.create_metadata_url(url)\n    column_maper_dict = self.get_property_to_sap_label_dict(metadata_url)\n    entities = []\n    for element in dirty_json[\"d\"][\"results\"]:\n        new_entity = {}\n        for key, object_of_interest in element.items():\n            if key not in [\"__metadata\", \"Photo\", \"\", \"Picture\"] and \"{\" not in str(\n                object_of_interest\n            ):\n                new_key = column_maper_dict.get(key)\n                if new_key:\n                    new_entity[new_key] = object_of_interest\n                else:\n                    new_entity[key] = object_of_interest\n        entities.append(new_entity)\n    return entities\n</code></pre>"},{"location":"references/sources/api/#viadot.sources.cloud_for_customers.CloudForCustomers.get_property_to_sap_label_dict","title":"<code>get_property_to_sap_label_dict(url=None)</code>","text":"<p>Create Dict that maps Property Name to value of SAP label.</p> <p>Property: Properties define the characteristics of the data.    SAP label: Labels are used for identification and for provision of content    information.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL to fetch metadata from.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, str]</code> <p>Dict[str, str]: Property Name to value of SAP label.</p> Source code in <code>src/viadot/sources/cloud_for_customers.py</code> <pre><code>def get_property_to_sap_label_dict(self, url: str | None = None) -&gt; dict[str, str]:\n    \"\"\"Create Dict that maps Property Name to value of SAP label.\n\n       Property: Properties define the characteristics of the data.\n       SAP label: Labels are used for identification and for provision of content\n       information.\n\n    Args:\n        url (str, optional): The URL to fetch metadata from.\n\n    Returns:\n        Dict[str, str]: Property Name to value of SAP label.\n    \"\"\"\n    column_mapping = {}\n    if url:\n        username = self.credentials.get(\"username\")\n        password = self.credentials.get(\"password\")\n        response = requests.get(\n            url,\n            auth=(username, password.get_secret_value()),\n            timeout=(3.05, 60 * 5),\n        )\n        for sentence in response.text.split(\"/&gt;\"):\n            result = re.search(\n                r'(?&lt;=Name=\")([^\"]+).+(sap:label=\")([^\"]+)+', sentence\n            )\n            if result:\n                key = result.groups(0)[0]\n                val = result.groups(0)[2]\n                column_mapping[key] = val\n    return column_mapping\n</code></pre>"},{"location":"references/sources/api/#viadot.sources.cloud_for_customers.CloudForCustomers.get_response","title":"<code>get_response(url, filter_params=None, timeout=(3.05, 60 * 30))</code>","text":"<p>Handle requests.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The url to request to.</p> required <code>filter_params</code> <code>Dict[str, Any]</code> <p>Additional parameters like filter, used in case of normal url.</p> <code>None</code> <code>timeout</code> <code>tuple</code> <p>The request time-out. Default is (3.05, 60 * 30).</p> <code>(3.05, 60 * 30)</code> <p>Returns:</p> Type Description <code>Response</code> <p>requests.models.Response.</p> Source code in <code>src/viadot/sources/cloud_for_customers.py</code> <pre><code>def get_response(\n    self,\n    url: str,\n    filter_params: dict[str, Any] | None = None,\n    timeout: tuple = (3.05, 60 * 30),\n) -&gt; requests.models.Response:\n    \"\"\"Handle requests.\n\n    Args:\n        url (str): The url to request to.\n        filter_params (Dict[str, Any], optional): Additional parameters like filter,\n            used in case of normal url.\n        timeout (tuple, optional): The request time-out. Default is (3.05, 60 * 30).\n\n    Returns:\n        requests.models.Response.\n    \"\"\"\n    username = self.credentials.get(\"username\")\n    password = self.credentials.get(\"password\")\n    return handle_api_response(\n        url=url,\n        params=filter_params,\n        auth=(username, password.get_secret_value()),\n        timeout=timeout,\n    )\n</code></pre>"},{"location":"references/sources/api/#viadot.sources.cloud_for_customers.CloudForCustomers.to_df","title":"<code>to_df(if_empty='warn', **kwargs)</code>","text":"<p>Download a table or report into a pandas DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>kwargs</code> <p>The parameters to pass to DataFrame constructor.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>df</code> <code>DataFrame</code> <p>DataFrame containing the records.</p> Source code in <code>src/viadot/sources/cloud_for_customers.py</code> <pre><code>def to_df(\n    self,\n    if_empty: Literal[\"warn\", \"skip\", \"fail\"] = \"warn\",\n    **kwargs,\n) -&gt; pd.DataFrame:\n    \"\"\"Download a table or report into a pandas DataFrame.\n\n    Args:\n        kwargs: The parameters to pass to DataFrame constructor.\n\n    Returns:\n        df (pandas.DataFrame): DataFrame containing the records.\n    \"\"\"\n    # Your implementation here\n    if if_empty == \"warn\":\n        self.logger.info(\"Warning: DataFrame is empty.\")\n    elif if_empty == \"skip\":\n        self.logger.info(\"Skipping due to empty DataFrame.\")\n    elif if_empty == \"fail\":\n        self.logger.info(\"Failing due to empty DataFrame.\")\n    else:\n        msg = \"Invalid value for if_empty parameter.\"\n        raise ValueError(msg)\n\n    url: str = kwargs.get(\"url\", \"\")\n    fields: list[str] = kwargs.get(\"fields\", [])\n    dtype: dict[str, Any] = kwargs.get(\"dtype\", {})\n    tests: dict[str, Any] = kwargs.get(\"tests\", {})\n\n    url = url or self.url\n    records = self.extract_records(url=url, report_url=self.report_url)\n    df = pd.DataFrame(data=records, **kwargs)\n\n    if dtype:\n        df = df.astype(dtype)\n\n    if fields:\n        return df[fields]\n\n    if tests:\n        validate(df=df, tests=tests)\n\n    return df\n</code></pre>"},{"location":"references/sources/api/#viadot.sources.customer_gauge.CustomerGauge","title":"<code>viadot.sources.customer_gauge.CustomerGauge</code>","text":"<p>               Bases: <code>Source</code></p> <p>A class to connect and download data using Customer Gauge API.</p> <p>Documentation for each of this API's gateways:     Responses gateway https://support.customergauge.com/support/solutions/         articles/5000875861-get-responses     Non-Responses gateway https://support.customergauge.com/support/solutions/         articles/5000877703-get-non-responses</p> Source code in <code>src/viadot/sources/customer_gauge.py</code> <pre><code>class CustomerGauge(Source):\n    \"\"\"A class to connect and download data using Customer Gauge API.\n\n    Documentation for each of this API's gateways:\n        Responses gateway https://support.customergauge.com/support/solutions/\n            articles/5000875861-get-responses\n        Non-Responses gateway https://support.customergauge.com/support/solutions/\n            articles/5000877703-get-non-responses\n    \"\"\"\n\n    API_URL = \"https://api.eu.customergauge.com/v7/rest/sync/\"\n\n    def __init__(\n        self,\n        *args,\n        credentials: CustomerGaugeCredentials | None = None,\n        config_key: str = \"customer_gauge\",\n        **kwargs,\n    ):\n        \"\"\"A class to connect and download data using Customer Gauge API.\n\n        Args:\n            credentials (CustomerGaugeCredentials, optional): Customer Gauge\n                credentials. Defaults to None.\n            config_key (str, optional): The key in the viadot config holding relevant\n                credentials. Defaults to \"customer_gauge\".\n\n        Raises:\n            CredentialError: If credentials are not provided in local_config or\n                directly as a parameter.\n        \"\"\"\n        raw_creds = credentials or get_source_credentials(config_key)\n        validated_creds = dict(CustomerGaugeCredentials(**raw_creds))\n\n        super().__init__(*args, credentials=validated_creds, **kwargs)\n\n        self.clean_json = None\n\n    def _get_token(self) -&gt; str:\n        \"\"\"Gets Bearer Token using POST request method.\n\n        Raises:\n            APIError: If token is not returned.\n\n        Returns:\n            str: Bearer Token value.\n        \"\"\"\n        url = \"https://auth.EU.customergauge.com/oauth2/token\"\n        client_id = self.credentials.get(\"client_id\", None)\n        client_secret = self.credentials.get(\"client_secret\", None)\n\n        headers = {\"Content-Type\": \"application/x-www-form-urlencoded\"}\n        body = {\n            \"grant_type\": \"client_credentials\",\n            \"client_id\": client_id,\n            \"client_secret\": client_secret,\n        }\n        api_response = handle_api_response(\n            url=url, params=body, headers=headers, method=\"POST\"\n        )\n        token = api_response.json().get(\"access_token\")\n\n        if token is None:\n            message = \"The token could not be generated. Check your credentials.\"\n            raise APIError(message)\n\n        return token\n\n    def _field_reference_unpacker(\n        self,\n        json_response: dict[str, Any],\n        field: str,\n    ) -&gt; dict[str, Any]:\n        \"\"\"Unpack and modify dictionaries within the specified field of a JSON response.\n\n        This function takes a JSON response and a field name. It processes dictionaries\n        within the specified field, checking if each dictionary contains\n        exactly two items. If a dictionary meets this criteria, it is transformed\n        into a new dictionary, where the first key becomes a key, and the second key\n        becomes its associated value\n\n        Args:\n            json_response (dict[str, Any], optional): JSON response with data.\n            field (str): The key (column) of the dictionary to be modified.\n\n        Returns:\n            dict[str, Any]: The JSON response with modified nested dictionaries\n            within the specified field.\n\n        Raises:\n            ValueError: If a dictionary within the specified field doesn't contain\n                exactly two items.\n        \"\"\"\n        result = {}\n        for _, dictionary in enumerate(json_response[field]):\n            if isinstance(dictionary, dict) and len(dictionary.items()) == 2:  # noqa: PLR2004\n                list_properties = list(dictionary.values())\n                result[list_properties[0]] = list_properties[1]\n            else:\n                message = (\n                    \"Dictionary within the specified field \"\n                    \"doesn't contain exactly two items.\"\n                )\n                raise ValueError(message)\n        if result:\n            json_response[field] = result\n\n        return json_response\n\n    def _nested_dict_transformer(\n        self,\n        json_response: dict[str, Any],\n        field: str,\n    ) -&gt; dict[str, Any]:\n        \"\"\"Modify nested dictionaries within the specified field of a JSON response.\n\n        This function takes a JSON response and a field name. It modifies nested\n        dictionaries within the specified field by adding an index and underscore\n        to the keys. The modified dictionary is then updated in the JSON response.\n\n        Args:\n            json_response (dict[str, Any], optional): JSON response with data.\n            field (str): The key (column) of the dictionary to be modified.\n\n        Returns:\n            dict[str, Any]: The JSON response with modified nested dictionaries\n                within the specified field.\n        \"\"\"\n        result = {}\n        try:\n            for i, dictionary in enumerate(json_response[field], start=1):\n                for key, value in dictionary.items():\n                    result[f\"{i}_{key}\"] = value\n            if result:\n                json_response[field] = result\n        except TypeError as type_error:\n            self.logger.exception(type_error)  # noqa: TRY401\n            raise\n\n        return json_response\n\n    def _get_json_response(\n        self,\n        url: str | None = None,\n        cursor: int | None = None,\n        pagesize: int = 1000,\n        date_field: str | None = None,\n        start_date: datetime | None = None,\n        end_date: datetime | None = None,\n    ) -&gt; dict[str, Any]:\n        \"\"\"Get JSON that contains data and cursor parameter value.\n\n        Args:\n            url (str, optional): URL API direction. Defaults to None.\n            cursor (int, optional): Cursor value to navigate to the page.\n                Defaults to None.\n            pagesize (int, optional): Number of responses (records) returned per page,\n                max value = 1000. Defaults to 1000. Defaults to 1000.\n            date_field (str, optional): Specifies the date type which filter date range.\n                Possible options: \"date_creation\", \"date_order\", \"date_sent\" or\n                \"date_survey_response\". Defaults to None.\n            start_date (datetime, optional): Defines the period start date in\n                yyyy-mm-dd format. Defaults to None.\n            end_date (datetime, optional): Defines the period end date in\n                yyyy-mm-dd format. Defaults to None.\n\n        Raises:\n            ValueError: If at least one date argument were provided and the rest is\n                missing. Needed all 3 or skip them.\n            APIError: If no response from API call.\n\n        Returns:\n            dict[str, Any]: JSON with data and cursor parameter value.\n        \"\"\"\n        params = {\n            \"per_page\": pagesize,\n            \"with[]\": [\"drivers\", \"tags\", \"questions\", \"properties\"],\n            \"cursor\": cursor,\n        }\n\n        if any([date_field, start_date, end_date]):\n            if all([date_field, start_date, end_date]):\n                params[\"period[field]\"] = (date_field,)\n                params[\"period[start]\"] = (start_date,)\n                params[\"period[end]\"] = end_date\n            else:\n                message = (\n                    \"Missing date arguments: 'date_field', 'start_date', \"\n                    \"'end_date'. Provide all 3 arguments or skip all of them.\"\n                )\n                raise ValueError(message)\n\n        header = {\"Authorization\": f\"Bearer {self._get_token()}\"}\n        api_response = handle_api_response(url=url, headers=header, params=params)\n        response = api_response.json()\n\n        if response is None:\n            message = \"No response.\"\n            raise APIError(message)\n\n        return response\n\n    def _get_cursor(self, json_response: dict[str, Any] | None = None) -&gt; int:\n        \"\"\"Returns cursor value that is needed to navigate to the next page.\n\n        Args:\n            json_response (dict[str, Any], optional): Dictionary with nested structure\n                that contains data and cursor parameter value. Defaults to None.\n\n        Raises:\n            ValueError: If cursor value not found.\n\n        Returns:\n            int: Cursor value.\n        \"\"\"\n        try:\n            cur = json_response[\"cursor\"][\"next\"]\n        except KeyError as error:\n            message = (\n                \"Provided argument doesn't contain 'cursor' value. \"\n                \"Pass json returned from the endpoint.\"\n            )\n            raise ValueError(message) from error\n\n        return cur\n\n    def _get_data(\n        self,\n        json_response: dict[str, Any] | None = None,\n    ) -&gt; list[dict[str, Any]]:\n        \"\"\"Extract the 'data' part of a JSON response as a list of dictionaries.\n\n        Args:\n            json_response (Dict[str, Any], optional): JSON object represented as a\n                nested dictionary that contains data and cursor parameter value.\n                Defaults to None.\n\n        Raises:\n            KeyError: If the 'data' key is not present in the provided JSON response.\n\n        Returns:\n            list[dict[str, Any]]: A list of dictionaries containing data from the 'data'\n                part of the JSON response.\n        \"\"\"\n        jsons_list = []\n        try:\n            jsons_list = json_response[\"data\"]\n        except KeyError:\n            message = (\n                \"Provided argument doesn't contain 'data' value. \"\n                \"Pass json returned from the endpoint.\"\n            )\n            self.logger.exception(message)\n            raise\n\n        return jsons_list\n\n    def _unpack_columns(\n        self,\n        json_list: list[dict[str, Any]] | None = None,\n        unpack_by_field_reference_cols: list[str] | None = None,\n        unpack_by_nested_dict_transformer: list[str] | None = None,\n    ) -&gt; list[dict[str, Any]]:\n        \"\"\"Function to unpack and modify specific columns in a list of dictionaries.\n\n        It is using one of two methods, chosen by the user.\n        If user would like to use field_reference_unpacker, he/she needs to provide\n        list of fields as strings in `unpack_by_field_reference_cols`  parameter,\n        if user would like to use nested_dict_transformer he/she needs to provide\n        list of fields as strings in unpack_by_nested_dict_transformer parameter.\n\n        Args:\n            json_list (list[dict[str, Any]): A list of dictionaries containing the data.\n            unpack_by_field_reference_cols (list[str]): Columns to unpack and modify\n                using `_field_reference_unpacker`. Defaults to None.\n            unpack_by_nested_dict_transformer (list[str]): Columns to unpack and\n                modify using `_nested_dict_transformer`. Defaults to None.\n\n        Raises:\n            ValueError: If 'json_list' is not provided.\n            ValueError: If specified columns do not exist in the JSON data.\n            ValueError: If columns are mentioned in both\n                'unpack_by_field_reference_cols' and 'unpack_by_nested_dict_transformer'\n\n        Returns:\n            list[dict[str, Any]]: The updated list of dictionaries after column\n                unpacking and modification.\n        \"\"\"\n        duplicated_cols = []\n\n        if json_list is None:\n            message = \"Input 'json_list' is required.\"\n            raise ValueError(message)\n\n        def unpack_columns(columns: list[str], unpack_function: dict[str, Any]):\n            json_list_clean = json_list.copy()\n            for field in columns:\n                if field in json_list_clean[0]:\n                    self.logger.info(\n                        f\"\"\"Unpacking column '{field}' with {unpack_function.__name__}\n                        method...\"\"\"\n                    )\n                    try:\n                        json_list_clean = list(\n                            map(\n                                lambda x, f=field: unpack_function(x, f),\n                                json_list_clean,\n                            )\n                        )\n                        self.logger.info(\n                            f\"All elements in '{field}' are unpacked successfully.\"\n                        )\n                    except ValueError:\n                        self.logger.info(\n                            f\"No transformation were made in '{field}',\"\n                            \"because didn't contain list of key-value data.\"\n                        )\n                else:\n                    self.logger.info(f\"Column '{field}' not found.\")\n            return json_list_clean\n\n        if unpack_by_field_reference_cols and unpack_by_nested_dict_transformer:\n            duplicated_cols = set(unpack_by_field_reference_cols).intersection(\n                set(unpack_by_nested_dict_transformer)\n            )\n        if duplicated_cols:\n            message = (\n                f\"{duplicated_cols} were mentioned in both\"\n                \"unpack_by_field_reference_cols and unpack_by_nested_dict_transformer.\"\n                \"It's not possible to apply two methods to the same field.\"\n            )\n            raise ValueError(message)\n\n        if unpack_by_field_reference_cols is not None:\n            json_list = unpack_columns(\n                columns=unpack_by_field_reference_cols,\n                unpack_function=self._field_reference_unpacker,\n            )\n\n        if unpack_by_nested_dict_transformer is not None:\n            json_list = unpack_columns(\n                columns=unpack_by_nested_dict_transformer,\n                unpack_function=self._nested_dict_transformer,\n            )\n\n        return json_list\n\n    def api_connection(\n        self,\n        endpoint: Literal[\"responses\", \"non-responses\"] = \"non-responses\",\n        cursor: int | None = None,\n        pagesize: int = 1000,\n        date_field: str | None = None,\n        start_date: datetime | None = None,\n        end_date: datetime | None = None,\n        total_load: bool = True,\n        unpack_by_field_reference_cols: list[str] | None = None,\n        unpack_by_nested_dict_transformer: list[str] | None = None,\n    ) -&gt; None:\n        \"\"\"General method to connect to Customer Gauge API and generate the response.\n\n        Args:\n            endpoint (Literal[\"responses\", \"non-responses\"], optional): Indicate which\n                endpoint to connect. Defaults to \"non-responses.\n            cursor (int, optional): Cursor value to navigate to the page.\n                Defaults to None.\n            pagesize (int, optional): Number of responses (records) returned per page,\n                max value = 1000. Defaults to 1000. Defaults to 1000.\n            date_field (str, optional): Specifies the date type which filter date range.\n                Possible options: \"date_creation\", \"date_order\", \"date_sent\" or\n                \"date_survey_response\". Defaults to None.\n            start_date (datetime, optional): Defines the period start date in\n                yyyy-mm-dd format. Defaults to None.\n            end_date (datetime, optional): Defines the period end date in\n                yyyy-mm-dd format. Defaults to None.\n            total_load (bool, optional): Indicate whether to download the data to the\n                latest. If 'False', only one API call is executed (up to 1000 records).\n                Defaults to True.\n            unpack_by_field_reference_cols (list[str]): Columns to unpack and modify\n                using `_field_reference_unpacker`. Defaults to None.\n            unpack_by_nested_dict_transformer (list[str]): Columns to unpack and modify\n                using `_nested_dict_transformer`. Defaults to None.\n        \"\"\"\n        url = f\"{self.API_URL}{endpoint}\"\n        self.logger.info(f\"Starting downloading data from {url} endpoint...\")\n\n        total_json = []\n        json_data = self._get_json_response(\n            url=url,\n            cursor=cursor,\n            pagesize=pagesize,\n            date_field=date_field,\n            start_date=start_date,\n            end_date=end_date,\n        )\n        cur = self._get_cursor(json_data)\n        jsn = self._get_data(json_data)\n\n        if total_load:\n            if cursor is None:\n                self.logger.info(\n                    f\"Downloading all the data from the {endpoint} endpoint.\"\n                    \"Process might take a few minutes...\"\n                )\n            else:\n                self.logger.info(\n                    f\"Downloading starting from the {cursor} cursor. \"\n                    \"Process might take a few minutes...\"\n                )\n            while jsn:\n                json_data = self._get_json_response(url=url, cursor=cur)\n                cur = self._get_cursor(json_data)\n                jsn = self._get_data(json_data)\n                total_json += jsn\n\n        self.clean_json = self._unpack_columns(\n            json_list=total_json,\n            unpack_by_field_reference_cols=unpack_by_field_reference_cols,\n            unpack_by_nested_dict_transformer=unpack_by_nested_dict_transformer,\n        )\n\n    def _flatten_json(\n        self, json_response: dict[str, Any] | None = None\n    ) -&gt; dict[str, Any]:\n        \"\"\"Flattens a nested structure of the JSON object.\n\n        Function flattens it into a single-level dictionary. It uses a nested\n        `flattify()` function to recursively combine nested keys in the JSON\n        object with '_' to create the flattened keys.\n\n        Args:\n            json_response (dict[str, Any], optional): JSON object represented as a\n                nested dictionary. Defaults to None.\n\n        Raises:\n            TypeError: If the 'json_response' not a dictionary.\n\n        Returns:\n            dict[str, Any]: The flattened dictionary.\n        \"\"\"\n        result = {}\n\n        if not isinstance(json_response, dict):\n            message = \"Input must be a dictionary.\"\n            raise TypeError(message)\n\n        def flattify(\n            field: dict[str, Any], key: str = \"\", out: dict[str, Any] | None = None\n        ):\n            if out is None:\n                out = result\n\n            if isinstance(field, dict):\n                for item in field:\n                    flattify(field[item], key + item + \"_\", out)\n            else:\n                out[key[:-1]] = field\n\n        flattify(json_response)\n\n        return result\n\n    def _remove_square_brackets(self, df: pd.DataFrame = None) -&gt; pd.DataFrame:\n        \"\"\"Replace square brackets \"[]\" with an empty string in a pandas DataFrame.\n\n        Args:\n            df (pd.DataFrame, optional): Replace square brackets \"[]\" with an empty\n                string in a pandas DataFrame. Defaults to None.\n\n        Returns:\n            pd.DataFrame: The modified DataFrame with square brackets replaced by\n                an empty string.\n        \"\"\"\n        df = df.astype(str)\n\n        return df.map(lambda x: x.strip(\"[]\"))\n\n    def _clean_drivers(self, drivers: str | None = None) -&gt; str:\n        \"\"\"Clean and format the 'drivers' data.\n\n        Args:\n            drivers (str, optional): Column name of the data to be cleaned.\n                Defaults to None.\n\n        Returns:\n            str: A cleaned and formatted string of driver data.\n        \"\"\"\n        return (\n            drivers.replace(\"{\", \"\")\n            .replace(\"}\", \"\")\n            .replace(\"'\", \"\")\n            .replace(\"label: \", \"\")\n        )\n\n    @add_viadot_metadata_columns\n    def to_df(\n        self,\n        if_empty: str = \"warn\",\n        validate_df_dict: dict[str, Any] | None = None,\n        anonymize: bool = False,\n        columns_to_anonymize: list[str] | None = None,\n        anonymize_method: Literal[\"mask\", \"hash\"] = \"mask\",\n        anonymize_value: str = \"***\",\n        date_column: str | None = None,\n        days: int | None = None,\n    ) -&gt; pd.DataFrame:\n        \"\"\"Generate a Pandas Data Frame with the data in the Response and metadata.\n\n        Args:\n            if_empty (str, optional): What to do if a fetch produce no data.\n                Defaults to \"warn\n            validate_df_dict (dict[str, Any], optional): A dictionary with optional list\n                of tests to verify the output dataframe. If defined, triggers the\n                `validate_df` task from task_utils. Defaults to None.\n            anonymize (bool, optional): Indicates if anonymize selected columns.\n                Defaults to False.\n            columns_to_anonymize (list[str], optional): List of columns to anonymize.\n                Defaults to None.\n            anonymize_method  (Literal[\"mask\", \"hash\"], optional): Method of\n                anonymizing data. \"mask\" -&gt; replace the data with \"value\" arg. \"hash\" -&gt;\n                replace the data with the hash value of an object (using `hash()`\n                method). Defaults to \"mask\".\n            anonymize_value (str, optional): Value to replace the data.\n                Defaults to \"***\".\n            date_column (str, optional): Name of the date column used to identify rows\n                that are older than a specified number of days. Defaults to None.\n            days (int, optional): The number of days beyond which we want to anonymize\n                the data, e.g. older than 2 years can be: 2*365. Defaults to None.\n\n        Returns:\n            pd.Dataframe: The response data as a Pandas Data Frame plus viadot metadata.\n        \"\"\"\n        super().to_df(if_empty=if_empty)\n\n        self.logger.info(\"Inserting data into the DataFrame...\")\n\n        data_frame = pd.DataFrame(list(map(self._flatten_json, self.clean_json)))\n        data_frame = self._remove_square_brackets(data_frame)\n\n        if \"drivers\" in list(data_frame.columns):\n            data_frame[\"drivers\"] = data_frame[\"drivers\"].apply(self._clean_drivers)\n        data_frame.columns = data_frame.columns.str.lower().str.replace(\" \", \"_\")\n\n        if data_frame.empty:\n            self._handle_if_empty(\n                if_empty=if_empty,\n                message=\"The response does not contain any data.\",\n            )\n        else:\n            self.logger.info(\n                \"Successfully downloaded data from the Customer Gauge API.\"\n            )\n\n        if validate_df_dict is not None:\n            validate(df=data_frame, tests=validate_df_dict)\n\n        if anonymize:\n            data_frame = anonymize_df(\n                df=data_frame,\n                columns=columns_to_anonymize,\n                method=anonymize_method,\n                mask_value=anonymize_value,\n                date_column=date_column,\n                days=days,\n            )\n            self.logger.info(\"Data Frame anonymized\")\n\n        return data_frame\n</code></pre>"},{"location":"references/sources/api/#viadot.sources.customer_gauge.CustomerGauge.__init__","title":"<code>__init__(*args, credentials=None, config_key='customer_gauge', **kwargs)</code>","text":"<p>A class to connect and download data using Customer Gauge API.</p> <p>Parameters:</p> Name Type Description Default <code>credentials</code> <code>CustomerGaugeCredentials</code> <p>Customer Gauge credentials. Defaults to None.</p> <code>None</code> <code>config_key</code> <code>str</code> <p>The key in the viadot config holding relevant credentials. Defaults to \"customer_gauge\".</p> <code>'customer_gauge'</code> <p>Raises:</p> Type Description <code>CredentialError</code> <p>If credentials are not provided in local_config or directly as a parameter.</p> Source code in <code>src/viadot/sources/customer_gauge.py</code> <pre><code>def __init__(\n    self,\n    *args,\n    credentials: CustomerGaugeCredentials | None = None,\n    config_key: str = \"customer_gauge\",\n    **kwargs,\n):\n    \"\"\"A class to connect and download data using Customer Gauge API.\n\n    Args:\n        credentials (CustomerGaugeCredentials, optional): Customer Gauge\n            credentials. Defaults to None.\n        config_key (str, optional): The key in the viadot config holding relevant\n            credentials. Defaults to \"customer_gauge\".\n\n    Raises:\n        CredentialError: If credentials are not provided in local_config or\n            directly as a parameter.\n    \"\"\"\n    raw_creds = credentials or get_source_credentials(config_key)\n    validated_creds = dict(CustomerGaugeCredentials(**raw_creds))\n\n    super().__init__(*args, credentials=validated_creds, **kwargs)\n\n    self.clean_json = None\n</code></pre>"},{"location":"references/sources/api/#viadot.sources.customer_gauge.CustomerGauge.api_connection","title":"<code>api_connection(endpoint='non-responses', cursor=None, pagesize=1000, date_field=None, start_date=None, end_date=None, total_load=True, unpack_by_field_reference_cols=None, unpack_by_nested_dict_transformer=None)</code>","text":"<p>General method to connect to Customer Gauge API and generate the response.</p> <p>Parameters:</p> Name Type Description Default <code>endpoint</code> <code>Literal['responses', 'non-responses']</code> <p>Indicate which endpoint to connect. Defaults to \"non-responses.</p> <code>'non-responses'</code> <code>cursor</code> <code>int</code> <p>Cursor value to navigate to the page. Defaults to None.</p> <code>None</code> <code>pagesize</code> <code>int</code> <p>Number of responses (records) returned per page, max value = 1000. Defaults to 1000. Defaults to 1000.</p> <code>1000</code> <code>date_field</code> <code>str</code> <p>Specifies the date type which filter date range. Possible options: \"date_creation\", \"date_order\", \"date_sent\" or \"date_survey_response\". Defaults to None.</p> <code>None</code> <code>start_date</code> <code>datetime</code> <p>Defines the period start date in yyyy-mm-dd format. Defaults to None.</p> <code>None</code> <code>end_date</code> <code>datetime</code> <p>Defines the period end date in yyyy-mm-dd format. Defaults to None.</p> <code>None</code> <code>total_load</code> <code>bool</code> <p>Indicate whether to download the data to the latest. If 'False', only one API call is executed (up to 1000 records). Defaults to True.</p> <code>True</code> <code>unpack_by_field_reference_cols</code> <code>list[str]</code> <p>Columns to unpack and modify using <code>_field_reference_unpacker</code>. Defaults to None.</p> <code>None</code> <code>unpack_by_nested_dict_transformer</code> <code>list[str]</code> <p>Columns to unpack and modify using <code>_nested_dict_transformer</code>. Defaults to None.</p> <code>None</code> Source code in <code>src/viadot/sources/customer_gauge.py</code> <pre><code>def api_connection(\n    self,\n    endpoint: Literal[\"responses\", \"non-responses\"] = \"non-responses\",\n    cursor: int | None = None,\n    pagesize: int = 1000,\n    date_field: str | None = None,\n    start_date: datetime | None = None,\n    end_date: datetime | None = None,\n    total_load: bool = True,\n    unpack_by_field_reference_cols: list[str] | None = None,\n    unpack_by_nested_dict_transformer: list[str] | None = None,\n) -&gt; None:\n    \"\"\"General method to connect to Customer Gauge API and generate the response.\n\n    Args:\n        endpoint (Literal[\"responses\", \"non-responses\"], optional): Indicate which\n            endpoint to connect. Defaults to \"non-responses.\n        cursor (int, optional): Cursor value to navigate to the page.\n            Defaults to None.\n        pagesize (int, optional): Number of responses (records) returned per page,\n            max value = 1000. Defaults to 1000. Defaults to 1000.\n        date_field (str, optional): Specifies the date type which filter date range.\n            Possible options: \"date_creation\", \"date_order\", \"date_sent\" or\n            \"date_survey_response\". Defaults to None.\n        start_date (datetime, optional): Defines the period start date in\n            yyyy-mm-dd format. Defaults to None.\n        end_date (datetime, optional): Defines the period end date in\n            yyyy-mm-dd format. Defaults to None.\n        total_load (bool, optional): Indicate whether to download the data to the\n            latest. If 'False', only one API call is executed (up to 1000 records).\n            Defaults to True.\n        unpack_by_field_reference_cols (list[str]): Columns to unpack and modify\n            using `_field_reference_unpacker`. Defaults to None.\n        unpack_by_nested_dict_transformer (list[str]): Columns to unpack and modify\n            using `_nested_dict_transformer`. Defaults to None.\n    \"\"\"\n    url = f\"{self.API_URL}{endpoint}\"\n    self.logger.info(f\"Starting downloading data from {url} endpoint...\")\n\n    total_json = []\n    json_data = self._get_json_response(\n        url=url,\n        cursor=cursor,\n        pagesize=pagesize,\n        date_field=date_field,\n        start_date=start_date,\n        end_date=end_date,\n    )\n    cur = self._get_cursor(json_data)\n    jsn = self._get_data(json_data)\n\n    if total_load:\n        if cursor is None:\n            self.logger.info(\n                f\"Downloading all the data from the {endpoint} endpoint.\"\n                \"Process might take a few minutes...\"\n            )\n        else:\n            self.logger.info(\n                f\"Downloading starting from the {cursor} cursor. \"\n                \"Process might take a few minutes...\"\n            )\n        while jsn:\n            json_data = self._get_json_response(url=url, cursor=cur)\n            cur = self._get_cursor(json_data)\n            jsn = self._get_data(json_data)\n            total_json += jsn\n\n    self.clean_json = self._unpack_columns(\n        json_list=total_json,\n        unpack_by_field_reference_cols=unpack_by_field_reference_cols,\n        unpack_by_nested_dict_transformer=unpack_by_nested_dict_transformer,\n    )\n</code></pre>"},{"location":"references/sources/api/#viadot.sources.customer_gauge.CustomerGauge.to_df","title":"<code>to_df(if_empty='warn', validate_df_dict=None, anonymize=False, columns_to_anonymize=None, anonymize_method='mask', anonymize_value='***', date_column=None, days=None)</code>","text":"<p>Generate a Pandas Data Frame with the data in the Response and metadata.</p> <p>Parameters:</p> Name Type Description Default <code>if_empty</code> <code>str</code> <p>What to do if a fetch produce no data. Defaults to \"warn</p> <code>'warn'</code> <code>validate_df_dict</code> <code>dict[str, Any]</code> <p>A dictionary with optional list of tests to verify the output dataframe. If defined, triggers the <code>validate_df</code> task from task_utils. Defaults to None.</p> <code>None</code> <code>anonymize</code> <code>bool</code> <p>Indicates if anonymize selected columns. Defaults to False.</p> <code>False</code> <code>columns_to_anonymize</code> <code>list[str]</code> <p>List of columns to anonymize. Defaults to None.</p> <code>None</code> <code>anonymize_method</code> <code> (Literal[\"mask\", \"hash\"]</code> <p>Method of anonymizing data. \"mask\" -&gt; replace the data with \"value\" arg. \"hash\" -&gt; replace the data with the hash value of an object (using <code>hash()</code> method). Defaults to \"mask\".</p> <code>'mask'</code> <code>anonymize_value</code> <code>str</code> <p>Value to replace the data. Defaults to \"***\".</p> <code>'***'</code> <code>date_column</code> <code>str</code> <p>Name of the date column used to identify rows that are older than a specified number of days. Defaults to None.</p> <code>None</code> <code>days</code> <code>int</code> <p>The number of days beyond which we want to anonymize the data, e.g. older than 2 years can be: 2*365. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.Dataframe: The response data as a Pandas Data Frame plus viadot metadata.</p> Source code in <code>src/viadot/sources/customer_gauge.py</code> <pre><code>@add_viadot_metadata_columns\ndef to_df(\n    self,\n    if_empty: str = \"warn\",\n    validate_df_dict: dict[str, Any] | None = None,\n    anonymize: bool = False,\n    columns_to_anonymize: list[str] | None = None,\n    anonymize_method: Literal[\"mask\", \"hash\"] = \"mask\",\n    anonymize_value: str = \"***\",\n    date_column: str | None = None,\n    days: int | None = None,\n) -&gt; pd.DataFrame:\n    \"\"\"Generate a Pandas Data Frame with the data in the Response and metadata.\n\n    Args:\n        if_empty (str, optional): What to do if a fetch produce no data.\n            Defaults to \"warn\n        validate_df_dict (dict[str, Any], optional): A dictionary with optional list\n            of tests to verify the output dataframe. If defined, triggers the\n            `validate_df` task from task_utils. Defaults to None.\n        anonymize (bool, optional): Indicates if anonymize selected columns.\n            Defaults to False.\n        columns_to_anonymize (list[str], optional): List of columns to anonymize.\n            Defaults to None.\n        anonymize_method  (Literal[\"mask\", \"hash\"], optional): Method of\n            anonymizing data. \"mask\" -&gt; replace the data with \"value\" arg. \"hash\" -&gt;\n            replace the data with the hash value of an object (using `hash()`\n            method). Defaults to \"mask\".\n        anonymize_value (str, optional): Value to replace the data.\n            Defaults to \"***\".\n        date_column (str, optional): Name of the date column used to identify rows\n            that are older than a specified number of days. Defaults to None.\n        days (int, optional): The number of days beyond which we want to anonymize\n            the data, e.g. older than 2 years can be: 2*365. Defaults to None.\n\n    Returns:\n        pd.Dataframe: The response data as a Pandas Data Frame plus viadot metadata.\n    \"\"\"\n    super().to_df(if_empty=if_empty)\n\n    self.logger.info(\"Inserting data into the DataFrame...\")\n\n    data_frame = pd.DataFrame(list(map(self._flatten_json, self.clean_json)))\n    data_frame = self._remove_square_brackets(data_frame)\n\n    if \"drivers\" in list(data_frame.columns):\n        data_frame[\"drivers\"] = data_frame[\"drivers\"].apply(self._clean_drivers)\n    data_frame.columns = data_frame.columns.str.lower().str.replace(\" \", \"_\")\n\n    if data_frame.empty:\n        self._handle_if_empty(\n            if_empty=if_empty,\n            message=\"The response does not contain any data.\",\n        )\n    else:\n        self.logger.info(\n            \"Successfully downloaded data from the Customer Gauge API.\"\n        )\n\n    if validate_df_dict is not None:\n        validate(df=data_frame, tests=validate_df_dict)\n\n    if anonymize:\n        data_frame = anonymize_df(\n            df=data_frame,\n            columns=columns_to_anonymize,\n            method=anonymize_method,\n            mask_value=anonymize_value,\n            date_column=date_column,\n            days=days,\n        )\n        self.logger.info(\"Data Frame anonymized\")\n\n    return data_frame\n</code></pre>"},{"location":"references/sources/api/#viadot.sources.epicor.Epicor","title":"<code>viadot.sources.epicor.Epicor</code>","text":"<p>               Bases: <code>Source</code></p> Source code in <code>src/viadot/sources/epicor.py</code> <pre><code>class Epicor(Source):\n    def __init__(\n        self,\n        base_url: str,\n        credentials: dict[str, Any] | None = None,\n        config_key: str | None = None,\n        validate_date_filter: bool = True,\n        start_date_field: str = \"BegInvoiceDate\",\n        end_date_field: str = \"EndInvoiceDate\",\n        *args,\n        **kwargs,\n    ):\n        \"\"\"Class to connect to Epicor API and pasere XML output into a pandas DataFrame.\n\n        Args:\n            base_url (str, required): Base url to Epicor.\n            filters_xml (str, required): Filters in form of XML. The date filter\n                 is required.\n            credentials (dict[str, Any], optional): Credentials to connect with\n            Epicor API containing host, port, username and password.Defaults to None.\n            config_key (str, optional): Credential key to dictionary where details\n                 are stored.\n            validate_date_filter (bool, optional): Whether or not validate xml\n                date filters. Defaults to True.\n            start_date_field (str, optional) The name of filters field containing\n                start date.Defaults to \"BegInvoiceDate\".\n            end_date_field (str, optional) The name of filters field containing\n                end date. Defaults to \"EndInvoiceDate\".\n        \"\"\"\n        raw_creds = credentials or get_source_credentials(config_key)\n        validated_creds = dict(EpicorCredentials(**raw_creds))\n\n        self.base_url = base_url\n        self.validate_date_filter = validate_date_filter\n        self.start_date_field = start_date_field\n        self.end_date_field = end_date_field\n\n        self.url = (\n            \"http://\"\n            + validated_creds[\"host\"]\n            + \":\"\n            + str(validated_creds[\"port\"])\n            + base_url\n        )\n\n        super().__init__(*args, credentials=validated_creds, **kwargs)\n\n    def generate_token(self) -&gt; str:\n        \"\"\"Function to generate API access token that is valid for 24 hours.\n\n        Returns:\n            str: Generated token.\n        \"\"\"\n        url = (\n            \"http://\"\n            + self.credentials[\"host\"]\n            + \":\"\n            + str(self.credentials[\"port\"])\n            + \"/api/security/token/\"\n        )\n\n        headers = {\n            \"Content-Type\": \"application/json\",\n            \"username\": self.credentials[\"username\"],\n            \"password\": self.credentials[\"password\"],\n        }\n\n        response = handle_api_response(url=url, headers=headers, method=\"POST\")\n        root = ET.fromstring(response.text)\n        return root.find(\"AccessToken\").text\n\n    def validate_filter(self, filters_xml: str) -&gt; None:\n        \"Function checking if user had specified date range filters.\"\n        root = ET.fromstring(filters_xml)\n        for child in root:\n            for subchild in child:\n                if (\n                    subchild.tag in (self.start_date_field, self.end_date_field)\n                ) and subchild.text is None:\n                    msg = \"Too much data. Please provide a date range filter.\"\n                    raise DataRangeError(msg)\n\n    def get_xml_response(self, filters_xml: str) -&gt; requests.models.Response:\n        \"Function for getting response from Epicor API.\"\n        if self.validate_date_filter is True:\n            self.validate_filter(filters_xml)\n        payload = filters_xml\n        headers = {\n            \"Content-Type\": \"application/xml\",\n            \"Authorization\": \"Bearer \" + self.generate_token(),\n        }\n        return handle_api_response(\n            url=self.url, headers=headers, data=payload, method=\"POST\"\n        )\n\n    def to_df(self, filters_xml: str) -&gt; pd.DataFrame:\n        \"\"\"Function for creating pandas DataFrame from Epicor API response.\n\n        Returns:\n            pd.DataFrame: Output DataFrame.\n        \"\"\"\n        data = self.get_xml_response(filters_xml)\n        if \"ORDER.HISTORY.DETAIL.QUERY\" in self.base_url:\n            df = parse_orders_xml(data)\n        elif \"CUSTOMER.QUERY\" in self.base_url:\n            df = parse_customer_xml(data)\n        elif \"ORDER.DETAIL.PROD.QUERY\" in self.base_url:\n            df = parse_open_orders_xml(data)\n        elif \"BOOKINGS.DETAIL.QUERY\" in self.base_url:\n            df = parse_bookings_xml(data)\n        else:\n            msg = f\"Parser for selected viev {self.base_url} is not avaiable\"\n            raise ValidationError(msg)\n\n        return df\n</code></pre>"},{"location":"references/sources/api/#viadot.sources.epicor.Epicor.__init__","title":"<code>__init__(base_url, credentials=None, config_key=None, validate_date_filter=True, start_date_field='BegInvoiceDate', end_date_field='EndInvoiceDate', *args, **kwargs)</code>","text":"<p>Class to connect to Epicor API and pasere XML output into a pandas DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>base_url</code> <code>(str, required)</code> <p>Base url to Epicor.</p> required <code>filters_xml</code> <code>(str, required)</code> <p>Filters in form of XML. The date filter  is required.</p> required <code>credentials</code> <code>dict[str, Any]</code> <p>Credentials to connect with</p> <code>None</code> <code>config_key</code> <code>str</code> <p>Credential key to dictionary where details  are stored.</p> <code>None</code> <code>validate_date_filter</code> <code>bool</code> <p>Whether or not validate xml date filters. Defaults to True.</p> <code>True</code> Source code in <code>src/viadot/sources/epicor.py</code> <pre><code>def __init__(\n    self,\n    base_url: str,\n    credentials: dict[str, Any] | None = None,\n    config_key: str | None = None,\n    validate_date_filter: bool = True,\n    start_date_field: str = \"BegInvoiceDate\",\n    end_date_field: str = \"EndInvoiceDate\",\n    *args,\n    **kwargs,\n):\n    \"\"\"Class to connect to Epicor API and pasere XML output into a pandas DataFrame.\n\n    Args:\n        base_url (str, required): Base url to Epicor.\n        filters_xml (str, required): Filters in form of XML. The date filter\n             is required.\n        credentials (dict[str, Any], optional): Credentials to connect with\n        Epicor API containing host, port, username and password.Defaults to None.\n        config_key (str, optional): Credential key to dictionary where details\n             are stored.\n        validate_date_filter (bool, optional): Whether or not validate xml\n            date filters. Defaults to True.\n        start_date_field (str, optional) The name of filters field containing\n            start date.Defaults to \"BegInvoiceDate\".\n        end_date_field (str, optional) The name of filters field containing\n            end date. Defaults to \"EndInvoiceDate\".\n    \"\"\"\n    raw_creds = credentials or get_source_credentials(config_key)\n    validated_creds = dict(EpicorCredentials(**raw_creds))\n\n    self.base_url = base_url\n    self.validate_date_filter = validate_date_filter\n    self.start_date_field = start_date_field\n    self.end_date_field = end_date_field\n\n    self.url = (\n        \"http://\"\n        + validated_creds[\"host\"]\n        + \":\"\n        + str(validated_creds[\"port\"])\n        + base_url\n    )\n\n    super().__init__(*args, credentials=validated_creds, **kwargs)\n</code></pre>"},{"location":"references/sources/api/#viadot.sources.epicor.Epicor.generate_token","title":"<code>generate_token()</code>","text":"<p>Function to generate API access token that is valid for 24 hours.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Generated token.</p> Source code in <code>src/viadot/sources/epicor.py</code> <pre><code>def generate_token(self) -&gt; str:\n    \"\"\"Function to generate API access token that is valid for 24 hours.\n\n    Returns:\n        str: Generated token.\n    \"\"\"\n    url = (\n        \"http://\"\n        + self.credentials[\"host\"]\n        + \":\"\n        + str(self.credentials[\"port\"])\n        + \"/api/security/token/\"\n    )\n\n    headers = {\n        \"Content-Type\": \"application/json\",\n        \"username\": self.credentials[\"username\"],\n        \"password\": self.credentials[\"password\"],\n    }\n\n    response = handle_api_response(url=url, headers=headers, method=\"POST\")\n    root = ET.fromstring(response.text)\n    return root.find(\"AccessToken\").text\n</code></pre>"},{"location":"references/sources/api/#viadot.sources.epicor.Epicor.get_xml_response","title":"<code>get_xml_response(filters_xml)</code>","text":"<p>Function for getting response from Epicor API.</p> Source code in <code>src/viadot/sources/epicor.py</code> <pre><code>def get_xml_response(self, filters_xml: str) -&gt; requests.models.Response:\n    \"Function for getting response from Epicor API.\"\n    if self.validate_date_filter is True:\n        self.validate_filter(filters_xml)\n    payload = filters_xml\n    headers = {\n        \"Content-Type\": \"application/xml\",\n        \"Authorization\": \"Bearer \" + self.generate_token(),\n    }\n    return handle_api_response(\n        url=self.url, headers=headers, data=payload, method=\"POST\"\n    )\n</code></pre>"},{"location":"references/sources/api/#viadot.sources.epicor.Epicor.to_df","title":"<code>to_df(filters_xml)</code>","text":"<p>Function for creating pandas DataFrame from Epicor API response.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Output DataFrame.</p> Source code in <code>src/viadot/sources/epicor.py</code> <pre><code>def to_df(self, filters_xml: str) -&gt; pd.DataFrame:\n    \"\"\"Function for creating pandas DataFrame from Epicor API response.\n\n    Returns:\n        pd.DataFrame: Output DataFrame.\n    \"\"\"\n    data = self.get_xml_response(filters_xml)\n    if \"ORDER.HISTORY.DETAIL.QUERY\" in self.base_url:\n        df = parse_orders_xml(data)\n    elif \"CUSTOMER.QUERY\" in self.base_url:\n        df = parse_customer_xml(data)\n    elif \"ORDER.DETAIL.PROD.QUERY\" in self.base_url:\n        df = parse_open_orders_xml(data)\n    elif \"BOOKINGS.DETAIL.QUERY\" in self.base_url:\n        df = parse_bookings_xml(data)\n    else:\n        msg = f\"Parser for selected viev {self.base_url} is not avaiable\"\n        raise ValidationError(msg)\n\n    return df\n</code></pre>"},{"location":"references/sources/api/#viadot.sources.epicor.Epicor.validate_filter","title":"<code>validate_filter(filters_xml)</code>","text":"<p>Function checking if user had specified date range filters.</p> Source code in <code>src/viadot/sources/epicor.py</code> <pre><code>def validate_filter(self, filters_xml: str) -&gt; None:\n    \"Function checking if user had specified date range filters.\"\n    root = ET.fromstring(filters_xml)\n    for child in root:\n        for subchild in child:\n            if (\n                subchild.tag in (self.start_date_field, self.end_date_field)\n            ) and subchild.text is None:\n                msg = \"Too much data. Please provide a date range filter.\"\n                raise DataRangeError(msg)\n</code></pre>"},{"location":"references/sources/api/#viadot.sources.eurostat.Eurostat","title":"<code>viadot.sources.eurostat.Eurostat</code>","text":"<p>               Bases: <code>Source</code></p> <p>Eurostat REST API v1 connector.</p> <p>This module provides functionalities for connecting to Eurostat  API and download the datasets. It includes the following features: - Pulling json file with all data from specific dataset. - Creating pandas Data Frame from pulled json file. - Creating dataset parameters validation if specified.</p> <p>Typical usage example:</p> <pre><code>eurostat = Eurostat()\n\neurostat.to_df(\n    dataset_code: str,\n    params: dict = None,\n    columns: list = None,\n    tests: dict = None,\n)\n</code></pre> Source code in <code>src/viadot/sources/eurostat.py</code> <pre><code>class Eurostat(Source):\n    \"\"\"Eurostat REST API v1 connector.\n\n    This module provides functionalities for connecting to Eurostat  API and download\n    the datasets. It includes the following features:\n    - Pulling json file with all data from specific dataset.\n    - Creating pandas Data Frame from pulled json file.\n    - Creating dataset parameters validation if specified.\n\n    Typical usage example:\n\n        eurostat = Eurostat()\n\n        eurostat.to_df(\n            dataset_code: str,\n            params: dict = None,\n            columns: list = None,\n            tests: dict = None,\n        )\n    \"\"\"\n\n    base_url = \"https://ec.europa.eu/eurostat/api/dissemination/statistics/1.0/data/\"\n\n    def __init__(\n        self,\n        *args,\n        dataset_code: str,\n        params: dict[str, str] | None = None,\n        columns: list[str] | None = None,\n        tests: dict | None = None,\n        **kwargs,\n    ):\n        \"\"\"Initialize the class with Eurostat API data fetching setup.\n\n        This method uses an HTTPS request to pull data from the Eurostat API.\n        No API registration or API key is required. Data is fetched based on the\n        parameters provided in the dynamic part of the URL.\n\n        Example URL:\n        Static part: https://ec.europa.eu/eurostat/api/dissemination/statistics/1.0/data\n        Dynamic part: /TEIBS020/?format=JSON&amp;lang=EN&amp;indic=BS-CSMCI-BAL\n\n        To retrieve specific data, parameters with codes must be provided in `params`.\n\n        Args:\n            dataset_code (str):\n                The code of the Eurostat dataset to be downloaded.\n            params (dict[str, str] | None, optional):\n                A dictionary with optional URL parameters. Each key is a parameter ID,\n                and the value is a specific parameter code, e.g.,\n                `params = {'unit': 'EUR'}` where \"unit\" is the parameter, and \"EUR\" is\n                the code. Only one code per parameter is allowed. Defaults to None.\n            columns (list[str] | None, optional):\n                A list of column names (as strings) that are required from the dataset.\n                Filters the data to only include the specified columns.\n                Defaults to None.\n            tests (dict | None, optional):\n                A dictionary containing test cases for the data, including:\n                - `column_size`: dict{column: size}\n                - `column_unique_values`: list[columns]\n                - `column_list_to_match`: list[columns]\n                - `dataset_row_count`: dict{'min': number, 'max': number}\n                - `column_match_regex`: dict{column: 'regex'}\n                - `column_sum`: dict{column: {'min': number, 'max': number}}.\n                Defaults to None.\n            **kwargs:\n                Additional arguments passed to the class initializer.\n        \"\"\"\n        self.dataset_code = dataset_code\n        self.params = params\n        self.columns = columns\n        self.tests = tests\n\n        super().__init__(*args, **kwargs)\n\n    def get_parameters_codes(self, url: str) -&gt; dict[str, list[str]]:\n        \"\"\"Retrieve available API request parameters and their codes.\n\n        Args:\n            url (str): URL built for API call.\n\n        Raises:\n            ValueError: If the response from the API is empty or invalid.\n\n        Returns:\n            dict[str, list[str]]: Key is a parameter, and value is a list of\n                available codes for the specified parameter.\n        \"\"\"\n        response = handle_api_response(url)\n        data = response.json()\n\n        available_params = data[\"id\"]\n\n        # Dictionary from JSON with keys and related codes values\n        dimension = data[\"dimension\"]\n\n        # Assigning list of available codes to specific parameters\n        params_and_codes = {}\n        for key in available_params:\n            if key in dimension:\n                codes = list(dimension[key][\"category\"][\"index\"].keys())\n                params_and_codes[key] = codes\n        return params_and_codes\n\n    def validate_params(\n        self, dataset_code: str, url: str, params: dict[str, str]\n    ) -&gt; None:\n        \"\"\"Validates given parameters against the available parameters in the dataset.\n\n        Important:\n            Each dataset in Eurostat has specific parameters that could be used for\n            filtering the data. For example dataset ILC_DI04 -Mean and median income by\n            household type - EU-SILC and ECHP surveys has parameter such as:\n            hhhtyp (Type of household), which can be filtered by specific available\n            code of this parameter, such as: 'total', 'a1' (single person),\n            'a1_dhc' (single person with dependent children). Please note that each\n            dataset has different parameters and different codes\n\n        Raises:\n            ValueError: If any of the `params` keys or values is not a string or\n            any of them is not available for specific dataset.\n        \"\"\"\n        # In order to make params validation, first we need to get params_and_codes.\n        key_codes = self.get_parameters_codes(url=url)\n\n        if key_codes is not None:\n            # Conversion keys and values on lower cases by using casefold\n            key_codes_after_conversion = {\n                k.casefold(): [v_elem.casefold() for v_elem in v]\n                for k, v in key_codes.items()\n            }\n            params_after_conversion = {\n                k.casefold(): v.casefold() for k, v in params.items()\n            }\n\n            # Comparing keys and values\n            non_available_keys = [\n                key\n                for key in params_after_conversion\n                if key not in key_codes_after_conversion\n            ]\n            non_available_codes = [\n                value\n                for key, value in params_after_conversion.items()\n                if key in key_codes_after_conversion\n                and value not in key_codes_after_conversion[key]\n            ]\n\n            # Error loggers\n\n            if non_available_keys:\n                self.logger.error(\n                    f\"Parameters: '{' | '.join(non_available_keys)}' are not in \"\n                    \"dataset. Please check your spelling!\"\n                )\n\n                self.logger.error(\n                    f\"Possible parameters: {' | '.join(key_codes.keys())}\"\n                )\n            if non_available_codes:\n                self.logger.error(\n                    f\"Parameters codes: '{' | '.join(non_available_codes)}' are not \"\n                    \"available. Please check your spelling!\\n\"\n                )\n                self.logger.error(\n                    \"You can find everything via link: https://ec.europa.eu/\"\n                    f\"eurostat/databrowser/view/{dataset_code}/default/table?lang=en\"\n                )\n\n            if non_available_keys or non_available_codes:\n                msg = \"Wrong parameters or codes were provided!\"\n                raise ValueError(msg)\n\n    def eurostat_dictionary_to_df(self, *signals: list[str]) -&gt; pd.DataFrame:\n        \"\"\"Function for creating DataFrame from JSON pulled from Eurostat.\n\n        Returns:\n            pd.DataFrame: Pandas DataFrame With 4 columns: index, geo, time, indicator\n        \"\"\"\n\n        class TSignal:\n            \"\"\"Class representing a signal with keys, indexes, labels, and name.\n\n            Attributes:\n            signal_keys_list : list[str]\n                A list of keys representing unique identifiers for the signal.\n            signal_index_list : list[str]\n                A list of index values corresponding to the keys of the signal.\n            signal_label_list : list[str]\n                A list of labels providing human-readable names for the\n                signal's keys.\n            signal_name : str\n                The name of the signal.\n            \"\"\"\n\n            signal_keys_list: list[str]\n            signal_index_list: list[str]\n            signal_label_list: list[str]\n            signal_name: str\n\n        # Dataframe creation\n        columns0 = signals[0].copy()\n        columns0.append(\"indicator\")\n        df = pd.DataFrame(columns=columns0)\n        indicator_list = []\n        index_list = []\n        signal_lists = []\n\n        # Get the dictionary from the inputs\n        eurostat_dictionary = signals[-1]\n\n        for signal in signals[0]:\n            signal_struct = TSignal()\n            signal_struct.signal_name = signal\n            signal_struct.signal_keys_list = list(\n                eurostat_dictionary[\"dimension\"][signal][\"category\"][\"index\"].keys()\n            )\n            signal_struct.signal_index_list = list(\n                eurostat_dictionary[\"dimension\"][signal][\"category\"][\"index\"].values()\n            )\n            signal_label_dict = eurostat_dictionary[\"dimension\"][signal][\"category\"][\n                \"label\"\n            ]\n            signal_struct.signal_label_list = [\n                signal_label_dict[i] for i in signal_struct.signal_keys_list\n            ]\n            signal_lists.append(signal_struct)\n\n        col_signal_temp = []\n        row_signal_temp = []\n        for row_index, row_label in zip(\n            signal_lists[0].signal_index_list,\n            signal_lists[0].signal_label_list,\n            strict=False,\n        ):  # rows\n            for col_index, col_label in zip(\n                signal_lists[1].signal_index_list,\n                signal_lists[1].signal_label_list,\n                strict=False,\n            ):  # cols\n                index = str(\n                    col_index + row_index * len(signal_lists[1].signal_label_list)\n                )\n                if index in eurostat_dictionary[\"value\"]:\n                    index_list.append(index)\n                    col_signal_temp.append(col_label)\n                    row_signal_temp.append(row_label)\n\n        indicator_list = [eurostat_dictionary[\"value\"][i] for i in index_list]\n        df.indicator = indicator_list\n        df[signal_lists[1].signal_name] = col_signal_temp\n        df[signal_lists[0].signal_name] = row_signal_temp\n\n        return df\n\n    @add_viadot_metadata_columns\n    def to_df(self) -&gt; pd.DataFrame:\n        \"\"\"Function responsible for getting response and creating DataFrame.\n\n        It is using method 'eurostat_dictionary_to_df' with validation\n        of provided parameters and their codes if needed.\n\n        Raises:\n            TypeError: If self.params is different type than a dictionary.\n            TypeError: If self.columns is different type than a list containing strings.\n            APIError: If there is an error with the API request.\n\n        Returns:\n            pd.DataFrame: Pandas DataFrame.\n        \"\"\"\n        # Checking if params and columns have correct type\n        if not isinstance(self.params, dict) and self.params is not None:\n            msg = \"Params should be a dictionary.\"\n            raise TypeError(msg)\n\n        if not isinstance(self.columns, list) and self.columns is not None:\n            msg = \"Requested columns should be provided as list of strings.\"\n            raise TypeError(msg)\n\n        # Creating url for connection with API\n        url = f\"{self.base_url}{self.dataset_code}?format=JSON&amp;lang=EN\"\n\n        # Making parameters validation\n        if self.params is not None:\n            self.validate_params(\n                dataset_code=self.dataset_code, url=url, params=self.params\n            )\n\n        # Getting response from API\n        try:\n            response = handle_api_response(url, params=self.params)\n        except APIError:\n            self.validate_params(\n                dataset_code=self.dataset_code, url=url, params=self.params\n            )\n        data = response.json()\n        data_frame = self.eurostat_dictionary_to_df([\"geo\", \"time\"], data)\n\n        # Merge data_frame with label and last updated date\n        label_col = pd.Series(str(data[\"label\"]), index=data_frame.index, name=\"label\")\n        last_updated__col = pd.Series(\n            str(data[\"updated\"]),\n            index=data_frame.index,\n            name=\"updated\",\n        )\n        data_frame = pd.concat([data_frame, label_col, last_updated__col], axis=1)\n\n        # Validation and transformation of requested column\n        if self.columns is not None:\n            filter_df_columns(data_frame=data_frame, columns=self.columns)\n\n        # Additional validation from utils\n        validate(df=data_frame, tests=self.tests)\n\n        return data_frame\n</code></pre>"},{"location":"references/sources/api/#viadot.sources.eurostat.Eurostat.__init__","title":"<code>__init__(*args, dataset_code, params=None, columns=None, tests=None, **kwargs)</code>","text":"<p>Initialize the class with Eurostat API data fetching setup.</p> <p>This method uses an HTTPS request to pull data from the Eurostat API. No API registration or API key is required. Data is fetched based on the parameters provided in the dynamic part of the URL.</p> <p>Example URL: Static part: https://ec.europa.eu/eurostat/api/dissemination/statistics/1.0/data Dynamic part: /TEIBS020/?format=JSON&amp;lang=EN&amp;indic=BS-CSMCI-BAL</p> <p>To retrieve specific data, parameters with codes must be provided in <code>params</code>.</p> <p>Parameters:</p> Name Type Description Default <code>dataset_code</code> <code>str</code> <p>The code of the Eurostat dataset to be downloaded.</p> required <code>params</code> <code>dict[str, str] | None</code> <p>A dictionary with optional URL parameters. Each key is a parameter ID, and the value is a specific parameter code, e.g., <code>params = {'unit': 'EUR'}</code> where \"unit\" is the parameter, and \"EUR\" is the code. Only one code per parameter is allowed. Defaults to None.</p> <code>None</code> <code>columns</code> <code>list[str] | None</code> <p>A list of column names (as strings) that are required from the dataset. Filters the data to only include the specified columns. Defaults to None.</p> <code>None</code> <code>tests</code> <code>dict | None</code> <p>A dictionary containing test cases for the data, including: - <code>column_size</code>: dict{column: size} - <code>column_unique_values</code>: list[columns] - <code>column_list_to_match</code>: list[columns] - <code>dataset_row_count</code>: dict{'min': number, 'max': number} - <code>column_match_regex</code>: dict{column: 'regex'} - <code>column_sum</code>: dict{column: {'min': number, 'max': number}}. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <p>Additional arguments passed to the class initializer.</p> <code>{}</code> Source code in <code>src/viadot/sources/eurostat.py</code> <pre><code>def __init__(\n    self,\n    *args,\n    dataset_code: str,\n    params: dict[str, str] | None = None,\n    columns: list[str] | None = None,\n    tests: dict | None = None,\n    **kwargs,\n):\n    \"\"\"Initialize the class with Eurostat API data fetching setup.\n\n    This method uses an HTTPS request to pull data from the Eurostat API.\n    No API registration or API key is required. Data is fetched based on the\n    parameters provided in the dynamic part of the URL.\n\n    Example URL:\n    Static part: https://ec.europa.eu/eurostat/api/dissemination/statistics/1.0/data\n    Dynamic part: /TEIBS020/?format=JSON&amp;lang=EN&amp;indic=BS-CSMCI-BAL\n\n    To retrieve specific data, parameters with codes must be provided in `params`.\n\n    Args:\n        dataset_code (str):\n            The code of the Eurostat dataset to be downloaded.\n        params (dict[str, str] | None, optional):\n            A dictionary with optional URL parameters. Each key is a parameter ID,\n            and the value is a specific parameter code, e.g.,\n            `params = {'unit': 'EUR'}` where \"unit\" is the parameter, and \"EUR\" is\n            the code. Only one code per parameter is allowed. Defaults to None.\n        columns (list[str] | None, optional):\n            A list of column names (as strings) that are required from the dataset.\n            Filters the data to only include the specified columns.\n            Defaults to None.\n        tests (dict | None, optional):\n            A dictionary containing test cases for the data, including:\n            - `column_size`: dict{column: size}\n            - `column_unique_values`: list[columns]\n            - `column_list_to_match`: list[columns]\n            - `dataset_row_count`: dict{'min': number, 'max': number}\n            - `column_match_regex`: dict{column: 'regex'}\n            - `column_sum`: dict{column: {'min': number, 'max': number}}.\n            Defaults to None.\n        **kwargs:\n            Additional arguments passed to the class initializer.\n    \"\"\"\n    self.dataset_code = dataset_code\n    self.params = params\n    self.columns = columns\n    self.tests = tests\n\n    super().__init__(*args, **kwargs)\n</code></pre>"},{"location":"references/sources/api/#viadot.sources.eurostat.Eurostat.eurostat_dictionary_to_df","title":"<code>eurostat_dictionary_to_df(*signals)</code>","text":"<p>Function for creating DataFrame from JSON pulled from Eurostat.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Pandas DataFrame With 4 columns: index, geo, time, indicator</p> Source code in <code>src/viadot/sources/eurostat.py</code> <pre><code>def eurostat_dictionary_to_df(self, *signals: list[str]) -&gt; pd.DataFrame:\n    \"\"\"Function for creating DataFrame from JSON pulled from Eurostat.\n\n    Returns:\n        pd.DataFrame: Pandas DataFrame With 4 columns: index, geo, time, indicator\n    \"\"\"\n\n    class TSignal:\n        \"\"\"Class representing a signal with keys, indexes, labels, and name.\n\n        Attributes:\n        signal_keys_list : list[str]\n            A list of keys representing unique identifiers for the signal.\n        signal_index_list : list[str]\n            A list of index values corresponding to the keys of the signal.\n        signal_label_list : list[str]\n            A list of labels providing human-readable names for the\n            signal's keys.\n        signal_name : str\n            The name of the signal.\n        \"\"\"\n\n        signal_keys_list: list[str]\n        signal_index_list: list[str]\n        signal_label_list: list[str]\n        signal_name: str\n\n    # Dataframe creation\n    columns0 = signals[0].copy()\n    columns0.append(\"indicator\")\n    df = pd.DataFrame(columns=columns0)\n    indicator_list = []\n    index_list = []\n    signal_lists = []\n\n    # Get the dictionary from the inputs\n    eurostat_dictionary = signals[-1]\n\n    for signal in signals[0]:\n        signal_struct = TSignal()\n        signal_struct.signal_name = signal\n        signal_struct.signal_keys_list = list(\n            eurostat_dictionary[\"dimension\"][signal][\"category\"][\"index\"].keys()\n        )\n        signal_struct.signal_index_list = list(\n            eurostat_dictionary[\"dimension\"][signal][\"category\"][\"index\"].values()\n        )\n        signal_label_dict = eurostat_dictionary[\"dimension\"][signal][\"category\"][\n            \"label\"\n        ]\n        signal_struct.signal_label_list = [\n            signal_label_dict[i] for i in signal_struct.signal_keys_list\n        ]\n        signal_lists.append(signal_struct)\n\n    col_signal_temp = []\n    row_signal_temp = []\n    for row_index, row_label in zip(\n        signal_lists[0].signal_index_list,\n        signal_lists[0].signal_label_list,\n        strict=False,\n    ):  # rows\n        for col_index, col_label in zip(\n            signal_lists[1].signal_index_list,\n            signal_lists[1].signal_label_list,\n            strict=False,\n        ):  # cols\n            index = str(\n                col_index + row_index * len(signal_lists[1].signal_label_list)\n            )\n            if index in eurostat_dictionary[\"value\"]:\n                index_list.append(index)\n                col_signal_temp.append(col_label)\n                row_signal_temp.append(row_label)\n\n    indicator_list = [eurostat_dictionary[\"value\"][i] for i in index_list]\n    df.indicator = indicator_list\n    df[signal_lists[1].signal_name] = col_signal_temp\n    df[signal_lists[0].signal_name] = row_signal_temp\n\n    return df\n</code></pre>"},{"location":"references/sources/api/#viadot.sources.eurostat.Eurostat.get_parameters_codes","title":"<code>get_parameters_codes(url)</code>","text":"<p>Retrieve available API request parameters and their codes.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>URL built for API call.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the response from the API is empty or invalid.</p> <p>Returns:</p> Type Description <code>dict[str, list[str]]</code> <p>dict[str, list[str]]: Key is a parameter, and value is a list of available codes for the specified parameter.</p> Source code in <code>src/viadot/sources/eurostat.py</code> <pre><code>def get_parameters_codes(self, url: str) -&gt; dict[str, list[str]]:\n    \"\"\"Retrieve available API request parameters and their codes.\n\n    Args:\n        url (str): URL built for API call.\n\n    Raises:\n        ValueError: If the response from the API is empty or invalid.\n\n    Returns:\n        dict[str, list[str]]: Key is a parameter, and value is a list of\n            available codes for the specified parameter.\n    \"\"\"\n    response = handle_api_response(url)\n    data = response.json()\n\n    available_params = data[\"id\"]\n\n    # Dictionary from JSON with keys and related codes values\n    dimension = data[\"dimension\"]\n\n    # Assigning list of available codes to specific parameters\n    params_and_codes = {}\n    for key in available_params:\n        if key in dimension:\n            codes = list(dimension[key][\"category\"][\"index\"].keys())\n            params_and_codes[key] = codes\n    return params_and_codes\n</code></pre>"},{"location":"references/sources/api/#viadot.sources.eurostat.Eurostat.to_df","title":"<code>to_df()</code>","text":"<p>Function responsible for getting response and creating DataFrame.</p> <p>It is using method 'eurostat_dictionary_to_df' with validation of provided parameters and their codes if needed.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If self.params is different type than a dictionary.</p> <code>TypeError</code> <p>If self.columns is different type than a list containing strings.</p> <code>APIError</code> <p>If there is an error with the API request.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Pandas DataFrame.</p> Source code in <code>src/viadot/sources/eurostat.py</code> <pre><code>@add_viadot_metadata_columns\ndef to_df(self) -&gt; pd.DataFrame:\n    \"\"\"Function responsible for getting response and creating DataFrame.\n\n    It is using method 'eurostat_dictionary_to_df' with validation\n    of provided parameters and their codes if needed.\n\n    Raises:\n        TypeError: If self.params is different type than a dictionary.\n        TypeError: If self.columns is different type than a list containing strings.\n        APIError: If there is an error with the API request.\n\n    Returns:\n        pd.DataFrame: Pandas DataFrame.\n    \"\"\"\n    # Checking if params and columns have correct type\n    if not isinstance(self.params, dict) and self.params is not None:\n        msg = \"Params should be a dictionary.\"\n        raise TypeError(msg)\n\n    if not isinstance(self.columns, list) and self.columns is not None:\n        msg = \"Requested columns should be provided as list of strings.\"\n        raise TypeError(msg)\n\n    # Creating url for connection with API\n    url = f\"{self.base_url}{self.dataset_code}?format=JSON&amp;lang=EN\"\n\n    # Making parameters validation\n    if self.params is not None:\n        self.validate_params(\n            dataset_code=self.dataset_code, url=url, params=self.params\n        )\n\n    # Getting response from API\n    try:\n        response = handle_api_response(url, params=self.params)\n    except APIError:\n        self.validate_params(\n            dataset_code=self.dataset_code, url=url, params=self.params\n        )\n    data = response.json()\n    data_frame = self.eurostat_dictionary_to_df([\"geo\", \"time\"], data)\n\n    # Merge data_frame with label and last updated date\n    label_col = pd.Series(str(data[\"label\"]), index=data_frame.index, name=\"label\")\n    last_updated__col = pd.Series(\n        str(data[\"updated\"]),\n        index=data_frame.index,\n        name=\"updated\",\n    )\n    data_frame = pd.concat([data_frame, label_col, last_updated__col], axis=1)\n\n    # Validation and transformation of requested column\n    if self.columns is not None:\n        filter_df_columns(data_frame=data_frame, columns=self.columns)\n\n    # Additional validation from utils\n    validate(df=data_frame, tests=self.tests)\n\n    return data_frame\n</code></pre>"},{"location":"references/sources/api/#viadot.sources.eurostat.Eurostat.validate_params","title":"<code>validate_params(dataset_code, url, params)</code>","text":"<p>Validates given parameters against the available parameters in the dataset.</p> Important <p>Each dataset in Eurostat has specific parameters that could be used for filtering the data. For example dataset ILC_DI04 -Mean and median income by household type - EU-SILC and ECHP surveys has parameter such as: hhhtyp (Type of household), which can be filtered by specific available code of this parameter, such as: 'total', 'a1' (single person), 'a1_dhc' (single person with dependent children). Please note that each dataset has different parameters and different codes</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If any of the <code>params</code> keys or values is not a string or</p> Source code in <code>src/viadot/sources/eurostat.py</code> <pre><code>def validate_params(\n    self, dataset_code: str, url: str, params: dict[str, str]\n) -&gt; None:\n    \"\"\"Validates given parameters against the available parameters in the dataset.\n\n    Important:\n        Each dataset in Eurostat has specific parameters that could be used for\n        filtering the data. For example dataset ILC_DI04 -Mean and median income by\n        household type - EU-SILC and ECHP surveys has parameter such as:\n        hhhtyp (Type of household), which can be filtered by specific available\n        code of this parameter, such as: 'total', 'a1' (single person),\n        'a1_dhc' (single person with dependent children). Please note that each\n        dataset has different parameters and different codes\n\n    Raises:\n        ValueError: If any of the `params` keys or values is not a string or\n        any of them is not available for specific dataset.\n    \"\"\"\n    # In order to make params validation, first we need to get params_and_codes.\n    key_codes = self.get_parameters_codes(url=url)\n\n    if key_codes is not None:\n        # Conversion keys and values on lower cases by using casefold\n        key_codes_after_conversion = {\n            k.casefold(): [v_elem.casefold() for v_elem in v]\n            for k, v in key_codes.items()\n        }\n        params_after_conversion = {\n            k.casefold(): v.casefold() for k, v in params.items()\n        }\n\n        # Comparing keys and values\n        non_available_keys = [\n            key\n            for key in params_after_conversion\n            if key not in key_codes_after_conversion\n        ]\n        non_available_codes = [\n            value\n            for key, value in params_after_conversion.items()\n            if key in key_codes_after_conversion\n            and value not in key_codes_after_conversion[key]\n        ]\n\n        # Error loggers\n\n        if non_available_keys:\n            self.logger.error(\n                f\"Parameters: '{' | '.join(non_available_keys)}' are not in \"\n                \"dataset. Please check your spelling!\"\n            )\n\n            self.logger.error(\n                f\"Possible parameters: {' | '.join(key_codes.keys())}\"\n            )\n        if non_available_codes:\n            self.logger.error(\n                f\"Parameters codes: '{' | '.join(non_available_codes)}' are not \"\n                \"available. Please check your spelling!\\n\"\n            )\n            self.logger.error(\n                \"You can find everything via link: https://ec.europa.eu/\"\n                f\"eurostat/databrowser/view/{dataset_code}/default/table?lang=en\"\n            )\n\n        if non_available_keys or non_available_codes:\n            msg = \"Wrong parameters or codes were provided!\"\n            raise ValueError(msg)\n</code></pre>"},{"location":"references/sources/api/#viadot.sources.exchange_rates.ExchangeRates","title":"<code>viadot.sources.exchange_rates.ExchangeRates</code>","text":"<p>               Bases: <code>Source</code></p> Source code in <code>src/viadot/sources/exchange_rates.py</code> <pre><code>class ExchangeRates(Source):\n    URL = \"https://api.apilayer.com/exchangerates_data/timeseries\"\n\n    def __init__(\n        self,\n        currency: Currency = \"USD\",\n        start_date: str = datetime.today().strftime(\"%Y-%m-%d\"),\n        end_date: str = datetime.today().strftime(\"%Y-%m-%d\"),\n        symbols: list[str] | None = None,\n        credentials: dict[str, Any] | None = None,\n        config_key: str | None = None,\n        *args,\n        **kwargs,\n    ):\n        \"\"\"Download data from https://api.apilayer.com/exchangerates_data/timeseries.\n\n        Args:\n            currency (Currency, optional): Base currency to which prices of searched\n                currencies are related. Defaults to \"USD\".\n            start_date (str, optional): Initial date for data search. Data range is\n                start_date -&gt; end_date, supported format 'yyyy-mm-dd'. Defaults to\n                    datetime.today().strftime(\"%Y-%m-%d\").\n            end_date (str, optional): See above. Defaults to\n                datetime.today().strftime(\"%Y-%m-%d\").\n            symbols (list, optional): List of ISO codes for which exchange rates from\n                base currency will be fetched. Defaults to [\"USD\", \"EUR\", \"GBP\", \"CHF\",\n                \"PLN\", \"DKK\", \"COP\", \"CZK\", \"SEK\", \"NOK\", \"ISK\" ].\n            credentials (Dict[str, Any], optional): The credentials to use. Defaults to\n                None.\n            config_key (str, optional): The key in the viadot config holding relevant\n                credentials.\n        \"\"\"\n        credentials = credentials or get_source_credentials(config_key)\n        if credentials is None:\n            msg = \"Please specify the credentials.\"\n            raise CredentialError(msg)\n        super().__init__(*args, credentials=credentials, **kwargs)\n\n        if not symbols:\n            symbols = [\n                \"USD\",\n                \"EUR\",\n                \"GBP\",\n                \"CHF\",\n                \"PLN\",\n                \"DKK\",\n                \"COP\",\n                \"CZK\",\n                \"SEK\",\n                \"NOK\",\n                \"ISK\",\n            ]\n\n        self.currency = currency\n        self.start_date = start_date\n        self.end_date = end_date\n        self.symbols = symbols\n        self._validate_symbols(self.symbols, self.currency)\n\n    def _validate_symbols(self, symbols: list[str], currency: str):\n        cur_list = [\n            \"USD\",\n            \"EUR\",\n            \"GBP\",\n            \"CHF\",\n            \"PLN\",\n            \"DKK\",\n            \"COP\",\n            \"CZK\",\n            \"SEK\",\n            \"NOK\",\n            \"ISK\",\n        ]\n\n        if currency not in cur_list:\n            msg = f\"The specified currency does not exist or is unsupported: {currency}\"\n            raise ValueError(msg)\n\n        for i in symbols:\n            if i not in cur_list:\n                msg = f\"The specified currency list item does not exist or is not supported: {i}\"\n                raise ValueError(msg)\n\n    def get_data(self) -&gt; dict[str, Any]:\n        \"\"\"Download data from the API.\n\n        Returns:\n            dict[str, Any]: The data from the API.\n        \"\"\"\n        headers = {\"apikey\": self.credentials[\"api_key\"]}\n        payload = {\n            \"start_date\": self.start_date,\n            \"end_date\": self.end_date,\n            \"base\": self.currency,\n            \"symbols\": \",\".join(self.symbols),\n        }\n        response = requests.request(\n            \"GET\", ExchangeRates.URL, headers=headers, params=payload, timeout=(3, 10)\n        )\n\n        return json.loads(response.text)\n\n    def to_records(self) -&gt; list[tuple]:\n        \"\"\"Download data and convert it to a list of records.\n\n        Returns:\n            list[tuple]: The records of the data.\n        \"\"\"\n        data = self.get_data()\n        records = []\n\n        for j in data[\"rates\"]:\n            records.append(j)\n            records.append(data[\"base\"])\n\n            for i in data[\"rates\"][j]:\n                records.append(data[\"rates\"][j][i])\n\n        return list(zip(*[iter(records)] * (2 + len(self.symbols)), strict=False))\n\n    def get_columns(self) -&gt; list[str]:\n        \"\"\"Return the columns of the data.\n\n        Returns:\n            list[str]: The columns of the data.\n        \"\"\"\n        return [\"Date\", \"Base\", *self.symbols]\n\n    def to_json(self) -&gt; dict[str, Any]:\n        \"\"\"Download data and convert it to a JSON.\n\n        Returns:\n            dict[str, Any]: The JSON with the data.\n        \"\"\"\n        records = self.to_records()\n        columns = self.get_columns()\n        records = [\n            dict(zip(columns, records[i], strict=False)) for i in range(len(records))\n        ]\n        json = {}\n        json[\"currencies\"] = records\n\n        return json\n\n    @add_viadot_metadata_columns\n    def to_df(\n        self,\n        tests: dict | None = None,\n    ) -&gt; pd.DataFrame:\n        \"\"\"Download data and convert it to a pandas DataFrame.\n\n        Args:\n            tests (dict | None, optional): The tests specification. Defaults to None.\n\n        Returns:\n            pd.DataFrame: The pandas DataFrame with the data.\n        \"\"\"\n        json = self.to_json()\n        df = pd.json_normalize(json[\"currencies\"])\n        df_clean = cleanup_df(df)\n\n        if tests:\n            validate(df=df_clean, tests=tests)\n\n        return df_clean\n</code></pre>"},{"location":"references/sources/api/#viadot.sources.exchange_rates.ExchangeRates.__init__","title":"<code>__init__(currency='USD', start_date=datetime.today().strftime('%Y-%m-%d'), end_date=datetime.today().strftime('%Y-%m-%d'), symbols=None, credentials=None, config_key=None, *args, **kwargs)</code>","text":"<p>Download data from https://api.apilayer.com/exchangerates_data/timeseries.</p> <p>Parameters:</p> Name Type Description Default <code>currency</code> <code>Currency</code> <p>Base currency to which prices of searched currencies are related. Defaults to \"USD\".</p> <code>'USD'</code> <code>start_date</code> <code>str</code> <p>Initial date for data search. Data range is start_date -&gt; end_date, supported format 'yyyy-mm-dd'. Defaults to     datetime.today().strftime(\"%Y-%m-%d\").</p> <code>strftime('%Y-%m-%d')</code> <code>end_date</code> <code>str</code> <p>See above. Defaults to datetime.today().strftime(\"%Y-%m-%d\").</p> <code>strftime('%Y-%m-%d')</code> <code>symbols</code> <code>list</code> <p>List of ISO codes for which exchange rates from base currency will be fetched. Defaults to [\"USD\", \"EUR\", \"GBP\", \"CHF\", \"PLN\", \"DKK\", \"COP\", \"CZK\", \"SEK\", \"NOK\", \"ISK\" ].</p> <code>None</code> <code>credentials</code> <code>Dict[str, Any]</code> <p>The credentials to use. Defaults to None.</p> <code>None</code> <code>config_key</code> <code>str</code> <p>The key in the viadot config holding relevant credentials.</p> <code>None</code> Source code in <code>src/viadot/sources/exchange_rates.py</code> <pre><code>def __init__(\n    self,\n    currency: Currency = \"USD\",\n    start_date: str = datetime.today().strftime(\"%Y-%m-%d\"),\n    end_date: str = datetime.today().strftime(\"%Y-%m-%d\"),\n    symbols: list[str] | None = None,\n    credentials: dict[str, Any] | None = None,\n    config_key: str | None = None,\n    *args,\n    **kwargs,\n):\n    \"\"\"Download data from https://api.apilayer.com/exchangerates_data/timeseries.\n\n    Args:\n        currency (Currency, optional): Base currency to which prices of searched\n            currencies are related. Defaults to \"USD\".\n        start_date (str, optional): Initial date for data search. Data range is\n            start_date -&gt; end_date, supported format 'yyyy-mm-dd'. Defaults to\n                datetime.today().strftime(\"%Y-%m-%d\").\n        end_date (str, optional): See above. Defaults to\n            datetime.today().strftime(\"%Y-%m-%d\").\n        symbols (list, optional): List of ISO codes for which exchange rates from\n            base currency will be fetched. Defaults to [\"USD\", \"EUR\", \"GBP\", \"CHF\",\n            \"PLN\", \"DKK\", \"COP\", \"CZK\", \"SEK\", \"NOK\", \"ISK\" ].\n        credentials (Dict[str, Any], optional): The credentials to use. Defaults to\n            None.\n        config_key (str, optional): The key in the viadot config holding relevant\n            credentials.\n    \"\"\"\n    credentials = credentials or get_source_credentials(config_key)\n    if credentials is None:\n        msg = \"Please specify the credentials.\"\n        raise CredentialError(msg)\n    super().__init__(*args, credentials=credentials, **kwargs)\n\n    if not symbols:\n        symbols = [\n            \"USD\",\n            \"EUR\",\n            \"GBP\",\n            \"CHF\",\n            \"PLN\",\n            \"DKK\",\n            \"COP\",\n            \"CZK\",\n            \"SEK\",\n            \"NOK\",\n            \"ISK\",\n        ]\n\n    self.currency = currency\n    self.start_date = start_date\n    self.end_date = end_date\n    self.symbols = symbols\n    self._validate_symbols(self.symbols, self.currency)\n</code></pre>"},{"location":"references/sources/api/#viadot.sources.exchange_rates.ExchangeRates.get_columns","title":"<code>get_columns()</code>","text":"<p>Return the columns of the data.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: The columns of the data.</p> Source code in <code>src/viadot/sources/exchange_rates.py</code> <pre><code>def get_columns(self) -&gt; list[str]:\n    \"\"\"Return the columns of the data.\n\n    Returns:\n        list[str]: The columns of the data.\n    \"\"\"\n    return [\"Date\", \"Base\", *self.symbols]\n</code></pre>"},{"location":"references/sources/api/#viadot.sources.exchange_rates.ExchangeRates.get_data","title":"<code>get_data()</code>","text":"<p>Download data from the API.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>dict[str, Any]: The data from the API.</p> Source code in <code>src/viadot/sources/exchange_rates.py</code> <pre><code>def get_data(self) -&gt; dict[str, Any]:\n    \"\"\"Download data from the API.\n\n    Returns:\n        dict[str, Any]: The data from the API.\n    \"\"\"\n    headers = {\"apikey\": self.credentials[\"api_key\"]}\n    payload = {\n        \"start_date\": self.start_date,\n        \"end_date\": self.end_date,\n        \"base\": self.currency,\n        \"symbols\": \",\".join(self.symbols),\n    }\n    response = requests.request(\n        \"GET\", ExchangeRates.URL, headers=headers, params=payload, timeout=(3, 10)\n    )\n\n    return json.loads(response.text)\n</code></pre>"},{"location":"references/sources/api/#viadot.sources.exchange_rates.ExchangeRates.to_df","title":"<code>to_df(tests=None)</code>","text":"<p>Download data and convert it to a pandas DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>tests</code> <code>dict | None</code> <p>The tests specification. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: The pandas DataFrame with the data.</p> Source code in <code>src/viadot/sources/exchange_rates.py</code> <pre><code>@add_viadot_metadata_columns\ndef to_df(\n    self,\n    tests: dict | None = None,\n) -&gt; pd.DataFrame:\n    \"\"\"Download data and convert it to a pandas DataFrame.\n\n    Args:\n        tests (dict | None, optional): The tests specification. Defaults to None.\n\n    Returns:\n        pd.DataFrame: The pandas DataFrame with the data.\n    \"\"\"\n    json = self.to_json()\n    df = pd.json_normalize(json[\"currencies\"])\n    df_clean = cleanup_df(df)\n\n    if tests:\n        validate(df=df_clean, tests=tests)\n\n    return df_clean\n</code></pre>"},{"location":"references/sources/api/#viadot.sources.exchange_rates.ExchangeRates.to_json","title":"<code>to_json()</code>","text":"<p>Download data and convert it to a JSON.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>dict[str, Any]: The JSON with the data.</p> Source code in <code>src/viadot/sources/exchange_rates.py</code> <pre><code>def to_json(self) -&gt; dict[str, Any]:\n    \"\"\"Download data and convert it to a JSON.\n\n    Returns:\n        dict[str, Any]: The JSON with the data.\n    \"\"\"\n    records = self.to_records()\n    columns = self.get_columns()\n    records = [\n        dict(zip(columns, records[i], strict=False)) for i in range(len(records))\n    ]\n    json = {}\n    json[\"currencies\"] = records\n\n    return json\n</code></pre>"},{"location":"references/sources/api/#viadot.sources.exchange_rates.ExchangeRates.to_records","title":"<code>to_records()</code>","text":"<p>Download data and convert it to a list of records.</p> <p>Returns:</p> Type Description <code>list[tuple]</code> <p>list[tuple]: The records of the data.</p> Source code in <code>src/viadot/sources/exchange_rates.py</code> <pre><code>def to_records(self) -&gt; list[tuple]:\n    \"\"\"Download data and convert it to a list of records.\n\n    Returns:\n        list[tuple]: The records of the data.\n    \"\"\"\n    data = self.get_data()\n    records = []\n\n    for j in data[\"rates\"]:\n        records.append(j)\n        records.append(data[\"base\"])\n\n        for i in data[\"rates\"][j]:\n            records.append(data[\"rates\"][j][i])\n\n    return list(zip(*[iter(records)] * (2 + len(self.symbols)), strict=False))\n</code></pre>"},{"location":"references/sources/api/#viadot.sources.genesys.Genesys","title":"<code>viadot.sources.genesys.Genesys</code>","text":"<p>               Bases: <code>Source</code></p> Source code in <code>src/viadot/sources/genesys.py</code> <pre><code>class Genesys(Source):\n    ENVIRONMENTS = (\n        \"cac1.pure.cloud\",\n        \"sae1.pure.cloud\",\n        \"mypurecloud.com\",\n        \"usw2.pure.cloud\",\n        \"aps1.pure.cloud\",\n        \"apne3.pure.cloud\",\n        \"apne2.pure.cloud\",\n        \"mypurecloud.com.au\",\n        \"mypurecloud.jp\",\n        \"mypurecloud.ie\",\n        \"mypurecloud.de\",\n        \"euw2.pure.cloud\",\n        \"euc2.pure.cloud\",\n        \"mec1.pure.cloud\",\n    )\n\n    def __init__(\n        self,\n        *args,\n        credentials: GenesysCredentials | None = None,\n        config_key: str = \"genesys\",\n        verbose: bool = False,\n        environment: str = \"mypurecloud.de\",\n        **kwargs,\n    ):\n        \"\"\"Genesys Cloud API connector.\n\n        Provides functionalities for connecting to Genesys Cloud API and downloading\n        generated reports. It includes the following features:\n\n        - Generate reports inside Genesys.\n        - Download the reports previously created.\n        - Direct connection to Genesys Cloud API, via GET method, to retrieve the data\n          without any report creation.\n        - Remove any report previously created.\n\n        Args:\n            credentials (Optional[GenesysCredentials], optional): Genesys credentials.\n                Defaults to None\n            config_key (str, optional): The key in the viadot config holding relevant\n                credentials. Defaults to \"genesys\".\n            verbose (bool, optional): Increase the details of the logs printed on the\n                screen. Defaults to False.\n            environment (str, optional): the domain that appears for Genesys Cloud\n                Environment based on the location of your Genesys Cloud organization.\n                Defaults to \"mypurecloud.de\".\n\n        Examples:\n            genesys = Genesys(\n                credentials=credentials,\n                config_key=config_key,\n                verbose=verbose,\n                environment=environment,\n            )\n            genesys.api_connection(\n                endpoint=endpoint,\n                queues_ids=queues_ids,\n                view_type=view_type,\n                view_type_time_sleep=view_type_time_sleep,\n                post_data_list=post_data_list,\n                normalization_sep=normalization_sep,\n            )\n            data_frame = genesys.to_df(\n                drop_duplicates=drop_duplicates,\n                validate_df_dict=validate_df_dict,\n            )\n\n        Raises:\n            CredentialError: If credentials are not provided in local_config or directly\n                as a parameter.\n            APIError: When the environment variable is not among the available.\n        \"\"\"\n        raw_creds = credentials or get_source_credentials(config_key)\n        validated_creds = dict(GenesysCredentials(**raw_creds))\n\n        super().__init__(*args, credentials=validated_creds, **kwargs)\n\n        self.verbose = verbose\n        self.data_returned = {}\n        self.new_report = \"{}\"  # ???\n\n        if environment in self.ENVIRONMENTS:\n            self.environment = environment\n        else:\n            raise APIError(\n                f\"Environment '{environment}' not available\"\n                + \" in Genesys Cloud Environments.\"\n            )\n\n    @property\n    def headers(self) -&gt; dict[str, Any]:\n        \"\"\"Get request headers.\n\n        Returns:\n            Dict[str, Any]: Request headers with token.\n        \"\"\"\n        client_id = self.credentials.get(\"client_id\", \"\")\n        client_secret = self.credentials.get(\"client_secret\", \"\")\n        authorization = base64.b64encode(\n            bytes(client_id + \":\" + client_secret, \"ISO-8859-1\")\n        ).decode(\"ascii\")\n        request_headers = {\n            \"Authorization\": f\"Basic {authorization}\",\n            \"Content-Type\": \"application/x-www-form-urlencoded\",\n        }\n        request_body = {\"grant_type\": \"client_credentials\"}\n        response = handle_api_response(\n            f\"https://login.{self.environment}/oauth/token\",\n            data=request_body,\n            headers=request_headers,\n            method=\"POST\",\n            timeout=3600,\n        )\n\n        if response.ok:\n            self.logger.info(\"Temporary authorization token was generated.\")\n        else:\n            self.logger.info(\n                f\"Failure: { response.status_code !s} - { response.reason }\"\n            )\n        response_json = response.json()\n\n        return {\n            \"Authorization\": f\"{ response_json['token_type'] }\"\n            + f\" { response_json['access_token']}\",\n            \"Content-Type\": \"application/json\",\n        }\n\n    def _api_call(\n        self,\n        endpoint: str,\n        post_data_list: list[str],\n        method: str,\n        params: dict[str, Any] | None = None,\n        time_between_api_call: float = 0.5,\n    ) -&gt; dict[str, Any]:\n        \"\"\"General method to connect to Genesys Cloud API and generate the response.\n\n        Args:\n            endpoint (str): Final end point to the API.\n            post_data_list (List[str]): List of string templates to generate json body.\n            method (str): Type of connection to the API. Defaults to \"POST\".\n            params (Optional[Dict[str, Any]], optional): Parameters to be passed into\n                the POST call. Defaults to None.\n            time_between_api_call (int, optional): The time, in seconds, to sleep the\n                call to the API. Defaults to 0.5.\n\n        Raises:\n            RuntimeError: There is no current event loop in asyncio thread.\n\n        Returns:\n            Dict[str, Any]: Genesys Cloud API response. When the endpoint requires to\n                create a report within Genesys Cloud, the response is just useless\n                information. The useful data must be downloaded from apps.{environment}\n                through another requests.\n        \"\"\"\n        limiter = AsyncLimiter(2, 15)\n        semaphore = asyncio.Semaphore(value=1)\n        url = f\"https://api.{self.environment}/api/v2/{endpoint}\"\n\n        async def generate_post():\n            for data_to_post in post_data_list:\n                payload = json.dumps(data_to_post)\n\n                async with aiohttp.ClientSession() as session:\n                    await semaphore.acquire()\n\n                    async with limiter:\n                        if method == \"POST\":\n                            async with session.post(\n                                url,\n                                headers=self.headers,\n                                data=payload,\n                            ) as resp:\n                                # global new_report\n                                self.new_report = await resp.read()\n                                message = \"Generated report export ---\"\n                                if self.verbose:\n                                    message += f\"\\n {payload}.\"\n                                    self.logger.info(message)\n\n                                semaphore.release()\n\n                        elif method == \"GET\":\n                            async with session.get(\n                                url,\n                                headers=self.headers,\n                                params=params,\n                            ) as resp:\n                                self.new_report = await resp.read()\n                                message = \"Connecting to Genesys Cloud\"\n                                if self.verbose:\n                                    message += f\": {params}.\"\n                                    self.logger.info(message)\n\n                                semaphore.release()\n\n                await asyncio.sleep(time_between_api_call)\n\n        try:\n            loop = asyncio.get_event_loop()\n        except RuntimeError as err:\n            if str(err).startswith(\"There is no current event loop in thread\"):\n                loop = asyncio.new_event_loop()\n                asyncio.set_event_loop(loop)\n            else:\n                raise\n        coroutine = generate_post()\n        loop.run_until_complete(coroutine)\n\n        return json.loads(self.new_report.decode(\"utf-8\"))\n\n    def _load_reporting_exports(\n        self,\n        page_size: int = 100,\n    ) -&gt; dict[str, Any]:\n        \"\"\"Consult the status of the reports created in Genesys Cloud.\n\n        Args:\n            page_size (int, optional): The number of items on page to print.\n                Defaults to 100.\n            verbose (bool, optional): Switch on/off for logging messages.\n                Defaults to False.\n\n        Raises:\n            APIError: Failed to loaded the exports from Genesys Cloud.\n\n        Returns:\n            Dict[str, Any]: Schedule genesys report.\n        \"\"\"\n        response = handle_api_response(\n            url=f\"https://api.{self.environment}/api/v2/\"\n            + f\"analytics/reporting/exports?pageSize={page_size}\",\n            headers=self.headers,\n            method=\"GET\",\n        )\n\n        response_ok = 200\n        if response.status_code == response_ok:\n            return response.json()\n\n        self.logger.error(f\"Failed to loaded all exports. - {response.content}\")\n        msg = \"Failed to loaded all exports.\"\n        raise APIError(msg)\n\n    def _get_reporting_exports_url(self, entities: list[str]) -&gt; tuple[list[str]]:\n        \"\"\"Collect all reports created in Genesys Cloud.\n\n        Args:\n            entities (List[str]): List of dictionaries with all the reports information\n                available in Genesys Cloud.\n\n        Returns:\n            Tuple[List[str]]: A tuple with Lists of IDs and URLs.\n        \"\"\"\n        ids = []\n        urls = []\n        status = []\n        for entity in entities:\n            ids.append(entity.get(\"id\"))\n            urls.append(entity.get(\"downloadUrl\"))\n            status.append(entity.get(\"status\"))\n\n        if \"FAILED\" in status:\n            self.logger.error(\"Some reports have not been successfully created.\")\n        if \"RUNNING\" in status:\n            self.logger.warning(\n                \"Some reports are still being created and can not be downloaded.\"\n            )\n        if self.verbose:\n            message = \"\".join(\n                [f\"\\t{i} -&gt; {j} \\n\" for i, j in zip(ids, status, strict=False)]\n            )\n            self.logger.info(f\"Report status:\\n{message}\")\n\n        return ids, urls\n\n    def _delete_report(self, report_id: str) -&gt; None:\n        \"\"\"Delete a particular report in Genesys Cloud.\n\n        Args:\n            report_id (str): Id of the report to be deleted.\n        \"\"\"\n        delete_response = handle_api_response(\n            url=f\"https://api.{self.environment}/api/v2/\"\n            + f\"analytics/reporting/exports/{report_id}\",\n            headers=self.headers,\n            method=\"DELETE\",\n        )\n        # Ok-ish responses (includes eg. 204 No Content)\n        ok_response_limit = 300\n        if delete_response.status_code &lt; ok_response_limit:\n            self.logger.info(\n                f\"Successfully deleted report '{report_id}' from Genesys API.\"\n            )\n        else:\n            self.logger.error(\n                f\"Failed to delete report '{report_id}' \"\n                + f\"from Genesys API. - {delete_response.content}\"\n            )\n\n    def _download_report(\n        self,\n        report_url: str,\n        drop_duplicates: bool = True,\n    ) -&gt; pd.DataFrame:\n        \"\"\"Download report from Genesys Cloud.\n\n        Args:\n            report_url (str): url to report, fetched from json response.\n            drop_duplicates (bool, optional): Decide if drop duplicates.\n                Defaults to True.\n\n        Returns:\n            pd.DataFrame: Data in a pandas DataFrame.\n        \"\"\"\n        response = handle_api_response(url=f\"{report_url}\", headers=self.headers)\n\n        # Ok-ish responses (includes eg. 204 No Content)\n        ok_response_limit = 300\n        if response.status_code &lt; ok_response_limit:\n            self.logger.info(\n                f\"Successfully downloaded report from Genesys API ('{report_url}').\"\n            )\n\n        else:\n            msg = (\n                \"Failed to download report from\"\n                + f\" Genesys API ('{report_url}'). - {response.content}\"\n            )\n            self.logger.error(msg)\n\n        dataframe = pd.read_csv(StringIO(response.content.decode(\"utf-8\")))\n\n        if drop_duplicates is True:\n            dataframe.drop_duplicates(inplace=True, ignore_index=True)\n\n        return dataframe\n\n    def _merge_conversations(self, data_to_merge: list) -&gt; pd.DataFrame:  # noqa: C901, PLR0912\n        \"\"\"Merge all the conversations data into a single data frame.\n\n        Args:\n            data_to_merge (list): List with all the conversations in json format.\n            Example for all levels data to merge:\n                {\n                \"conversations\": [\n                    {\n                        **** LEVEL 0 data ****\n                        \"participants\": [\n                            {\n                                **** LEVEL 1 data ****\n                                \"sessions\": [\n                                    {\n                                        \"agentBullseyeRing\": 1,\n                                        **** LEVEL 2 data ****\n                                        \"mediaEndpointStats\": [\n                                            {\n                                                **** LEVEL 3 data ****\n                                            },\n                                        ],\n                                        \"metrics\": [\n                                            {\n                                                **** LEVEL 3 data ****\n                                            },\n                                        ],\n                                        \"segments\": [\n                                            {\n                                                **** LEVEL 3 data ****\n                                            },\n                                            {\n                                                **** LEVEL 3 data ****\n                                            },\n                                        ],\n                                    }\n                                ],\n                            },\n                            {\n                                \"participantId\": \"xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\",\n                                **** LEVEL 1 data ****\n                                \"sessions\": [\n                                    {\n                                        **** LEVEL 2 data ****\n                                        \"mediaEndpointStats\": [\n                                            {\n                                                **** LEVEL 3 data ****\n                                            }\n                                        ],\n                                        \"flow\": {\n                                            **** LEVEL 2 data ****\n                                        },\n                                        \"metrics\": [\n                                            {\n                                                **** LEVEL 3 data ****\n                                            },\n                                        ],\n                                        \"segments\": [\n                                            {\n                                                **** LEVEL 3 data ****\n                                            },\n                                        ],\n                                    }\n                                ],\n                            },\n                        ],\n                    }\n                ],\n                \"totalHits\": 100,\n            }\n\n        Returns:\n            DataFrame: A single data frame with all the content.\n        \"\"\"\n        # LEVEL 0\n        df0 = pd.json_normalize(data_to_merge)\n        df0.drop([\"participants\"], axis=1, inplace=True)\n\n        # LEVEL 1\n        df1 = pd.json_normalize(\n            data_to_merge,\n            record_path=[\"participants\"],\n            meta=[\"conversationId\"],\n        )\n        df1.drop([\"sessions\"], axis=1, inplace=True)\n\n        # LEVEL 2\n        df2 = pd.json_normalize(\n            data_to_merge,\n            record_path=[\"participants\", \"sessions\"],\n            meta=[\n                [\"participants\", \"externalContactId\"],\n                [\"participants\", \"participantId\"],\n            ],\n            errors=\"ignore\",\n            sep=\"_\",\n        )\n        # Columns that will be the reference for the next LEVEL\n        df2.rename(\n            columns={\n                \"participants_externalContactId\": \"externalContactId\",\n                \"participants_participantId\": \"participantId\",\n            },\n            inplace=True,\n        )\n        for key in [\"metrics\", \"segments\", \"mediaEndpointStats\"]:\n            try:\n                df2.drop([key], axis=1, inplace=True)\n            except KeyError as err:\n                self.logger.info(f\"Key {err} not appearing in the response.\")\n\n        # LEVEL 3\n        conversations_df = {}\n        for i, conversation in enumerate(data_to_merge):\n            # Not all \"sessions\" have the same data, and that creates\n            #   problems of standardization\n            # Empty data will be added to columns where there is not to avoid\n            #   future errors.\n            for j, entry_0 in enumerate(conversation[\"participants\"]):\n                for key in list(entry_0.keys()):\n                    if key == \"sessions\":\n                        for k, entry_1 in enumerate(entry_0[key]):\n                            if \"metrics\" not in list(entry_1.keys()):\n                                conversation[\"participants\"][j][key][k][\"metrics\"] = []\n                            if \"segments\" not in list(entry_1.keys()):\n                                conversation[\"participants\"][j][key][k][\"segments\"] = []\n                            if \"mediaEndpointStats\" not in list(entry_1.keys()):\n                                conversation[\"participants\"][j][key][k][\n                                    \"mediaEndpointStats\"\n                                ] = []\n\n            # LEVEL 3 metrics\n            df3_1 = pd.json_normalize(\n                conversation,\n                record_path=[\"participants\", \"sessions\", \"metrics\"],\n                meta=[\n                    [\"participants\", \"sessions\", \"sessionId\"],\n                ],\n                errors=\"ignore\",\n                record_prefix=\"metrics_\",\n                sep=\"_\",\n            )\n            df3_1.rename(\n                columns={\"participants_sessions_sessionId\": \"sessionId\"}, inplace=True\n            )\n\n            # LEVEL 3 segments\n            df3_2 = pd.json_normalize(\n                conversation,\n                record_path=[\"participants\", \"sessions\", \"segments\"],\n                meta=[\n                    [\"participants\", \"sessions\", \"sessionId\"],\n                ],\n                errors=\"ignore\",\n                record_prefix=\"segments_\",\n                sep=\"_\",\n            )\n            df3_2.rename(\n                columns={\"participants_sessions_sessionId\": \"sessionId\"}, inplace=True\n            )\n\n            # LEVEL 3 mediaEndpointStats\n            df3_3 = pd.json_normalize(\n                conversation,\n                record_path=[\"participants\", \"sessions\", \"mediaEndpointStats\"],\n                meta=[\n                    [\"participants\", \"sessions\", \"sessionId\"],\n                ],\n                errors=\"ignore\",\n                record_prefix=\"mediaEndpointStats_\",\n                sep=\"_\",\n            )\n            df3_3.rename(\n                columns={\"participants_sessions_sessionId\": \"sessionId\"}, inplace=True\n            )\n\n            # merging all LEVELs 3 from the same conversation\n            dff3_tmp = pd.concat([df3_1, df3_2])\n            dff3 = pd.concat([dff3_tmp, df3_3])\n\n            conversations_df.update({i: dff3})\n\n        # MERGING ALL LEVELS\n        # LEVELS 3\n        for i_3, key in enumerate(list(conversations_df.keys())):\n            if i_3 == 0:\n                dff3_f = conversations_df[key]\n            else:\n                dff3_f = pd.concat([dff3_f, conversations_df[key]])\n\n        # LEVEL 3 with LEVEL 2\n        dff2 = pd.merge(dff3_f, df2, how=\"outer\", on=[\"sessionId\"])\n\n        # LEVEL 2 with LEVEL 1\n        dff1 = pd.merge(\n            df1, dff2, how=\"outer\", on=[\"externalContactId\", \"participantId\"]\n        )\n\n        # LEVEL 1 with LEVEL 0\n        return pd.merge(df0, dff1, how=\"outer\", on=[\"conversationId\"])\n\n    # This is way too complicated for what it's doing...\n    def api_connection(  # noqa: PLR0912, PLR0915, C901.\n        self,\n        endpoint: str | None = None,\n        queues_ids: list[str] | None = None,\n        view_type: str | None = None,\n        view_type_time_sleep: int = 10,\n        post_data_list: list[dict[str, Any]] | None = None,\n        time_between_api_call: float = 0.5,\n        normalization_sep: str = \".\",\n    ) -&gt; None:\n        \"\"\"General method to connect to Genesys Cloud API and generate the response.\n\n        Args:\n            endpoint (Optional[str], optional): Final end point to the API.\n                Defaults to None.\n\n                Custom endpoints have specific key words, and parameters:\n                Example:\n                    - \"routing/queues/{id}/members\": \"routing_queues_members\"\n                    - members_ids = [\"xxxxxxxxx\", \"xxxxxxxxx\", ...]\n            queues_ids (Optional[List[str]], optional): List of queues ids to consult\n                the members. Defaults to None.\n            view_type (Optional[str], optional): The type of view export job to be\n                created. Defaults to None.\n            view_type_time_sleep (int, optional): Waiting time to retrieve data from\n                Genesys Cloud API. Defaults to 10.\n            post_data_list (Optional[List[Dict[str, Any]]], optional): List of string\n                templates to generate json body in POST calls to the API.\n                Defaults to None.\n            time_between_api_call (int, optional): The time, in seconds, to sleep the\n                call to the API. Defaults to 0.5.\n            normalization_sep (str, optional): Nested records will generate names\n                separated by sep. Defaults to \".\".\n\n        Raises:\n            APIError: Some or No reports were not created.\n            APIError: At different endpoints:\n                - 'analytics/conversations/details/query': only one body must be used.\n                - 'routing_queues_members': extra parameter `queues_ids` must be\n                    included.\n        \"\"\"\n        self.logger.info(\n            f\"Connecting to the Genesys Cloud using the endpoint: {endpoint}\"\n        )\n\n        if endpoint == \"analytics/reporting/exports\":\n            self._api_call(\n                endpoint=endpoint,\n                post_data_list=post_data_list,\n                time_between_api_call=time_between_api_call,\n                method=\"POST\",\n            )\n\n            msg = (\n                f\"Waiting {view_type_time_sleep} seconds for\"\n                + \" caching data from Genesys Cloud API.\"\n            )\n            self.logger.info(msg)\n            time.sleep(view_type_time_sleep)\n\n            request_json = self._load_reporting_exports()\n            entities = request_json[\"entities\"]\n\n            if isinstance(entities, list):\n                ids, urls = self._get_reporting_exports_url(entities)\n                if len(entities) != len(post_data_list):\n                    self.logger.warning(\n                        f\"There are {len(entities)} available reports in Genesys, \"\n                        f\"and where sent {len(post_data_list)} reports. \"\n                        \"Unsed reports will be removed.\"\n                    )\n            else:\n                APIError(\n                    \"There are no reports to be downloaded.\"\n                    f\"May be {view_type_time_sleep} should be increased.\"\n                )\n\n            # download and delete reports created\n            count = 0\n            raise_api_error = False\n            for qid, url in zip(ids, urls, strict=False):\n                if url is not None:\n                    df_downloaded = self._download_report(report_url=url)\n\n                    time.sleep(1.0)\n                    # remove resume rows\n                    if view_type in [\"queue_performance_detail_view\"]:\n                        criteria = (\n                            df_downloaded[\"Queue Id\"]\n                            .apply(lambda x: str(x).split(\";\"))\n                            .apply(lambda x: not len(x) &gt; 1)\n                        )\n                        df_downloaded = df_downloaded[criteria]\n\n                    self.data_returned.update({count: df_downloaded})\n                else:\n                    self.logger.error(\n                        f\"Report id {qid} didn't have time to be created. \"\n                        + \"Consider increasing the `view_type_time_sleep` parameter \"\n                        + f\"&gt;&gt; {view_type_time_sleep} seconds to allow Genesys Cloud \"\n                        + \"to conclude the report creation.\"\n                    )\n                    raise_api_error = True\n\n                self._delete_report(qid)\n\n                count += 1  # noqa: SIM113\n\n            if raise_api_error:\n                msg = \"Some reports creation failed.\"\n                raise APIError(msg)\n\n        elif endpoint == \"analytics/conversations/details/query\":\n            if len(post_data_list) &gt; 1:\n                msg = \"Not available more than one body for this end-point.\"\n                raise APIError(msg)\n\n            stop_loop = False\n            page_counter = post_data_list[0][\"paging\"][\"pageNumber\"]\n            self.logger.info(\n                \"Restructuring the response in order to be able to insert it into a \"\n                + \"data frame.\\n\\tThis task could take a few minutes.\\n\"\n            )\n            while not stop_loop:\n                report = self._api_call(\n                    endpoint=endpoint,\n                    post_data_list=post_data_list,\n                    time_between_api_call=time_between_api_call,\n                    method=\"POST\",\n                )\n\n                merged_data_frame = self._merge_conversations(report[\"conversations\"])\n                self.data_returned.update(\n                    {\n                        int(post_data_list[0][\"paging\"][\"pageNumber\"])\n                        - 1: merged_data_frame\n                    }\n                )\n\n                if page_counter == 1:\n                    max_calls = int(np.ceil(report[\"totalHits\"] / 100))\n                if page_counter == max_calls:\n                    stop_loop = True\n\n                post_data_list[0][\"paging\"][\"pageNumber\"] += 1\n                page_counter += 1\n\n        elif endpoint in [\"routing/queues\", \"users\"]:\n            page = 1\n            self.logger.info(\n                \"Restructuring the response in order to be able to insert it into a \"\n                + \"data frame.\\n\\tThis task could take a few minutes.\\n\"\n            )\n            while True:\n                if endpoint == \"routing/queues\":\n                    params = {\"pageSize\": 500, \"pageNumber\": page}\n                elif endpoint == \"users\":\n                    params = {\n                        \"pageSize\": 500,\n                        \"pageNumber\": page,\n                        \"expand\": \"presence,dateLastLogin,groups\"\n                        + \",employerInfo,lasttokenissued\",\n                        \"state\": \"any\",\n                    }\n                response = self._api_call(\n                    endpoint=endpoint,\n                    post_data_list=post_data_list,\n                    time_between_api_call=time_between_api_call,\n                    method=\"GET\",\n                    params=params,\n                )\n\n                if response[\"entities\"]:\n                    df_response = pd.json_normalize(\n                        response[\"entities\"],\n                        sep=normalization_sep,\n                    )\n                    self.data_returned.update({page - 1: df_response})\n\n                    page += 1\n                else:\n                    break\n\n        elif endpoint == \"routing_queues_members\":\n            counter = 0\n            if queues_ids is None:\n                self.logger.error(\n                    \"This endpoint requires `queues_ids` parameter to work.\"\n                )\n                APIError(\"This endpoint requires `queues_ids` parameter to work.\")\n\n            for qid in queues_ids:\n                self.logger.info(f\"Downloading Agents information from Queue: {qid}\")\n                page = 1\n                while True:\n                    response = self._api_call(\n                        endpoint=f\"routing/queues/{qid}/members\",\n                        params={\"pageSize\": 100, \"pageNumber\": page},\n                        post_data_list=post_data_list,\n                        time_between_api_call=time_between_api_call,\n                        method=\"GET\",\n                    )\n\n                    if response[\"entities\"]:\n                        df_response = pd.json_normalize(response[\"entities\"])\n                        # drop personal information\n                        columns_to_drop = {\n                            \"user.addresses\",\n                            \"user.primaryContactInfo\",\n                            \"user.images\",\n                        }.intersection(df_response.columns)\n                        df_response.drop(\n                            columns_to_drop,\n                            axis=1,\n                            inplace=True,\n                        )\n                        self.data_returned.update({counter: df_response})\n\n                        page += 1\n                        counter += 1\n                    else:\n                        break\n\n    @add_viadot_metadata_columns\n    def to_df(\n        self,\n        if_empty: str = \"warn\",\n        **kwargs,\n    ) -&gt; pd.DataFrame:\n        \"\"\"Generate a pandas DataFrame from self.data_returned.\n\n        Args:\n            drop_duplicates (bool, optional): Remove duplicates from the DataFrame.\n                Defaults to False.\n            validate_df_dict (Optional[Dict[str, Any]], optional): A dictionary with\n                optional list of tests to verify the output dataframe. Defaults to None.\n\n        Returns:\n            pd.Dataframe: The response data as a pandas DataFrame plus viadot metadata.\n        \"\"\"\n        drop_duplicates = kwargs.get(\"drop_duplicates\", False)\n        validate_df_dict = kwargs.get(\"validate_df_dict\", None)\n        super().to_df(if_empty=if_empty)\n\n        for key in list(self.data_returned.keys()):\n            if key == 0:\n                data_frame = self.data_returned[key]\n            else:\n                data_frame = pd.concat([data_frame, self.data_returned[key]])\n\n        if drop_duplicates:\n            data_frame.drop_duplicates(inplace=True)\n\n        if validate_df_dict:\n            validate(df=data_frame, tests=validate_df_dict)\n\n        if len(self.data_returned) == 0:\n            data_frame = pd.DataFrame()\n            self._handle_if_empty(\n                if_empty=if_empty,\n                message=\"The response does not contain any data.\",\n            )\n        else:\n            data_frame.reset_index(inplace=True, drop=True)\n\n        return data_frame\n</code></pre>"},{"location":"references/sources/api/#viadot.sources.genesys.Genesys.headers","title":"<code>headers</code>  <code>property</code>","text":"<p>Get request headers.</p> <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dict[str, Any]: Request headers with token.</p>"},{"location":"references/sources/api/#viadot.sources.genesys.Genesys.__init__","title":"<code>__init__(*args, credentials=None, config_key='genesys', verbose=False, environment='mypurecloud.de', **kwargs)</code>","text":"<p>Genesys Cloud API connector.</p> <p>Provides functionalities for connecting to Genesys Cloud API and downloading generated reports. It includes the following features:</p> <ul> <li>Generate reports inside Genesys.</li> <li>Download the reports previously created.</li> <li>Direct connection to Genesys Cloud API, via GET method, to retrieve the data   without any report creation.</li> <li>Remove any report previously created.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>credentials</code> <code>Optional[GenesysCredentials]</code> <p>Genesys credentials. Defaults to None</p> <code>None</code> <code>config_key</code> <code>str</code> <p>The key in the viadot config holding relevant credentials. Defaults to \"genesys\".</p> <code>'genesys'</code> <code>verbose</code> <code>bool</code> <p>Increase the details of the logs printed on the screen. Defaults to False.</p> <code>False</code> <code>environment</code> <code>str</code> <p>the domain that appears for Genesys Cloud Environment based on the location of your Genesys Cloud organization. Defaults to \"mypurecloud.de\".</p> <code>'mypurecloud.de'</code> <p>Examples:</p> <p>genesys = Genesys(     credentials=credentials,     config_key=config_key,     verbose=verbose,     environment=environment, ) genesys.api_connection(     endpoint=endpoint,     queues_ids=queues_ids,     view_type=view_type,     view_type_time_sleep=view_type_time_sleep,     post_data_list=post_data_list,     normalization_sep=normalization_sep, ) data_frame = genesys.to_df(     drop_duplicates=drop_duplicates,     validate_df_dict=validate_df_dict, )</p> <p>Raises:</p> Type Description <code>CredentialError</code> <p>If credentials are not provided in local_config or directly as a parameter.</p> <code>APIError</code> <p>When the environment variable is not among the available.</p> Source code in <code>src/viadot/sources/genesys.py</code> <pre><code>def __init__(\n    self,\n    *args,\n    credentials: GenesysCredentials | None = None,\n    config_key: str = \"genesys\",\n    verbose: bool = False,\n    environment: str = \"mypurecloud.de\",\n    **kwargs,\n):\n    \"\"\"Genesys Cloud API connector.\n\n    Provides functionalities for connecting to Genesys Cloud API and downloading\n    generated reports. It includes the following features:\n\n    - Generate reports inside Genesys.\n    - Download the reports previously created.\n    - Direct connection to Genesys Cloud API, via GET method, to retrieve the data\n      without any report creation.\n    - Remove any report previously created.\n\n    Args:\n        credentials (Optional[GenesysCredentials], optional): Genesys credentials.\n            Defaults to None\n        config_key (str, optional): The key in the viadot config holding relevant\n            credentials. Defaults to \"genesys\".\n        verbose (bool, optional): Increase the details of the logs printed on the\n            screen. Defaults to False.\n        environment (str, optional): the domain that appears for Genesys Cloud\n            Environment based on the location of your Genesys Cloud organization.\n            Defaults to \"mypurecloud.de\".\n\n    Examples:\n        genesys = Genesys(\n            credentials=credentials,\n            config_key=config_key,\n            verbose=verbose,\n            environment=environment,\n        )\n        genesys.api_connection(\n            endpoint=endpoint,\n            queues_ids=queues_ids,\n            view_type=view_type,\n            view_type_time_sleep=view_type_time_sleep,\n            post_data_list=post_data_list,\n            normalization_sep=normalization_sep,\n        )\n        data_frame = genesys.to_df(\n            drop_duplicates=drop_duplicates,\n            validate_df_dict=validate_df_dict,\n        )\n\n    Raises:\n        CredentialError: If credentials are not provided in local_config or directly\n            as a parameter.\n        APIError: When the environment variable is not among the available.\n    \"\"\"\n    raw_creds = credentials or get_source_credentials(config_key)\n    validated_creds = dict(GenesysCredentials(**raw_creds))\n\n    super().__init__(*args, credentials=validated_creds, **kwargs)\n\n    self.verbose = verbose\n    self.data_returned = {}\n    self.new_report = \"{}\"  # ???\n\n    if environment in self.ENVIRONMENTS:\n        self.environment = environment\n    else:\n        raise APIError(\n            f\"Environment '{environment}' not available\"\n            + \" in Genesys Cloud Environments.\"\n        )\n</code></pre>"},{"location":"references/sources/api/#viadot.sources.genesys.Genesys.api_connection","title":"<code>api_connection(endpoint=None, queues_ids=None, view_type=None, view_type_time_sleep=10, post_data_list=None, time_between_api_call=0.5, normalization_sep='.')</code>","text":"<p>General method to connect to Genesys Cloud API and generate the response.</p> <p>Parameters:</p> Name Type Description Default <code>endpoint</code> <code>Optional[str]</code> <p>Final end point to the API. Defaults to None.</p> <p>Custom endpoints have specific key words, and parameters: Example:     - \"routing/queues/{id}/members\": \"routing_queues_members\"     - members_ids = [\"xxxxxxxxx\", \"xxxxxxxxx\", ...]</p> <code>None</code> <code>queues_ids</code> <code>Optional[List[str]]</code> <p>List of queues ids to consult the members. Defaults to None.</p> <code>None</code> <code>view_type</code> <code>Optional[str]</code> <p>The type of view export job to be created. Defaults to None.</p> <code>None</code> <code>view_type_time_sleep</code> <code>int</code> <p>Waiting time to retrieve data from Genesys Cloud API. Defaults to 10.</p> <code>10</code> <code>post_data_list</code> <code>Optional[List[Dict[str, Any]]]</code> <p>List of string templates to generate json body in POST calls to the API. Defaults to None.</p> <code>None</code> <code>time_between_api_call</code> <code>int</code> <p>The time, in seconds, to sleep the call to the API. Defaults to 0.5.</p> <code>0.5</code> <code>normalization_sep</code> <code>str</code> <p>Nested records will generate names separated by sep. Defaults to \".\".</p> <code>'.'</code> <p>Raises:</p> Type Description <code>APIError</code> <p>Some or No reports were not created.</p> <code>APIError</code> <p>At different endpoints: - 'analytics/conversations/details/query': only one body must be used. - 'routing_queues_members': extra parameter <code>queues_ids</code> must be     included.</p> Source code in <code>src/viadot/sources/genesys.py</code> <pre><code>def api_connection(  # noqa: PLR0912, PLR0915, C901.\n    self,\n    endpoint: str | None = None,\n    queues_ids: list[str] | None = None,\n    view_type: str | None = None,\n    view_type_time_sleep: int = 10,\n    post_data_list: list[dict[str, Any]] | None = None,\n    time_between_api_call: float = 0.5,\n    normalization_sep: str = \".\",\n) -&gt; None:\n    \"\"\"General method to connect to Genesys Cloud API and generate the response.\n\n    Args:\n        endpoint (Optional[str], optional): Final end point to the API.\n            Defaults to None.\n\n            Custom endpoints have specific key words, and parameters:\n            Example:\n                - \"routing/queues/{id}/members\": \"routing_queues_members\"\n                - members_ids = [\"xxxxxxxxx\", \"xxxxxxxxx\", ...]\n        queues_ids (Optional[List[str]], optional): List of queues ids to consult\n            the members. Defaults to None.\n        view_type (Optional[str], optional): The type of view export job to be\n            created. Defaults to None.\n        view_type_time_sleep (int, optional): Waiting time to retrieve data from\n            Genesys Cloud API. Defaults to 10.\n        post_data_list (Optional[List[Dict[str, Any]]], optional): List of string\n            templates to generate json body in POST calls to the API.\n            Defaults to None.\n        time_between_api_call (int, optional): The time, in seconds, to sleep the\n            call to the API. Defaults to 0.5.\n        normalization_sep (str, optional): Nested records will generate names\n            separated by sep. Defaults to \".\".\n\n    Raises:\n        APIError: Some or No reports were not created.\n        APIError: At different endpoints:\n            - 'analytics/conversations/details/query': only one body must be used.\n            - 'routing_queues_members': extra parameter `queues_ids` must be\n                included.\n    \"\"\"\n    self.logger.info(\n        f\"Connecting to the Genesys Cloud using the endpoint: {endpoint}\"\n    )\n\n    if endpoint == \"analytics/reporting/exports\":\n        self._api_call(\n            endpoint=endpoint,\n            post_data_list=post_data_list,\n            time_between_api_call=time_between_api_call,\n            method=\"POST\",\n        )\n\n        msg = (\n            f\"Waiting {view_type_time_sleep} seconds for\"\n            + \" caching data from Genesys Cloud API.\"\n        )\n        self.logger.info(msg)\n        time.sleep(view_type_time_sleep)\n\n        request_json = self._load_reporting_exports()\n        entities = request_json[\"entities\"]\n\n        if isinstance(entities, list):\n            ids, urls = self._get_reporting_exports_url(entities)\n            if len(entities) != len(post_data_list):\n                self.logger.warning(\n                    f\"There are {len(entities)} available reports in Genesys, \"\n                    f\"and where sent {len(post_data_list)} reports. \"\n                    \"Unsed reports will be removed.\"\n                )\n        else:\n            APIError(\n                \"There are no reports to be downloaded.\"\n                f\"May be {view_type_time_sleep} should be increased.\"\n            )\n\n        # download and delete reports created\n        count = 0\n        raise_api_error = False\n        for qid, url in zip(ids, urls, strict=False):\n            if url is not None:\n                df_downloaded = self._download_report(report_url=url)\n\n                time.sleep(1.0)\n                # remove resume rows\n                if view_type in [\"queue_performance_detail_view\"]:\n                    criteria = (\n                        df_downloaded[\"Queue Id\"]\n                        .apply(lambda x: str(x).split(\";\"))\n                        .apply(lambda x: not len(x) &gt; 1)\n                    )\n                    df_downloaded = df_downloaded[criteria]\n\n                self.data_returned.update({count: df_downloaded})\n            else:\n                self.logger.error(\n                    f\"Report id {qid} didn't have time to be created. \"\n                    + \"Consider increasing the `view_type_time_sleep` parameter \"\n                    + f\"&gt;&gt; {view_type_time_sleep} seconds to allow Genesys Cloud \"\n                    + \"to conclude the report creation.\"\n                )\n                raise_api_error = True\n\n            self._delete_report(qid)\n\n            count += 1  # noqa: SIM113\n\n        if raise_api_error:\n            msg = \"Some reports creation failed.\"\n            raise APIError(msg)\n\n    elif endpoint == \"analytics/conversations/details/query\":\n        if len(post_data_list) &gt; 1:\n            msg = \"Not available more than one body for this end-point.\"\n            raise APIError(msg)\n\n        stop_loop = False\n        page_counter = post_data_list[0][\"paging\"][\"pageNumber\"]\n        self.logger.info(\n            \"Restructuring the response in order to be able to insert it into a \"\n            + \"data frame.\\n\\tThis task could take a few minutes.\\n\"\n        )\n        while not stop_loop:\n            report = self._api_call(\n                endpoint=endpoint,\n                post_data_list=post_data_list,\n                time_between_api_call=time_between_api_call,\n                method=\"POST\",\n            )\n\n            merged_data_frame = self._merge_conversations(report[\"conversations\"])\n            self.data_returned.update(\n                {\n                    int(post_data_list[0][\"paging\"][\"pageNumber\"])\n                    - 1: merged_data_frame\n                }\n            )\n\n            if page_counter == 1:\n                max_calls = int(np.ceil(report[\"totalHits\"] / 100))\n            if page_counter == max_calls:\n                stop_loop = True\n\n            post_data_list[0][\"paging\"][\"pageNumber\"] += 1\n            page_counter += 1\n\n    elif endpoint in [\"routing/queues\", \"users\"]:\n        page = 1\n        self.logger.info(\n            \"Restructuring the response in order to be able to insert it into a \"\n            + \"data frame.\\n\\tThis task could take a few minutes.\\n\"\n        )\n        while True:\n            if endpoint == \"routing/queues\":\n                params = {\"pageSize\": 500, \"pageNumber\": page}\n            elif endpoint == \"users\":\n                params = {\n                    \"pageSize\": 500,\n                    \"pageNumber\": page,\n                    \"expand\": \"presence,dateLastLogin,groups\"\n                    + \",employerInfo,lasttokenissued\",\n                    \"state\": \"any\",\n                }\n            response = self._api_call(\n                endpoint=endpoint,\n                post_data_list=post_data_list,\n                time_between_api_call=time_between_api_call,\n                method=\"GET\",\n                params=params,\n            )\n\n            if response[\"entities\"]:\n                df_response = pd.json_normalize(\n                    response[\"entities\"],\n                    sep=normalization_sep,\n                )\n                self.data_returned.update({page - 1: df_response})\n\n                page += 1\n            else:\n                break\n\n    elif endpoint == \"routing_queues_members\":\n        counter = 0\n        if queues_ids is None:\n            self.logger.error(\n                \"This endpoint requires `queues_ids` parameter to work.\"\n            )\n            APIError(\"This endpoint requires `queues_ids` parameter to work.\")\n\n        for qid in queues_ids:\n            self.logger.info(f\"Downloading Agents information from Queue: {qid}\")\n            page = 1\n            while True:\n                response = self._api_call(\n                    endpoint=f\"routing/queues/{qid}/members\",\n                    params={\"pageSize\": 100, \"pageNumber\": page},\n                    post_data_list=post_data_list,\n                    time_between_api_call=time_between_api_call,\n                    method=\"GET\",\n                )\n\n                if response[\"entities\"]:\n                    df_response = pd.json_normalize(response[\"entities\"])\n                    # drop personal information\n                    columns_to_drop = {\n                        \"user.addresses\",\n                        \"user.primaryContactInfo\",\n                        \"user.images\",\n                    }.intersection(df_response.columns)\n                    df_response.drop(\n                        columns_to_drop,\n                        axis=1,\n                        inplace=True,\n                    )\n                    self.data_returned.update({counter: df_response})\n\n                    page += 1\n                    counter += 1\n                else:\n                    break\n</code></pre>"},{"location":"references/sources/api/#viadot.sources.genesys.Genesys.to_df","title":"<code>to_df(if_empty='warn', **kwargs)</code>","text":"<p>Generate a pandas DataFrame from self.data_returned.</p> <p>Parameters:</p> Name Type Description Default <code>drop_duplicates</code> <code>bool</code> <p>Remove duplicates from the DataFrame. Defaults to False.</p> required <code>validate_df_dict</code> <code>Optional[Dict[str, Any]]</code> <p>A dictionary with optional list of tests to verify the output dataframe. Defaults to None.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.Dataframe: The response data as a pandas DataFrame plus viadot metadata.</p> Source code in <code>src/viadot/sources/genesys.py</code> <pre><code>@add_viadot_metadata_columns\ndef to_df(\n    self,\n    if_empty: str = \"warn\",\n    **kwargs,\n) -&gt; pd.DataFrame:\n    \"\"\"Generate a pandas DataFrame from self.data_returned.\n\n    Args:\n        drop_duplicates (bool, optional): Remove duplicates from the DataFrame.\n            Defaults to False.\n        validate_df_dict (Optional[Dict[str, Any]], optional): A dictionary with\n            optional list of tests to verify the output dataframe. Defaults to None.\n\n    Returns:\n        pd.Dataframe: The response data as a pandas DataFrame plus viadot metadata.\n    \"\"\"\n    drop_duplicates = kwargs.get(\"drop_duplicates\", False)\n    validate_df_dict = kwargs.get(\"validate_df_dict\", None)\n    super().to_df(if_empty=if_empty)\n\n    for key in list(self.data_returned.keys()):\n        if key == 0:\n            data_frame = self.data_returned[key]\n        else:\n            data_frame = pd.concat([data_frame, self.data_returned[key]])\n\n    if drop_duplicates:\n        data_frame.drop_duplicates(inplace=True)\n\n    if validate_df_dict:\n        validate(df=data_frame, tests=validate_df_dict)\n\n    if len(self.data_returned) == 0:\n        data_frame = pd.DataFrame()\n        self._handle_if_empty(\n            if_empty=if_empty,\n            message=\"The response does not contain any data.\",\n        )\n    else:\n        data_frame.reset_index(inplace=True, drop=True)\n\n    return data_frame\n</code></pre>"},{"location":"references/sources/api/#viadot.sources.hubspot.Hubspot","title":"<code>viadot.sources.hubspot.Hubspot</code>","text":"<p>               Bases: <code>Source</code></p> <p>A class that connects and extracts data from Hubspot API.</p> Documentation is available here <p>https://developers.hubspot.com/docs/api/crm/understanding-the-crm.</p> Connector allows to pull data in two ways <ul> <li>using base API for crm schemas as an endpoint     (eg. \"contacts\", \"\"line_items\", \"deals\", ...),</li> <li>using full url as endpoint.</li> </ul> Source code in <code>src/viadot/sources/hubspot.py</code> <pre><code>class Hubspot(Source):\n    \"\"\"A class that connects and extracts data from Hubspot API.\n\n    Documentation is available here:\n        https://developers.hubspot.com/docs/api/crm/understanding-the-crm.\n\n    Connector allows to pull data in two ways:\n        - using base API for crm schemas as an endpoint\n            (eg. \"contacts\", \"\"line_items\", \"deals\", ...),\n        - using full url as endpoint.\n    \"\"\"\n\n    API_URL = \"https://api.hubapi.com\"\n\n    def __init__(\n        self,\n        *args,\n        credentials: HubspotCredentials | None = None,\n        config_key: str = \"hubspot\",\n        **kwargs,\n    ):\n        \"\"\"Create an instance of Hubspot.\n\n        Args:\n            credentials (Optional[HubspotCredentials], optional): Hubspot credentials.\n                Defaults to None.\n            config_key (str, optional): The key in the viadot config holding relevant\n                credentials. Defaults to \"hubspot\".\n\n        Examples:\n            hubspot = Hubspot(\n                credentials=credentials,\n                config_key=config_key,\n            )\n            hubspot.api_connection(\n                endpoint=endpoint,\n                filters=filters,\n                properties=properties,\n                nrows=nrows,\n            )\n            data_frame = hubspot.to_df()\n\n        Raises:\n            CredentialError: If credentials are not provided in local_config or\n                directly as a parameter.\n        \"\"\"\n        raw_creds = credentials or get_source_credentials(config_key)\n        validated_creds = dict(HubspotCredentials(**raw_creds))\n        super().__init__(*args, credentials=validated_creds, **kwargs)\n\n        self.full_dataset = None\n\n    def _date_to_unixtimestamp(self, date: str | None = None) -&gt; int:\n        \"\"\"Convert date from \"yyyy-mm-dd\" to Unix Timestamp.\n\n        (SECONDS SINCE JAN 01 1970. (UTC)). For example:\n                1680774921 SECONDS SINCE JAN 01 1970. (UTC) -&gt; 11:55:49 AM 2023-04-06.\n\n        Args:\n            date (Optional[str], optional): Input date in format \"yyyy-mm-dd\".\n                Defaults to None.\n\n        Returns:\n            int: Number of seconds that passed since 1970-01-01 until \"date\".\n        \"\"\"\n        return int(datetime.timestamp(datetime.strptime(date, \"%Y-%m-%d\")) * 1000)\n\n    def _get_api_url(\n        self,\n        endpoint: str | None = None,\n        filters: dict[str, Any] | None = None,\n        properties: list[Any] | None = None,\n    ) -&gt; str:\n        \"\"\"Generates full url for Hubspot API given filters and parameters.\n\n        Args:\n            endpoint (Optional[str], optional): API endpoint for an individual request.\n                Defaults to None.\n            filters (Optional[Dict[str, Any]], optional): Filters defined for the API\n                body in specific order. Defaults to None.\n            properties (Optional[List[Any]], optional): List of user-defined columns to\n                be pulled from the API. Defaults to None.\n\n        Returns:\n            str: The final URL API.\n        \"\"\"\n        if self.API_URL in endpoint:\n            url = endpoint\n        elif endpoint.startswith(\"hubdb\"):\n            url = f\"{self.API_URL}/{endpoint}\"\n        else:\n            if filters:\n                url = f\"{self.API_URL}/crm/v3/objects/{endpoint}/search/?limit=100&amp;\"\n            else:\n                url = f\"{self.API_URL}/crm/v3/objects/{endpoint}/?limit=100&amp;\"\n\n            if properties and len(properties) &gt; 0:\n                url += f'properties={\",\".join(properties)}&amp;'\n\n        return url\n\n    def _format_filters(\n        self,\n        filters: list[dict[str, Any]] | None,\n    ) -&gt; list[dict[str, Any]]:\n        \"\"\"API body (filters) conversion from a user defined to API language.\n\n        Note: Right now only converts date to Unix Timestamp.\n\n        Args:\n            filters (Optional[List[Dict[str, Any]]]): List of filters in JSON format.\n\n        Returns:\n            List[Dict[str, Any]]: List of cleaned filters in JSON format.\n        \"\"\"\n        for item in filters:\n            for subitem in item[\"filters\"]:\n                for key in list(subitem.keys()):\n                    lookup = subitem[key]\n                    regex = re.findall(r\"\\d+-\\d+-\\d+\", lookup)\n                    if regex:\n                        regex = self._date_to_unixtimestamp(lookup)\n                        subitem[key] = f\"{regex}\"\n\n        return filters\n\n    def _get_api_body(self, filters: list[dict[str, Any]]):\n        \"\"\"Clean the filters body and converts to a JSON formatted value.\n\n        Args:\n            filters (List[Dict[str, Any]]): Filters dictionary that will be passed to\n                Hubspot API. Defaults to {}.\n\n        Example:\n                    filters = {\n                                \"filters\": [\n                                    {\n                                    \"propertyName\": \"createdate\",\n                                    \"operator\": \"BETWEEN\",\n                                    \"highValue\": \"2023-03-27\",\n                                    \"value\": \"2023-03-26\"\n                                    }\n                                ]\n                            }\n                Operators between the min and max value are listed below:\n                [IN, NOT_HAS_PROPERTY, LT, EQ, GT, NOT_IN, GTE, CONTAINS_TOKEN,\n                    HAS_PROPERTY, LTE, NOT_CONTAINS_TOKEN, BETWEEN, NEQ]\n                LT - Less than\n                LTE - Less than or equal to\n                GT - Greater than\n                GTE - Greater than or equal to\n                EQ - Equal to\n                NEQ - Not equal to\n                BETWEEN - Within the specified range. In your request, use key-value\n                    pairs to set highValue and value. Refer to the example above.\n                IN - Included within the specified list. This operator is\n                    case-sensitive, so inputted values must be in lowercase.\n                NOT_IN - Not included within the specified list\n                HAS_PROPERTY - Has a value for the specified property\n                NOT_HAS_PROPERTY - Doesn't have a value for the specified property\n                CONTAINS_TOKEN - Contains a token. In your request, you can use\n                    wildcards (*) to complete a partial search. For example, use the\n                    value *@hubspot.com to retrieve contacts with a HubSpot email\n                    address.\n                NOT_CONTAINS_TOKEN  -Doesn't contain a token\n\n        Returns:\n            Dict: Filters with a JSON format.\n        \"\"\"\n        return json.dumps({\"filterGroups\": filters, \"limit\": 100})\n\n    def _api_call(\n        self,\n        url: str | None = None,\n        body: str | None = None,\n        method: str | None = None,\n    ) -&gt; dict | None:\n        \"\"\"General method to connect to Hubspot API and generate the response.\n\n        Args:\n            url (Optional[str], optional): Hubspot API url. Defaults to None.\n            body (Optional[str], optional): Filters that will be pushed to the API body.\n                Defaults to None.\n            method (Optional[str], optional): Method of the API call. Defaults to None.\n\n        Raises:\n            APIError: When the `status_code` is different to 200.\n\n        Returns:\n            Dict: API response in JSON format.\n        \"\"\"\n        headers = {\n            \"Authorization\": f'Bearer {self.credentials[\"token\"]}',\n            \"Content-Type\": \"application/json\",\n        }\n\n        response = handle_api_response(\n            url=url, headers=headers, data=body, method=method\n        )\n\n        response_ok = 200\n        if response.status_code == response_ok:\n            return response.json()\n\n        self.logger.error(f\"Failed to load response content. - {response.content}\")\n        msg = \"Failed to load all exports.\"\n        raise APIError(msg)\n\n    def _get_offset_from_response(\n        self, api_response: dict[str, Any]\n    ) -&gt; tuple[str] | None:\n        \"\"\"Assign offset type/value depending on keys in API response.\n\n        Args:\n            api_response (Dict[str, Any]): API response in JSON format.\n\n        Returns:\n            tuple: Tuple in order: (offset_type, offset_value)\n        \"\"\"\n        if \"paging\" in api_response:\n            offset_type = \"after\"\n            offset_value = api_response[\"paging\"][\"next\"][f\"{offset_type}\"]\n\n        elif \"offset\" in api_response:\n            offset_type = \"offset\"\n            offset_value = api_response[\"offset\"]\n\n        else:\n            offset_type = None\n            offset_value = None\n\n        return (offset_type, offset_value)\n\n    def api_connection(\n        self,\n        endpoint: str | None = None,\n        filters: list[dict[str, Any]] | None = None,\n        properties: list[Any] | None = None,\n        nrows: int = 1000,\n    ) -&gt; None:\n        \"\"\"General method to connect to Hubspot API and generate the response.\n\n        Args:\n            endpoint (Optional[str], optional): API endpoint for an individual request.\n                Defaults to None.\n            filters (Optional[List[Dict[str, Any]]], optional): Filters defined for the\n                API body in specific order. Defaults to None.\n\n        Example:\n                    filters=[\n                        {\n                            \"filters\": [\n                                {\n                                    \"propertyName\": \"createdate\",\n                                    \"operator\": \"BETWEEN\",\n                                    \"highValue\": \"1642636800000\",\n                                    \"value\": \"1641995200000\",\n                                },\n                                {\n                                    \"propertyName\": \"email\",\n                                    \"operator\": \"CONTAINS_TOKEN\",\n                                    \"value\": \"*@xxxx.xx\",\n                                },\n                            ]\n                        }\n                    ],\n            properties (Optional[List[Any]], optional): List of user-defined columns to\n                be pulled from the API. Defaults to None.\n            nrows (int, optional): Max number of rows to pull during execution.\n                Defaults to 1000.\n\n        Raises:\n            APIError: Failed to download data from the endpoint.\n        \"\"\"\n        url = self._get_api_url(\n            endpoint=endpoint,\n            filters=filters,\n            properties=properties,\n        )\n        if filters:\n            filters_formatted = self._format_filters(filters)\n            body = self._get_api_body(filters=filters_formatted)\n            method = \"POST\"\n            partition = self._api_call(url=url, body=body, method=method)\n            self.full_dataset = partition[\"results\"]\n\n            while \"paging\" in partition and len(self.full_dataset) &lt; nrows:\n                body = json.loads(self._get_api_body(filters=filters_formatted))\n                body[\"after\"] = partition[\"paging\"][\"next\"][\"after\"]\n                partition = self._api_call(\n                    url=url, body=json.dumps(body), method=method\n                )\n                self.full_dataset.extend(partition[\"results\"])\n\n        else:\n            method = \"GET\"\n            partition = self._api_call(url=url, method=method)\n            self.full_dataset = partition[next(iter(partition.keys()))]\n\n            offset_type, offset_value = self._get_offset_from_response(partition)\n\n            while offset_value and len(self.full_dataset) &lt; nrows:\n                url = self._get_api_url(\n                    endpoint=endpoint,\n                    properties=properties,\n                    filters=filters,\n                )\n                url += f\"{offset_type}={offset_value}\"\n\n                partition = self._api_call(url=url, method=method)\n                self.full_dataset.extend(partition[next(iter(partition.keys()))])\n\n                offset_type, offset_value = self._get_offset_from_response(partition)\n\n    @add_viadot_metadata_columns\n    def to_df(\n        self,\n        if_empty: str = \"warn\",\n    ) -&gt; pd.DataFrame:\n        \"\"\"Generate a pandas DataFrame with the data in the Response and metadata.\n\n        Args:\n            if_empty (str, optional): What to do if a fetch produce no data.\n                Defaults to \"warn\n\n        Returns:\n            pd.Dataframe: The response data as a pandas DataFrame plus viadot metadata.\n        \"\"\"\n        super().to_df(if_empty=if_empty)\n\n        data_frame = pd.json_normalize(self.full_dataset)\n\n        if data_frame.empty:\n            self._handle_if_empty(\n                if_empty=if_empty,\n                message=\"The response does not contain any data.\",\n            )\n        else:\n            self.logger.info(\"Successfully downloaded data from the Mindful API.\")\n\n        return data_frame\n</code></pre>"},{"location":"references/sources/api/#viadot.sources.hubspot.Hubspot.__init__","title":"<code>__init__(*args, credentials=None, config_key='hubspot', **kwargs)</code>","text":"<p>Create an instance of Hubspot.</p> <p>Parameters:</p> Name Type Description Default <code>credentials</code> <code>Optional[HubspotCredentials]</code> <p>Hubspot credentials. Defaults to None.</p> <code>None</code> <code>config_key</code> <code>str</code> <p>The key in the viadot config holding relevant credentials. Defaults to \"hubspot\".</p> <code>'hubspot'</code> <p>Examples:</p> <p>hubspot = Hubspot(     credentials=credentials,     config_key=config_key, ) hubspot.api_connection(     endpoint=endpoint,     filters=filters,     properties=properties,     nrows=nrows, ) data_frame = hubspot.to_df()</p> <p>Raises:</p> Type Description <code>CredentialError</code> <p>If credentials are not provided in local_config or directly as a parameter.</p> Source code in <code>src/viadot/sources/hubspot.py</code> <pre><code>def __init__(\n    self,\n    *args,\n    credentials: HubspotCredentials | None = None,\n    config_key: str = \"hubspot\",\n    **kwargs,\n):\n    \"\"\"Create an instance of Hubspot.\n\n    Args:\n        credentials (Optional[HubspotCredentials], optional): Hubspot credentials.\n            Defaults to None.\n        config_key (str, optional): The key in the viadot config holding relevant\n            credentials. Defaults to \"hubspot\".\n\n    Examples:\n        hubspot = Hubspot(\n            credentials=credentials,\n            config_key=config_key,\n        )\n        hubspot.api_connection(\n            endpoint=endpoint,\n            filters=filters,\n            properties=properties,\n            nrows=nrows,\n        )\n        data_frame = hubspot.to_df()\n\n    Raises:\n        CredentialError: If credentials are not provided in local_config or\n            directly as a parameter.\n    \"\"\"\n    raw_creds = credentials or get_source_credentials(config_key)\n    validated_creds = dict(HubspotCredentials(**raw_creds))\n    super().__init__(*args, credentials=validated_creds, **kwargs)\n\n    self.full_dataset = None\n</code></pre>"},{"location":"references/sources/api/#viadot.sources.hubspot.Hubspot.api_connection","title":"<code>api_connection(endpoint=None, filters=None, properties=None, nrows=1000)</code>","text":"<p>General method to connect to Hubspot API and generate the response.</p> <p>Parameters:</p> Name Type Description Default <code>endpoint</code> <code>Optional[str]</code> <p>API endpoint for an individual request. Defaults to None.</p> <code>None</code> <code>filters</code> <code>Optional[List[Dict[str, Any]]]</code> <p>Filters defined for the API body in specific order. Defaults to None.</p> <code>None</code> Example <p>filters=[     {         \"filters\": [             {                 \"propertyName\": \"createdate\",                 \"operator\": \"BETWEEN\",                 \"highValue\": \"1642636800000\",                 \"value\": \"1641995200000\",             },             {                 \"propertyName\": \"email\",                 \"operator\": \"CONTAINS_TOKEN\",                 \"value\": \"*@xxxx.xx\",             },         ]     } ],</p> <pre><code>properties (Optional[List[Any]], optional): List of user-defined columns to\n    be pulled from the API. Defaults to None.\nnrows (int, optional): Max number of rows to pull during execution.\n    Defaults to 1000.\n</code></pre> <p>Raises:</p> Type Description <code>APIError</code> <p>Failed to download data from the endpoint.</p> Source code in <code>src/viadot/sources/hubspot.py</code> <pre><code>def api_connection(\n    self,\n    endpoint: str | None = None,\n    filters: list[dict[str, Any]] | None = None,\n    properties: list[Any] | None = None,\n    nrows: int = 1000,\n) -&gt; None:\n    \"\"\"General method to connect to Hubspot API and generate the response.\n\n    Args:\n        endpoint (Optional[str], optional): API endpoint for an individual request.\n            Defaults to None.\n        filters (Optional[List[Dict[str, Any]]], optional): Filters defined for the\n            API body in specific order. Defaults to None.\n\n    Example:\n                filters=[\n                    {\n                        \"filters\": [\n                            {\n                                \"propertyName\": \"createdate\",\n                                \"operator\": \"BETWEEN\",\n                                \"highValue\": \"1642636800000\",\n                                \"value\": \"1641995200000\",\n                            },\n                            {\n                                \"propertyName\": \"email\",\n                                \"operator\": \"CONTAINS_TOKEN\",\n                                \"value\": \"*@xxxx.xx\",\n                            },\n                        ]\n                    }\n                ],\n        properties (Optional[List[Any]], optional): List of user-defined columns to\n            be pulled from the API. Defaults to None.\n        nrows (int, optional): Max number of rows to pull during execution.\n            Defaults to 1000.\n\n    Raises:\n        APIError: Failed to download data from the endpoint.\n    \"\"\"\n    url = self._get_api_url(\n        endpoint=endpoint,\n        filters=filters,\n        properties=properties,\n    )\n    if filters:\n        filters_formatted = self._format_filters(filters)\n        body = self._get_api_body(filters=filters_formatted)\n        method = \"POST\"\n        partition = self._api_call(url=url, body=body, method=method)\n        self.full_dataset = partition[\"results\"]\n\n        while \"paging\" in partition and len(self.full_dataset) &lt; nrows:\n            body = json.loads(self._get_api_body(filters=filters_formatted))\n            body[\"after\"] = partition[\"paging\"][\"next\"][\"after\"]\n            partition = self._api_call(\n                url=url, body=json.dumps(body), method=method\n            )\n            self.full_dataset.extend(partition[\"results\"])\n\n    else:\n        method = \"GET\"\n        partition = self._api_call(url=url, method=method)\n        self.full_dataset = partition[next(iter(partition.keys()))]\n\n        offset_type, offset_value = self._get_offset_from_response(partition)\n\n        while offset_value and len(self.full_dataset) &lt; nrows:\n            url = self._get_api_url(\n                endpoint=endpoint,\n                properties=properties,\n                filters=filters,\n            )\n            url += f\"{offset_type}={offset_value}\"\n\n            partition = self._api_call(url=url, method=method)\n            self.full_dataset.extend(partition[next(iter(partition.keys()))])\n\n            offset_type, offset_value = self._get_offset_from_response(partition)\n</code></pre>"},{"location":"references/sources/api/#viadot.sources.hubspot.Hubspot.to_df","title":"<code>to_df(if_empty='warn')</code>","text":"<p>Generate a pandas DataFrame with the data in the Response and metadata.</p> <p>Parameters:</p> Name Type Description Default <code>if_empty</code> <code>str</code> <p>What to do if a fetch produce no data. Defaults to \"warn</p> <code>'warn'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.Dataframe: The response data as a pandas DataFrame plus viadot metadata.</p> Source code in <code>src/viadot/sources/hubspot.py</code> <pre><code>@add_viadot_metadata_columns\ndef to_df(\n    self,\n    if_empty: str = \"warn\",\n) -&gt; pd.DataFrame:\n    \"\"\"Generate a pandas DataFrame with the data in the Response and metadata.\n\n    Args:\n        if_empty (str, optional): What to do if a fetch produce no data.\n            Defaults to \"warn\n\n    Returns:\n        pd.Dataframe: The response data as a pandas DataFrame plus viadot metadata.\n    \"\"\"\n    super().to_df(if_empty=if_empty)\n\n    data_frame = pd.json_normalize(self.full_dataset)\n\n    if data_frame.empty:\n        self._handle_if_empty(\n            if_empty=if_empty,\n            message=\"The response does not contain any data.\",\n        )\n    else:\n        self.logger.info(\"Successfully downloaded data from the Mindful API.\")\n\n    return data_frame\n</code></pre>"},{"location":"references/sources/api/#viadot.sources.mediatool.Mediatool","title":"<code>viadot.sources.mediatool.Mediatool</code>","text":"<p>               Bases: <code>Source</code></p> <p>Class implementing the Mediatool API.</p> <p>Download data from Mediatool platform. Using Mediatool class user is able to download organizations, media entries, campaigns, vehicles, and media types data.</p> Source code in <code>src/viadot/sources/mediatool.py</code> <pre><code>class Mediatool(Source):\n    \"\"\"Class implementing the Mediatool API.\n\n    Download data from Mediatool platform. Using Mediatool class user is able to\n    download organizations, media entries, campaigns, vehicles, and media types data.\n    \"\"\"\n\n    def __init__(\n        self,\n        *args,\n        credentials: MediatoolCredentials | None = None,\n        config_key: str | None = None,\n        user_id: str | None = None,\n        **kwargs,\n    ):\n        \"\"\"Create an instance of the Mediatool class.\n\n        Args:\n            credentials (MediatoolCredentials, optional): Meditaool credentials.\n                Defaults to None.\n            config_key (str, optional): The key in the viadot config holding relevant\n                credentials. Defaults to None.\n            user_id (str, optional): User ID. Defaults to None.\n        \"\"\"\n        credentials = credentials or get_source_credentials(config_key) or None\n        if credentials is None:\n            message = \"Missing credentials.\"\n            raise CredentialError(message)\n\n        validated_creds = dict(MediatoolCredentials(**credentials))\n        super().__init__(*args, credentials=validated_creds, **kwargs)\n\n        self.header = {\"Authorization\": f\"Bearer {credentials.get('token')}\"}\n        self.user_id = user_id or credentials.get(\"user_id\")\n\n    def _get_organizations(\n        self,\n        user_id: str | None = None,\n    ) -&gt; list[dict[str, str]]:\n        \"\"\"Get organizations data based on the user ID.\n\n        Args:\n            user_id (str, optional): User ID. Defaults to None.\n\n        Returns:\n            list[dict[str, str]]: A list of dicts will be returned.\n        \"\"\"\n        user_id = user_id or self.user_id\n        url_organizations = f\"https://api.mediatool.com/users/{user_id}/organizations\"\n\n        response = handle_api_response(\n            url=url_organizations,\n            headers=self.header,\n            method=\"GET\",\n        )\n        response_dict = json.loads(response.text)\n        organizations = response_dict[\"organizations\"]\n\n        list_organizations = []\n        for org in organizations:\n            list_organizations.append(\n                {\n                    \"_id\": org.get(\"_id\"),\n                    \"name\": org.get(\"name\"),\n                    \"abbreviation\": org.get(\"abbreviation\"),\n                }\n            )\n\n        return list_organizations\n\n    def _get_media_entries(\n        self,\n        organization_id: str,\n    ) -&gt; list[dict[str, str]]:\n        \"\"\"Data for media entries.\n\n        Args:\n            organization_id (str): Organization ID.\n\n        Returns:\n            list[dict[str, str]]: A list of dicts will be returned.\n        \"\"\"\n        url = (\n            \"https://api.mediatool.com/searchmediaentries?q=\"\n            + f'{{\"organizationId\": \"{organization_id}\"}}'\n        )\n\n        response = handle_api_response(\n            url=url,\n            headers=self.header,\n            method=\"GET\",\n        )\n        response_dict = json.loads(response.text)\n\n        return response_dict[\"mediaEntries\"]\n\n    def _get_vehicles(\n        self,\n        vehicle_ids: list[str],\n    ) -&gt; list[dict[str, str]]:\n        \"\"\"Vehicles data based on the organization IDs.\n\n        Args:\n            vehicle_ids (list[str]): List of organization IDs.\n\n        Raises:\n            APIError: Mediatool API does not recognise the vehicle id.\n\n        Returns:\n            list[dict[str, str]]: A list of dicts will be returned.\n        \"\"\"\n        response_list = []\n        missing_vehicles = []\n\n        for vid in vehicle_ids:\n            url = f\"https://api.mediatool.com/vehicles/{vid}\"\n            try:\n                response = handle_api_response(\n                    url=url,\n                    headers=self.header,\n                    method=\"GET\",\n                )\n            except APIError:\n                missing_vehicles.append(vid)\n            else:\n                response_dict = json.loads(response.text)\n                response_list.append(response_dict[\"vehicle\"])\n\n        if missing_vehicles:\n            self.logger.error(f\"Vehicle were not found for: {missing_vehicles}.\")\n\n        return response_list\n\n    def _get_campaigns(\n        self,\n        organization_id: str,\n    ) -&gt; list[dict[str, str]]:\n        \"\"\"Campaign data based on the organization ID.\n\n        Args:\n            organization_id (str): Organization ID.\n\n        Returns:\n            list[dict[str, str]]: A list of dicts will be returned.\n        \"\"\"\n        url_campaigns = (\n            f\"https://api.mediatool.com/organizations/{organization_id}/campaigns\"\n        )\n\n        response = handle_api_response(\n            url=url_campaigns,\n            headers=self.header,\n            method=\"GET\",\n        )\n        response_dict = json.loads(response.text)\n\n        return response_dict[\"campaigns\"]\n\n    def _get_media_types(\n        self,\n        media_type_ids: list[str],\n    ) -&gt; list[dict[str, str]]:\n        \"\"\"Media types data based on the media types ID.\n\n        Args:\n            media_type_ids (list[str]): List of media type IDs.\n\n        Returns:\n            list[dict[str, str]]: A list of dicts will be returned.\n        \"\"\"\n        list_media_types = []\n        for id_media_type in media_type_ids:\n            response = handle_api_response(\n                url=f\"https://api.mediatool.com/mediatypes/{id_media_type}\",\n                headers=self.header,\n                method=\"GET\",\n            )\n            response_dict = json.loads(response.text)\n            list_media_types.append(\n                {\n                    \"_id\": response_dict.get(\"mediaType\").get(\"_id\"),\n                    \"name\": response_dict.get(\"mediaType\").get(\"name\"),\n                    \"type\": response_dict.get(\"mediaType\").get(\"type\"),\n                }\n            )\n\n        return list_media_types\n\n    def _to_records(\n        self,\n        endpoint: Literal[\n            \"organizations\", \"media_entries\", \"vehicles\", \"campaigns\", \"media_types\"\n        ],\n        organization_id: str | None = None,\n        vehicle_ids: list[str] | None = None,\n        media_type_ids: list[str] | None = None,\n    ) -&gt; list[dict[str, str]]:\n        \"\"\"Connects to the Mediatool API and retrieves data for the specified endpoint.\n\n        Args:\n            endpoint (Literal[\"organizations\", \"media_entries\", \"vehicles\", \"campaigns\",\n                \"media_types\"]): The API endpoint to fetch data from.\n            organization_id (str, optional): Organization ID. Defaults to None.\n            vehicle_ids (list[str]): List of organization IDs. Defaults to None.\n            media_type_ids (list[str]): List of media type IDs. Defaults to None.\n\n        Returns:\n            list[dict[str, str]]: A list of records containing the retrieved data.\n        \"\"\"\n        if endpoint == \"organizations\":\n            return self._get_organizations(self.user_id)\n\n        if endpoint == \"media_entries\":\n            return self._get_media_entries(organization_id=organization_id)\n\n        if endpoint == \"vehicles\":\n            return self._get_vehicles(vehicle_ids=vehicle_ids)\n\n        if endpoint == \"campaigns\":\n            return self._get_campaigns(organization_id=organization_id)\n\n        if endpoint == \"media_types\":\n            return self._get_media_types(media_type_ids=media_type_ids)\n        return None\n\n    def fetch_and_transform(\n        self,\n        endpoint: Literal[\n            \"organizations\", \"media_entries\", \"vehicles\", \"campaigns\", \"media_types\"\n        ],\n        organization_id: str | None = None,\n        vehicle_ids: list[str] | None = None,\n        media_type_ids: list[str] | None = None,\n        columns: list[str] | None = None,\n        if_empty: str = \"warn\",\n        add_endpoint_suffix: bool = True,\n    ) -&gt; pd.DataFrame:\n        \"\"\"Pandas Data Frame with the data in the Response object and metadata.\n\n        Args:\n            endpoint (Literal[\"organizations\", \"media_entries\", \"vehicles\", \"campaigns\",\n                \"media_types\"]): The API endpoint to fetch data from.\n            organization_id (str, optional): Organization ID. Defaults to None.\n            vehicle_ids (list[str]): List of organization IDs. Defaults to None.\n            media_type_ids (list[str]): List of media type IDs. Defaults to None.\n            columns (list[str], optional): If provided, a list of column names to\n                include in the DataFrame.By default, all columns will be included.\n                Defaults to None.\n            if_empty (str, optional): What to do if a fetch produce no data.\n                Defaults to \"warn\n            add_endpoint_suffix (bool, optional): If True, appends the endpoint name\n                to column names in the format {column_name}_{endpoint} to ensure\n                uniqueness in combined DataFrame from multiple endpoints.\n                Defaults to True.\n\n        Returns:\n            pd.Dataframe: The response data as a Pandas Data Frame plus viadot metadata.\n        \"\"\"\n        records = self._to_records(\n            endpoint, organization_id, vehicle_ids, media_type_ids\n        )\n\n        data_frame = pd.DataFrame.from_dict(records)  # type: ignore\n\n        if endpoint == \"campaigns\":\n            data_frame.replace(\n                to_replace=[r\"\\\\t|\\\\n|\\\\r\", \"\\t|\\n|\\r\"],\n                value=[\"\", \"\"],\n                regex=True,\n                inplace=True,\n            )\n\n        if add_endpoint_suffix:\n            data_frame = data_frame.rename(\n                columns={\n                    column_name: f\"{column_name}_{endpoint}\"\n                    for column_name in data_frame.columns\n                }\n            )\n\n        if columns:\n            if set(columns).issubset(set(data_frame.columns)):\n                data_frame = data_frame[columns]\n            elif not set(columns).issubset(set(data_frame.columns)):\n                self.logger.error(\n                    f\"Columns '{', '.join(columns)}' are incorrect. \"\n                    + \"Whole dictionary for 'mediaEntries' will be returned.\"\n                )\n\n        if data_frame.empty:\n            self._handle_if_empty(\n                if_empty=if_empty,\n                message=\"The response does not contain any data.\",\n            )\n        else:\n            self.logger.info(\n                \"Successfully downloaded data from \"\n                + f\"the Mediatool API ({endpoint}).\"\n            )\n\n        return data_frame\n\n    @add_viadot_metadata_columns\n    def to_df(\n        self,\n        organization_ids: list[str],\n        media_entries_columns: list[str] | None = None,\n        if_empty: Literal[\"warn\"] | Literal[\"skip\"] | Literal[\"fail\"] = \"warn\",\n    ) -&gt; pd.DataFrame:\n        \"\"\"Fetches, transforms, and combines data from Mediatool API endpoints.\n\n        Args:\n            organization_ids (list[str]): List of organization IDs.\n            media_entries_columns (list[str], optional): Columns to get from media\n                entries. Defaults to None.\n            if_empty (Literal[warn, skip, fail], optional): What to do if there is no\n                data. Defaults to \"warn\".\n\n        Raises:\n            ValueError: Raised when no organizations are defined or an organization ID\n            is not found in the organizations list.\n\n        Returns:\n            pd.DataFrame: DataFrame containing the combined data from the specified\n            endpoints.\n        \"\"\"\n        if not organization_ids:\n            message = \"'organization_ids' must be a non-empty list.\"\n            raise ValueError(message)\n\n        # first method ORGANIZATIONS\n        df_organizations = self.fetch_and_transform(endpoint=\"organizations\")\n\n        list_of_organizations_df = []\n        for organization_id in organization_ids:\n            if organization_id in df_organizations[\"_id_organizations\"].unique():\n                self.logger.info(f\"Downloading data for: {organization_id} ...\")\n\n                # extract media entries per organization\n                df_media_entries = self.fetch_and_transform(\n                    endpoint=\"media_entries\",\n                    organization_id=organization_id,\n                    columns=media_entries_columns,\n                    add_endpoint_suffix=False,\n                )\n\n                unique_vehicle_ids = df_media_entries[\"vehicleId\"].unique()\n                unique_media_type_ids = df_media_entries[\"mediaTypeId\"].unique()\n\n                # extract vehicles\n                df_vehicles = self.fetch_and_transform(\n                    endpoint=\"vehicles\", vehicle_ids=unique_vehicle_ids\n                )\n\n                # extract campaigns\n                df_campaigns = self.fetch_and_transform(\n                    endpoint=\"campaigns\",\n                    organization_id=organization_id,\n                )\n\n                # extract media types\n                df_media_types = self.fetch_and_transform(\n                    endpoint=\"media_types\",\n                    media_type_ids=unique_media_type_ids,\n                )\n\n                # join media entries &amp; organizations\n                df_merged_entries_orgs = join_dfs(\n                    df_left=df_media_entries,\n                    df_right=df_organizations,\n                    left_on=\"organizationId\",\n                    right_on=\"_id_organizations\",\n                    columns_from_right_df=[\n                        \"_id_organizations\",\n                        \"name_organizations\",\n                        \"abbreviation_organizations\",\n                    ],\n                    how=\"left\",\n                )\n\n                # join the previous merge &amp; campaigns\n                df_merged_campaigns = join_dfs(\n                    df_left=df_merged_entries_orgs,\n                    df_right=df_campaigns,\n                    left_on=\"campaignId\",\n                    right_on=\"_id_campaigns\",\n                    columns_from_right_df=[\n                        \"_id_campaigns\",\n                        \"name_campaigns\",\n                        \"conventionalName_campaigns\",\n                    ],\n                    how=\"left\",\n                )\n\n                # join the previous merge &amp; vehicles\n                df_merged_vehicles = join_dfs(\n                    df_left=df_merged_campaigns,\n                    df_right=df_vehicles,\n                    left_on=\"vehicleId\",\n                    right_on=\"_id_vehicles\",\n                    columns_from_right_df=[\"_id_vehicles\", \"name_vehicles\"],\n                    how=\"left\",\n                )\n\n                # join the previous merge &amp; media types\n                df_merged_media_types = join_dfs(\n                    df_left=df_merged_vehicles,\n                    df_right=df_media_types,\n                    left_on=\"mediaTypeId\",\n                    right_on=\"_id_media_types\",\n                    columns_from_right_df=[\"_id_media_types\", \"name_media_types\"],\n                    how=\"left\",\n                )\n\n                list_of_organizations_df.append(df_merged_media_types)\n\n            else:\n                message = (\n                    f\"Organization - {organization_id} not found in organizations list.\"\n                )\n                raise ValueError(message)\n\n        df_final = pd.concat(list_of_organizations_df)\n\n        if df_final.empty:\n            self._handle_if_empty(if_empty)\n\n        return df_final\n</code></pre>"},{"location":"references/sources/api/#viadot.sources.mediatool.Mediatool.__init__","title":"<code>__init__(*args, credentials=None, config_key=None, user_id=None, **kwargs)</code>","text":"<p>Create an instance of the Mediatool class.</p> <p>Parameters:</p> Name Type Description Default <code>credentials</code> <code>MediatoolCredentials</code> <p>Meditaool credentials. Defaults to None.</p> <code>None</code> <code>config_key</code> <code>str</code> <p>The key in the viadot config holding relevant credentials. Defaults to None.</p> <code>None</code> <code>user_id</code> <code>str</code> <p>User ID. Defaults to None.</p> <code>None</code> Source code in <code>src/viadot/sources/mediatool.py</code> <pre><code>def __init__(\n    self,\n    *args,\n    credentials: MediatoolCredentials | None = None,\n    config_key: str | None = None,\n    user_id: str | None = None,\n    **kwargs,\n):\n    \"\"\"Create an instance of the Mediatool class.\n\n    Args:\n        credentials (MediatoolCredentials, optional): Meditaool credentials.\n            Defaults to None.\n        config_key (str, optional): The key in the viadot config holding relevant\n            credentials. Defaults to None.\n        user_id (str, optional): User ID. Defaults to None.\n    \"\"\"\n    credentials = credentials or get_source_credentials(config_key) or None\n    if credentials is None:\n        message = \"Missing credentials.\"\n        raise CredentialError(message)\n\n    validated_creds = dict(MediatoolCredentials(**credentials))\n    super().__init__(*args, credentials=validated_creds, **kwargs)\n\n    self.header = {\"Authorization\": f\"Bearer {credentials.get('token')}\"}\n    self.user_id = user_id or credentials.get(\"user_id\")\n</code></pre>"},{"location":"references/sources/api/#viadot.sources.mediatool.Mediatool.fetch_and_transform","title":"<code>fetch_and_transform(endpoint, organization_id=None, vehicle_ids=None, media_type_ids=None, columns=None, if_empty='warn', add_endpoint_suffix=True)</code>","text":"<p>Pandas Data Frame with the data in the Response object and metadata.</p> <p>Parameters:</p> Name Type Description Default <code>organization_id</code> <code>str</code> <p>Organization ID. Defaults to None.</p> <code>None</code> <code>vehicle_ids</code> <code>list[str]</code> <p>List of organization IDs. Defaults to None.</p> <code>None</code> <code>media_type_ids</code> <code>list[str]</code> <p>List of media type IDs. Defaults to None.</p> <code>None</code> <code>columns</code> <code>list[str]</code> <p>If provided, a list of column names to include in the DataFrame.By default, all columns will be included. Defaults to None.</p> <code>None</code> <code>if_empty</code> <code>str</code> <p>What to do if a fetch produce no data. Defaults to \"warn</p> <code>'warn'</code> <code>add_endpoint_suffix</code> <code>bool</code> <p>If True, appends the endpoint name to column names in the format {column_name}_{endpoint} to ensure uniqueness in combined DataFrame from multiple endpoints. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.Dataframe: The response data as a Pandas Data Frame plus viadot metadata.</p> Source code in <code>src/viadot/sources/mediatool.py</code> <pre><code>def fetch_and_transform(\n    self,\n    endpoint: Literal[\n        \"organizations\", \"media_entries\", \"vehicles\", \"campaigns\", \"media_types\"\n    ],\n    organization_id: str | None = None,\n    vehicle_ids: list[str] | None = None,\n    media_type_ids: list[str] | None = None,\n    columns: list[str] | None = None,\n    if_empty: str = \"warn\",\n    add_endpoint_suffix: bool = True,\n) -&gt; pd.DataFrame:\n    \"\"\"Pandas Data Frame with the data in the Response object and metadata.\n\n    Args:\n        endpoint (Literal[\"organizations\", \"media_entries\", \"vehicles\", \"campaigns\",\n            \"media_types\"]): The API endpoint to fetch data from.\n        organization_id (str, optional): Organization ID. Defaults to None.\n        vehicle_ids (list[str]): List of organization IDs. Defaults to None.\n        media_type_ids (list[str]): List of media type IDs. Defaults to None.\n        columns (list[str], optional): If provided, a list of column names to\n            include in the DataFrame.By default, all columns will be included.\n            Defaults to None.\n        if_empty (str, optional): What to do if a fetch produce no data.\n            Defaults to \"warn\n        add_endpoint_suffix (bool, optional): If True, appends the endpoint name\n            to column names in the format {column_name}_{endpoint} to ensure\n            uniqueness in combined DataFrame from multiple endpoints.\n            Defaults to True.\n\n    Returns:\n        pd.Dataframe: The response data as a Pandas Data Frame plus viadot metadata.\n    \"\"\"\n    records = self._to_records(\n        endpoint, organization_id, vehicle_ids, media_type_ids\n    )\n\n    data_frame = pd.DataFrame.from_dict(records)  # type: ignore\n\n    if endpoint == \"campaigns\":\n        data_frame.replace(\n            to_replace=[r\"\\\\t|\\\\n|\\\\r\", \"\\t|\\n|\\r\"],\n            value=[\"\", \"\"],\n            regex=True,\n            inplace=True,\n        )\n\n    if add_endpoint_suffix:\n        data_frame = data_frame.rename(\n            columns={\n                column_name: f\"{column_name}_{endpoint}\"\n                for column_name in data_frame.columns\n            }\n        )\n\n    if columns:\n        if set(columns).issubset(set(data_frame.columns)):\n            data_frame = data_frame[columns]\n        elif not set(columns).issubset(set(data_frame.columns)):\n            self.logger.error(\n                f\"Columns '{', '.join(columns)}' are incorrect. \"\n                + \"Whole dictionary for 'mediaEntries' will be returned.\"\n            )\n\n    if data_frame.empty:\n        self._handle_if_empty(\n            if_empty=if_empty,\n            message=\"The response does not contain any data.\",\n        )\n    else:\n        self.logger.info(\n            \"Successfully downloaded data from \"\n            + f\"the Mediatool API ({endpoint}).\"\n        )\n\n    return data_frame\n</code></pre>"},{"location":"references/sources/api/#viadot.sources.mediatool.Mediatool.to_df","title":"<code>to_df(organization_ids, media_entries_columns=None, if_empty='warn')</code>","text":"<p>Fetches, transforms, and combines data from Mediatool API endpoints.</p> <p>Parameters:</p> Name Type Description Default <code>organization_ids</code> <code>list[str]</code> <p>List of organization IDs.</p> required <code>media_entries_columns</code> <code>list[str]</code> <p>Columns to get from media entries. Defaults to None.</p> <code>None</code> <code>if_empty</code> <code>Literal[warn, skip, fail]</code> <p>What to do if there is no data. Defaults to \"warn\".</p> <code>'warn'</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>Raised when no organizations are defined or an organization ID</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: DataFrame containing the combined data from the specified</p> <code>DataFrame</code> <p>endpoints.</p> Source code in <code>src/viadot/sources/mediatool.py</code> <pre><code>@add_viadot_metadata_columns\ndef to_df(\n    self,\n    organization_ids: list[str],\n    media_entries_columns: list[str] | None = None,\n    if_empty: Literal[\"warn\"] | Literal[\"skip\"] | Literal[\"fail\"] = \"warn\",\n) -&gt; pd.DataFrame:\n    \"\"\"Fetches, transforms, and combines data from Mediatool API endpoints.\n\n    Args:\n        organization_ids (list[str]): List of organization IDs.\n        media_entries_columns (list[str], optional): Columns to get from media\n            entries. Defaults to None.\n        if_empty (Literal[warn, skip, fail], optional): What to do if there is no\n            data. Defaults to \"warn\".\n\n    Raises:\n        ValueError: Raised when no organizations are defined or an organization ID\n        is not found in the organizations list.\n\n    Returns:\n        pd.DataFrame: DataFrame containing the combined data from the specified\n        endpoints.\n    \"\"\"\n    if not organization_ids:\n        message = \"'organization_ids' must be a non-empty list.\"\n        raise ValueError(message)\n\n    # first method ORGANIZATIONS\n    df_organizations = self.fetch_and_transform(endpoint=\"organizations\")\n\n    list_of_organizations_df = []\n    for organization_id in organization_ids:\n        if organization_id in df_organizations[\"_id_organizations\"].unique():\n            self.logger.info(f\"Downloading data for: {organization_id} ...\")\n\n            # extract media entries per organization\n            df_media_entries = self.fetch_and_transform(\n                endpoint=\"media_entries\",\n                organization_id=organization_id,\n                columns=media_entries_columns,\n                add_endpoint_suffix=False,\n            )\n\n            unique_vehicle_ids = df_media_entries[\"vehicleId\"].unique()\n            unique_media_type_ids = df_media_entries[\"mediaTypeId\"].unique()\n\n            # extract vehicles\n            df_vehicles = self.fetch_and_transform(\n                endpoint=\"vehicles\", vehicle_ids=unique_vehicle_ids\n            )\n\n            # extract campaigns\n            df_campaigns = self.fetch_and_transform(\n                endpoint=\"campaigns\",\n                organization_id=organization_id,\n            )\n\n            # extract media types\n            df_media_types = self.fetch_and_transform(\n                endpoint=\"media_types\",\n                media_type_ids=unique_media_type_ids,\n            )\n\n            # join media entries &amp; organizations\n            df_merged_entries_orgs = join_dfs(\n                df_left=df_media_entries,\n                df_right=df_organizations,\n                left_on=\"organizationId\",\n                right_on=\"_id_organizations\",\n                columns_from_right_df=[\n                    \"_id_organizations\",\n                    \"name_organizations\",\n                    \"abbreviation_organizations\",\n                ],\n                how=\"left\",\n            )\n\n            # join the previous merge &amp; campaigns\n            df_merged_campaigns = join_dfs(\n                df_left=df_merged_entries_orgs,\n                df_right=df_campaigns,\n                left_on=\"campaignId\",\n                right_on=\"_id_campaigns\",\n                columns_from_right_df=[\n                    \"_id_campaigns\",\n                    \"name_campaigns\",\n                    \"conventionalName_campaigns\",\n                ],\n                how=\"left\",\n            )\n\n            # join the previous merge &amp; vehicles\n            df_merged_vehicles = join_dfs(\n                df_left=df_merged_campaigns,\n                df_right=df_vehicles,\n                left_on=\"vehicleId\",\n                right_on=\"_id_vehicles\",\n                columns_from_right_df=[\"_id_vehicles\", \"name_vehicles\"],\n                how=\"left\",\n            )\n\n            # join the previous merge &amp; media types\n            df_merged_media_types = join_dfs(\n                df_left=df_merged_vehicles,\n                df_right=df_media_types,\n                left_on=\"mediaTypeId\",\n                right_on=\"_id_media_types\",\n                columns_from_right_df=[\"_id_media_types\", \"name_media_types\"],\n                how=\"left\",\n            )\n\n            list_of_organizations_df.append(df_merged_media_types)\n\n        else:\n            message = (\n                f\"Organization - {organization_id} not found in organizations list.\"\n            )\n            raise ValueError(message)\n\n    df_final = pd.concat(list_of_organizations_df)\n\n    if df_final.empty:\n        self._handle_if_empty(if_empty)\n\n    return df_final\n</code></pre>"},{"location":"references/sources/api/#viadot.sources.mindful.Mindful","title":"<code>viadot.sources.mindful.Mindful</code>","text":"<p>               Bases: <code>Source</code></p> <p>Class implementing the Mindful API.</p> <p>Documentation for this API is available at: https://apidocs.surveydynamix.com/.</p> Source code in <code>src/viadot/sources/mindful.py</code> <pre><code>class Mindful(Source):\n    \"\"\"Class implementing the Mindful API.\n\n    Documentation for this API is available at: https://apidocs.surveydynamix.com/.\n    \"\"\"\n\n    ENDPOINTS = (\"interactions\", \"responses\", \"surveys\")\n\n    def __init__(\n        self,\n        *args,\n        credentials: MindfulCredentials | None = None,\n        config_key: str = \"mindful\",\n        region: Literal[\"us1\", \"us2\", \"us3\", \"ca1\", \"eu1\", \"au1\"] = \"eu1\",\n        **kwargs,\n    ):\n        \"\"\"Create a Mindful instance.\n\n        Args:\n            credentials (Optional[MindfulCredentials], optional): Mindful credentials.\n                Defaults to None.\n            config_key (str, optional): The key in the viadot config holding relevant\n                credentials. Defaults to \"mindful\".\n            region (Literal[us1, us2, us3, ca1, eu1, au1], optional): Survey Dynamix\n                region from where to interact with the mindful API. Defaults to \"eu1\"\n                English (United Kingdom).\n\n        Examples:\n            mindful = Mindful(\n                credentials=credentials,\n                config_key=config_key,\n                region=region,\n            )\n            mindful.api_connection(\n                endpoint=endpoint,\n                date_interval=date_interval,\n                limit=limit,\n            )\n            data_frame = mindful.to_df()\n        \"\"\"\n        credentials = credentials or get_source_credentials(config_key) or None\n        if credentials is None:\n            msg = \"Missing credentials.\"\n            raise CredentialError(msg)\n\n        validated_creds = dict(MindfulCredentials(**credentials))\n        super().__init__(*args, credentials=validated_creds, **kwargs)\n\n        self.auth = (credentials[\"customer_uuid\"], credentials[\"auth_token\"])\n        if region != \"us1\":\n            self.region = f\"{region}.\"\n        else:\n            self.region = \"\"\n\n    def _mindful_api_response(\n        self,\n        params: dict[str, Any] | None = None,\n        endpoint: str = \"\",\n    ) -&gt; Response:\n        \"\"\"Call to Mindful API given an endpoint.\n\n        Args:\n            params (Optional[Dict[str, Any]], optional): Parameters to be passed into\n                the request. Defaults to None.\n            endpoint (str, optional): API endpoint for an individual request.\n                Defaults to \"\".\n\n        Returns:\n            Response: request object with the response from the Mindful API.\n        \"\"\"\n        return handle_api_response(\n            url=f\"https://{self.region}surveydynamix.com/api/{endpoint}\",\n            params=params,\n            method=\"GET\",\n            auth=HTTPBasicAuth(*self.auth),\n        )\n\n    def api_connection(\n        self,\n        endpoint: Literal[\"interactions\", \"responses\", \"surveys\"] = \"surveys\",\n        date_interval: list[date] | None = None,\n        limit: int = 1000,\n    ) -&gt; None:\n        \"\"\"General method to connect to Survey Dynamix API and generate the response.\n\n        Args:\n            endpoint (Literal[\"interactions\", \"responses\", \"surveys\"], optional): API\n                endpoint for an individual request. Defaults to \"surveys\".\n            date_interval (Optional[List[date]], optional): Date time range detailing\n                the starting date and the ending date. If no range is passed, one day of\n                data since this moment will be retrieved. Defaults to None.\n            limit (int, optional): The number of matching interactions to return.\n                Defaults to 1000.\n\n        Raises:\n            ValueError: Not available endpoint.\n            APIError: Failed to download data from the endpoint.\n        \"\"\"\n        if endpoint not in self.ENDPOINTS:\n            raise ValueError(\n                f\"Survey Dynamix endpoint: '{endpoint}',\"\n                + \" is not available through Mindful viadot connector.\"\n            )\n\n        if (\n            date_interval is None\n            or all(list(map(isinstance, date_interval, [date] * len(date_interval))))\n            is False\n        ):\n            reference_date = date.today()\n            date_interval = [reference_date - timedelta(days=1), reference_date]\n\n            self.logger.warning(\n                \"No `date_interval` parameter was defined, or was erroneously \"\n                + \"defined. `date_interval` parameter must have the folloing \"\n                + \"structure:\\n\\t[`date_0`, `date_1`], having that `date_1` &gt; \"\n                + \"`date_0`.\\nBy default, one day of data, from \"\n                + f\"{date_interval[0].strftime('%Y-%m-%d')} to \"\n                + f\"{date_interval[1].strftime('%Y-%m-%d')}, will be obtained.\"\n            )\n\n        params = {\n            \"_limit\": limit,\n            \"start_date\": f\"{date_interval[0]}\",\n            \"end_date\": f\"{date_interval[1]}\",\n        }\n\n        if endpoint == \"surveys\":\n            del params[\"start_date\"]\n            del params[\"end_date\"]\n\n        response = self._mindful_api_response(\n            endpoint=endpoint,\n            params=params,\n        )\n        response_ok = 200\n        no_data_code = 204\n        if response.status_code == response_ok:\n            self.logger.info(\n                f\"Successfully downloaded '{endpoint}' data from mindful API.\"\n            )\n            self.data = StringIO(response.content.decode(\"utf-8\"))\n        elif response.status_code == no_data_code and not response.content.decode():\n            self.logger.warning(\n                f\"There are not '{endpoint}' data to download from\"\n                + f\" {date_interval[0]} to {date_interval[1]}.\"\n            )\n            self.data = json.dumps({})\n        else:\n            self.logger.error(\n                f\"Failed to downloaded '{endpoint}' data. - {response.content}\"\n            )\n            msg = f\"Failed to downloaded '{endpoint}' data.\"\n            raise APIError(msg)\n\n    @add_viadot_metadata_columns\n    def to_df(\n        self,\n        if_empty: str = \"warn\",\n    ) -&gt; pd.DataFrame:\n        \"\"\"Download the data to a pandas DataFrame.\n\n        Args:\n            if_empty (str, optional): What to do if a fetch produce no data.\n                Defaults to \"warn\n\n        Returns:\n            pd.Dataframe: The response data as a pandas DataFrame plus viadot metadata.\n        \"\"\"\n        super().to_df(if_empty=if_empty)\n\n        data_frame = pd.read_json(self.data)\n\n        if data_frame.empty:\n            self._handle_if_empty(\n                if_empty=if_empty,\n                message=\"The response does not contain any data.\",\n            )\n        else:\n            self.logger.info(\"Successfully downloaded data from the Mindful API.\")\n\n        return data_frame\n</code></pre>"},{"location":"references/sources/api/#viadot.sources.mindful.Mindful.__init__","title":"<code>__init__(*args, credentials=None, config_key='mindful', region='eu1', **kwargs)</code>","text":"<p>Create a Mindful instance.</p> <p>Parameters:</p> Name Type Description Default <code>credentials</code> <code>Optional[MindfulCredentials]</code> <p>Mindful credentials. Defaults to None.</p> <code>None</code> <code>config_key</code> <code>str</code> <p>The key in the viadot config holding relevant credentials. Defaults to \"mindful\".</p> <code>'mindful'</code> <code>region</code> <code>Literal[us1, us2, us3, ca1, eu1, au1]</code> <p>Survey Dynamix region from where to interact with the mindful API. Defaults to \"eu1\" English (United Kingdom).</p> <code>'eu1'</code> <p>Examples:</p> <p>mindful = Mindful(     credentials=credentials,     config_key=config_key,     region=region, ) mindful.api_connection(     endpoint=endpoint,     date_interval=date_interval,     limit=limit, ) data_frame = mindful.to_df()</p> Source code in <code>src/viadot/sources/mindful.py</code> <pre><code>def __init__(\n    self,\n    *args,\n    credentials: MindfulCredentials | None = None,\n    config_key: str = \"mindful\",\n    region: Literal[\"us1\", \"us2\", \"us3\", \"ca1\", \"eu1\", \"au1\"] = \"eu1\",\n    **kwargs,\n):\n    \"\"\"Create a Mindful instance.\n\n    Args:\n        credentials (Optional[MindfulCredentials], optional): Mindful credentials.\n            Defaults to None.\n        config_key (str, optional): The key in the viadot config holding relevant\n            credentials. Defaults to \"mindful\".\n        region (Literal[us1, us2, us3, ca1, eu1, au1], optional): Survey Dynamix\n            region from where to interact with the mindful API. Defaults to \"eu1\"\n            English (United Kingdom).\n\n    Examples:\n        mindful = Mindful(\n            credentials=credentials,\n            config_key=config_key,\n            region=region,\n        )\n        mindful.api_connection(\n            endpoint=endpoint,\n            date_interval=date_interval,\n            limit=limit,\n        )\n        data_frame = mindful.to_df()\n    \"\"\"\n    credentials = credentials or get_source_credentials(config_key) or None\n    if credentials is None:\n        msg = \"Missing credentials.\"\n        raise CredentialError(msg)\n\n    validated_creds = dict(MindfulCredentials(**credentials))\n    super().__init__(*args, credentials=validated_creds, **kwargs)\n\n    self.auth = (credentials[\"customer_uuid\"], credentials[\"auth_token\"])\n    if region != \"us1\":\n        self.region = f\"{region}.\"\n    else:\n        self.region = \"\"\n</code></pre>"},{"location":"references/sources/api/#viadot.sources.mindful.Mindful.api_connection","title":"<code>api_connection(endpoint='surveys', date_interval=None, limit=1000)</code>","text":"<p>General method to connect to Survey Dynamix API and generate the response.</p> <p>Parameters:</p> Name Type Description Default <code>endpoint</code> <code>Literal['interactions', 'responses', 'surveys']</code> <p>API endpoint for an individual request. Defaults to \"surveys\".</p> <code>'surveys'</code> <code>date_interval</code> <code>Optional[List[date]]</code> <p>Date time range detailing the starting date and the ending date. If no range is passed, one day of data since this moment will be retrieved. Defaults to None.</p> <code>None</code> <code>limit</code> <code>int</code> <p>The number of matching interactions to return. Defaults to 1000.</p> <code>1000</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>Not available endpoint.</p> <code>APIError</code> <p>Failed to download data from the endpoint.</p> Source code in <code>src/viadot/sources/mindful.py</code> <pre><code>def api_connection(\n    self,\n    endpoint: Literal[\"interactions\", \"responses\", \"surveys\"] = \"surveys\",\n    date_interval: list[date] | None = None,\n    limit: int = 1000,\n) -&gt; None:\n    \"\"\"General method to connect to Survey Dynamix API and generate the response.\n\n    Args:\n        endpoint (Literal[\"interactions\", \"responses\", \"surveys\"], optional): API\n            endpoint for an individual request. Defaults to \"surveys\".\n        date_interval (Optional[List[date]], optional): Date time range detailing\n            the starting date and the ending date. If no range is passed, one day of\n            data since this moment will be retrieved. Defaults to None.\n        limit (int, optional): The number of matching interactions to return.\n            Defaults to 1000.\n\n    Raises:\n        ValueError: Not available endpoint.\n        APIError: Failed to download data from the endpoint.\n    \"\"\"\n    if endpoint not in self.ENDPOINTS:\n        raise ValueError(\n            f\"Survey Dynamix endpoint: '{endpoint}',\"\n            + \" is not available through Mindful viadot connector.\"\n        )\n\n    if (\n        date_interval is None\n        or all(list(map(isinstance, date_interval, [date] * len(date_interval))))\n        is False\n    ):\n        reference_date = date.today()\n        date_interval = [reference_date - timedelta(days=1), reference_date]\n\n        self.logger.warning(\n            \"No `date_interval` parameter was defined, or was erroneously \"\n            + \"defined. `date_interval` parameter must have the folloing \"\n            + \"structure:\\n\\t[`date_0`, `date_1`], having that `date_1` &gt; \"\n            + \"`date_0`.\\nBy default, one day of data, from \"\n            + f\"{date_interval[0].strftime('%Y-%m-%d')} to \"\n            + f\"{date_interval[1].strftime('%Y-%m-%d')}, will be obtained.\"\n        )\n\n    params = {\n        \"_limit\": limit,\n        \"start_date\": f\"{date_interval[0]}\",\n        \"end_date\": f\"{date_interval[1]}\",\n    }\n\n    if endpoint == \"surveys\":\n        del params[\"start_date\"]\n        del params[\"end_date\"]\n\n    response = self._mindful_api_response(\n        endpoint=endpoint,\n        params=params,\n    )\n    response_ok = 200\n    no_data_code = 204\n    if response.status_code == response_ok:\n        self.logger.info(\n            f\"Successfully downloaded '{endpoint}' data from mindful API.\"\n        )\n        self.data = StringIO(response.content.decode(\"utf-8\"))\n    elif response.status_code == no_data_code and not response.content.decode():\n        self.logger.warning(\n            f\"There are not '{endpoint}' data to download from\"\n            + f\" {date_interval[0]} to {date_interval[1]}.\"\n        )\n        self.data = json.dumps({})\n    else:\n        self.logger.error(\n            f\"Failed to downloaded '{endpoint}' data. - {response.content}\"\n        )\n        msg = f\"Failed to downloaded '{endpoint}' data.\"\n        raise APIError(msg)\n</code></pre>"},{"location":"references/sources/api/#viadot.sources.mindful.Mindful.to_df","title":"<code>to_df(if_empty='warn')</code>","text":"<p>Download the data to a pandas DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>if_empty</code> <code>str</code> <p>What to do if a fetch produce no data. Defaults to \"warn</p> <code>'warn'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.Dataframe: The response data as a pandas DataFrame plus viadot metadata.</p> Source code in <code>src/viadot/sources/mindful.py</code> <pre><code>@add_viadot_metadata_columns\ndef to_df(\n    self,\n    if_empty: str = \"warn\",\n) -&gt; pd.DataFrame:\n    \"\"\"Download the data to a pandas DataFrame.\n\n    Args:\n        if_empty (str, optional): What to do if a fetch produce no data.\n            Defaults to \"warn\n\n    Returns:\n        pd.Dataframe: The response data as a pandas DataFrame plus viadot metadata.\n    \"\"\"\n    super().to_df(if_empty=if_empty)\n\n    data_frame = pd.read_json(self.data)\n\n    if data_frame.empty:\n        self._handle_if_empty(\n            if_empty=if_empty,\n            message=\"The response does not contain any data.\",\n        )\n    else:\n        self.logger.info(\"Successfully downloaded data from the Mindful API.\")\n\n    return data_frame\n</code></pre>"},{"location":"references/sources/api/#viadot.sources._minio.MinIO","title":"<code>viadot.sources._minio.MinIO</code>","text":"<p>               Bases: <code>Source</code></p> Source code in <code>src/viadot/sources/_minio.py</code> <pre><code>class MinIO(Source):\n    def __init__(\n        self,\n        credentials: MinIOCredentials | None = None,\n        config_key: str | None = None,\n        *args,\n        **kwargs,\n    ):\n        \"\"\"A class for interacting with MinIO.\n\n        Interact with MinIO in a more Pythonic, user-friendly, and robust way than the\n        official minio client.\n\n        Args:\n        credentials (MinIOCredentials): MinIO credentials.\n        config_key (str, optional): The key in the viadot config holding relevant\n            credentials.\n        \"\"\"\n        raw_creds = credentials or get_source_credentials(config_key) or {}\n        validated_creds = MinIOCredentials(**raw_creds).dict(\n            by_alias=True\n        )  # validate the credentials\n\n        super().__init__(*args, credentials=validated_creds, **kwargs)\n\n        self.endpoint = self.credentials.get(\"endpoint\")\n        self.access_key = self.credentials.get(\"access_key\")\n        self.secret_key = self.credentials.get(\"secret_key\")\n        self.bucket = self.credentials.get(\"bucket\")\n        self.secure = self.credentials.get(\"secure\")\n        self.verify = self.credentials.get(\"verify\")\n\n        self.http_scheme = \"https\" if self.secure is True else \"http\"\n\n        self.client = Minio(\n            self.endpoint,\n            access_key=self.access_key,\n            secret_key=self.secret_key,\n            secure=self.secure,\n            http_client=urllib3.PoolManager(\n                timeout=urllib3.Timeout.DEFAULT_TIMEOUT,\n                retries=urllib3.Retry(\n                    connect=1,\n                    read=3,\n                    total=2,\n                    backoff_factor=1,\n                    status_forcelist=[500, 502, 503, 504],\n                ),\n                cert_reqs=\"CERT_REQUIRED\" if self.verify else \"CERT_NONE\",\n            ),\n        )\n\n        self.storage_options = {\n            \"key\": self.access_key,\n            \"secret\": self.secret_key,\n            \"client_kwargs\": {\n                \"endpoint_url\": f\"{self.http_scheme}://\" + self.endpoint,\n                \"use_ssl\": self.secure,\n                \"verify\": self.verify,\n            },\n        }\n\n        self.fs = s3fs.S3FileSystem(\n            key=self.storage_options[\"key\"],\n            secret=self.storage_options[\"secret\"],\n            client_kwargs=self.storage_options[\"client_kwargs\"],\n            anon=False,\n        )\n\n    def from_arrow(\n        self,\n        table: pa.Table,\n        schema_name: str | None = None,\n        table_name: str | None = None,\n        path: str | Path | None = None,\n        basename_template: str | None = None,\n        partition_cols: list[str] | None = None,\n        if_exists: Literal[\"error\", \"delete_matching\", \"overwrite_or_ignore\"] = \"error\",\n    ) -&gt; None:\n        \"\"\"Create a Parquet dataset on MinIO from a PyArrow Table.\n\n        Uses multi-part upload to upload the table in chunks, speeding up the\n        process by using multithreading and avoiding upload size limits.\n\n        Either both `schema_name` and `table_name` or only `path` must be provided.\n\n        `path` allows specifying an arbitrary path, while `schema_name` and `table_name`\n        provide a shortcut for creating a data lakehouse-like structure of\n        `s3://&lt;bucket&gt;/&lt;schema_name&gt;/&lt;table_name&gt;/&lt;table_name&gt;.parquet`.\n\n        For more information on partitioning, see\n        https://arrow.apache.org/docs/python/generated/pyarrow.parquet.write_to_dataset.html#pyarrow-parquet-write-to-dataset\n\n        Args:\n            table (pa.Table): The table to upload.\n            schema_name (str, optional): The name of the schema directory. Defaults to\n                None.\n            table_name (str, optional): The name of the table directory. Defaults to\n                None.\n            path (str | Path, optional): The path to the destination file. Defaults to\n                None.\n            basename_template (str, optional): A template string used to generate\n                basenames of written data files. The token '{i}' will be replaced with\n                an automatically incremented integer. Defaults to None.\n            partition_cols (list[str], optional): The columns to partition by. Defaults\n                to None.\n            if_exists (Literal[\"error\", \"delete_matching\", \"overwrite_or_ignore\"],\n                optional): What to do if the dataset already exists.\n        \"\"\"\n        fqn_or_path = (schema_name and table_name) or (\n            path and not (schema_name or table_name)\n        )\n        if not fqn_or_path:\n            msg = \"Either both `schema_name` and `table_name` or only `path` must be provided.\"\n            raise ValueError(msg)\n\n        # We need to create the dirs here as PyArrow also tries to create the bucket,\n        # which shouldn't be allowed for whomever is executing this code.\n        self.logger.debug(f\"Creating directory for table {table_name}...\")\n        path = path or f\"{schema_name}/{table_name}\"\n        self.fs.makedirs(path, exist_ok=True)\n        self.logger.debug(\"Directory has been created successfully.\")\n\n        # Write the data.\n        pq.write_to_dataset(\n            table,\n            root_path=path,\n            partition_cols=partition_cols,\n            existing_data_behavior=if_exists,\n            basename_template=basename_template,\n            filesystem=self.fs,\n            max_rows_per_file=1024 * 1024,\n            create_dir=False,  # Required as Arrow attempts to create the bucket, too.\n        )\n\n    def from_df(\n        self,\n        df: pd.DataFrame,\n        schema_name: str | None = None,\n        table_name: str | None = None,\n        path: str | Path | None = None,\n        basename_template: str | None = None,\n        partition_cols: list[str] | None = None,\n        if_exists: Literal[\"error\", \"delete_matching\", \"overwrite_or_ignore\"] = \"error\",\n    ) -&gt; None:\n        \"\"\"Create a Parquet dataset on MinIO from a PyArrow Table.\n\n        Uses multi-part upload to upload the table in chunks, speeding up the\n        process by using multithreading and avoiding upload size limits.\n\n        Either both `schema_name` and `table_name` or only `path` must be provided.\n\n        `path` allows specifying an arbitrary path, while `schema_name` and `table_name`\n        provide a shortcut for creating a data lakehouse-like structure of\n        `s3://&lt;bucket&gt;/&lt;schema_name&gt;/&lt;table_name&gt;/&lt;table_name&gt;.parquet`.\n\n        For more information on partitioning, see\n        https://arrow.apache.org/docs/python/generated/pyarrow.parquet.write_to_dataset.html#pyarrow-parquet-write-to-dataset\n\n        Args:\n            df (pd.DataFrame): The DataFrame to upload.\n            schema_name (str, optional): The name of the schema directory. Defaults to\n                None.\n            table_name (str, optional): The name of the table directory. Defaults to\n                None.\n            path (str | Path, optional): The path to the destination file. Defaults to\n                None.\n            basename_template (str, optional): A template string used to generate\n                basenames of written data files. The token '{i}' will be replaced with\n                an automatically incremented integer. Defaults to None.\n            partition_cols (list[str], optional): The columns to partition by. Defaults\n                to None.\n            if_exists (Literal[\"error\", \"delete_matching\", \"overwrite_or_ignore\"],\n                optional): What to do if the dataset already exists.\n        \"\"\"\n        table = pa.Table.from_pandas(df)\n\n        return self.from_arrow(\n            table=table,\n            schema_name=schema_name,\n            table_name=table_name,\n            path=path,\n            basename_template=basename_template,\n            partition_cols=partition_cols,\n            if_exists=if_exists,\n        )\n\n    def ls(self, path: str) -&gt; Generator[str, None, None]:\n        \"\"\"List files and directories under `path`.\n\n        List operation can be slow if there are a lot of objects, hence using a\n        generator.\n\n        Args:\n            path (str): The path which contents should be listed.\n\n        Yields:\n            Generator[str, None, None]: Contents (files and directories) of `path`.\n        \"\"\"\n        for obj in self.client.list_objects(self.bucket, path):\n            yield obj.object_name\n\n    def rm(self, path: str, recursive: bool = False) -&gt; None:\n        \"\"\"Remove a file or directory from MinIO.\n\n        Args:\n            path (str): The path to the file to remove.\n        \"\"\"\n        if recursive:\n            bucket = path.split(\"/\")[2]\n            prefix = \"/\".join(path.split(\"/\")[3:])\n            objects = self.client.list_objects(bucket, prefix=prefix, recursive=True)\n            for obj in objects:\n                self.client.remove_object(bucket, obj.object_name)\n            return\n\n        self.client.remove_object(self.bucket, path)\n\n    def _check_if_file_exists(self, path: str) -&gt; bool:\n        try:\n            self.client.stat_object(self.bucket, path)\n        except S3Error as e:\n            if \"Object does not exist\" in e.message:\n                return False\n            raise\n        else:\n            return True\n\n    def check_connection(self) -&gt; None:\n        \"\"\"Verify connectivity to the MinIO endpoint.\"\"\"\n        try:\n            self.client.bucket_exists(self.bucket)\n        except NewConnectionError as e:\n            msg = f\"Connection to MinIO endpoint '{self.endpoint}' failed with error: \\n{e}\"\n            msg += \"Please check your credentials and try again.\"\n\n            raise ValueError(msg) from e\n        except Exception as e:\n            msg = f\"Connection to MinIO endpoint '{self.endpoint}' failed with error: \\n{e}\"\n            raise ValueError(msg) from e\n        self.logger.info(\"Connection successful!\")\n</code></pre>"},{"location":"references/sources/api/#viadot.sources._minio.MinIO.__init__","title":"<code>__init__(credentials=None, config_key=None, *args, **kwargs)</code>","text":"<p>A class for interacting with MinIO.</p> <p>Interact with MinIO in a more Pythonic, user-friendly, and robust way than the official minio client.</p> <p>Args: credentials (MinIOCredentials): MinIO credentials. config_key (str, optional): The key in the viadot config holding relevant     credentials.</p> Source code in <code>src/viadot/sources/_minio.py</code> <pre><code>def __init__(\n    self,\n    credentials: MinIOCredentials | None = None,\n    config_key: str | None = None,\n    *args,\n    **kwargs,\n):\n    \"\"\"A class for interacting with MinIO.\n\n    Interact with MinIO in a more Pythonic, user-friendly, and robust way than the\n    official minio client.\n\n    Args:\n    credentials (MinIOCredentials): MinIO credentials.\n    config_key (str, optional): The key in the viadot config holding relevant\n        credentials.\n    \"\"\"\n    raw_creds = credentials or get_source_credentials(config_key) or {}\n    validated_creds = MinIOCredentials(**raw_creds).dict(\n        by_alias=True\n    )  # validate the credentials\n\n    super().__init__(*args, credentials=validated_creds, **kwargs)\n\n    self.endpoint = self.credentials.get(\"endpoint\")\n    self.access_key = self.credentials.get(\"access_key\")\n    self.secret_key = self.credentials.get(\"secret_key\")\n    self.bucket = self.credentials.get(\"bucket\")\n    self.secure = self.credentials.get(\"secure\")\n    self.verify = self.credentials.get(\"verify\")\n\n    self.http_scheme = \"https\" if self.secure is True else \"http\"\n\n    self.client = Minio(\n        self.endpoint,\n        access_key=self.access_key,\n        secret_key=self.secret_key,\n        secure=self.secure,\n        http_client=urllib3.PoolManager(\n            timeout=urllib3.Timeout.DEFAULT_TIMEOUT,\n            retries=urllib3.Retry(\n                connect=1,\n                read=3,\n                total=2,\n                backoff_factor=1,\n                status_forcelist=[500, 502, 503, 504],\n            ),\n            cert_reqs=\"CERT_REQUIRED\" if self.verify else \"CERT_NONE\",\n        ),\n    )\n\n    self.storage_options = {\n        \"key\": self.access_key,\n        \"secret\": self.secret_key,\n        \"client_kwargs\": {\n            \"endpoint_url\": f\"{self.http_scheme}://\" + self.endpoint,\n            \"use_ssl\": self.secure,\n            \"verify\": self.verify,\n        },\n    }\n\n    self.fs = s3fs.S3FileSystem(\n        key=self.storage_options[\"key\"],\n        secret=self.storage_options[\"secret\"],\n        client_kwargs=self.storage_options[\"client_kwargs\"],\n        anon=False,\n    )\n</code></pre>"},{"location":"references/sources/api/#viadot.sources._minio.MinIO.check_connection","title":"<code>check_connection()</code>","text":"<p>Verify connectivity to the MinIO endpoint.</p> Source code in <code>src/viadot/sources/_minio.py</code> <pre><code>def check_connection(self) -&gt; None:\n    \"\"\"Verify connectivity to the MinIO endpoint.\"\"\"\n    try:\n        self.client.bucket_exists(self.bucket)\n    except NewConnectionError as e:\n        msg = f\"Connection to MinIO endpoint '{self.endpoint}' failed with error: \\n{e}\"\n        msg += \"Please check your credentials and try again.\"\n\n        raise ValueError(msg) from e\n    except Exception as e:\n        msg = f\"Connection to MinIO endpoint '{self.endpoint}' failed with error: \\n{e}\"\n        raise ValueError(msg) from e\n    self.logger.info(\"Connection successful!\")\n</code></pre>"},{"location":"references/sources/api/#viadot.sources._minio.MinIO.from_arrow","title":"<code>from_arrow(table, schema_name=None, table_name=None, path=None, basename_template=None, partition_cols=None, if_exists='error')</code>","text":"<p>Create a Parquet dataset on MinIO from a PyArrow Table.</p> <p>Uses multi-part upload to upload the table in chunks, speeding up the process by using multithreading and avoiding upload size limits.</p> <p>Either both <code>schema_name</code> and <code>table_name</code> or only <code>path</code> must be provided.</p> <p><code>path</code> allows specifying an arbitrary path, while <code>schema_name</code> and <code>table_name</code> provide a shortcut for creating a data lakehouse-like structure of <code>s3://&lt;bucket&gt;/&lt;schema_name&gt;/&lt;table_name&gt;/&lt;table_name&gt;.parquet</code>.</p> <p>For more information on partitioning, see https://arrow.apache.org/docs/python/generated/pyarrow.parquet.write_to_dataset.html#pyarrow-parquet-write-to-dataset</p> <p>Parameters:</p> Name Type Description Default <code>table</code> <code>Table</code> <p>The table to upload.</p> required <code>schema_name</code> <code>str</code> <p>The name of the schema directory. Defaults to None.</p> <code>None</code> <code>table_name</code> <code>str</code> <p>The name of the table directory. Defaults to None.</p> <code>None</code> <code>path</code> <code>str | Path</code> <p>The path to the destination file. Defaults to None.</p> <code>None</code> <code>basename_template</code> <code>str</code> <p>A template string used to generate basenames of written data files. The token '{i}' will be replaced with an automatically incremented integer. Defaults to None.</p> <code>None</code> <code>partition_cols</code> <code>list[str]</code> <p>The columns to partition by. Defaults to None.</p> <code>None</code> Source code in <code>src/viadot/sources/_minio.py</code> <pre><code>def from_arrow(\n    self,\n    table: pa.Table,\n    schema_name: str | None = None,\n    table_name: str | None = None,\n    path: str | Path | None = None,\n    basename_template: str | None = None,\n    partition_cols: list[str] | None = None,\n    if_exists: Literal[\"error\", \"delete_matching\", \"overwrite_or_ignore\"] = \"error\",\n) -&gt; None:\n    \"\"\"Create a Parquet dataset on MinIO from a PyArrow Table.\n\n    Uses multi-part upload to upload the table in chunks, speeding up the\n    process by using multithreading and avoiding upload size limits.\n\n    Either both `schema_name` and `table_name` or only `path` must be provided.\n\n    `path` allows specifying an arbitrary path, while `schema_name` and `table_name`\n    provide a shortcut for creating a data lakehouse-like structure of\n    `s3://&lt;bucket&gt;/&lt;schema_name&gt;/&lt;table_name&gt;/&lt;table_name&gt;.parquet`.\n\n    For more information on partitioning, see\n    https://arrow.apache.org/docs/python/generated/pyarrow.parquet.write_to_dataset.html#pyarrow-parquet-write-to-dataset\n\n    Args:\n        table (pa.Table): The table to upload.\n        schema_name (str, optional): The name of the schema directory. Defaults to\n            None.\n        table_name (str, optional): The name of the table directory. Defaults to\n            None.\n        path (str | Path, optional): The path to the destination file. Defaults to\n            None.\n        basename_template (str, optional): A template string used to generate\n            basenames of written data files. The token '{i}' will be replaced with\n            an automatically incremented integer. Defaults to None.\n        partition_cols (list[str], optional): The columns to partition by. Defaults\n            to None.\n        if_exists (Literal[\"error\", \"delete_matching\", \"overwrite_or_ignore\"],\n            optional): What to do if the dataset already exists.\n    \"\"\"\n    fqn_or_path = (schema_name and table_name) or (\n        path and not (schema_name or table_name)\n    )\n    if not fqn_or_path:\n        msg = \"Either both `schema_name` and `table_name` or only `path` must be provided.\"\n        raise ValueError(msg)\n\n    # We need to create the dirs here as PyArrow also tries to create the bucket,\n    # which shouldn't be allowed for whomever is executing this code.\n    self.logger.debug(f\"Creating directory for table {table_name}...\")\n    path = path or f\"{schema_name}/{table_name}\"\n    self.fs.makedirs(path, exist_ok=True)\n    self.logger.debug(\"Directory has been created successfully.\")\n\n    # Write the data.\n    pq.write_to_dataset(\n        table,\n        root_path=path,\n        partition_cols=partition_cols,\n        existing_data_behavior=if_exists,\n        basename_template=basename_template,\n        filesystem=self.fs,\n        max_rows_per_file=1024 * 1024,\n        create_dir=False,  # Required as Arrow attempts to create the bucket, too.\n    )\n</code></pre>"},{"location":"references/sources/api/#viadot.sources._minio.MinIO.from_df","title":"<code>from_df(df, schema_name=None, table_name=None, path=None, basename_template=None, partition_cols=None, if_exists='error')</code>","text":"<p>Create a Parquet dataset on MinIO from a PyArrow Table.</p> <p>Uses multi-part upload to upload the table in chunks, speeding up the process by using multithreading and avoiding upload size limits.</p> <p>Either both <code>schema_name</code> and <code>table_name</code> or only <code>path</code> must be provided.</p> <p><code>path</code> allows specifying an arbitrary path, while <code>schema_name</code> and <code>table_name</code> provide a shortcut for creating a data lakehouse-like structure of <code>s3://&lt;bucket&gt;/&lt;schema_name&gt;/&lt;table_name&gt;/&lt;table_name&gt;.parquet</code>.</p> <p>For more information on partitioning, see https://arrow.apache.org/docs/python/generated/pyarrow.parquet.write_to_dataset.html#pyarrow-parquet-write-to-dataset</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The DataFrame to upload.</p> required <code>schema_name</code> <code>str</code> <p>The name of the schema directory. Defaults to None.</p> <code>None</code> <code>table_name</code> <code>str</code> <p>The name of the table directory. Defaults to None.</p> <code>None</code> <code>path</code> <code>str | Path</code> <p>The path to the destination file. Defaults to None.</p> <code>None</code> <code>basename_template</code> <code>str</code> <p>A template string used to generate basenames of written data files. The token '{i}' will be replaced with an automatically incremented integer. Defaults to None.</p> <code>None</code> <code>partition_cols</code> <code>list[str]</code> <p>The columns to partition by. Defaults to None.</p> <code>None</code> Source code in <code>src/viadot/sources/_minio.py</code> <pre><code>def from_df(\n    self,\n    df: pd.DataFrame,\n    schema_name: str | None = None,\n    table_name: str | None = None,\n    path: str | Path | None = None,\n    basename_template: str | None = None,\n    partition_cols: list[str] | None = None,\n    if_exists: Literal[\"error\", \"delete_matching\", \"overwrite_or_ignore\"] = \"error\",\n) -&gt; None:\n    \"\"\"Create a Parquet dataset on MinIO from a PyArrow Table.\n\n    Uses multi-part upload to upload the table in chunks, speeding up the\n    process by using multithreading and avoiding upload size limits.\n\n    Either both `schema_name` and `table_name` or only `path` must be provided.\n\n    `path` allows specifying an arbitrary path, while `schema_name` and `table_name`\n    provide a shortcut for creating a data lakehouse-like structure of\n    `s3://&lt;bucket&gt;/&lt;schema_name&gt;/&lt;table_name&gt;/&lt;table_name&gt;.parquet`.\n\n    For more information on partitioning, see\n    https://arrow.apache.org/docs/python/generated/pyarrow.parquet.write_to_dataset.html#pyarrow-parquet-write-to-dataset\n\n    Args:\n        df (pd.DataFrame): The DataFrame to upload.\n        schema_name (str, optional): The name of the schema directory. Defaults to\n            None.\n        table_name (str, optional): The name of the table directory. Defaults to\n            None.\n        path (str | Path, optional): The path to the destination file. Defaults to\n            None.\n        basename_template (str, optional): A template string used to generate\n            basenames of written data files. The token '{i}' will be replaced with\n            an automatically incremented integer. Defaults to None.\n        partition_cols (list[str], optional): The columns to partition by. Defaults\n            to None.\n        if_exists (Literal[\"error\", \"delete_matching\", \"overwrite_or_ignore\"],\n            optional): What to do if the dataset already exists.\n    \"\"\"\n    table = pa.Table.from_pandas(df)\n\n    return self.from_arrow(\n        table=table,\n        schema_name=schema_name,\n        table_name=table_name,\n        path=path,\n        basename_template=basename_template,\n        partition_cols=partition_cols,\n        if_exists=if_exists,\n    )\n</code></pre>"},{"location":"references/sources/api/#viadot.sources._minio.MinIO.ls","title":"<code>ls(path)</code>","text":"<p>List files and directories under <code>path</code>.</p> <p>List operation can be slow if there are a lot of objects, hence using a generator.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path which contents should be listed.</p> required <p>Yields:</p> Type Description <code>str</code> <p>Generator[str, None, None]: Contents (files and directories) of <code>path</code>.</p> Source code in <code>src/viadot/sources/_minio.py</code> <pre><code>def ls(self, path: str) -&gt; Generator[str, None, None]:\n    \"\"\"List files and directories under `path`.\n\n    List operation can be slow if there are a lot of objects, hence using a\n    generator.\n\n    Args:\n        path (str): The path which contents should be listed.\n\n    Yields:\n        Generator[str, None, None]: Contents (files and directories) of `path`.\n    \"\"\"\n    for obj in self.client.list_objects(self.bucket, path):\n        yield obj.object_name\n</code></pre>"},{"location":"references/sources/api/#viadot.sources._minio.MinIO.rm","title":"<code>rm(path, recursive=False)</code>","text":"<p>Remove a file or directory from MinIO.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the file to remove.</p> required Source code in <code>src/viadot/sources/_minio.py</code> <pre><code>def rm(self, path: str, recursive: bool = False) -&gt; None:\n    \"\"\"Remove a file or directory from MinIO.\n\n    Args:\n        path (str): The path to the file to remove.\n    \"\"\"\n    if recursive:\n        bucket = path.split(\"/\")[2]\n        prefix = \"/\".join(path.split(\"/\")[3:])\n        objects = self.client.list_objects(bucket, prefix=prefix, recursive=True)\n        for obj in objects:\n            self.client.remove_object(bucket, obj.object_name)\n        return\n\n    self.client.remove_object(self.bucket, path)\n</code></pre>"},{"location":"references/sources/api/#viadot.sources.outlook.Outlook","title":"<code>viadot.sources.outlook.Outlook</code>","text":"<p>               Bases: <code>Source</code></p> <p>Class implementing the Outlook API.</p> Documentation for this API is available at <p>https://o365.github.io/python-o365/latest/getting_started.html.</p> Source code in <code>src/viadot/sources/outlook.py</code> <pre><code>class Outlook(Source):\n    \"\"\"Class implementing the Outlook API.\n\n    Documentation for this API is available at:\n        https://o365.github.io/python-o365/latest/getting_started.html.\n    \"\"\"\n\n    UTC = timezone.utc\n\n    def __init__(\n        self,\n        *args: list[Any],\n        credentials: dict[str, Any] | None = None,\n        config_key: str = \"outlook\",\n        **kwargs: dict[str, Any],\n    ):\n        \"\"\"Outlook connector build for fetching Outlook API source.\n\n        Data are fetched from start to end date range. If start or end date are not\n        provided then flow fetched data from yesterday by default.\n\n        Args:\n            credentials (Optional[OutlookCredentials], optional): Outlook credentials.\n                Defaults to None\n            config_key (str, optional): The key in the viadot config holding relevant\n                credentials. Defaults to \"outlook\".\n\n        Examples:\n            outlook = Outlook(\n                config_key=config_key,\n            )\n            outlook.api_connection(\n                mailbox_name=mailbox_name,\n                request_retries=request_retries,\n                start_date=start_date,\n                end_date=end_date,\n                limit=limit,\n                address_limit=address_limit,\n                outbox_list=outbox_list,\n            )\n            data_frame = outlook.to_df()\n\n        Raises:\n            CredentialError: If credentials are not provided in local_config or\n                directly as a parameter.\n        \"\"\"\n        raw_creds = credentials or get_source_credentials(config_key)\n        validated_creds = dict(OutlookCredentials(**raw_creds))\n\n        super().__init__(*args, credentials=validated_creds, **kwargs)\n\n    @staticmethod\n    def _get_subfolders(\n        folder_structure: dict,\n        folder: MailBox,\n        key_concat: str = \"\",\n    ) -&gt; dict[str, list] | None:\n        \"\"\"Retrieve all the subfolder in a MailBox folder.\n\n        Args:\n            folder_structure (dict): Dictionary where to save the data.\n            folder (MailBox): The MailBox folder from where to extract the subfolders.\n            key_concat (str, optional) Previous Mailbox folder structure to add to\n                the actual subfolder. Defaults to \"\".\n\n        Returns:\n            Dict[str, List]: `folder_structure` dictionary is returned once\n                it is updated.\n        \"\"\"\n        if key_concat:\n            tmp_key = key_concat.split(\"|\")\n            key_concat = key_concat.replace(f\"|{tmp_key[-1]}\", \"\")\n\n        for subfolder in folder.get_folders():\n            if subfolder:\n                folder_structure.update(\n                    {\n                        \"|\".join([key_concat, folder.name, subfolder.name]).lstrip(\n                            \"|\"\n                        ): subfolder\n                    }\n                )\n\n        if folder_structure:\n            return folder_structure\n\n        return None\n\n    def _get_all_folders(\n        self,\n        mailbox: MailBox,\n    ) -&gt; dict:\n        \"\"\"To retrieve all folders from a Mailbox object.\n\n        Args:\n            mailbox (MailBox): Outlook Mailbox object from where to extract all\n                folder structure.\n\n        Returns:\n            dict: Every single folder and subfolder is returned as\n                \"parent (sub)folder|(sub)folder\": Mailbox.\n        \"\"\"\n        dict_folders = self._get_subfolders({}, mailbox)\n        final_dict_folders = dict_folders.copy()\n\n        # Get all subfolders.\n        while_dict_folders = {\"key\": \"value\"}\n        while len(while_dict_folders) != 0:\n            while_dict_folders = {}\n            for key, value in list(dict_folders.items()):\n                tmp_dict_folders = self._get_subfolders({}, value, key_concat=key)\n                if tmp_dict_folders:\n                    final_dict_folders.update(tmp_dict_folders)\n                    while_dict_folders.update(tmp_dict_folders)\n\n            dict_folders = while_dict_folders.copy()\n\n        return final_dict_folders\n\n    # TODO: should be refactored.\n    def _get_messages_from_mailbox(  # noqa: C901, PLR0912\n        self,\n        mailbox_name: str,\n        dict_folder: dict,\n        date_range_start_time: datetime,\n        date_range_end_time: datetime,\n        limit: int = 10000,\n        address_limit: int = 8000,\n        outbox_list: list[str] | None = None,\n    ) -&gt; list:\n        \"\"\"To retrieve all messages from all the mailboxes passed in the dictionary.\n\n        Args:\n            mailbox_name (str): Mailbox name.\n            dict_folder (dict): Mailboxes dictionary holder, with the following\n                structure: \"parent (sub)folder|(sub)folder\": Mailbox.\n            date_range_start_time (datetime): Start date from where to stract data.\n            date_range_end_time (datetime): End data up to where to stract data.\n            limit (int, optional): Number of fetched top messages. Defaults to 10000.\n            address_limit (int, optional): The maximum number of accepted characters in\n                the sum of all email names. Defaults to 8000.\n            outbox_list (List[str], optional): List of outbox folders to differenciate\n                between Inboxes and Outboxes. Defaults to [\"Sent Items\"].\n\n        Returns:\n            list: A list with all messages from all Mailboxes.\n        \"\"\"\n        if not outbox_list:\n            outbox_list = [\"Sent Items\"]\n\n        data = []\n        for key, value in list(dict_folder.items()):\n            count = 0\n            for message in value.get_messages(limit=limit):\n                received_time = message.received\n                date_obj = datetime.fromisoformat(str(received_time))\n                if (\n                    date_range_start_time.replace(tzinfo=self.UTC)\n                    &lt; date_obj\n                    &lt; date_range_end_time.replace(tzinfo=self.UTC)\n                ):\n                    count += 1\n                    fetched = message.to_api_data()\n                    sender_mail = fetched.get(\"from\", None)\n                    if sender_mail is not None:\n                        sender_mail = fetched[\"from\"][\"emailAddress\"][\"address\"]\n                    recivers_list = fetched.get(\"toRecipients\")\n                    recivers = \" \"\n\n                    if recivers_list is not None:\n                        for reciver in recivers_list:\n                            add_string = f\", {reciver['emailAddress']['address']}\"\n                            if (\n                                sum(list(map(len, [recivers, add_string])))\n                                &gt;= address_limit\n                            ):\n                                break\n\n                            recivers += add_string\n\n                    categories = \" \"\n                    if message.categories is not None:\n                        categories = \", \".join(\n                            categories for categories in message.categories\n                        )\n\n                    conversation_index = \" \"\n                    if message.conversation_index is not None:\n                        conversation_index = message.conversation_index\n\n                    if isinstance(message.subject, str):\n                        subject = message.subject.replace(\"\\t\", \" \")\n                    else:\n                        subject = message.subject\n\n                    row = {\n                        \"(sub)folder\": value.name,\n                        \"conversation ID\": fetched.get(\"conversationId\"),\n                        \"conversation index\": conversation_index,\n                        \"categories\": categories,\n                        \"sender\": sender_mail,\n                        \"subject\": subject,\n                        \"recivers\": recivers.strip(\", \"),\n                        \"received_time\": fetched.get(\"receivedDateTime\"),\n                        \"mail_adress\": mailbox_name.split(\"@\")[0]\n                        .replace(\".\", \"_\")\n                        .replace(\"-\", \"_\"),\n                    }\n                    if any(x.lower() in key.lower() for x in outbox_list):\n                        row[\"Inbox\"] = False\n                    else:\n                        row[\"Inbox\"] = True\n\n                    data.append(row)\n\n            if count &gt; 0:\n                self.logger.info(f\"folder: {key.ljust(76, '-')}  messages: {count}\")\n\n        return data\n\n    def api_connection(\n        self,\n        mailbox_name: str | None = None,\n        request_retries: int = 10,\n        start_date: str | None = None,\n        end_date: str | None = None,\n        limit: int = 10000,\n        address_limit: int = 8000,\n        outbox_list: list[str] | None = None,\n    ) -&gt; pd.DataFrame:\n        \"\"\"Download all the messages stored in a MailBox folder and subfolders.\n\n        Args:\n            mailbox_name (Optional[str], optional): Mailbox name. Defaults to None.\n            request_retries (int, optional): How many times retries to authorizate.\n                Defaults to 10.\n            start_date (Optional[str], optional): A filtering start date parameter e.g.\n                \"2022-01-01\". Defaults to None.\n            end_date (Optional[str], optional): A filtering end date parameter e.g.\n                \"2022-01-02\". Defaults to None.\n            limit (int, optional): Number of fetched top messages. Defaults to 10000.\n            address_limit (int, optional): The maximum number of accepted characters in\n                the sum of all email names. Defaults to 8000.\n            outbox_list (List[str], optional): List of outbox folders to differentiate\n                between Inboxes and Outboxes. Defaults to [\"Sent Items\"].\n\n        Returns:\n            pd.DataFrame: All messages are stored in a pandas framework.\n        \"\"\"\n        if not outbox_list:\n            outbox_list = [\"Sent Items\"]\n        account = Account(\n            (self.credentials[\"client_id\"], self.credentials[\"client_secret\"]),\n            auth_flow_type=\"credentials\",\n            tenant_id=self.credentials[\"tenant_id\"],\n            main_resource=mailbox_name,\n            request_retries=request_retries,\n        )\n\n        if account.authenticate():\n            self.logger.info(f\"{mailbox_name} Authenticated!\")\n        else:\n            msg = \"Failed to authenticate.\"\n            raise ValueError(msg)\n\n        mailbox_obj = account.mailbox()\n\n        if start_date is not None and end_date is not None:\n            date_range_end_time = datetime.strptime(end_date, \"%Y-%m-%d\")\n            date_range_start_time = datetime.strptime(start_date, \"%Y-%m-%d\")\n        else:\n            date_range_start_time = date.today() - timedelta(days=1)\n            date_range_end_time = date.today()\n\n            min_time = datetime.min.time()\n            date_range_end_time = datetime.combine(date_range_end_time, min_time)\n            date_range_start_time = datetime.combine(date_range_start_time, min_time)\n\n        final_dict_folders = self._get_all_folders(mailbox_obj)\n\n        self.data = self._get_messages_from_mailbox(\n            mailbox_name=mailbox_name,\n            dict_folder=final_dict_folders,\n            date_range_start_time=date_range_start_time,\n            date_range_end_time=date_range_end_time,\n            limit=limit,\n            address_limit=address_limit,\n            outbox_list=outbox_list,\n        )\n\n    @add_viadot_metadata_columns\n    def to_df(\n        self,\n        if_empty: str = \"warn\",\n    ) -&gt; pd.DataFrame:\n        \"\"\"Generate a pandas DataFrame with the data.\n\n        Args:\n            if_empty (str, optional): What to do if a fetch produce no data.\n                Defaults to \"warn\n\n        Returns:\n            pd.Dataframe: The response data as a pandas DataFrame plus viadot metadata.\n        \"\"\"\n        super().to_df(if_empty=if_empty)\n\n        data_frame = pd.DataFrame(self.data)\n\n        if data_frame.empty:\n            self._handle_if_empty(\n                if_empty=\"warn\",\n                message=\"No data was got from the Mail Box for those days\",\n            )\n        else:\n            self.logger.info(\"Successfully downloaded data from the Mindful API.\")\n\n        return data_frame\n</code></pre>"},{"location":"references/sources/api/#viadot.sources.outlook.Outlook.__init__","title":"<code>__init__(*args, credentials=None, config_key='outlook', **kwargs)</code>","text":"<p>Outlook connector build for fetching Outlook API source.</p> <p>Data are fetched from start to end date range. If start or end date are not provided then flow fetched data from yesterday by default.</p> <p>Parameters:</p> Name Type Description Default <code>credentials</code> <code>Optional[OutlookCredentials]</code> <p>Outlook credentials. Defaults to None</p> <code>None</code> <code>config_key</code> <code>str</code> <p>The key in the viadot config holding relevant credentials. Defaults to \"outlook\".</p> <code>'outlook'</code> <p>Examples:</p> <p>outlook = Outlook(     config_key=config_key, ) outlook.api_connection(     mailbox_name=mailbox_name,     request_retries=request_retries,     start_date=start_date,     end_date=end_date,     limit=limit,     address_limit=address_limit,     outbox_list=outbox_list, ) data_frame = outlook.to_df()</p> <p>Raises:</p> Type Description <code>CredentialError</code> <p>If credentials are not provided in local_config or directly as a parameter.</p> Source code in <code>src/viadot/sources/outlook.py</code> <pre><code>def __init__(\n    self,\n    *args: list[Any],\n    credentials: dict[str, Any] | None = None,\n    config_key: str = \"outlook\",\n    **kwargs: dict[str, Any],\n):\n    \"\"\"Outlook connector build for fetching Outlook API source.\n\n    Data are fetched from start to end date range. If start or end date are not\n    provided then flow fetched data from yesterday by default.\n\n    Args:\n        credentials (Optional[OutlookCredentials], optional): Outlook credentials.\n            Defaults to None\n        config_key (str, optional): The key in the viadot config holding relevant\n            credentials. Defaults to \"outlook\".\n\n    Examples:\n        outlook = Outlook(\n            config_key=config_key,\n        )\n        outlook.api_connection(\n            mailbox_name=mailbox_name,\n            request_retries=request_retries,\n            start_date=start_date,\n            end_date=end_date,\n            limit=limit,\n            address_limit=address_limit,\n            outbox_list=outbox_list,\n        )\n        data_frame = outlook.to_df()\n\n    Raises:\n        CredentialError: If credentials are not provided in local_config or\n            directly as a parameter.\n    \"\"\"\n    raw_creds = credentials or get_source_credentials(config_key)\n    validated_creds = dict(OutlookCredentials(**raw_creds))\n\n    super().__init__(*args, credentials=validated_creds, **kwargs)\n</code></pre>"},{"location":"references/sources/api/#viadot.sources.outlook.Outlook.api_connection","title":"<code>api_connection(mailbox_name=None, request_retries=10, start_date=None, end_date=None, limit=10000, address_limit=8000, outbox_list=None)</code>","text":"<p>Download all the messages stored in a MailBox folder and subfolders.</p> <p>Parameters:</p> Name Type Description Default <code>mailbox_name</code> <code>Optional[str]</code> <p>Mailbox name. Defaults to None.</p> <code>None</code> <code>request_retries</code> <code>int</code> <p>How many times retries to authorizate. Defaults to 10.</p> <code>10</code> <code>start_date</code> <code>Optional[str]</code> <p>A filtering start date parameter e.g. \"2022-01-01\". Defaults to None.</p> <code>None</code> <code>end_date</code> <code>Optional[str]</code> <p>A filtering end date parameter e.g. \"2022-01-02\". Defaults to None.</p> <code>None</code> <code>limit</code> <code>int</code> <p>Number of fetched top messages. Defaults to 10000.</p> <code>10000</code> <code>address_limit</code> <code>int</code> <p>The maximum number of accepted characters in the sum of all email names. Defaults to 8000.</p> <code>8000</code> <code>outbox_list</code> <code>List[str]</code> <p>List of outbox folders to differentiate between Inboxes and Outboxes. Defaults to [\"Sent Items\"].</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: All messages are stored in a pandas framework.</p> Source code in <code>src/viadot/sources/outlook.py</code> <pre><code>def api_connection(\n    self,\n    mailbox_name: str | None = None,\n    request_retries: int = 10,\n    start_date: str | None = None,\n    end_date: str | None = None,\n    limit: int = 10000,\n    address_limit: int = 8000,\n    outbox_list: list[str] | None = None,\n) -&gt; pd.DataFrame:\n    \"\"\"Download all the messages stored in a MailBox folder and subfolders.\n\n    Args:\n        mailbox_name (Optional[str], optional): Mailbox name. Defaults to None.\n        request_retries (int, optional): How many times retries to authorizate.\n            Defaults to 10.\n        start_date (Optional[str], optional): A filtering start date parameter e.g.\n            \"2022-01-01\". Defaults to None.\n        end_date (Optional[str], optional): A filtering end date parameter e.g.\n            \"2022-01-02\". Defaults to None.\n        limit (int, optional): Number of fetched top messages. Defaults to 10000.\n        address_limit (int, optional): The maximum number of accepted characters in\n            the sum of all email names. Defaults to 8000.\n        outbox_list (List[str], optional): List of outbox folders to differentiate\n            between Inboxes and Outboxes. Defaults to [\"Sent Items\"].\n\n    Returns:\n        pd.DataFrame: All messages are stored in a pandas framework.\n    \"\"\"\n    if not outbox_list:\n        outbox_list = [\"Sent Items\"]\n    account = Account(\n        (self.credentials[\"client_id\"], self.credentials[\"client_secret\"]),\n        auth_flow_type=\"credentials\",\n        tenant_id=self.credentials[\"tenant_id\"],\n        main_resource=mailbox_name,\n        request_retries=request_retries,\n    )\n\n    if account.authenticate():\n        self.logger.info(f\"{mailbox_name} Authenticated!\")\n    else:\n        msg = \"Failed to authenticate.\"\n        raise ValueError(msg)\n\n    mailbox_obj = account.mailbox()\n\n    if start_date is not None and end_date is not None:\n        date_range_end_time = datetime.strptime(end_date, \"%Y-%m-%d\")\n        date_range_start_time = datetime.strptime(start_date, \"%Y-%m-%d\")\n    else:\n        date_range_start_time = date.today() - timedelta(days=1)\n        date_range_end_time = date.today()\n\n        min_time = datetime.min.time()\n        date_range_end_time = datetime.combine(date_range_end_time, min_time)\n        date_range_start_time = datetime.combine(date_range_start_time, min_time)\n\n    final_dict_folders = self._get_all_folders(mailbox_obj)\n\n    self.data = self._get_messages_from_mailbox(\n        mailbox_name=mailbox_name,\n        dict_folder=final_dict_folders,\n        date_range_start_time=date_range_start_time,\n        date_range_end_time=date_range_end_time,\n        limit=limit,\n        address_limit=address_limit,\n        outbox_list=outbox_list,\n    )\n</code></pre>"},{"location":"references/sources/api/#viadot.sources.outlook.Outlook.to_df","title":"<code>to_df(if_empty='warn')</code>","text":"<p>Generate a pandas DataFrame with the data.</p> <p>Parameters:</p> Name Type Description Default <code>if_empty</code> <code>str</code> <p>What to do if a fetch produce no data. Defaults to \"warn</p> <code>'warn'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.Dataframe: The response data as a pandas DataFrame plus viadot metadata.</p> Source code in <code>src/viadot/sources/outlook.py</code> <pre><code>@add_viadot_metadata_columns\ndef to_df(\n    self,\n    if_empty: str = \"warn\",\n) -&gt; pd.DataFrame:\n    \"\"\"Generate a pandas DataFrame with the data.\n\n    Args:\n        if_empty (str, optional): What to do if a fetch produce no data.\n            Defaults to \"warn\n\n    Returns:\n        pd.Dataframe: The response data as a pandas DataFrame plus viadot metadata.\n    \"\"\"\n    super().to_df(if_empty=if_empty)\n\n    data_frame = pd.DataFrame(self.data)\n\n    if data_frame.empty:\n        self._handle_if_empty(\n            if_empty=\"warn\",\n            message=\"No data was got from the Mail Box for those days\",\n        )\n    else:\n        self.logger.info(\"Successfully downloaded data from the Mindful API.\")\n\n    return data_frame\n</code></pre>"},{"location":"references/sources/api/#viadot.sources.salesforce.Salesforce","title":"<code>viadot.sources.salesforce.Salesforce</code>","text":"<p>               Bases: <code>Source</code></p> <p>Class implementing the Salesforce API.</p> Documentation for this API is available at <p>https://developer.salesforce.com/docs/apis.</p> Source code in <code>src/viadot/sources/salesforce.py</code> <pre><code>class Salesforce(Source):\n    \"\"\"Class implementing the Salesforce API.\n\n    Documentation for this API is available at:\n        https://developer.salesforce.com/docs/apis.\n    \"\"\"\n\n    def __init__(\n        self,\n        *args,\n        credentials: SalesforceCredentials | None = None,\n        config_key: str = \"salesforce\",\n        env: Literal[\"DEV\", \"QA\", \"PROD\"] = \"DEV\",\n        domain: str = \"test\",\n        client_id: str = \"viadot\",\n        **kwargs,\n    ):\n        \"\"\"A class for downloading data from Salesforce.\n\n        Args:\n            credentials (dict(str, any), optional): Salesforce credentials as a\n                dictionary. Defaults to None.\n            config_key (str, optional): The key in the viadot config holding relevant\n                credentials. Defaults to \"salesforce\".\n            env (Literal[\"DEV\", \"QA\", \"PROD\"], optional): Environment information,\n                provides information about credential and connection configuration.\n                Defaults to 'DEV'.\n            domain (str, optional): Domain of a connection. Defaults to 'test'\n                (sandbox). Can only be added if a username/password/security token\n                is provided.\n            client_id (str, optional): Client id, keep track of API calls.\n                Defaults to 'viadot'.\n        \"\"\"\n        credentials = credentials or get_source_credentials(config_key)\n\n        if not (\n            credentials.get(\"username\")\n            and credentials.get(\"password\")\n            and credentials.get(\"token\")\n        ):\n            message = \"'username', 'password' and 'token' credentials are required.\"\n            raise CredentialError(message)\n\n        validated_creds = dict(SalesforceCredentials(**credentials))\n        super().__init__(*args, credentials=validated_creds, **kwargs)\n\n        if env.upper() == \"DEV\" or env.upper() == \"QA\":\n            self.salesforce = SimpleSalesforce(\n                username=self.credentials[\"username\"],\n                password=self.credentials[\"password\"],\n                security_token=self.credentials[\"token\"],\n                domain=domain,\n                client_id=client_id,\n            )\n\n        elif env.upper() == \"PROD\":\n            self.salesforce = SimpleSalesforce(\n                username=self.credentials[\"username\"],\n                password=self.credentials[\"password\"],\n                security_token=self.credentials[\"token\"],\n            )\n\n        else:\n            message = \"The only available environments are DEV, QA, and PROD.\"\n            raise ValueError(message)\n\n    @add_viadot_metadata_columns\n    def to_df(\n        self,\n        query: str | None = None,\n        table: str | None = None,\n        columns: list[str] | None = None,\n        if_empty: Literal[\"warn\", \"skip\", \"fail\"] = \"warn\",\n    ) -&gt; pd.DataFrame:\n        \"\"\"Downloads data from Salesforce API and returns the DataFrame.\n\n        Args:\n            if_empty (str, optional): What to do if a fetch produce no data.\n                Defaults to \"warn\n            query (str, optional): The query to be used to download the data.\n                Defaults to None.\n            table (str, optional): Table name. Defaults to None.\n            columns (list[str], optional): List of required columns. Requires `table`\n                to be specified. Defaults to None.\n\n        Returns:\n            pd.DataFrame: Selected rows from Salesforce.\n        \"\"\"\n        if not query:\n            columns_str = \", \".join(columns) if columns else \"FIELDS(STANDARD)\"\n            query = f\"SELECT {columns_str} FROM {table}\"  # noqa: S608\n\n        data = self.salesforce.query(query).get(\"records\")\n\n        # Remove metadata from the data\n        for record in data:\n            record.pop(\"attributes\")\n\n        df = pd.DataFrame(data)\n\n        if df.empty:\n            self._handle_if_empty(\n                if_empty=if_empty,\n                message=\"The response does not contain any data.\",\n            )\n        else:\n            self.logger.info(\"Successfully downloaded data from the Salesforce API.\")\n\n        return df\n\n    def upsert(\n        self,\n        df: pd.DataFrame,\n        table: str,\n        external_id: str | None = None,\n        raise_on_error: bool = False,\n    ) -&gt; bool | None:\n        \"\"\"Upsert data into Salesforce.\n\n        Args:\n            df (pd.DataFrame): Selected rows from Salesforce.\n            table (str): Table name.\n            external_id (str, optional): External ID. Defaults to None.\n            raise_on_error (bool, optional): Whether to raise on error.\n                Defaults to False.\n\n        Returns:\n            bool: True if all records where processed successfully. None when df empty.\n\n        Raises:\n            ValueError: If the external_id is provided but not found in the DataFrame.\n            ValueError: If upserting a record fails and raise_on_error is True.\n        \"\"\"\n        if df.empty:\n            self._handle_if_empty()\n            return None\n\n        if external_id and external_id not in df.columns:\n            msg = f\"Passed DataFrame does not contain column '{external_id}'.\"\n            raise ValueError(msg)\n\n        table_to_upsert = getattr(self.salesforce, table)\n        records = df.to_dict(\"records\")\n        records_cp = records.copy()\n\n        for record in records_cp:\n            response = 0\n            if external_id:\n                if record[external_id] is None:\n                    continue\n                merge_key = f\"{external_id}/{record[external_id]}\"\n                record.pop(external_id)\n            else:\n                merge_key = record.pop(\"Id\")\n\n            try:\n                response = table_to_upsert.upsert(data=record, record_id=merge_key)\n            except SalesforceMalformedRequest as e:\n                msg = f\"Upsert of record {merge_key} failed.\"\n                if raise_on_error:\n                    raise ValueError(msg) from e\n                self.logger.warning(msg)\n\n            codes = {200: \"updated\", 201: \"created\", 204: \"updated\"}\n\n            if response not in codes:\n                msg = f\"Upsert failed for record: \\n{record} with response {response}\"\n                if raise_on_error:\n                    raise ValueError(msg)\n                self.logger.warning(msg)\n            else:\n                self.logger.info(f\"Successfully {codes[response]} record {merge_key}.\")\n\n        self.logger.info(\n            f\"Successfully upserted {len(records)} records into table '{table}'.\"\n        )\n\n        return True\n</code></pre>"},{"location":"references/sources/api/#viadot.sources.salesforce.Salesforce.__init__","title":"<code>__init__(*args, credentials=None, config_key='salesforce', env='DEV', domain='test', client_id='viadot', **kwargs)</code>","text":"<p>A class for downloading data from Salesforce.</p> <p>Parameters:</p> Name Type Description Default <code>credentials</code> <code>dict(str, any)</code> <p>Salesforce credentials as a dictionary. Defaults to None.</p> <code>None</code> <code>config_key</code> <code>str</code> <p>The key in the viadot config holding relevant credentials. Defaults to \"salesforce\".</p> <code>'salesforce'</code> <code>env</code> <code>Literal['DEV', 'QA', 'PROD']</code> <p>Environment information, provides information about credential and connection configuration. Defaults to 'DEV'.</p> <code>'DEV'</code> <code>domain</code> <code>str</code> <p>Domain of a connection. Defaults to 'test' (sandbox). Can only be added if a username/password/security token is provided.</p> <code>'test'</code> <code>client_id</code> <code>str</code> <p>Client id, keep track of API calls. Defaults to 'viadot'.</p> <code>'viadot'</code> Source code in <code>src/viadot/sources/salesforce.py</code> <pre><code>def __init__(\n    self,\n    *args,\n    credentials: SalesforceCredentials | None = None,\n    config_key: str = \"salesforce\",\n    env: Literal[\"DEV\", \"QA\", \"PROD\"] = \"DEV\",\n    domain: str = \"test\",\n    client_id: str = \"viadot\",\n    **kwargs,\n):\n    \"\"\"A class for downloading data from Salesforce.\n\n    Args:\n        credentials (dict(str, any), optional): Salesforce credentials as a\n            dictionary. Defaults to None.\n        config_key (str, optional): The key in the viadot config holding relevant\n            credentials. Defaults to \"salesforce\".\n        env (Literal[\"DEV\", \"QA\", \"PROD\"], optional): Environment information,\n            provides information about credential and connection configuration.\n            Defaults to 'DEV'.\n        domain (str, optional): Domain of a connection. Defaults to 'test'\n            (sandbox). Can only be added if a username/password/security token\n            is provided.\n        client_id (str, optional): Client id, keep track of API calls.\n            Defaults to 'viadot'.\n    \"\"\"\n    credentials = credentials or get_source_credentials(config_key)\n\n    if not (\n        credentials.get(\"username\")\n        and credentials.get(\"password\")\n        and credentials.get(\"token\")\n    ):\n        message = \"'username', 'password' and 'token' credentials are required.\"\n        raise CredentialError(message)\n\n    validated_creds = dict(SalesforceCredentials(**credentials))\n    super().__init__(*args, credentials=validated_creds, **kwargs)\n\n    if env.upper() == \"DEV\" or env.upper() == \"QA\":\n        self.salesforce = SimpleSalesforce(\n            username=self.credentials[\"username\"],\n            password=self.credentials[\"password\"],\n            security_token=self.credentials[\"token\"],\n            domain=domain,\n            client_id=client_id,\n        )\n\n    elif env.upper() == \"PROD\":\n        self.salesforce = SimpleSalesforce(\n            username=self.credentials[\"username\"],\n            password=self.credentials[\"password\"],\n            security_token=self.credentials[\"token\"],\n        )\n\n    else:\n        message = \"The only available environments are DEV, QA, and PROD.\"\n        raise ValueError(message)\n</code></pre>"},{"location":"references/sources/api/#viadot.sources.salesforce.Salesforce.to_df","title":"<code>to_df(query=None, table=None, columns=None, if_empty='warn')</code>","text":"<p>Downloads data from Salesforce API and returns the DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>if_empty</code> <code>str</code> <p>What to do if a fetch produce no data. Defaults to \"warn</p> <code>'warn'</code> <code>query</code> <code>str</code> <p>The query to be used to download the data. Defaults to None.</p> <code>None</code> <code>table</code> <code>str</code> <p>Table name. Defaults to None.</p> <code>None</code> <code>columns</code> <code>list[str]</code> <p>List of required columns. Requires <code>table</code> to be specified. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Selected rows from Salesforce.</p> Source code in <code>src/viadot/sources/salesforce.py</code> <pre><code>@add_viadot_metadata_columns\ndef to_df(\n    self,\n    query: str | None = None,\n    table: str | None = None,\n    columns: list[str] | None = None,\n    if_empty: Literal[\"warn\", \"skip\", \"fail\"] = \"warn\",\n) -&gt; pd.DataFrame:\n    \"\"\"Downloads data from Salesforce API and returns the DataFrame.\n\n    Args:\n        if_empty (str, optional): What to do if a fetch produce no data.\n            Defaults to \"warn\n        query (str, optional): The query to be used to download the data.\n            Defaults to None.\n        table (str, optional): Table name. Defaults to None.\n        columns (list[str], optional): List of required columns. Requires `table`\n            to be specified. Defaults to None.\n\n    Returns:\n        pd.DataFrame: Selected rows from Salesforce.\n    \"\"\"\n    if not query:\n        columns_str = \", \".join(columns) if columns else \"FIELDS(STANDARD)\"\n        query = f\"SELECT {columns_str} FROM {table}\"  # noqa: S608\n\n    data = self.salesforce.query(query).get(\"records\")\n\n    # Remove metadata from the data\n    for record in data:\n        record.pop(\"attributes\")\n\n    df = pd.DataFrame(data)\n\n    if df.empty:\n        self._handle_if_empty(\n            if_empty=if_empty,\n            message=\"The response does not contain any data.\",\n        )\n    else:\n        self.logger.info(\"Successfully downloaded data from the Salesforce API.\")\n\n    return df\n</code></pre>"},{"location":"references/sources/api/#viadot.sources.salesforce.Salesforce.upsert","title":"<code>upsert(df, table, external_id=None, raise_on_error=False)</code>","text":"<p>Upsert data into Salesforce.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Selected rows from Salesforce.</p> required <code>table</code> <code>str</code> <p>Table name.</p> required <code>external_id</code> <code>str</code> <p>External ID. Defaults to None.</p> <code>None</code> <code>raise_on_error</code> <code>bool</code> <p>Whether to raise on error. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool | None</code> <p>True if all records where processed successfully. None when df empty.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the external_id is provided but not found in the DataFrame.</p> <code>ValueError</code> <p>If upserting a record fails and raise_on_error is True.</p> Source code in <code>src/viadot/sources/salesforce.py</code> <pre><code>def upsert(\n    self,\n    df: pd.DataFrame,\n    table: str,\n    external_id: str | None = None,\n    raise_on_error: bool = False,\n) -&gt; bool | None:\n    \"\"\"Upsert data into Salesforce.\n\n    Args:\n        df (pd.DataFrame): Selected rows from Salesforce.\n        table (str): Table name.\n        external_id (str, optional): External ID. Defaults to None.\n        raise_on_error (bool, optional): Whether to raise on error.\n            Defaults to False.\n\n    Returns:\n        bool: True if all records where processed successfully. None when df empty.\n\n    Raises:\n        ValueError: If the external_id is provided but not found in the DataFrame.\n        ValueError: If upserting a record fails and raise_on_error is True.\n    \"\"\"\n    if df.empty:\n        self._handle_if_empty()\n        return None\n\n    if external_id and external_id not in df.columns:\n        msg = f\"Passed DataFrame does not contain column '{external_id}'.\"\n        raise ValueError(msg)\n\n    table_to_upsert = getattr(self.salesforce, table)\n    records = df.to_dict(\"records\")\n    records_cp = records.copy()\n\n    for record in records_cp:\n        response = 0\n        if external_id:\n            if record[external_id] is None:\n                continue\n            merge_key = f\"{external_id}/{record[external_id]}\"\n            record.pop(external_id)\n        else:\n            merge_key = record.pop(\"Id\")\n\n        try:\n            response = table_to_upsert.upsert(data=record, record_id=merge_key)\n        except SalesforceMalformedRequest as e:\n            msg = f\"Upsert of record {merge_key} failed.\"\n            if raise_on_error:\n                raise ValueError(msg) from e\n            self.logger.warning(msg)\n\n        codes = {200: \"updated\", 201: \"created\", 204: \"updated\"}\n\n        if response not in codes:\n            msg = f\"Upsert failed for record: \\n{record} with response {response}\"\n            if raise_on_error:\n                raise ValueError(msg)\n            self.logger.warning(msg)\n        else:\n            self.logger.info(f\"Successfully {codes[response]} record {merge_key}.\")\n\n    self.logger.info(\n        f\"Successfully upserted {len(records)} records into table '{table}'.\"\n    )\n\n    return True\n</code></pre>"},{"location":"references/sources/api/#viadot.sources.sharepoint.Sharepoint","title":"<code>viadot.sources.sharepoint.Sharepoint</code>","text":"<p>               Bases: <code>Source</code></p> Source code in <code>src/viadot/sources/sharepoint.py</code> <pre><code>class Sharepoint(Source):\n    DEFAULT_NA_VALUES = tuple(STR_NA_VALUES)\n\n    def __init__(\n        self,\n        credentials: SharepointCredentials = None,\n        config_key: str | None = None,\n        *args,\n        **kwargs,\n    ):\n        \"\"\"Download Excel files from Sharepoint.\n\n        Args:\n        credentials (SharepointCredentials): Sharepoint credentials.\n        config_key (str, optional): The key in the viadot config holding relevant\n            credentials.\n        \"\"\"\n        raw_creds = credentials or get_source_credentials(config_key) or {}\n        validated_creds = dict(SharepointCredentials(**raw_creds))\n        super().__init__(*args, credentials=validated_creds, **kwargs)\n\n    def get_connection(self) -&gt; sharepy.session.SharePointSession:\n        \"\"\"Establishe a connection to SharePoint.\n\n        Returns:\n            sharepy.session.SharePointSession: A session object representing\n                the authenticated connection.\n\n        Raises:\n            CredentialError: If authentication to SharePoint fails due to incorrect\n                credentials.\n        \"\"\"\n        try:\n            connection = sharepy.connect(\n                site=self.credentials.get(\"site\"),\n                username=self.credentials.get(\"username\"),\n                password=self.credentials.get(\"password\"),\n            )\n        except AuthError as e:\n            site = self.credentials.get(\"site\")\n            msg = f\"Could not authenticate to {site} with provided credentials.\"\n            raise CredentialError(msg) from e\n        return connection\n\n    def download_file(self, url: str, to_path: list | str) -&gt; None:\n        \"\"\"Download a file from Sharepoint to specific location.\n\n        Args:\n            url (str): The URL of the file to be downloaded.\n            to_path (str): Where to download the file.\n\n        Example:\n            download_file(\n                url=\"https://{tenant_name}.sharepoint.com/sites/{directory}/Shared%20Documents/Dashboard/file\",\n                to_path=\"file.xlsx\"\n            )\n        \"\"\"\n        conn = self.get_connection()\n        conn.getfile(\n            url=url,\n            filename=to_path,\n        )\n        conn.close()\n\n    def scan_sharepoint_folder(self, url: str) -&gt; list[str]:\n        \"\"\"Scan Sharepoint folder to get all file URLs of all files within it.\n\n        Args:\n            url (str): The URL of the folder to scan.\n\n        Raises:\n            ValueError: If the provided URL does not contain the expected '/sites/'\n                segment.\n\n        Returns:\n            list[str]: List of URLs pointing to each file within the specified\n                SharePoint folder.\n        \"\"\"\n        conn = self.get_connection()\n\n        parsed_url = urlparse(url)\n        path_parts = parsed_url.path.split(\"/\")\n        if \"sites\" in path_parts:\n            site_index = (\n                path_parts.index(\"sites\") + 2\n            )  # +2 to include 'sites' and the next segment\n            site_url = f\"{parsed_url.scheme}://{parsed_url.netloc}{'/'.join(path_parts[:site_index])}\"\n            library = \"/\".join(path_parts[site_index:])\n        else:\n            message = \"URL does not contain '/sites/' segment.\"\n            raise ValueError(message)\n\n        # -&gt; site_url = company.sharepoint.com/sites/site_name/\n        # -&gt; library = /shared_documents/folder/sub_folder/final_folder\n        endpoint = (\n            f\"{site_url}/_api/web/GetFolderByServerRelativeUrl('{library}')/Files\"\n        )\n        response = conn.get(endpoint)\n        files = response.json().get(\"d\", {}).get(\"results\", [])\n\n        return [f\"{site_url}/{library}{file['Name']}\" for file in files]\n\n    def _get_file_extension(self, url: str) -&gt; str:\n        \"\"\"Extracts the file extension from a given URL.\n\n        Parameters:\n            url (str): The URL from which to extract the file extension.\n\n        Returns:\n            str: The file extension, including the leading dot (e.g., '.xlsx').\n        \"\"\"\n        # Parse the URL to get the path\n        parsed_url = urlparse(url)\n        return Path(parsed_url.path).suffix\n\n    def _download_file_stream(self, url: str, **kwargs) -&gt; pd.ExcelFile:\n        \"\"\"Download the contents of a file from SharePoint.\n\n        Returns the data as an in-memory byte stream.\n\n        Args:\n            url (str): The URL of the file to download.\n\n        Returns:\n            io.BytesIO: An in-memory byte stream containing the file content.\n        \"\"\"\n        if \"nrows\" in kwargs:\n            msg = \"Parameter 'nrows' is not supported.\"\n            raise ValueError(msg)\n\n        conn = self.get_connection()\n\n        self.logger.info(f\"Downloading data from {url}...\")\n        try:\n            response = conn.get(url)\n            response.raise_for_status()  # Raise an exception for HTTP errors\n        except requests.exceptions.HTTPError as e:\n            if e.response.status_code == HTTPStatus.FORBIDDEN:\n                self.logger.exception(f\"Access denied to file: {url}\")\n            else:\n                self.logger.exception(f\"HTTP error when accessing {url}\")\n            raise\n        except Exception:\n            self.logger.exception(f\"Failed to download file: {url}\")\n            raise\n\n        bytes_stream = io.BytesIO(response.content)\n\n        try:\n            return pd.ExcelFile(bytes_stream)\n        except ValueError:\n            self.logger.exception(f\"Invalid Excel file: {url}\")\n            raise\n\n    def _is_file(self, url: str) -&gt; bool:\n        \"\"\"Determines whether a provided URL points to a file based on its structure.\n\n        This function uses a regular expression to check if the URL ends with a\n        common file extension. It does not make any network requests and purely\n        relies on the URL structure for its determination.\n\n        Parameters:\n        url (str): The URL to be checked.\n\n        Returns:\n        bool: True if the URL is identified as a file based on its extension,\n            False otherwise.\n\n        Example:\n        &gt;&gt;&gt; _is_file(\"https://example.com/file.xlsx\")\n        True\n        &gt;&gt;&gt; _is_file(\"https://example.com/folder/\")\n        False\n        &gt;&gt;&gt; _is_file(\"https://example.com/folder\")\n        False\n        \"\"\"\n        # Regular expression for matching file extensions\n        file_extension_pattern = re.compile(r\"\\.[a-zA-Z0-9]+$\")\n\n        return bool(file_extension_pattern.search(url))\n\n    def _handle_multiple_files(\n        self,\n        url: str,\n        file_sheet_mapping: dict,\n        na_values: list[str] | None = None,\n        **kwargs,\n    ):\n        \"\"\"Handle downloading and parsing multiple Excel files from a SharePoint folder.\n\n        Args:\n            url (str): The base URL of the SharePoint folder containing the files.\n            file_sheet_mapping (dict): A dictionary mapping file names to sheet names\n                or indexes. The keys are file names, and the values are sheet\n                names/indices.\n            na_values (Optional[list[str]]): Additional strings to recognize as NA/NaN.\n\n        Returns:\n            pd.DataFrame: A concatenated DataFrame containing the data from all\n                specified files and sheets.\n\n        Raises:\n            ValueError: If the file extension is not supported.\n        \"\"\"\n        dfs = []\n        for file, sheet in file_sheet_mapping.items():\n            file_url = url + file\n            try:\n                df = self._load_and_parse(\n                    file_url=file_url, sheet_name=sheet, na_values=na_values, **kwargs\n                )\n                dfs.append(df)\n            except Exception:\n                self.logger.exception(f\"Failed to load file: {file_url}\")\n                continue\n        if not dfs:\n            self.logger.warning(\"No valid Excel files were loaded.\")\n            return pd.DataFrame()\n        return pd.concat(validate_and_reorder_dfs_columns(dfs))\n\n    def _load_and_parse(\n        self,\n        file_url: str,\n        sheet_name: str | list[str] | None = None,\n        na_values: list[str] | None = None,\n        **kwargs,\n    ):\n        \"\"\"Loads and parses an Excel file from a URL.\n\n        Args:\n            file_url (str): The URL of the file to download and parse.\n            sheet_name (Optional[Union[str, list[str]]]): The name(s) or index(es) of\n                the sheet(s) to parse. If None, all sheets are parsed.\n            na_values (Optional[list[str]]): Additional strings to recognize as NA/NaN.\n            **kwargs: Additional keyword arguments to pass to the pandas read function.\n\n        Returns:\n            pd.DataFrame: The parsed data as a pandas DataFrame.\n\n        Raises:\n            ValueError: If the file extension is not supported.\n        \"\"\"\n        file_extension = self._get_file_extension(file_url)\n        if file_extension != \".xlsx\":\n            self.logger.error(\n                f\"Unsupported file extension: {file_extension} for file: {file_url}\"\n            )\n            msg = \"Only Excel (.xlsx) files are supported.\"\n            raise ValueError(msg)\n        file_stream = self._download_file_stream(file_url)\n        return self._parse_excel(file_stream, sheet_name, na_values, **kwargs)\n\n    def _parse_excel(\n        self,\n        excel_file: pd.ExcelFile,\n        sheet_name: str | list[str] | None = None,\n        na_values: list[str] | None = None,\n        **kwargs,\n    ):\n        \"\"\"Parses an Excel file into a DataFrame. Casts all columns to string.\n\n        Args:\n            excel_file: An ExcelFile object containing the data to parse.\n            sheet_name (Optional[Union[str, list[str]]]): The name(s) or index(es) of\n                the sheet(s) to parse. If None, all sheets are parsed.\n            na_values (Optional[list[str]]): Additional strings to recognize as NA/NaN.\n            **kwargs: Additional keyword arguments to pass to the pandas read function.\n\n        Returns:\n            pd.DataFrame: The parsed data as a pandas DataFrame.\n        \"\"\"\n        return pd.concat(\n            [\n                excel_file.parse(\n                    sheet,\n                    keep_default_na=False,\n                    na_values=na_values or list(self.DEFAULT_NA_VALUES),\n                    dtype=str,  # Ensure all columns are read as strings\n                    **kwargs,\n                )\n                for sheet in ([sheet_name] if sheet_name else excel_file.sheet_names)\n            ]\n        )\n\n    @add_viadot_metadata_columns\n    def to_df(\n        self,\n        url: str,\n        sheet_name: str | list[str] | None = None,\n        if_empty: Literal[\"warn\", \"skip\", \"fail\"] = \"warn\",\n        tests: dict[str, Any] | None = None,\n        file_sheet_mapping: dict[str, str | int | list[str]] | None = None,\n        na_values: list[str] | None = None,\n        **kwargs,\n    ) -&gt; pd.DataFrame:\n        \"\"\"Load an Excel file or files from a SharePoint URL into a pandas DataFrame.\n\n        This method handles downloading the file(s), parsing the content, and converting\n        it into a pandas DataFrame. It supports both single file URLs and folder URLs\n        with multiple files.\n\n        Args:\n            url (str): The URL of the file to be downloaded.\n            sheet_name (Optional[Union[str, list, int]], optional): Strings are used for\n                sheet names. Integers are used in zero-indexed sheet positions (chart\n                sheets do not count as a sheet position). Lists of strings/integers are\n                used to request multiple sheets. Specify None to get all worksheets.\n                Defaults to None.\n            if_empty (Literal[\"warn\", \"skip\", \"fail\"], optional): Action to take if\n                the DataFrame is empty.\n                - \"warn\": Logs a warning.\n                - \"skip\": Skips the operation.\n                - \"fail\": Raises an error.\n                Defaults to \"warn\".\n            tests (Dict[str, Any], optional): A dictionary with optional list of tests\n                to verify the output dataframe. If defined, triggers the `validate`\n                function from utils. Defaults to None.\n            file_sheet_mapping (dict[str, Union[str, int, list[str]]], optional):\n                Mapping of file names to sheet names or indices. The keys are file names\n                and the values are sheet names/indices. Used when multiple files are\n                involved. Defaults to None.\n            na_values (list[str], optional): Additional strings to recognize as NA/NaN.\n                If list passed, the specific NA values for each column will be\n                recognized. Defaults to None.\n            kwargs (dict[str, Any], optional): Keyword arguments to pass to\n                pd.ExcelFile.parse(). Note that `nrows` is not supported.\n\n        Returns:\n            pd.DataFrame: The resulting data as a pandas DataFrame.\n\n        Raises:\n            ValueError: If the file extension is not supported or if `if_empty` is set\n                to \"fail\" and the DataFrame is empty.\n            SKIP: If `if_empty` is set to \"skip\" and the DataFrame is empty.\n        \"\"\"\n        if self._is_file(url):\n            df = self._load_and_parse(\n                file_url=url, sheet_name=sheet_name, na_values=na_values, **kwargs\n            )\n        elif file_sheet_mapping:\n            df = self._handle_multiple_files(\n                url=url,\n                file_sheet_mapping=file_sheet_mapping,\n                na_values=na_values,\n                **kwargs,\n            )\n        else:\n            list_of_urls = self.scan_sharepoint_folder(url)\n            excel_urls = [\n                file_url\n                for file_url in list_of_urls\n                if self._get_file_extension(file_url) == \".xlsx\"\n            ]\n            if not excel_urls:\n                self.logger.warning(f\"No Excel files found in folder: {url}\")\n                return pd.DataFrame()\n            dfs = [\n                self._load_and_parse(\n                    file_url=file_url,\n                    sheet_name=sheet_name,\n                    na_values=na_values,\n                    **kwargs,\n                )\n                for file_url in excel_urls\n            ]\n            df = pd.concat(validate_and_reorder_dfs_columns(dfs))\n\n        if df.empty:\n            try:\n                self._handle_if_empty(if_empty)\n            except SKIP:\n                return pd.DataFrame()\n        else:\n            self.logger.info(f\"Successfully downloaded {len(df)} rows of data.\")\n\n        df_clean = cleanup_df(df)\n\n        if tests:\n            validate(df=df_clean, tests=tests)\n\n        return df_clean\n</code></pre>"},{"location":"references/sources/api/#viadot.sources.sharepoint.Sharepoint.__init__","title":"<code>__init__(credentials=None, config_key=None, *args, **kwargs)</code>","text":"<p>Download Excel files from Sharepoint.</p> <p>Args: credentials (SharepointCredentials): Sharepoint credentials. config_key (str, optional): The key in the viadot config holding relevant     credentials.</p> Source code in <code>src/viadot/sources/sharepoint.py</code> <pre><code>def __init__(\n    self,\n    credentials: SharepointCredentials = None,\n    config_key: str | None = None,\n    *args,\n    **kwargs,\n):\n    \"\"\"Download Excel files from Sharepoint.\n\n    Args:\n    credentials (SharepointCredentials): Sharepoint credentials.\n    config_key (str, optional): The key in the viadot config holding relevant\n        credentials.\n    \"\"\"\n    raw_creds = credentials or get_source_credentials(config_key) or {}\n    validated_creds = dict(SharepointCredentials(**raw_creds))\n    super().__init__(*args, credentials=validated_creds, **kwargs)\n</code></pre>"},{"location":"references/sources/api/#viadot.sources.sharepoint.Sharepoint.download_file","title":"<code>download_file(url, to_path)</code>","text":"<p>Download a file from Sharepoint to specific location.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL of the file to be downloaded.</p> required <code>to_path</code> <code>str</code> <p>Where to download the file.</p> required Example <p>download_file(     url=\"https://{tenant_name}.sharepoint.com/sites/{directory}/Shared%20Documents/Dashboard/file\",     to_path=\"file.xlsx\" )</p> Source code in <code>src/viadot/sources/sharepoint.py</code> <pre><code>def download_file(self, url: str, to_path: list | str) -&gt; None:\n    \"\"\"Download a file from Sharepoint to specific location.\n\n    Args:\n        url (str): The URL of the file to be downloaded.\n        to_path (str): Where to download the file.\n\n    Example:\n        download_file(\n            url=\"https://{tenant_name}.sharepoint.com/sites/{directory}/Shared%20Documents/Dashboard/file\",\n            to_path=\"file.xlsx\"\n        )\n    \"\"\"\n    conn = self.get_connection()\n    conn.getfile(\n        url=url,\n        filename=to_path,\n    )\n    conn.close()\n</code></pre>"},{"location":"references/sources/api/#viadot.sources.sharepoint.Sharepoint.get_connection","title":"<code>get_connection()</code>","text":"<p>Establishe a connection to SharePoint.</p> <p>Returns:</p> Type Description <code>SharePointSession</code> <p>sharepy.session.SharePointSession: A session object representing the authenticated connection.</p> <p>Raises:</p> Type Description <code>CredentialError</code> <p>If authentication to SharePoint fails due to incorrect credentials.</p> Source code in <code>src/viadot/sources/sharepoint.py</code> <pre><code>def get_connection(self) -&gt; sharepy.session.SharePointSession:\n    \"\"\"Establishe a connection to SharePoint.\n\n    Returns:\n        sharepy.session.SharePointSession: A session object representing\n            the authenticated connection.\n\n    Raises:\n        CredentialError: If authentication to SharePoint fails due to incorrect\n            credentials.\n    \"\"\"\n    try:\n        connection = sharepy.connect(\n            site=self.credentials.get(\"site\"),\n            username=self.credentials.get(\"username\"),\n            password=self.credentials.get(\"password\"),\n        )\n    except AuthError as e:\n        site = self.credentials.get(\"site\")\n        msg = f\"Could not authenticate to {site} with provided credentials.\"\n        raise CredentialError(msg) from e\n    return connection\n</code></pre>"},{"location":"references/sources/api/#viadot.sources.sharepoint.Sharepoint.scan_sharepoint_folder","title":"<code>scan_sharepoint_folder(url)</code>","text":"<p>Scan Sharepoint folder to get all file URLs of all files within it.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL of the folder to scan.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the provided URL does not contain the expected '/sites/' segment.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: List of URLs pointing to each file within the specified SharePoint folder.</p> Source code in <code>src/viadot/sources/sharepoint.py</code> <pre><code>def scan_sharepoint_folder(self, url: str) -&gt; list[str]:\n    \"\"\"Scan Sharepoint folder to get all file URLs of all files within it.\n\n    Args:\n        url (str): The URL of the folder to scan.\n\n    Raises:\n        ValueError: If the provided URL does not contain the expected '/sites/'\n            segment.\n\n    Returns:\n        list[str]: List of URLs pointing to each file within the specified\n            SharePoint folder.\n    \"\"\"\n    conn = self.get_connection()\n\n    parsed_url = urlparse(url)\n    path_parts = parsed_url.path.split(\"/\")\n    if \"sites\" in path_parts:\n        site_index = (\n            path_parts.index(\"sites\") + 2\n        )  # +2 to include 'sites' and the next segment\n        site_url = f\"{parsed_url.scheme}://{parsed_url.netloc}{'/'.join(path_parts[:site_index])}\"\n        library = \"/\".join(path_parts[site_index:])\n    else:\n        message = \"URL does not contain '/sites/' segment.\"\n        raise ValueError(message)\n\n    # -&gt; site_url = company.sharepoint.com/sites/site_name/\n    # -&gt; library = /shared_documents/folder/sub_folder/final_folder\n    endpoint = (\n        f\"{site_url}/_api/web/GetFolderByServerRelativeUrl('{library}')/Files\"\n    )\n    response = conn.get(endpoint)\n    files = response.json().get(\"d\", {}).get(\"results\", [])\n\n    return [f\"{site_url}/{library}{file['Name']}\" for file in files]\n</code></pre>"},{"location":"references/sources/api/#viadot.sources.sharepoint.Sharepoint.to_df","title":"<code>to_df(url, sheet_name=None, if_empty='warn', tests=None, file_sheet_mapping=None, na_values=None, **kwargs)</code>","text":"<p>Load an Excel file or files from a SharePoint URL into a pandas DataFrame.</p> <p>This method handles downloading the file(s), parsing the content, and converting it into a pandas DataFrame. It supports both single file URLs and folder URLs with multiple files.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The URL of the file to be downloaded.</p> required <code>sheet_name</code> <code>Optional[Union[str, list, int]]</code> <p>Strings are used for sheet names. Integers are used in zero-indexed sheet positions (chart sheets do not count as a sheet position). Lists of strings/integers are used to request multiple sheets. Specify None to get all worksheets. Defaults to None.</p> <code>None</code> <code>if_empty</code> <code>Literal['warn', 'skip', 'fail']</code> <p>Action to take if the DataFrame is empty. - \"warn\": Logs a warning. - \"skip\": Skips the operation. - \"fail\": Raises an error. Defaults to \"warn\".</p> <code>'warn'</code> <code>tests</code> <code>Dict[str, Any]</code> <p>A dictionary with optional list of tests to verify the output dataframe. If defined, triggers the <code>validate</code> function from utils. Defaults to None.</p> <code>None</code> <code>file_sheet_mapping</code> <code>dict[str, Union[str, int, list[str]]]</code> <p>Mapping of file names to sheet names or indices. The keys are file names and the values are sheet names/indices. Used when multiple files are involved. Defaults to None.</p> <code>None</code> <code>na_values</code> <code>list[str]</code> <p>Additional strings to recognize as NA/NaN. If list passed, the specific NA values for each column will be recognized. Defaults to None.</p> <code>None</code> <code>kwargs</code> <code>dict[str, Any]</code> <p>Keyword arguments to pass to pd.ExcelFile.parse(). Note that <code>nrows</code> is not supported.</p> <code>{}</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: The resulting data as a pandas DataFrame.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the file extension is not supported or if <code>if_empty</code> is set to \"fail\" and the DataFrame is empty.</p> <code>SKIP</code> <p>If <code>if_empty</code> is set to \"skip\" and the DataFrame is empty.</p> Source code in <code>src/viadot/sources/sharepoint.py</code> <pre><code>@add_viadot_metadata_columns\ndef to_df(\n    self,\n    url: str,\n    sheet_name: str | list[str] | None = None,\n    if_empty: Literal[\"warn\", \"skip\", \"fail\"] = \"warn\",\n    tests: dict[str, Any] | None = None,\n    file_sheet_mapping: dict[str, str | int | list[str]] | None = None,\n    na_values: list[str] | None = None,\n    **kwargs,\n) -&gt; pd.DataFrame:\n    \"\"\"Load an Excel file or files from a SharePoint URL into a pandas DataFrame.\n\n    This method handles downloading the file(s), parsing the content, and converting\n    it into a pandas DataFrame. It supports both single file URLs and folder URLs\n    with multiple files.\n\n    Args:\n        url (str): The URL of the file to be downloaded.\n        sheet_name (Optional[Union[str, list, int]], optional): Strings are used for\n            sheet names. Integers are used in zero-indexed sheet positions (chart\n            sheets do not count as a sheet position). Lists of strings/integers are\n            used to request multiple sheets. Specify None to get all worksheets.\n            Defaults to None.\n        if_empty (Literal[\"warn\", \"skip\", \"fail\"], optional): Action to take if\n            the DataFrame is empty.\n            - \"warn\": Logs a warning.\n            - \"skip\": Skips the operation.\n            - \"fail\": Raises an error.\n            Defaults to \"warn\".\n        tests (Dict[str, Any], optional): A dictionary with optional list of tests\n            to verify the output dataframe. If defined, triggers the `validate`\n            function from utils. Defaults to None.\n        file_sheet_mapping (dict[str, Union[str, int, list[str]]], optional):\n            Mapping of file names to sheet names or indices. The keys are file names\n            and the values are sheet names/indices. Used when multiple files are\n            involved. Defaults to None.\n        na_values (list[str], optional): Additional strings to recognize as NA/NaN.\n            If list passed, the specific NA values for each column will be\n            recognized. Defaults to None.\n        kwargs (dict[str, Any], optional): Keyword arguments to pass to\n            pd.ExcelFile.parse(). Note that `nrows` is not supported.\n\n    Returns:\n        pd.DataFrame: The resulting data as a pandas DataFrame.\n\n    Raises:\n        ValueError: If the file extension is not supported or if `if_empty` is set\n            to \"fail\" and the DataFrame is empty.\n        SKIP: If `if_empty` is set to \"skip\" and the DataFrame is empty.\n    \"\"\"\n    if self._is_file(url):\n        df = self._load_and_parse(\n            file_url=url, sheet_name=sheet_name, na_values=na_values, **kwargs\n        )\n    elif file_sheet_mapping:\n        df = self._handle_multiple_files(\n            url=url,\n            file_sheet_mapping=file_sheet_mapping,\n            na_values=na_values,\n            **kwargs,\n        )\n    else:\n        list_of_urls = self.scan_sharepoint_folder(url)\n        excel_urls = [\n            file_url\n            for file_url in list_of_urls\n            if self._get_file_extension(file_url) == \".xlsx\"\n        ]\n        if not excel_urls:\n            self.logger.warning(f\"No Excel files found in folder: {url}\")\n            return pd.DataFrame()\n        dfs = [\n            self._load_and_parse(\n                file_url=file_url,\n                sheet_name=sheet_name,\n                na_values=na_values,\n                **kwargs,\n            )\n            for file_url in excel_urls\n        ]\n        df = pd.concat(validate_and_reorder_dfs_columns(dfs))\n\n    if df.empty:\n        try:\n            self._handle_if_empty(if_empty)\n        except SKIP:\n            return pd.DataFrame()\n    else:\n        self.logger.info(f\"Successfully downloaded {len(df)} rows of data.\")\n\n    df_clean = cleanup_df(df)\n\n    if tests:\n        validate(df=df_clean, tests=tests)\n\n    return df_clean\n</code></pre>"},{"location":"references/sources/api/#viadot.sources.supermetrics.Supermetrics","title":"<code>viadot.sources.supermetrics.Supermetrics</code>","text":"<p>               Bases: <code>Source</code></p> <p>Implements methods for querying and interacting with the Supermetrics API.</p> <p>This class provides functionality to query data from the Supermetrics API, which is a tool used for accessing data from various data sources. The API documentation can be found at: https://supermetrics.com/docs/product-api-getting-started/. For information on usage limits, please refer to: https://supermetrics.com/docs/product-api-usage-limits/.</p> <pre><code>config_key (str, optional):\n    The key in the viadot configuration that holds the relevant credentials\n    for the API. Defaults to None.\ncredentials (dict of str to any, optional):\n    A dictionary containing the credentials needed for the API connection\n    configuration, specifically `api_key` and `user`. Defaults to None.\nquery_params (dict of str to any, optional):\n    A dictionary containing the parameters to pass to the GET query.\n    These parameters define the specifics of the data request. Defaults to None.\n    For a full specification of possible parameters, see:\n    https://supermetrics.com/docs/product-api-get-data/.\n</code></pre> Source code in <code>src/viadot/sources/supermetrics.py</code> <pre><code>class Supermetrics(Source):\n    \"\"\"Implements methods for querying and interacting with the Supermetrics API.\n\n    This class provides functionality to query data from the Supermetrics API,\n    which is a tool used for accessing data from various data sources. The API\n    documentation can be found at:\n    https://supermetrics.com/docs/product-api-getting-started/. For information\n    on usage limits, please refer to:\n    https://supermetrics.com/docs/product-api-usage-limits/.\n\n    Args:\n    ----\n        config_key (str, optional):\n            The key in the viadot configuration that holds the relevant credentials\n            for the API. Defaults to None.\n        credentials (dict of str to any, optional):\n            A dictionary containing the credentials needed for the API connection\n            configuration, specifically `api_key` and `user`. Defaults to None.\n        query_params (dict of str to any, optional):\n            A dictionary containing the parameters to pass to the GET query.\n            These parameters define the specifics of the data request. Defaults to None.\n            For a full specification of possible parameters, see:\n            https://supermetrics.com/docs/product-api-get-data/.\n\n    \"\"\"\n\n    BASE_URL = \"https://api.supermetrics.com/enterprise/v2/query/data/json\"\n\n    def __init__(\n        self,\n        *args,\n        credentials: dict[str, Any] | None = None,\n        config_key: str | None = None,\n        query_params: dict[str, Any] | None = None,\n        **kwargs,\n    ) -&gt; None:\n        \"\"\"Initialize the Supermetrics object.\n\n        This constructor sets up the necessary components to interact with the\n        Supermetrics API, including the credentials and any query parameters.\n\n        Args:\n        ----\n            credentials (SupermetricsCredentials, optional):\n                An instance of `SupermetricsCredentials` containing the API key and\n                user email for authentication. Defaults to None.\n            config_key (str, optional):\n                The key in the viadot configuration that holds the relevant credentials\n                for the API. Defaults to None.\n            query_params (dict of str to any, optional):\n                A dictionary containing the parameters to pass to the GET query. These\n                parameters define the specifics of the data request. Defaults to None.\n\n        \"\"\"\n        raw_creds = credentials or get_source_credentials(config_key)\n        validated_creds = dict(SupermetricsCredentials(**raw_creds))\n\n        super().__init__(*args, credentials=validated_creds, **kwargs)\n\n        self.query_params = query_params\n\n    def to_json(self, timeout: tuple = (3.05, 60 * 30)) -&gt; dict[str, Any]:\n        \"\"\"Download query results to a dictionary.\n\n        This method executes the query against the Supermetrics API and retrieves\n        the results as a JSON dictionary.\n\n        Args:\n        ----\n            timeout (tuple of float, optional):\n                A tuple specifying the timeout values for the request. The first value\n                is the timeout for connection issues, and the second value is\n                the timeout for query execution. Defaults to (3.05, 1800), which\n                provides a short timeout for connection issues and a longer timeout\n                for the query execution.\n\n        Returns:\n        -------\n            dict:\n                The response from the Supermetrics API, returned as a JSON dictionary.\n\n        Raises:\n        ------\n            ValueError:\n                Raised if the query parameters are not set before calling this method.\n\n        \"\"\"\n        if not self.query_params:\n            msg = \"Please build the query first\"\n            raise ValueError(msg)\n\n        params = {\"json\": json.dumps(self.query_params)}\n        headers = {\"Authorization\": f\"Bearer {self.credentials.get('api_key')}\"}\n\n        response = handle_api_response(\n            url=Supermetrics.BASE_URL,\n            params=params,\n            headers=headers,\n            timeout=timeout,\n        )\n        return response.json()\n\n    @classmethod\n    def _get_col_names_google_analytics(\n        cls,\n        response: dict,\n    ) -&gt; list[str]:\n        \"\"\"Get column names from Google Analytics data.\n\n        This method extracts the column names from the JSON response received\n        from a Google Analytics API call.\n\n        Args:\n        ----\n            response (dict):\n                A dictionary containing the JSON response from the API call.\n\n        Returns:\n        -------\n            list of str:\n                A list of column names extracted from the Google Analytics data.\n\n        Raises:\n        ------\n            ValueError:\n                Raised if no data is returned in the response or if the column names\n                cannot be determined.\n\n        \"\"\"\n        is_pivoted = any(\n            field[\"field_split\"] == \"column\"\n            for field in response[\"meta\"][\"query\"][\"fields\"]\n        )\n\n        if is_pivoted:\n            if not response[\"data\"]:\n                msg = \"Couldn't find column names as query returned no data.\"\n                raise ValueError(msg)\n            columns = response[\"data\"][0]\n        else:\n            cols_meta = response[\"meta\"][\"query\"][\"fields\"]\n            columns = [col_meta[\"field_name\"] for col_meta in cols_meta]\n        return columns\n\n    @classmethod\n    def _get_col_names_other(cls, response: dict) -&gt; list[str]:\n        \"\"\"Get column names from non-Google Analytics data.\n\n        This method extracts the column names from the JSON response received\n        from an API call that is not related to Google Analytics.\n\n        Args:\n        ----\n            response (dict):\n                A dictionary containing the JSON response from the API call.\n\n        Returns:\n        -------\n            list of str:\n                A list of column names extracted from the non-Google Analytics data.\n\n        \"\"\"\n        cols_meta = response[\"meta\"][\"query\"][\"fields\"]\n        return [col_meta[\"field_name\"] for col_meta in cols_meta]\n\n    def _get_col_names(self) -&gt; list[str]:\n        \"\"\"Get column names based on the data type.\n\n        This method determines the appropriate column names for the data based\n        on its type, whether it's Google Analytics data or another type.\n\n        Returns:\n        -------\n            list of str:\n                A list of column names based on the data type.\n\n        Raises:\n        ------\n            ValueError:\n                Raised if the column names cannot be determined.\n\n        \"\"\"\n        response: dict = self.to_json()\n        if self.query_params[\"ds_id\"] == \"GA\":\n            return Supermetrics._get_col_names_google_analytics(response)\n\n        return Supermetrics._get_col_names_other(response)\n\n    @add_viadot_metadata_columns\n    def to_df(\n        self,\n        if_empty: str = \"warn\",\n        query_params: dict[str, Any] | None = None,\n    ) -&gt; pd.DataFrame:\n        \"\"\"Download data into a pandas DataFrame.\n\n        This method retrieves data from the Supermetrics API and loads it into\n        a pandas DataFrame.\n\n        Args:\n        ----\n            if_empty (str, optional):\n                Specifies the action to take if the query returns no data.\n                Options include \"fail\" to raise an error or \"ignore\" to return\n                an empty DataFrame. Defaults to \"fail\".\n\n        Returns:\n        -------\n            pd.DataFrame:\n                A pandas DataFrame containing the JSON data retrieved from the API.\n\n        Raises:\n        ------\n            ValueError:\n                Raised if the DataFrame is empty and `if_empty` is set to \"fail\".\n\n        \"\"\"\n        # Use provided query_params or default to the instance's query_params\n        if query_params is not None:\n            self.query_params = query_params\n\n        if not self.query_params:\n            msg = \"Query parameters are required to fetch data.\"\n            raise ValueError(msg)\n\n        self.query_params[\"api_key\"] = self.credentials.get(\"api_key\")\n\n        try:\n            columns = self._get_col_names()\n        except ValueError:\n            columns = None\n\n        data = self.to_json()[\"data\"]\n\n        if data:\n            df = pd.DataFrame(data[1:], columns=columns).replace(\"\", np.nan)\n        else:\n            df = pd.DataFrame(columns=columns)\n\n        if df.empty:\n            self._handle_if_empty(if_empty)\n\n        return df\n</code></pre>"},{"location":"references/sources/api/#viadot.sources.supermetrics.Supermetrics.__init__","title":"<code>__init__(*args, credentials=None, config_key=None, query_params=None, **kwargs)</code>","text":"<p>Initialize the Supermetrics object.</p> <p>This constructor sets up the necessary components to interact with the Supermetrics API, including the credentials and any query parameters.</p> <pre><code>credentials (SupermetricsCredentials, optional):\n    An instance of `SupermetricsCredentials` containing the API key and\n    user email for authentication. Defaults to None.\nconfig_key (str, optional):\n    The key in the viadot configuration that holds the relevant credentials\n    for the API. Defaults to None.\nquery_params (dict of str to any, optional):\n    A dictionary containing the parameters to pass to the GET query. These\n    parameters define the specifics of the data request. Defaults to None.\n</code></pre> Source code in <code>src/viadot/sources/supermetrics.py</code> <pre><code>def __init__(\n    self,\n    *args,\n    credentials: dict[str, Any] | None = None,\n    config_key: str | None = None,\n    query_params: dict[str, Any] | None = None,\n    **kwargs,\n) -&gt; None:\n    \"\"\"Initialize the Supermetrics object.\n\n    This constructor sets up the necessary components to interact with the\n    Supermetrics API, including the credentials and any query parameters.\n\n    Args:\n    ----\n        credentials (SupermetricsCredentials, optional):\n            An instance of `SupermetricsCredentials` containing the API key and\n            user email for authentication. Defaults to None.\n        config_key (str, optional):\n            The key in the viadot configuration that holds the relevant credentials\n            for the API. Defaults to None.\n        query_params (dict of str to any, optional):\n            A dictionary containing the parameters to pass to the GET query. These\n            parameters define the specifics of the data request. Defaults to None.\n\n    \"\"\"\n    raw_creds = credentials or get_source_credentials(config_key)\n    validated_creds = dict(SupermetricsCredentials(**raw_creds))\n\n    super().__init__(*args, credentials=validated_creds, **kwargs)\n\n    self.query_params = query_params\n</code></pre>"},{"location":"references/sources/api/#viadot.sources.supermetrics.Supermetrics.to_df","title":"<code>to_df(if_empty='warn', query_params=None)</code>","text":"<p>Download data into a pandas DataFrame.</p> <p>This method retrieves data from the Supermetrics API and loads it into a pandas DataFrame.</p> <pre><code>if_empty (str, optional):\n    Specifies the action to take if the query returns no data.\n    Options include \"fail\" to raise an error or \"ignore\" to return\n    an empty DataFrame. Defaults to \"fail\".\n</code></pre> <pre><code>pd.DataFrame:\n    A pandas DataFrame containing the JSON data retrieved from the API.\n</code></pre> <pre><code>ValueError:\n    Raised if the DataFrame is empty and `if_empty` is set to \"fail\".\n</code></pre> Source code in <code>src/viadot/sources/supermetrics.py</code> <pre><code>@add_viadot_metadata_columns\ndef to_df(\n    self,\n    if_empty: str = \"warn\",\n    query_params: dict[str, Any] | None = None,\n) -&gt; pd.DataFrame:\n    \"\"\"Download data into a pandas DataFrame.\n\n    This method retrieves data from the Supermetrics API and loads it into\n    a pandas DataFrame.\n\n    Args:\n    ----\n        if_empty (str, optional):\n            Specifies the action to take if the query returns no data.\n            Options include \"fail\" to raise an error or \"ignore\" to return\n            an empty DataFrame. Defaults to \"fail\".\n\n    Returns:\n    -------\n        pd.DataFrame:\n            A pandas DataFrame containing the JSON data retrieved from the API.\n\n    Raises:\n    ------\n        ValueError:\n            Raised if the DataFrame is empty and `if_empty` is set to \"fail\".\n\n    \"\"\"\n    # Use provided query_params or default to the instance's query_params\n    if query_params is not None:\n        self.query_params = query_params\n\n    if not self.query_params:\n        msg = \"Query parameters are required to fetch data.\"\n        raise ValueError(msg)\n\n    self.query_params[\"api_key\"] = self.credentials.get(\"api_key\")\n\n    try:\n        columns = self._get_col_names()\n    except ValueError:\n        columns = None\n\n    data = self.to_json()[\"data\"]\n\n    if data:\n        df = pd.DataFrame(data[1:], columns=columns).replace(\"\", np.nan)\n    else:\n        df = pd.DataFrame(columns=columns)\n\n    if df.empty:\n        self._handle_if_empty(if_empty)\n\n    return df\n</code></pre>"},{"location":"references/sources/api/#viadot.sources.supermetrics.Supermetrics.to_json","title":"<code>to_json(timeout=(3.05, 60 * 30))</code>","text":"<p>Download query results to a dictionary.</p> <p>This method executes the query against the Supermetrics API and retrieves the results as a JSON dictionary.</p> <pre><code>timeout (tuple of float, optional):\n    A tuple specifying the timeout values for the request. The first value\n    is the timeout for connection issues, and the second value is\n    the timeout for query execution. Defaults to (3.05, 1800), which\n    provides a short timeout for connection issues and a longer timeout\n    for the query execution.\n</code></pre> <pre><code>dict:\n    The response from the Supermetrics API, returned as a JSON dictionary.\n</code></pre> <pre><code>ValueError:\n    Raised if the query parameters are not set before calling this method.\n</code></pre> Source code in <code>src/viadot/sources/supermetrics.py</code> <pre><code>def to_json(self, timeout: tuple = (3.05, 60 * 30)) -&gt; dict[str, Any]:\n    \"\"\"Download query results to a dictionary.\n\n    This method executes the query against the Supermetrics API and retrieves\n    the results as a JSON dictionary.\n\n    Args:\n    ----\n        timeout (tuple of float, optional):\n            A tuple specifying the timeout values for the request. The first value\n            is the timeout for connection issues, and the second value is\n            the timeout for query execution. Defaults to (3.05, 1800), which\n            provides a short timeout for connection issues and a longer timeout\n            for the query execution.\n\n    Returns:\n    -------\n        dict:\n            The response from the Supermetrics API, returned as a JSON dictionary.\n\n    Raises:\n    ------\n        ValueError:\n            Raised if the query parameters are not set before calling this method.\n\n    \"\"\"\n    if not self.query_params:\n        msg = \"Please build the query first\"\n        raise ValueError(msg)\n\n    params = {\"json\": json.dumps(self.query_params)}\n    headers = {\"Authorization\": f\"Bearer {self.credentials.get('api_key')}\"}\n\n    response = handle_api_response(\n        url=Supermetrics.BASE_URL,\n        params=params,\n        headers=headers,\n        timeout=timeout,\n    )\n    return response.json()\n</code></pre>"},{"location":"references/sources/api/#viadot.sources.uk_carbon_intensity.UKCarbonIntensity","title":"<code>viadot.sources.uk_carbon_intensity.UKCarbonIntensity</code>","text":"<p>               Bases: <code>Source</code></p> Source code in <code>src/viadot/sources/uk_carbon_intensity.py</code> <pre><code>class UKCarbonIntensity(Source):\n    def __init__(self, *args, api_url: str | None = None, **kwargs):\n        \"\"\"Fetch data of Carbon Intensity of the UK Power Grid.\n\n        Documentation for this source API is located\n        at: https://carbon-intensity.github.io/api-definitions/#carbon-intensity-api-v2-0-0\n\n        Parameters\n        ----------\n        api_url : str, optional\n        The URL endpoint to call, by default None\n        \"\"\"\n        super().__init__(*args, **kwargs)\n        self.api_url = api_url\n        self.API_ENDPOINT = \"https://api.carbonintensity.org.uk\"\n\n    def to_json(self) -&gt; dict:\n        \"\"\"Creates json file.\"\"\"\n        url = f\"{self.API_ENDPOINT}{self.api_url}\"\n        headers = {\"Accept\": \"application/json\"}\n        response = requests.get(url, params={}, headers=headers, timeout=10)\n        if response.ok:\n            return response.json()\n        raise f\"Error {response.json()}\"\n\n    @add_viadot_metadata_columns\n    def to_df(self, if_empty: str = \"warn\") -&gt; pd.DataFrame:\n        \"\"\"Returns a pandas DataFrame with flattened data.\n\n        Returns:\n            pandas.DataFrame: A Pandas DataFrame\n        \"\"\"\n        from_ = []\n        to = []\n        forecast = []\n        actual = []\n        max_ = []\n        average = []\n        min_ = []\n        index = []\n        json_data = self.to_json()\n\n        if not json_data:\n            self._handle_if_empty(if_empty)\n\n        for row in json_data[\"data\"]:\n            from_.append(row[\"from\"])\n            to.append(row[\"to\"])\n            index.append(row[\"intensity\"][\"index\"])\n            try:\n                forecast.append(row[\"intensity\"][\"forecast\"])\n                actual.append(row[\"intensity\"][\"actual\"])\n                df = pd.DataFrame(\n                    {\n                        \"from\": from_,\n                        \"to\": to,\n                        \"forecast\": forecast,\n                        \"actual\": actual,\n                        \"index\": index,\n                    }\n                )\n            except KeyError:\n                max_.append(row[\"intensity\"][\"max\"])\n                average.append(row[\"intensity\"][\"average\"])\n                min_.append(row[\"intensity\"][\"min\"])\n                df = pd.DataFrame(\n                    {\n                        \"from\": from_,\n                        \"to\": to,\n                        \"max\": max_,\n                        \"average\": average,\n                        \"min\": min_,\n                    }\n                )\n        return df\n\n    def query(self) -&gt; None:\n        \"\"\"Queries the API.\"\"\"\n        ...\n</code></pre>"},{"location":"references/sources/api/#viadot.sources.uk_carbon_intensity.UKCarbonIntensity.__init__","title":"<code>__init__(*args, api_url=None, **kwargs)</code>","text":"<p>Fetch data of Carbon Intensity of the UK Power Grid.</p> <p>Documentation for this source API is located at: https://carbon-intensity.github.io/api-definitions/#carbon-intensity-api-v2-0-0</p>"},{"location":"references/sources/api/#viadot.sources.uk_carbon_intensity.UKCarbonIntensity.__init__--parameters","title":"Parameters","text":"<p>api_url : str, optional The URL endpoint to call, by default None</p> Source code in <code>src/viadot/sources/uk_carbon_intensity.py</code> <pre><code>def __init__(self, *args, api_url: str | None = None, **kwargs):\n    \"\"\"Fetch data of Carbon Intensity of the UK Power Grid.\n\n    Documentation for this source API is located\n    at: https://carbon-intensity.github.io/api-definitions/#carbon-intensity-api-v2-0-0\n\n    Parameters\n    ----------\n    api_url : str, optional\n    The URL endpoint to call, by default None\n    \"\"\"\n    super().__init__(*args, **kwargs)\n    self.api_url = api_url\n    self.API_ENDPOINT = \"https://api.carbonintensity.org.uk\"\n</code></pre>"},{"location":"references/sources/api/#viadot.sources.uk_carbon_intensity.UKCarbonIntensity.query","title":"<code>query()</code>","text":"<p>Queries the API.</p> Source code in <code>src/viadot/sources/uk_carbon_intensity.py</code> <pre><code>def query(self) -&gt; None:\n    \"\"\"Queries the API.\"\"\"\n    ...\n</code></pre>"},{"location":"references/sources/api/#viadot.sources.uk_carbon_intensity.UKCarbonIntensity.to_df","title":"<code>to_df(if_empty='warn')</code>","text":"<p>Returns a pandas DataFrame with flattened data.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pandas.DataFrame: A Pandas DataFrame</p> Source code in <code>src/viadot/sources/uk_carbon_intensity.py</code> <pre><code>@add_viadot_metadata_columns\ndef to_df(self, if_empty: str = \"warn\") -&gt; pd.DataFrame:\n    \"\"\"Returns a pandas DataFrame with flattened data.\n\n    Returns:\n        pandas.DataFrame: A Pandas DataFrame\n    \"\"\"\n    from_ = []\n    to = []\n    forecast = []\n    actual = []\n    max_ = []\n    average = []\n    min_ = []\n    index = []\n    json_data = self.to_json()\n\n    if not json_data:\n        self._handle_if_empty(if_empty)\n\n    for row in json_data[\"data\"]:\n        from_.append(row[\"from\"])\n        to.append(row[\"to\"])\n        index.append(row[\"intensity\"][\"index\"])\n        try:\n            forecast.append(row[\"intensity\"][\"forecast\"])\n            actual.append(row[\"intensity\"][\"actual\"])\n            df = pd.DataFrame(\n                {\n                    \"from\": from_,\n                    \"to\": to,\n                    \"forecast\": forecast,\n                    \"actual\": actual,\n                    \"index\": index,\n                }\n            )\n        except KeyError:\n            max_.append(row[\"intensity\"][\"max\"])\n            average.append(row[\"intensity\"][\"average\"])\n            min_.append(row[\"intensity\"][\"min\"])\n            df = pd.DataFrame(\n                {\n                    \"from\": from_,\n                    \"to\": to,\n                    \"max\": max_,\n                    \"average\": average,\n                    \"min\": min_,\n                }\n            )\n    return df\n</code></pre>"},{"location":"references/sources/api/#viadot.sources.uk_carbon_intensity.UKCarbonIntensity.to_json","title":"<code>to_json()</code>","text":"<p>Creates json file.</p> Source code in <code>src/viadot/sources/uk_carbon_intensity.py</code> <pre><code>def to_json(self) -&gt; dict:\n    \"\"\"Creates json file.\"\"\"\n    url = f\"{self.API_ENDPOINT}{self.api_url}\"\n    headers = {\"Accept\": \"application/json\"}\n    response = requests.get(url, params={}, headers=headers, timeout=10)\n    if response.ok:\n        return response.json()\n    raise f\"Error {response.json()}\"\n</code></pre>"},{"location":"references/sources/api/#viadot.sources.vid_club.VidClub","title":"<code>viadot.sources.vid_club.VidClub</code>","text":"<p>               Bases: <code>Source</code></p> <p>A class implementing the Vid Club API.</p> <p>Documentation for this API is located at: https://evps01.envoo.net/vipapi/ There are 4 endpoints where to get the data.</p> Source code in <code>src/viadot/sources/vid_club.py</code> <pre><code>class VidClub(Source):\n    \"\"\"A class implementing the Vid Club API.\n\n    Documentation for this API is located at: https://evps01.envoo.net/vipapi/\n    There are 4 endpoints where to get the data.\n    \"\"\"\n\n    def __init__(\n        self,\n        *args,\n        endpoint: Literal[\"jobs\", \"product\", \"company\", \"survey\"] | None = None,\n        from_date: str = \"2022-03-22\",\n        to_date: str | None = None,\n        items_per_page: int = 100,\n        region: Literal[\"bg\", \"hu\", \"hr\", \"pl\", \"ro\", \"si\", \"all\"] | None = None,\n        days_interval: int = 30,\n        cols_to_drop: list[str] | None = None,\n        vid_club_credentials: dict[str, Any] | None = None,\n        validate_df_dict: dict | None = None,\n        timeout: int = 3600,\n        **kwargs,\n    ):\n        \"\"\"Create an instance of VidClub.\n\n        Args:\n            endpoint (Literal[\"jobs\", \"product\", \"company\", \"survey\"], optional): The\n                endpoint source to be accessed. Defaults to None.\n            from_date (str, optional): Start date for the query, by default is the\n                oldest date in the data 2022-03-22.\n            to_date (str, optional): End date for the query. By default None,\n                which will be executed as datetime.today().strftime(\"%Y-%m-%d\") in code.\n            items_per_page (int, optional): Number of entries per page. Defaults to 100.\n            region (Literal[\"bg\", \"hu\", \"hr\", \"pl\", \"ro\", \"si\", \"all\"], optional):\n                Region filter for the query. Defaults to None\n                (parameter is not used in url). [December 2023 status: value 'all'\n                does not work for company and jobs]\n            days_interval (int, optional): Days specified in date range per API call\n                (test showed that 30-40 is optimal for performance). Defaults to 30.\n            cols_to_drop (List[str], optional): List of columns to drop.\n                Defaults to None.\n            vid_club_credentials (Dict[str, Any], optional): Stores the credentials\n                information. Defaults to None.\n            validate_df_dict (dict, optional): A dictionary with optional list of tests\n                to verify the output\n                dataframe. If defined, triggers the `validate_df` task from task_utils.\n                Defaults to None.\n            timeout (int, optional): The time (in seconds) to wait while running this\n                task before a timeout occurs. Defaults to 3600.\n        \"\"\"\n        self.endpoint = endpoint\n        self.from_date = from_date\n        self.to_date = to_date\n        self.items_per_page = items_per_page\n        self.region = region\n        self.days_interval = days_interval\n        self.cols_to_drop = cols_to_drop\n        self.vid_club_credentials = vid_club_credentials\n        self.validate_df_dict = validate_df_dict\n        self.timeout = timeout\n\n        self.headers = {\n            \"Authorization\": \"Bearer \" + vid_club_credentials[\"token\"],\n            \"Content-Type\": \"application/json\",\n        }\n\n        super().__init__(credentials=vid_club_credentials, *args, **kwargs)  # noqa: B026\n\n    def build_query(\n        self,\n        from_date: str,\n        to_date: str,\n        api_url: str,\n        items_per_page: int,\n        endpoint: Literal[\"jobs\", \"product\", \"company\", \"survey\"] | None = None,\n        region: Literal[\"bg\", \"hu\", \"hr\", \"pl\", \"ro\", \"si\", \"all\"] | None = None,\n    ) -&gt; str:\n        \"\"\"Builds the query from the inputs.\n\n        Args:\n            from_date (str): Start date for the query.\n            to_date (str): End date for the query, if empty, will be executed as\n                datetime.today().strftime(\"%Y-%m-%d\").\n            api_url (str): Generic part of the URL to Vid Club API.\n            items_per_page (int): number of entries per page.\n            endpoint (Literal[\"jobs\", \"product\", \"company\", \"survey\"], optional):\n                The endpoint source to be accessed. Defaults to None.\n            region (Literal[\"bg\", \"hu\", \"hr\", \"pl\", \"ro\", \"si\", \"all\"], optional):\n                Region filter for the query. Defaults to None\n                (parameter is not used in url). [December 2023 status: value 'all'\n                does not work for company and jobs]\n\n        Returns:\n            str: Final query with all filters added.\n\n        Raises:\n            ValidationError: If any source different than the ones in the list are used.\n        \"\"\"\n        if endpoint in [\"jobs\", \"product\", \"company\"]:\n            region_url_string = f\"&amp;region={region}\" if region else \"\"\n            url = (\n                f\"\"\"{api_url}{endpoint}?from={from_date}&amp;to={to_date}\"\"\"\n                f\"\"\"{region_url_string}&amp;limit={items_per_page}\"\"\"\n            )\n        elif endpoint == \"survey\":\n            url = f\"{api_url}{endpoint}?language=en&amp;type=question\"\n        else:\n            msg = \"Pick one these sources: jobs, product, company, survey\"\n            raise ValidationError(msg)\n        return url\n\n    def intervals(\n        self, from_date: str, to_date: str, days_interval: int\n    ) -&gt; tuple[list[str], list[str]]:\n        \"\"\"Breaks dates range into smaller by provided days interval.\n\n        Args:\n            from_date (str): Start date for the query in \"%Y-%m-%d\" format.\n            to_date (str): End date for the query, if empty, will be executed as\n                datetime.today().strftime(\"%Y-%m-%d\").\n            days_interval (int): Days specified in date range per api call\n                (test showed that 30-40 is optimal for performance).\n\n        Returns:\n            List[str], List[str]: Starts and Ends lists that contains information\n                about date ranges for specific period and time interval.\n\n        Raises:\n            ValidationError: If the final date of the query is before the start date.\n        \"\"\"\n        if to_date is None:\n            to_date = datetime.today().strftime(\"%Y-%m-%d\")\n\n        end_date = datetime.strptime(to_date, \"%Y-%m-%d\").date()\n        start_date = datetime.strptime(from_date, \"%Y-%m-%d\").date()\n\n        from_date_obj = datetime.strptime(from_date, \"%Y-%m-%d\")\n\n        to_date_obj = datetime.strptime(to_date, \"%Y-%m-%d\")\n        delta = to_date_obj - from_date_obj\n\n        if delta.days &lt; 0:\n            msg = \"to_date cannot be earlier than from_date.\"\n            raise ValidationError(msg)\n\n        interval = timedelta(days=days_interval)\n        starts = []\n        ends = []\n\n        period_start = start_date\n        while period_start &lt; end_date:\n            period_end = min(period_start + interval, end_date)\n            starts.append(period_start.strftime(\"%Y-%m-%d\"))\n            ends.append(period_end.strftime(\"%Y-%m-%d\"))\n            period_start = period_end\n        if len(starts) == 0 and len(ends) == 0:\n            starts.append(from_date)\n            ends.append(to_date)\n        return starts, ends\n\n    def check_connection(\n        self,\n        endpoint: Literal[\"jobs\", \"product\", \"company\", \"survey\"] | None = None,\n        from_date: str = \"2022-03-22\",\n        to_date: str | None = None,\n        items_per_page: int = 100,\n        region: Literal[\"bg\", \"hu\", \"hr\", \"pl\", \"ro\", \"si\", \"all\"] | None = None,\n        url: str | None = None,\n    ) -&gt; tuple[dict[str, Any], str]:\n        \"\"\"Initiate first connection to API to retrieve piece of data.\n\n        With information about type of pagination in API URL.\n        This option is added because type of pagination for endpoints is being changed\n        in the future from page number to 'next' id.\n\n        Args:\n            endpoint (Literal[\"jobs\", \"product\", \"company\", \"survey\"], optional):\n                The endpoint source to be accessed. Defaults to None.\n            from_date (str, optional): Start date for the query, by default is the\n                oldest date in the data 2022-03-22.\n            to_date (str, optional): End date for the query. By default None,\n                which will be executed as datetime.today().strftime(\"%Y-%m-%d\") in code.\n            items_per_page (int, optional): Number of entries per page.\n                100 entries by default.\n            region (Literal[\"bg\", \"hu\", \"hr\", \"pl\", \"ro\", \"si\", \"all\"], optional):\n                Region filter for the query. Defaults to None\n                (parameter is not used in url). [December 2023 status: value 'all'\n                does not work for company and jobs]\n            url (str, optional): Generic part of the URL to Vid Club API.\n                Defaults to None.\n\n        Returns:\n            Tuple[Dict[str, Any], str]: Dictionary with first response from API with\n                JSON containing data and used URL string.\n\n        Raises:\n            ValidationError: If from_date is earlier than 2022-03-22.\n            ValidationError: If to_date is earlier than from_date.\n        \"\"\"\n        if from_date &lt; \"2022-03-22\":\n            msg = \"from_date cannot be earlier than 2022-03-22.\"\n            raise ValidationError(msg)\n\n        if to_date &lt; from_date:\n            msg = \"to_date cannot be earlier than from_date.\"\n            raise ValidationError(msg)\n\n        if url is None:\n            url = self.credentials[\"url\"]\n\n        first_url = self.build_query(\n            endpoint=endpoint,\n            from_date=from_date,\n            to_date=to_date,\n            api_url=url,\n            items_per_page=items_per_page,\n            region=region,\n        )\n        headers = self.headers\n        response = handle_api_response(url=first_url, headers=headers, method=\"GET\")\n        response = response.json()\n        return (response, first_url)\n\n    def get_response(\n        self,\n        endpoint: Literal[\"jobs\", \"product\", \"company\", \"survey\"] | None = None,\n        from_date: str = \"2022-03-22\",\n        to_date: str | None = None,\n        items_per_page: int = 100,\n        region: Literal[\"bg\", \"hu\", \"hr\", \"pl\", \"ro\", \"si\", \"all\"] | None = None,\n    ) -&gt; pd.DataFrame:\n        \"\"\"Basing on the pagination type retrieved using check_connection function.\n\n        It gets the response from the API queried and transforms it into DataFrame.\n\n        Args:\n            endpoint (Literal[\"jobs\", \"product\", \"company\", \"survey\"], optional):\n                The endpoint source to be accessed. Defaults to None.\n            from_date (str, optional): Start date for the query, by default is the\n                oldest date in the data 2022-03-22.\n            to_date (str, optional): End date for the query. By default None,\n                which will be executed as datetime.today().strftime(\"%Y-%m-%d\") in code.\n            items_per_page (int, optional): Number of entries per page.\n                100 entries by default.\n            region (Literal[\"bg\", \"hu\", \"hr\", \"pl\", \"ro\", \"si\", \"all\"], optional):\n                Region filter for the query. Defaults to None\n                (parameter is not used in url). [December 2023 status: value 'all'\n                does not work for company and jobs]\n\n        Returns:\n            pd.DataFrame: Table of the data carried in the response.\n\n        Raises:\n            ValidationError: If any source different than the ones in the list are used.\n        \"\"\"\n        headers = self.headers\n        if endpoint not in [\"jobs\", \"product\", \"company\", \"survey\"]:\n            msg = \"The source has to be: jobs, product, company or survey\"\n            raise ValidationError(msg)\n        if to_date is None:\n            to_date = datetime.today().strftime(\"%Y-%m-%d\")\n\n        response, first_url = self.check_connection(\n            endpoint=endpoint,\n            from_date=from_date,\n            to_date=to_date,\n            items_per_page=items_per_page,\n            region=region,\n        )\n\n        if isinstance(response, dict):\n            keys_list = list(response.keys())\n        elif isinstance(response, list):\n            keys_list = list(response[0].keys())\n        else:\n            keys_list = []\n\n        ind = \"next\" in keys_list\n\n        if \"data\" in keys_list:\n            df = pd.json_normalize(response[\"data\"])\n            df = pd.DataFrame(df)\n            length = df.shape[0]\n            page = 1\n\n            while length == items_per_page:\n                if ind is True:\n                    next_page = response[\"next\"]\n                    url = f\"{first_url}&amp;next={next_page}\"\n                else:\n                    page += 1\n                    url = f\"{first_url}&amp;page={page}\"\n                response_api = handle_api_response(\n                    url=url, headers=headers, method=\"GET\"\n                )\n                response = response_api.json()\n                df_page = pd.json_normalize(response[\"data\"])\n                df_page = pd.DataFrame(df_page)\n                if endpoint == \"product\":\n                    df_page = df_page.transpose()\n                length = df_page.shape[0]\n                df = pd.concat((df, df_page), axis=0)\n        else:\n            df = pd.DataFrame(response)\n\n        return df\n\n    def to_df(\n        self,\n        if_empty: Literal[\"warn\", \"skip\", \"fail\"] = \"warn\",\n    ) -&gt; pd.DataFrame:\n        \"\"\"Looping get_response and iterating by date ranges defined in intervals.\n\n        Stores outputs as DataFrames in a list. At the end, daframes are concatenated\n        in one and dropped duplicates that would appear when quering.\n\n        Args:\n            if_empty (Literal[\"warn\", \"skip\", \"fail\"], optional): What to do if a fetch\n                produce no data. Defaults to \"warn\n\n        Returns:\n            pd.DataFrame: Dataframe of the concatanated data carried in the responses.\n        \"\"\"\n        starts, ends = self.intervals(\n            from_date=self.from_date,\n            to_date=self.to_date,\n            days_interval=self.days_interval,\n        )\n\n        dfs_list = []\n        if len(starts) &gt; 0 and len(ends) &gt; 0:\n            for start, end in zip(starts, ends, strict=False):\n                self.logger.info(f\"ingesting data for dates [{start}]-[{end}]...\")\n                df = self.get_response(\n                    endpoint=self.endpoint,\n                    from_date=start,\n                    to_date=end,\n                    items_per_page=self.items_per_page,\n                    region=self.region,\n                )\n                dfs_list.append(df)\n                if len(dfs_list) &gt; 1:\n                    df = pd.concat(dfs_list, axis=0, ignore_index=True)\n                else:\n                    df = pd.DataFrame(dfs_list[0])\n        else:\n            df = self.get_response(\n                endpoint=self.endpoint,\n                from_date=self.from_date,\n                to_date=self.to_date,\n                items_per_page=self.items_per_page,\n                region=self.region,\n            )\n        list_columns = df.columns[df.map(lambda x: isinstance(x, list)).any()].tolist()\n        for i in list_columns:\n            df[i] = df[i].apply(lambda x: tuple(x) if isinstance(x, list) else x)\n        df.drop_duplicates(inplace=True)\n\n        if self.cols_to_drop is not None:\n            if isinstance(self.cols_to_drop, list):\n                try:\n                    self.logger.info(\n                        f\"Dropping following columns: {self.cols_to_drop}...\"\n                    )\n                    df.drop(columns=self.cols_to_drop, inplace=True, errors=\"raise\")\n                except KeyError:\n                    self.logger.exception(\n                        f\"\"\"Column(s): {self.cols_to_drop} don't exist in the DataFrame.\n                        No columns were dropped. Returning full DataFrame...\"\"\"\n                    )\n                    self.logger.info(f\"Existing columns: {df.columns}\")\n            else:\n                msg = \"Provide columns to drop in a List.\"\n                raise TypeError(msg)\n\n        if df.empty:\n            self._handle_if_empty(if_empty=if_empty)\n\n        return df\n</code></pre>"},{"location":"references/sources/api/#viadot.sources.vid_club.VidClub.__init__","title":"<code>__init__(*args, endpoint=None, from_date='2022-03-22', to_date=None, items_per_page=100, region=None, days_interval=30, cols_to_drop=None, vid_club_credentials=None, validate_df_dict=None, timeout=3600, **kwargs)</code>","text":"<p>Create an instance of VidClub.</p> <p>Parameters:</p> Name Type Description Default <code>endpoint</code> <code>Literal['jobs', 'product', 'company', 'survey']</code> <p>The endpoint source to be accessed. Defaults to None.</p> <code>None</code> <code>from_date</code> <code>str</code> <p>Start date for the query, by default is the oldest date in the data 2022-03-22.</p> <code>'2022-03-22'</code> <code>to_date</code> <code>str</code> <p>End date for the query. By default None, which will be executed as datetime.today().strftime(\"%Y-%m-%d\") in code.</p> <code>None</code> <code>items_per_page</code> <code>int</code> <p>Number of entries per page. Defaults to 100.</p> <code>100</code> <code>region</code> <code>Literal['bg', 'hu', 'hr', 'pl', 'ro', 'si', 'all']</code> <p>Region filter for the query. Defaults to None (parameter is not used in url). [December 2023 status: value 'all' does not work for company and jobs]</p> <code>None</code> <code>days_interval</code> <code>int</code> <p>Days specified in date range per API call (test showed that 30-40 is optimal for performance). Defaults to 30.</p> <code>30</code> <code>cols_to_drop</code> <code>List[str]</code> <p>List of columns to drop. Defaults to None.</p> <code>None</code> <code>vid_club_credentials</code> <code>Dict[str, Any]</code> <p>Stores the credentials information. Defaults to None.</p> <code>None</code> <code>validate_df_dict</code> <code>dict</code> <p>A dictionary with optional list of tests to verify the output dataframe. If defined, triggers the <code>validate_df</code> task from task_utils. Defaults to None.</p> <code>None</code> <code>timeout</code> <code>int</code> <p>The time (in seconds) to wait while running this task before a timeout occurs. Defaults to 3600.</p> <code>3600</code> Source code in <code>src/viadot/sources/vid_club.py</code> <pre><code>def __init__(\n    self,\n    *args,\n    endpoint: Literal[\"jobs\", \"product\", \"company\", \"survey\"] | None = None,\n    from_date: str = \"2022-03-22\",\n    to_date: str | None = None,\n    items_per_page: int = 100,\n    region: Literal[\"bg\", \"hu\", \"hr\", \"pl\", \"ro\", \"si\", \"all\"] | None = None,\n    days_interval: int = 30,\n    cols_to_drop: list[str] | None = None,\n    vid_club_credentials: dict[str, Any] | None = None,\n    validate_df_dict: dict | None = None,\n    timeout: int = 3600,\n    **kwargs,\n):\n    \"\"\"Create an instance of VidClub.\n\n    Args:\n        endpoint (Literal[\"jobs\", \"product\", \"company\", \"survey\"], optional): The\n            endpoint source to be accessed. Defaults to None.\n        from_date (str, optional): Start date for the query, by default is the\n            oldest date in the data 2022-03-22.\n        to_date (str, optional): End date for the query. By default None,\n            which will be executed as datetime.today().strftime(\"%Y-%m-%d\") in code.\n        items_per_page (int, optional): Number of entries per page. Defaults to 100.\n        region (Literal[\"bg\", \"hu\", \"hr\", \"pl\", \"ro\", \"si\", \"all\"], optional):\n            Region filter for the query. Defaults to None\n            (parameter is not used in url). [December 2023 status: value 'all'\n            does not work for company and jobs]\n        days_interval (int, optional): Days specified in date range per API call\n            (test showed that 30-40 is optimal for performance). Defaults to 30.\n        cols_to_drop (List[str], optional): List of columns to drop.\n            Defaults to None.\n        vid_club_credentials (Dict[str, Any], optional): Stores the credentials\n            information. Defaults to None.\n        validate_df_dict (dict, optional): A dictionary with optional list of tests\n            to verify the output\n            dataframe. If defined, triggers the `validate_df` task from task_utils.\n            Defaults to None.\n        timeout (int, optional): The time (in seconds) to wait while running this\n            task before a timeout occurs. Defaults to 3600.\n    \"\"\"\n    self.endpoint = endpoint\n    self.from_date = from_date\n    self.to_date = to_date\n    self.items_per_page = items_per_page\n    self.region = region\n    self.days_interval = days_interval\n    self.cols_to_drop = cols_to_drop\n    self.vid_club_credentials = vid_club_credentials\n    self.validate_df_dict = validate_df_dict\n    self.timeout = timeout\n\n    self.headers = {\n        \"Authorization\": \"Bearer \" + vid_club_credentials[\"token\"],\n        \"Content-Type\": \"application/json\",\n    }\n\n    super().__init__(credentials=vid_club_credentials, *args, **kwargs)  # noqa: B026\n</code></pre>"},{"location":"references/sources/api/#viadot.sources.vid_club.VidClub.build_query","title":"<code>build_query(from_date, to_date, api_url, items_per_page, endpoint=None, region=None)</code>","text":"<p>Builds the query from the inputs.</p> <p>Parameters:</p> Name Type Description Default <code>from_date</code> <code>str</code> <p>Start date for the query.</p> required <code>to_date</code> <code>str</code> <p>End date for the query, if empty, will be executed as datetime.today().strftime(\"%Y-%m-%d\").</p> required <code>api_url</code> <code>str</code> <p>Generic part of the URL to Vid Club API.</p> required <code>items_per_page</code> <code>int</code> <p>number of entries per page.</p> required <code>endpoint</code> <code>Literal['jobs', 'product', 'company', 'survey']</code> <p>The endpoint source to be accessed. Defaults to None.</p> <code>None</code> <code>region</code> <code>Literal['bg', 'hu', 'hr', 'pl', 'ro', 'si', 'all']</code> <p>Region filter for the query. Defaults to None (parameter is not used in url). [December 2023 status: value 'all' does not work for company and jobs]</p> <code>None</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Final query with all filters added.</p> <p>Raises:</p> Type Description <code>ValidationError</code> <p>If any source different than the ones in the list are used.</p> Source code in <code>src/viadot/sources/vid_club.py</code> <pre><code>def build_query(\n    self,\n    from_date: str,\n    to_date: str,\n    api_url: str,\n    items_per_page: int,\n    endpoint: Literal[\"jobs\", \"product\", \"company\", \"survey\"] | None = None,\n    region: Literal[\"bg\", \"hu\", \"hr\", \"pl\", \"ro\", \"si\", \"all\"] | None = None,\n) -&gt; str:\n    \"\"\"Builds the query from the inputs.\n\n    Args:\n        from_date (str): Start date for the query.\n        to_date (str): End date for the query, if empty, will be executed as\n            datetime.today().strftime(\"%Y-%m-%d\").\n        api_url (str): Generic part of the URL to Vid Club API.\n        items_per_page (int): number of entries per page.\n        endpoint (Literal[\"jobs\", \"product\", \"company\", \"survey\"], optional):\n            The endpoint source to be accessed. Defaults to None.\n        region (Literal[\"bg\", \"hu\", \"hr\", \"pl\", \"ro\", \"si\", \"all\"], optional):\n            Region filter for the query. Defaults to None\n            (parameter is not used in url). [December 2023 status: value 'all'\n            does not work for company and jobs]\n\n    Returns:\n        str: Final query with all filters added.\n\n    Raises:\n        ValidationError: If any source different than the ones in the list are used.\n    \"\"\"\n    if endpoint in [\"jobs\", \"product\", \"company\"]:\n        region_url_string = f\"&amp;region={region}\" if region else \"\"\n        url = (\n            f\"\"\"{api_url}{endpoint}?from={from_date}&amp;to={to_date}\"\"\"\n            f\"\"\"{region_url_string}&amp;limit={items_per_page}\"\"\"\n        )\n    elif endpoint == \"survey\":\n        url = f\"{api_url}{endpoint}?language=en&amp;type=question\"\n    else:\n        msg = \"Pick one these sources: jobs, product, company, survey\"\n        raise ValidationError(msg)\n    return url\n</code></pre>"},{"location":"references/sources/api/#viadot.sources.vid_club.VidClub.check_connection","title":"<code>check_connection(endpoint=None, from_date='2022-03-22', to_date=None, items_per_page=100, region=None, url=None)</code>","text":"<p>Initiate first connection to API to retrieve piece of data.</p> <p>With information about type of pagination in API URL. This option is added because type of pagination for endpoints is being changed in the future from page number to 'next' id.</p> <p>Parameters:</p> Name Type Description Default <code>endpoint</code> <code>Literal['jobs', 'product', 'company', 'survey']</code> <p>The endpoint source to be accessed. Defaults to None.</p> <code>None</code> <code>from_date</code> <code>str</code> <p>Start date for the query, by default is the oldest date in the data 2022-03-22.</p> <code>'2022-03-22'</code> <code>to_date</code> <code>str</code> <p>End date for the query. By default None, which will be executed as datetime.today().strftime(\"%Y-%m-%d\") in code.</p> <code>None</code> <code>items_per_page</code> <code>int</code> <p>Number of entries per page. 100 entries by default.</p> <code>100</code> <code>region</code> <code>Literal['bg', 'hu', 'hr', 'pl', 'ro', 'si', 'all']</code> <p>Region filter for the query. Defaults to None (parameter is not used in url). [December 2023 status: value 'all' does not work for company and jobs]</p> <code>None</code> <code>url</code> <code>str</code> <p>Generic part of the URL to Vid Club API. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple[dict[str, Any], str]</code> <p>Tuple[Dict[str, Any], str]: Dictionary with first response from API with JSON containing data and used URL string.</p> <p>Raises:</p> Type Description <code>ValidationError</code> <p>If from_date is earlier than 2022-03-22.</p> <code>ValidationError</code> <p>If to_date is earlier than from_date.</p> Source code in <code>src/viadot/sources/vid_club.py</code> <pre><code>def check_connection(\n    self,\n    endpoint: Literal[\"jobs\", \"product\", \"company\", \"survey\"] | None = None,\n    from_date: str = \"2022-03-22\",\n    to_date: str | None = None,\n    items_per_page: int = 100,\n    region: Literal[\"bg\", \"hu\", \"hr\", \"pl\", \"ro\", \"si\", \"all\"] | None = None,\n    url: str | None = None,\n) -&gt; tuple[dict[str, Any], str]:\n    \"\"\"Initiate first connection to API to retrieve piece of data.\n\n    With information about type of pagination in API URL.\n    This option is added because type of pagination for endpoints is being changed\n    in the future from page number to 'next' id.\n\n    Args:\n        endpoint (Literal[\"jobs\", \"product\", \"company\", \"survey\"], optional):\n            The endpoint source to be accessed. Defaults to None.\n        from_date (str, optional): Start date for the query, by default is the\n            oldest date in the data 2022-03-22.\n        to_date (str, optional): End date for the query. By default None,\n            which will be executed as datetime.today().strftime(\"%Y-%m-%d\") in code.\n        items_per_page (int, optional): Number of entries per page.\n            100 entries by default.\n        region (Literal[\"bg\", \"hu\", \"hr\", \"pl\", \"ro\", \"si\", \"all\"], optional):\n            Region filter for the query. Defaults to None\n            (parameter is not used in url). [December 2023 status: value 'all'\n            does not work for company and jobs]\n        url (str, optional): Generic part of the URL to Vid Club API.\n            Defaults to None.\n\n    Returns:\n        Tuple[Dict[str, Any], str]: Dictionary with first response from API with\n            JSON containing data and used URL string.\n\n    Raises:\n        ValidationError: If from_date is earlier than 2022-03-22.\n        ValidationError: If to_date is earlier than from_date.\n    \"\"\"\n    if from_date &lt; \"2022-03-22\":\n        msg = \"from_date cannot be earlier than 2022-03-22.\"\n        raise ValidationError(msg)\n\n    if to_date &lt; from_date:\n        msg = \"to_date cannot be earlier than from_date.\"\n        raise ValidationError(msg)\n\n    if url is None:\n        url = self.credentials[\"url\"]\n\n    first_url = self.build_query(\n        endpoint=endpoint,\n        from_date=from_date,\n        to_date=to_date,\n        api_url=url,\n        items_per_page=items_per_page,\n        region=region,\n    )\n    headers = self.headers\n    response = handle_api_response(url=first_url, headers=headers, method=\"GET\")\n    response = response.json()\n    return (response, first_url)\n</code></pre>"},{"location":"references/sources/api/#viadot.sources.vid_club.VidClub.get_response","title":"<code>get_response(endpoint=None, from_date='2022-03-22', to_date=None, items_per_page=100, region=None)</code>","text":"<p>Basing on the pagination type retrieved using check_connection function.</p> <p>It gets the response from the API queried and transforms it into DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>endpoint</code> <code>Literal['jobs', 'product', 'company', 'survey']</code> <p>The endpoint source to be accessed. Defaults to None.</p> <code>None</code> <code>from_date</code> <code>str</code> <p>Start date for the query, by default is the oldest date in the data 2022-03-22.</p> <code>'2022-03-22'</code> <code>to_date</code> <code>str</code> <p>End date for the query. By default None, which will be executed as datetime.today().strftime(\"%Y-%m-%d\") in code.</p> <code>None</code> <code>items_per_page</code> <code>int</code> <p>Number of entries per page. 100 entries by default.</p> <code>100</code> <code>region</code> <code>Literal['bg', 'hu', 'hr', 'pl', 'ro', 'si', 'all']</code> <p>Region filter for the query. Defaults to None (parameter is not used in url). [December 2023 status: value 'all' does not work for company and jobs]</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Table of the data carried in the response.</p> <p>Raises:</p> Type Description <code>ValidationError</code> <p>If any source different than the ones in the list are used.</p> Source code in <code>src/viadot/sources/vid_club.py</code> <pre><code>def get_response(\n    self,\n    endpoint: Literal[\"jobs\", \"product\", \"company\", \"survey\"] | None = None,\n    from_date: str = \"2022-03-22\",\n    to_date: str | None = None,\n    items_per_page: int = 100,\n    region: Literal[\"bg\", \"hu\", \"hr\", \"pl\", \"ro\", \"si\", \"all\"] | None = None,\n) -&gt; pd.DataFrame:\n    \"\"\"Basing on the pagination type retrieved using check_connection function.\n\n    It gets the response from the API queried and transforms it into DataFrame.\n\n    Args:\n        endpoint (Literal[\"jobs\", \"product\", \"company\", \"survey\"], optional):\n            The endpoint source to be accessed. Defaults to None.\n        from_date (str, optional): Start date for the query, by default is the\n            oldest date in the data 2022-03-22.\n        to_date (str, optional): End date for the query. By default None,\n            which will be executed as datetime.today().strftime(\"%Y-%m-%d\") in code.\n        items_per_page (int, optional): Number of entries per page.\n            100 entries by default.\n        region (Literal[\"bg\", \"hu\", \"hr\", \"pl\", \"ro\", \"si\", \"all\"], optional):\n            Region filter for the query. Defaults to None\n            (parameter is not used in url). [December 2023 status: value 'all'\n            does not work for company and jobs]\n\n    Returns:\n        pd.DataFrame: Table of the data carried in the response.\n\n    Raises:\n        ValidationError: If any source different than the ones in the list are used.\n    \"\"\"\n    headers = self.headers\n    if endpoint not in [\"jobs\", \"product\", \"company\", \"survey\"]:\n        msg = \"The source has to be: jobs, product, company or survey\"\n        raise ValidationError(msg)\n    if to_date is None:\n        to_date = datetime.today().strftime(\"%Y-%m-%d\")\n\n    response, first_url = self.check_connection(\n        endpoint=endpoint,\n        from_date=from_date,\n        to_date=to_date,\n        items_per_page=items_per_page,\n        region=region,\n    )\n\n    if isinstance(response, dict):\n        keys_list = list(response.keys())\n    elif isinstance(response, list):\n        keys_list = list(response[0].keys())\n    else:\n        keys_list = []\n\n    ind = \"next\" in keys_list\n\n    if \"data\" in keys_list:\n        df = pd.json_normalize(response[\"data\"])\n        df = pd.DataFrame(df)\n        length = df.shape[0]\n        page = 1\n\n        while length == items_per_page:\n            if ind is True:\n                next_page = response[\"next\"]\n                url = f\"{first_url}&amp;next={next_page}\"\n            else:\n                page += 1\n                url = f\"{first_url}&amp;page={page}\"\n            response_api = handle_api_response(\n                url=url, headers=headers, method=\"GET\"\n            )\n            response = response_api.json()\n            df_page = pd.json_normalize(response[\"data\"])\n            df_page = pd.DataFrame(df_page)\n            if endpoint == \"product\":\n                df_page = df_page.transpose()\n            length = df_page.shape[0]\n            df = pd.concat((df, df_page), axis=0)\n    else:\n        df = pd.DataFrame(response)\n\n    return df\n</code></pre>"},{"location":"references/sources/api/#viadot.sources.vid_club.VidClub.intervals","title":"<code>intervals(from_date, to_date, days_interval)</code>","text":"<p>Breaks dates range into smaller by provided days interval.</p> <p>Parameters:</p> Name Type Description Default <code>from_date</code> <code>str</code> <p>Start date for the query in \"%Y-%m-%d\" format.</p> required <code>to_date</code> <code>str</code> <p>End date for the query, if empty, will be executed as datetime.today().strftime(\"%Y-%m-%d\").</p> required <code>days_interval</code> <code>int</code> <p>Days specified in date range per api call (test showed that 30-40 is optimal for performance).</p> required <p>Returns:</p> Type Description <code>tuple[list[str], list[str]]</code> <p>List[str], List[str]: Starts and Ends lists that contains information about date ranges for specific period and time interval.</p> <p>Raises:</p> Type Description <code>ValidationError</code> <p>If the final date of the query is before the start date.</p> Source code in <code>src/viadot/sources/vid_club.py</code> <pre><code>def intervals(\n    self, from_date: str, to_date: str, days_interval: int\n) -&gt; tuple[list[str], list[str]]:\n    \"\"\"Breaks dates range into smaller by provided days interval.\n\n    Args:\n        from_date (str): Start date for the query in \"%Y-%m-%d\" format.\n        to_date (str): End date for the query, if empty, will be executed as\n            datetime.today().strftime(\"%Y-%m-%d\").\n        days_interval (int): Days specified in date range per api call\n            (test showed that 30-40 is optimal for performance).\n\n    Returns:\n        List[str], List[str]: Starts and Ends lists that contains information\n            about date ranges for specific period and time interval.\n\n    Raises:\n        ValidationError: If the final date of the query is before the start date.\n    \"\"\"\n    if to_date is None:\n        to_date = datetime.today().strftime(\"%Y-%m-%d\")\n\n    end_date = datetime.strptime(to_date, \"%Y-%m-%d\").date()\n    start_date = datetime.strptime(from_date, \"%Y-%m-%d\").date()\n\n    from_date_obj = datetime.strptime(from_date, \"%Y-%m-%d\")\n\n    to_date_obj = datetime.strptime(to_date, \"%Y-%m-%d\")\n    delta = to_date_obj - from_date_obj\n\n    if delta.days &lt; 0:\n        msg = \"to_date cannot be earlier than from_date.\"\n        raise ValidationError(msg)\n\n    interval = timedelta(days=days_interval)\n    starts = []\n    ends = []\n\n    period_start = start_date\n    while period_start &lt; end_date:\n        period_end = min(period_start + interval, end_date)\n        starts.append(period_start.strftime(\"%Y-%m-%d\"))\n        ends.append(period_end.strftime(\"%Y-%m-%d\"))\n        period_start = period_end\n    if len(starts) == 0 and len(ends) == 0:\n        starts.append(from_date)\n        ends.append(to_date)\n    return starts, ends\n</code></pre>"},{"location":"references/sources/api/#viadot.sources.vid_club.VidClub.to_df","title":"<code>to_df(if_empty='warn')</code>","text":"<p>Looping get_response and iterating by date ranges defined in intervals.</p> <p>Stores outputs as DataFrames in a list. At the end, daframes are concatenated in one and dropped duplicates that would appear when quering.</p> <p>Parameters:</p> Name Type Description Default <code>if_empty</code> <code>Literal['warn', 'skip', 'fail']</code> <p>What to do if a fetch produce no data. Defaults to \"warn</p> <code>'warn'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Dataframe of the concatanated data carried in the responses.</p> Source code in <code>src/viadot/sources/vid_club.py</code> <pre><code>def to_df(\n    self,\n    if_empty: Literal[\"warn\", \"skip\", \"fail\"] = \"warn\",\n) -&gt; pd.DataFrame:\n    \"\"\"Looping get_response and iterating by date ranges defined in intervals.\n\n    Stores outputs as DataFrames in a list. At the end, daframes are concatenated\n    in one and dropped duplicates that would appear when quering.\n\n    Args:\n        if_empty (Literal[\"warn\", \"skip\", \"fail\"], optional): What to do if a fetch\n            produce no data. Defaults to \"warn\n\n    Returns:\n        pd.DataFrame: Dataframe of the concatanated data carried in the responses.\n    \"\"\"\n    starts, ends = self.intervals(\n        from_date=self.from_date,\n        to_date=self.to_date,\n        days_interval=self.days_interval,\n    )\n\n    dfs_list = []\n    if len(starts) &gt; 0 and len(ends) &gt; 0:\n        for start, end in zip(starts, ends, strict=False):\n            self.logger.info(f\"ingesting data for dates [{start}]-[{end}]...\")\n            df = self.get_response(\n                endpoint=self.endpoint,\n                from_date=start,\n                to_date=end,\n                items_per_page=self.items_per_page,\n                region=self.region,\n            )\n            dfs_list.append(df)\n            if len(dfs_list) &gt; 1:\n                df = pd.concat(dfs_list, axis=0, ignore_index=True)\n            else:\n                df = pd.DataFrame(dfs_list[0])\n    else:\n        df = self.get_response(\n            endpoint=self.endpoint,\n            from_date=self.from_date,\n            to_date=self.to_date,\n            items_per_page=self.items_per_page,\n            region=self.region,\n        )\n    list_columns = df.columns[df.map(lambda x: isinstance(x, list)).any()].tolist()\n    for i in list_columns:\n        df[i] = df[i].apply(lambda x: tuple(x) if isinstance(x, list) else x)\n    df.drop_duplicates(inplace=True)\n\n    if self.cols_to_drop is not None:\n        if isinstance(self.cols_to_drop, list):\n            try:\n                self.logger.info(\n                    f\"Dropping following columns: {self.cols_to_drop}...\"\n                )\n                df.drop(columns=self.cols_to_drop, inplace=True, errors=\"raise\")\n            except KeyError:\n                self.logger.exception(\n                    f\"\"\"Column(s): {self.cols_to_drop} don't exist in the DataFrame.\n                    No columns were dropped. Returning full DataFrame...\"\"\"\n                )\n                self.logger.info(f\"Existing columns: {df.columns}\")\n        else:\n            msg = \"Provide columns to drop in a List.\"\n            raise TypeError(msg)\n\n    if df.empty:\n        self._handle_if_empty(if_empty=if_empty)\n\n    return df\n</code></pre>"},{"location":"references/sources/database/","title":"Database sources","text":""},{"location":"references/sources/database/#viadot.sources.azure_data_lake.AzureDataLake","title":"<code>viadot.sources.azure_data_lake.AzureDataLake</code>","text":"<p>               Bases: <code>Source</code></p> Source code in <code>src/viadot/sources/azure_data_lake.py</code> <pre><code>class AzureDataLake(Source):\n    def __init__(\n        self,\n        path: str | None = None,\n        gen: int = 2,\n        credentials: dict[str, Any] | None = None,\n        config_key: str | None = None,\n        *args,\n        **kwargs,\n    ):\n        \"\"\"A class for pulling data from the Azure Data Lakes (gen1 and gen2).\n\n        You can either connect to the lake in general:\n        lake = AzureDataLake(); lake.exists(\"a/b/c.csv\")\n\n        or to a particular path:\n        lake = AzureDataLake(path=\"a/b/c.csv\"); lake.exists()`\n\n        Args:\n        credentials (Dict[str, Any], optional): A dictionary containing\n            the following credentials: `account_name`, `tenant_id`,\n            `client_id`, and `client_secret`.\n        config_key (str, optional): The key in the viadot config holding\n            relevant credentials.\n        \"\"\"\n        credentials = credentials or get_source_credentials(config_key)\n        credentials = {key.lower(): value for key, value in credentials.items()}\n\n        required_credentials = (\n            \"account_name\",\n            \"azure_tenant_id\",\n            \"azure_client_id\",\n            \"azure_client_secret\",\n        )\n        required_credentials_are_provided = all(\n            rc in credentials for rc in required_credentials\n        )\n\n        if credentials is None:\n            msg = \"Please provide the credentials.\"\n            raise CredentialError(msg)\n\n        if not required_credentials_are_provided:\n            msg = \"Please provide all required credentials.\"\n            raise CredentialError(msg)\n\n        super().__init__(*args, credentials=credentials, **kwargs)\n\n        storage_account_name = self.credentials[\"account_name\"]\n        tenant_id = self.credentials[\"azure_tenant_id\"]\n        client_id = self.credentials[\"azure_client_id\"]\n        client_secret = self.credentials[\"azure_client_secret\"]\n\n        self.path = path\n        self.gen = gen\n        self.storage_options = {\n            \"azure_tenant_id\": tenant_id,\n            \"azure_client_id\": client_id,\n            \"azure_client_secret\": client_secret,\n        }\n        if gen == 1:\n            self.fs = AzureDatalakeFileSystem(\n                store_name=storage_account_name,\n                tenant_id=tenant_id,\n                client_id=client_id,\n                client_secret=client_secret,\n            )\n            self.base_url = f\"adl://{storage_account_name}\"\n        elif gen == 2:  # noqa: PLR2004\n            self.storage_options[\"account_name\"] = storage_account_name\n            self.fs = AzureBlobFileSystem(\n                account_name=storage_account_name,\n                tenant_id=tenant_id,\n                client_id=client_id,\n                client_secret=client_secret,\n            )\n            self.base_url = \"az://\"\n\n    def upload(\n        self,\n        from_path: str,\n        to_path: str | None = None,\n        recursive: bool = False,\n        overwrite: bool = False,\n    ) -&gt; None:\n        \"\"\"Upload file(s) to the lake.\n\n        Args:\n            from_path (str): Path to the local file(s) to be uploaded.\n            to_path (str): Path to the destination file/folder\n            recursive (bool): Set this to true if working with directories.\n            overwrite (bool): Whether to overwrite the file(s) if they exist.\n\n        Example:\n        ```python\n        from viadot.sources import AzureDataLake\n\n        lake = AzureDataLake()\n        lake.upload(from_path='tests/test.csv', to_path=\"sandbox/test.csv\")\n        ```\n        \"\"\"\n        if self.gen == 1:\n            msg = \"Azure Data Lake Gen1 does not support simple file upload.\"\n            raise NotImplementedError(msg)\n\n        to_path = to_path or self.path\n\n        self.logger.info(f\"Uploading file(s) from '{from_path}' to '{to_path}'...\")\n\n        try:\n            self.fs.upload(\n                lpath=from_path,\n                rpath=to_path,\n                recursive=recursive,\n                overwrite=overwrite,\n            )\n        except FileExistsError as e:\n            if recursive:\n                msg = f\"At least one file in '{to_path}' already exists. Specify `overwrite=True` to overwrite.\"\n            else:\n                msg = f\"The file '{to_path}' already exists. Specify `overwrite=True` to overwrite.\"\n            raise FileExistsError(msg) from e\n\n        self.logger.info(\n            f\"Successfully uploaded file(s) from '{from_path}' to '{to_path}'.\"\n        )\n\n    def exists(self, path: str | None = None) -&gt; bool:\n        \"\"\"Check if a location exists in Azure Data Lake.\n\n        Args:\n            path (str): The path to check. Can be a file or a directory.\n\n        Example:\n        ```python\n        from viadot.sources import AzureDataLake\n\n        lake = AzureDataLake(gen=1)\n        lake.exists(\"tests/test.csv\")\n        ```\n\n        Returns:\n            bool: Whether the paths exists.\n        \"\"\"\n        path = path or self.path\n        return self.fs.exists(path)\n\n    def download(\n        self,\n        to_path: str,\n        from_path: str | None = None,\n        recursive: bool = False,\n        overwrite: bool = True,\n    ) -&gt; None:\n        \"\"\"Download file(s) from the lake.\n\n        Args:\n            to_path (str): _description_\n            from_path (str | None, optional): _description_. Defaults to None.\n            recursive (bool, optional): _description_. Defaults to False.\n            overwrite (bool, optional): _description_. Defaults to True.\n\n        Raises:\n            NotImplementedError: _description_\n        \"\"\"\n        if overwrite is False:\n            msg = \"Currently, only the default behavior (overwrite) is available.\"\n            raise NotImplementedError(msg)\n\n        from_path = from_path or self.path\n        self.fs.download(rpath=from_path, lpath=to_path, recursive=recursive)\n\n    @add_viadot_metadata_columns\n    def to_df(\n        self,\n        path: str | None = None,\n        sep: str = \"\\t\",\n        quoting: int = 0,\n        lineterminator: str | None = None,\n        error_bad_lines: bool | None = None,\n    ) -&gt; pd.DataFrame:\n        r\"\"\"Download a file from the lake and return it as a pandas DataFrame.\n\n        Args:\n            path (str, optional): _description_. Defaults to None.\n            sep (str, optional): _description_. Defaults to \"\\t\".\n            quoting (int, optional): _description_. Defaults to 0.\n            lineterminator (str, optional): _description_. Defaults to None.\n            error_bad_lines (bool, optional): _description_. Defaults to None.\n\n        Raises:\n            ValueError: _description_\n\n        Returns:\n            _type_: _description_\n        \"\"\"\n        if quoting is None:\n            quoting = 0\n\n        path = path or self.path\n        url = self.base_url + \"/\" + path.strip(\"/\")\n\n        if url.endswith(\".csv\"):\n            df = pd.read_csv(\n                url,\n                storage_options=self.storage_options,\n                sep=sep,\n                quoting=quoting,\n                lineterminator=lineterminator,\n                error_bad_lines=error_bad_lines,\n            )\n        elif url.endswith(\".parquet\"):\n            df = pd.read_parquet(url, storage_options=self.storage_options)\n        else:\n            msg = \"Only CSV and parquet formats are supported.\"\n            raise ValueError(msg)\n\n        return df\n\n    def ls(self, path: str | None = None) -&gt; list[str]:\n        \"\"\"Returns a list of files in a path.\n\n        Args:\n            path (str, optional): Path to a folder. Defaults to None.\n        \"\"\"\n        path = path or self.path\n        return self.fs.ls(path)\n\n    def rm(self, path: str | None = None, recursive: bool = False) -&gt; None:\n        \"\"\"Delete files in a path.\n\n        Args:\n            path (str, optional): Path to a folder. Defaults to None.\n            recursive (bool, optional): Whether to delete files recursively.\n                Defaults to False.\n        \"\"\"\n        path = path or self.path\n        self.fs.rm(path, recursive=recursive)\n\n    def cp(\n        self,\n        from_path: str | None = None,\n        to_path: str | None = None,\n        recursive: bool = False,\n    ) -&gt; None:\n        \"\"\"Copies the contents of `from_path` to `to_path`.\n\n        Args:\n            from_path (str, optional): Path from which to copy file(s). Defaults to\n                None.\n            to_path (str, optional): Path where to copy file(s). Defaults to None.\n            recursive (bool, optional): Whether to copy file(s) recursively. Defaults to\n                False.\n        \"\"\"\n        from_path = from_path or self.path\n        self.fs.cp(from_path, to_path, recursive=recursive)\n\n    def from_df(\n        self,\n        df: pd.DataFrame,\n        path: str | None = None,\n        sep: str = \"\\t\",\n        overwrite: bool = False,\n    ) -&gt; None:\n        r\"\"\"Upload a pandas `DataFrame` to a file on Azure Data Lake.\n\n        Args:\n            df (pd.DataFrame): The pandas `DataFrame` to upload.\n            path (str, optional): The destination path. Defaults to None.\n            sep (str, optional): The separator to use in the `to_csv()` function.\n                Defaults to \"\\t\".\n            overwrite (bool): Whether to overwrite the file if it exist.\n        \"\"\"\n        path = path or self.path\n\n        extension = path.split(\".\")[-1]\n        if extension not in (\"csv\", \"parquet\"):\n            if \".\" not in path:\n                msg = \"Please provide the full path to the file.\"\n            else:\n                msg = \"Accepted file formats are 'csv' and 'parquet'.\"\n            raise ValueError(msg)\n\n        file_name = path.split(\"/\")[-1]\n        if extension == \"csv\":\n            # Can do it simply like this if ADLS accesses are set up correctly\n            # url = os.path.join(self.base_url, path)\n            # df.to_csv(url, storage_options=self.storage_options)\n            df.to_csv(file_name, index=False, sep=sep)\n        else:\n            df.to_parquet(file_name, index=False)\n\n        self.upload(from_path=file_name, to_path=path, overwrite=overwrite)\n\n        Path(file_name).unlink()\n</code></pre>"},{"location":"references/sources/database/#viadot.sources.azure_data_lake.AzureDataLake.__init__","title":"<code>__init__(path=None, gen=2, credentials=None, config_key=None, *args, **kwargs)</code>","text":"<p>A class for pulling data from the Azure Data Lakes (gen1 and gen2).</p> <p>You can either connect to the lake in general: lake = AzureDataLake(); lake.exists(\"a/b/c.csv\")</p> <p>or to a particular path: lake = AzureDataLake(path=\"a/b/c.csv\"); lake.exists()`</p> <p>credentials (Dict[str, Any], optional): A dictionary containing     the following credentials: <code>account_name</code>, <code>tenant_id</code>,     <code>client_id</code>, and <code>client_secret</code>. config_key (str, optional): The key in the viadot config holding     relevant credentials.</p> Source code in <code>src/viadot/sources/azure_data_lake.py</code> <pre><code>def __init__(\n    self,\n    path: str | None = None,\n    gen: int = 2,\n    credentials: dict[str, Any] | None = None,\n    config_key: str | None = None,\n    *args,\n    **kwargs,\n):\n    \"\"\"A class for pulling data from the Azure Data Lakes (gen1 and gen2).\n\n    You can either connect to the lake in general:\n    lake = AzureDataLake(); lake.exists(\"a/b/c.csv\")\n\n    or to a particular path:\n    lake = AzureDataLake(path=\"a/b/c.csv\"); lake.exists()`\n\n    Args:\n    credentials (Dict[str, Any], optional): A dictionary containing\n        the following credentials: `account_name`, `tenant_id`,\n        `client_id`, and `client_secret`.\n    config_key (str, optional): The key in the viadot config holding\n        relevant credentials.\n    \"\"\"\n    credentials = credentials or get_source_credentials(config_key)\n    credentials = {key.lower(): value for key, value in credentials.items()}\n\n    required_credentials = (\n        \"account_name\",\n        \"azure_tenant_id\",\n        \"azure_client_id\",\n        \"azure_client_secret\",\n    )\n    required_credentials_are_provided = all(\n        rc in credentials for rc in required_credentials\n    )\n\n    if credentials is None:\n        msg = \"Please provide the credentials.\"\n        raise CredentialError(msg)\n\n    if not required_credentials_are_provided:\n        msg = \"Please provide all required credentials.\"\n        raise CredentialError(msg)\n\n    super().__init__(*args, credentials=credentials, **kwargs)\n\n    storage_account_name = self.credentials[\"account_name\"]\n    tenant_id = self.credentials[\"azure_tenant_id\"]\n    client_id = self.credentials[\"azure_client_id\"]\n    client_secret = self.credentials[\"azure_client_secret\"]\n\n    self.path = path\n    self.gen = gen\n    self.storage_options = {\n        \"azure_tenant_id\": tenant_id,\n        \"azure_client_id\": client_id,\n        \"azure_client_secret\": client_secret,\n    }\n    if gen == 1:\n        self.fs = AzureDatalakeFileSystem(\n            store_name=storage_account_name,\n            tenant_id=tenant_id,\n            client_id=client_id,\n            client_secret=client_secret,\n        )\n        self.base_url = f\"adl://{storage_account_name}\"\n    elif gen == 2:  # noqa: PLR2004\n        self.storage_options[\"account_name\"] = storage_account_name\n        self.fs = AzureBlobFileSystem(\n            account_name=storage_account_name,\n            tenant_id=tenant_id,\n            client_id=client_id,\n            client_secret=client_secret,\n        )\n        self.base_url = \"az://\"\n</code></pre>"},{"location":"references/sources/database/#viadot.sources.azure_data_lake.AzureDataLake.cp","title":"<code>cp(from_path=None, to_path=None, recursive=False)</code>","text":"<p>Copies the contents of <code>from_path</code> to <code>to_path</code>.</p> <p>Parameters:</p> Name Type Description Default <code>from_path</code> <code>str</code> <p>Path from which to copy file(s). Defaults to None.</p> <code>None</code> <code>to_path</code> <code>str</code> <p>Path where to copy file(s). Defaults to None.</p> <code>None</code> <code>recursive</code> <code>bool</code> <p>Whether to copy file(s) recursively. Defaults to False.</p> <code>False</code> Source code in <code>src/viadot/sources/azure_data_lake.py</code> <pre><code>def cp(\n    self,\n    from_path: str | None = None,\n    to_path: str | None = None,\n    recursive: bool = False,\n) -&gt; None:\n    \"\"\"Copies the contents of `from_path` to `to_path`.\n\n    Args:\n        from_path (str, optional): Path from which to copy file(s). Defaults to\n            None.\n        to_path (str, optional): Path where to copy file(s). Defaults to None.\n        recursive (bool, optional): Whether to copy file(s) recursively. Defaults to\n            False.\n    \"\"\"\n    from_path = from_path or self.path\n    self.fs.cp(from_path, to_path, recursive=recursive)\n</code></pre>"},{"location":"references/sources/database/#viadot.sources.azure_data_lake.AzureDataLake.download","title":"<code>download(to_path, from_path=None, recursive=False, overwrite=True)</code>","text":"<p>Download file(s) from the lake.</p> <p>Parameters:</p> Name Type Description Default <code>to_path</code> <code>str</code> <p>description</p> required <code>from_path</code> <code>str | None</code> <p>description. Defaults to None.</p> <code>None</code> <code>recursive</code> <code>bool</code> <p>description. Defaults to False.</p> <code>False</code> <code>overwrite</code> <code>bool</code> <p>description. Defaults to True.</p> <code>True</code> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>description</p> Source code in <code>src/viadot/sources/azure_data_lake.py</code> <pre><code>def download(\n    self,\n    to_path: str,\n    from_path: str | None = None,\n    recursive: bool = False,\n    overwrite: bool = True,\n) -&gt; None:\n    \"\"\"Download file(s) from the lake.\n\n    Args:\n        to_path (str): _description_\n        from_path (str | None, optional): _description_. Defaults to None.\n        recursive (bool, optional): _description_. Defaults to False.\n        overwrite (bool, optional): _description_. Defaults to True.\n\n    Raises:\n        NotImplementedError: _description_\n    \"\"\"\n    if overwrite is False:\n        msg = \"Currently, only the default behavior (overwrite) is available.\"\n        raise NotImplementedError(msg)\n\n    from_path = from_path or self.path\n    self.fs.download(rpath=from_path, lpath=to_path, recursive=recursive)\n</code></pre>"},{"location":"references/sources/database/#viadot.sources.azure_data_lake.AzureDataLake.exists","title":"<code>exists(path=None)</code>","text":"<p>Check if a location exists in Azure Data Lake.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to check. Can be a file or a directory.</p> <code>None</code> <p>Example: <pre><code>from viadot.sources import AzureDataLake\n\nlake = AzureDataLake(gen=1)\nlake.exists(\"tests/test.csv\")\n</code></pre></p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>Whether the paths exists.</p> Source code in <code>src/viadot/sources/azure_data_lake.py</code> <pre><code>def exists(self, path: str | None = None) -&gt; bool:\n    \"\"\"Check if a location exists in Azure Data Lake.\n\n    Args:\n        path (str): The path to check. Can be a file or a directory.\n\n    Example:\n    ```python\n    from viadot.sources import AzureDataLake\n\n    lake = AzureDataLake(gen=1)\n    lake.exists(\"tests/test.csv\")\n    ```\n\n    Returns:\n        bool: Whether the paths exists.\n    \"\"\"\n    path = path or self.path\n    return self.fs.exists(path)\n</code></pre>"},{"location":"references/sources/database/#viadot.sources.azure_data_lake.AzureDataLake.from_df","title":"<code>from_df(df, path=None, sep='\\t', overwrite=False)</code>","text":"<p>Upload a pandas <code>DataFrame</code> to a file on Azure Data Lake.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The pandas <code>DataFrame</code> to upload.</p> required <code>path</code> <code>str</code> <p>The destination path. Defaults to None.</p> <code>None</code> <code>sep</code> <code>str</code> <p>The separator to use in the <code>to_csv()</code> function. Defaults to \"\\t\".</p> <code>'\\t'</code> <code>overwrite</code> <code>bool</code> <p>Whether to overwrite the file if it exist.</p> <code>False</code> Source code in <code>src/viadot/sources/azure_data_lake.py</code> <pre><code>def from_df(\n    self,\n    df: pd.DataFrame,\n    path: str | None = None,\n    sep: str = \"\\t\",\n    overwrite: bool = False,\n) -&gt; None:\n    r\"\"\"Upload a pandas `DataFrame` to a file on Azure Data Lake.\n\n    Args:\n        df (pd.DataFrame): The pandas `DataFrame` to upload.\n        path (str, optional): The destination path. Defaults to None.\n        sep (str, optional): The separator to use in the `to_csv()` function.\n            Defaults to \"\\t\".\n        overwrite (bool): Whether to overwrite the file if it exist.\n    \"\"\"\n    path = path or self.path\n\n    extension = path.split(\".\")[-1]\n    if extension not in (\"csv\", \"parquet\"):\n        if \".\" not in path:\n            msg = \"Please provide the full path to the file.\"\n        else:\n            msg = \"Accepted file formats are 'csv' and 'parquet'.\"\n        raise ValueError(msg)\n\n    file_name = path.split(\"/\")[-1]\n    if extension == \"csv\":\n        # Can do it simply like this if ADLS accesses are set up correctly\n        # url = os.path.join(self.base_url, path)\n        # df.to_csv(url, storage_options=self.storage_options)\n        df.to_csv(file_name, index=False, sep=sep)\n    else:\n        df.to_parquet(file_name, index=False)\n\n    self.upload(from_path=file_name, to_path=path, overwrite=overwrite)\n\n    Path(file_name).unlink()\n</code></pre>"},{"location":"references/sources/database/#viadot.sources.azure_data_lake.AzureDataLake.ls","title":"<code>ls(path=None)</code>","text":"<p>Returns a list of files in a path.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to a folder. Defaults to None.</p> <code>None</code> Source code in <code>src/viadot/sources/azure_data_lake.py</code> <pre><code>def ls(self, path: str | None = None) -&gt; list[str]:\n    \"\"\"Returns a list of files in a path.\n\n    Args:\n        path (str, optional): Path to a folder. Defaults to None.\n    \"\"\"\n    path = path or self.path\n    return self.fs.ls(path)\n</code></pre>"},{"location":"references/sources/database/#viadot.sources.azure_data_lake.AzureDataLake.rm","title":"<code>rm(path=None, recursive=False)</code>","text":"<p>Delete files in a path.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to a folder. Defaults to None.</p> <code>None</code> <code>recursive</code> <code>bool</code> <p>Whether to delete files recursively. Defaults to False.</p> <code>False</code> Source code in <code>src/viadot/sources/azure_data_lake.py</code> <pre><code>def rm(self, path: str | None = None, recursive: bool = False) -&gt; None:\n    \"\"\"Delete files in a path.\n\n    Args:\n        path (str, optional): Path to a folder. Defaults to None.\n        recursive (bool, optional): Whether to delete files recursively.\n            Defaults to False.\n    \"\"\"\n    path = path or self.path\n    self.fs.rm(path, recursive=recursive)\n</code></pre>"},{"location":"references/sources/database/#viadot.sources.azure_data_lake.AzureDataLake.to_df","title":"<code>to_df(path=None, sep='\\t', quoting=0, lineterminator=None, error_bad_lines=None)</code>","text":"<p>Download a file from the lake and return it as a pandas DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>description. Defaults to None.</p> <code>None</code> <code>sep</code> <code>str</code> <p>description. Defaults to \"\\t\".</p> <code>'\\t'</code> <code>quoting</code> <code>int</code> <p>description. Defaults to 0.</p> <code>0</code> <code>lineterminator</code> <code>str</code> <p>description. Defaults to None.</p> <code>None</code> <code>error_bad_lines</code> <code>bool</code> <p>description. Defaults to None.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>description</p> <p>Returns:</p> Name Type Description <code>_type_</code> <code>DataFrame</code> <p>description</p> Source code in <code>src/viadot/sources/azure_data_lake.py</code> <pre><code>@add_viadot_metadata_columns\ndef to_df(\n    self,\n    path: str | None = None,\n    sep: str = \"\\t\",\n    quoting: int = 0,\n    lineterminator: str | None = None,\n    error_bad_lines: bool | None = None,\n) -&gt; pd.DataFrame:\n    r\"\"\"Download a file from the lake and return it as a pandas DataFrame.\n\n    Args:\n        path (str, optional): _description_. Defaults to None.\n        sep (str, optional): _description_. Defaults to \"\\t\".\n        quoting (int, optional): _description_. Defaults to 0.\n        lineterminator (str, optional): _description_. Defaults to None.\n        error_bad_lines (bool, optional): _description_. Defaults to None.\n\n    Raises:\n        ValueError: _description_\n\n    Returns:\n        _type_: _description_\n    \"\"\"\n    if quoting is None:\n        quoting = 0\n\n    path = path or self.path\n    url = self.base_url + \"/\" + path.strip(\"/\")\n\n    if url.endswith(\".csv\"):\n        df = pd.read_csv(\n            url,\n            storage_options=self.storage_options,\n            sep=sep,\n            quoting=quoting,\n            lineterminator=lineterminator,\n            error_bad_lines=error_bad_lines,\n        )\n    elif url.endswith(\".parquet\"):\n        df = pd.read_parquet(url, storage_options=self.storage_options)\n    else:\n        msg = \"Only CSV and parquet formats are supported.\"\n        raise ValueError(msg)\n\n    return df\n</code></pre>"},{"location":"references/sources/database/#viadot.sources.azure_data_lake.AzureDataLake.upload","title":"<code>upload(from_path, to_path=None, recursive=False, overwrite=False)</code>","text":"<p>Upload file(s) to the lake.</p> <p>Parameters:</p> Name Type Description Default <code>from_path</code> <code>str</code> <p>Path to the local file(s) to be uploaded.</p> required <code>to_path</code> <code>str</code> <p>Path to the destination file/folder</p> <code>None</code> <code>recursive</code> <code>bool</code> <p>Set this to true if working with directories.</p> <code>False</code> <code>overwrite</code> <code>bool</code> <p>Whether to overwrite the file(s) if they exist.</p> <code>False</code> <p>Example: <pre><code>from viadot.sources import AzureDataLake\n\nlake = AzureDataLake()\nlake.upload(from_path='tests/test.csv', to_path=\"sandbox/test.csv\")\n</code></pre></p> Source code in <code>src/viadot/sources/azure_data_lake.py</code> <pre><code>def upload(\n    self,\n    from_path: str,\n    to_path: str | None = None,\n    recursive: bool = False,\n    overwrite: bool = False,\n) -&gt; None:\n    \"\"\"Upload file(s) to the lake.\n\n    Args:\n        from_path (str): Path to the local file(s) to be uploaded.\n        to_path (str): Path to the destination file/folder\n        recursive (bool): Set this to true if working with directories.\n        overwrite (bool): Whether to overwrite the file(s) if they exist.\n\n    Example:\n    ```python\n    from viadot.sources import AzureDataLake\n\n    lake = AzureDataLake()\n    lake.upload(from_path='tests/test.csv', to_path=\"sandbox/test.csv\")\n    ```\n    \"\"\"\n    if self.gen == 1:\n        msg = \"Azure Data Lake Gen1 does not support simple file upload.\"\n        raise NotImplementedError(msg)\n\n    to_path = to_path or self.path\n\n    self.logger.info(f\"Uploading file(s) from '{from_path}' to '{to_path}'...\")\n\n    try:\n        self.fs.upload(\n            lpath=from_path,\n            rpath=to_path,\n            recursive=recursive,\n            overwrite=overwrite,\n        )\n    except FileExistsError as e:\n        if recursive:\n            msg = f\"At least one file in '{to_path}' already exists. Specify `overwrite=True` to overwrite.\"\n        else:\n            msg = f\"The file '{to_path}' already exists. Specify `overwrite=True` to overwrite.\"\n        raise FileExistsError(msg) from e\n\n    self.logger.info(\n        f\"Successfully uploaded file(s) from '{from_path}' to '{to_path}'.\"\n    )\n</code></pre>"},{"location":"references/sources/database/#viadot.sources.azure_sql.AzureSQL","title":"<code>viadot.sources.azure_sql.AzureSQL</code>","text":"<p>               Bases: <code>SQLServer</code></p> <p>Azure SQL connector class.</p> Source code in <code>src/viadot/sources/azure_sql.py</code> <pre><code>class AzureSQL(SQLServer):\n    \"\"\"Azure SQL connector class.\"\"\"\n\n    def __init__(self, *args, config_key: str = \"AZURE_SQL\", **kwargs):\n        \"\"\"Initialize the AzureSQL connector.\n\n        This constructor sets up the Azure SQL connector with the specified\n        configuration key. It allows for additional positional and keyword arguments\n        to be passed to the parent SQLServer class.\n\n        Args:\n            *args: Variable length argument list passed to the parent class.\n            config_key (str, optional): The configuration key used to retrieve\n                connection settings. Defaults to \"AZURE_SQL\".\n            **kwargs: Additional keyword arguments passed to the parent class.\n        \"\"\"\n        super().__init__(*args, config_key=config_key, **kwargs)\n\n    def bulk_insert(\n        self,\n        table: str,\n        schema: str | None = None,\n        source_path: str | None = None,\n        sep: str | None = \"\\t\",\n        if_exists: Literal[\"append\", \"replace\"] = \"append\",\n    ) -&gt; bool:\n        r\"\"\"Function to bulk insert.\n\n        Args:\n            table (str): Table name.\n            schema (str, optional): Schema name. Defaults to None.\n            source_path (str, optional): Full path to a data file. Defaults to one.\n            sep (str, optional):  field terminator to be used for char and\n                widechar data files. Defaults to \"\\t\".\n            if_exists (Literal[\"append\", \"replace\"] , optional): What to do if the table\n                already exists. Defaults to \"append\".\n        \"\"\"\n        if schema is None:\n            schema = self.DEFAULT_SCHEMA\n        fqn = f\"{schema}.{table}\"\n        insert_sql = f\"\"\"\n            BULK INSERT {fqn} FROM '{source_path}'\n            WITH (\n                CHECK_CONSTRAINTS,\n                DATA_SOURCE='{self.credentials['data_source']}',\n                DATAFILETYPE='char',\n                FIELDTERMINATOR='{sep}',\n                ROWTERMINATOR='0x0a',\n                FIRSTROW=2,\n                KEEPIDENTITY,\n                TABLOCK,\n                CODEPAGE='65001'\n            );\n        \"\"\"\n        if if_exists == \"replace\":\n            self.run(f\"DELETE FROM {schema}.{table}\")  # noqa: S608\n        self.run(insert_sql)\n        return True\n\n    def create_external_database(\n        self,\n        external_database_name: str,\n        storage_account_name: str,\n        container_name: str,\n        sas_token: str,\n        master_key_password: str,\n        credential_name: str | None = None,\n    ) -&gt; None:\n        \"\"\"Create an external database.\n\n        Used to eg. execute BULK INSERT or OPENROWSET queries.\n\n        Args:\n            external_database_name (str): The name of the extrnal source (db)\n                to be created.\n            storage_account_name (str): The name of the Azure storage account.\n            container_name (str): The name of the container which should\n                become the \"database\".\n            sas_token (str): The SAS token to be used as the credential.\n                Note that the auth system in Azure is pretty broken and you might need\n                to paste here your storage account's account key instead.\n            master_key_password (str): The password for the database master key of your\n                Azure SQL Database.\n            credential_name (str): How to name the SAS credential. This is really\n                an Azure internal thing and can be anything.\n                By default '{external_database_name}_credential`.\n        \"\"\"\n        # stupid Microsoft thing\n        if sas_token.startswith(\"?\"):\n            sas_token = sas_token[1:]\n\n        if credential_name is None:\n            credential_name = f\"{external_database_name}_credential\"\n\n        create_master_key_sql = (\n            f\"CREATE MASTER KEY ENCRYPTION BY PASSWORD = {master_key_password}\"\n        )\n\n        create_external_db_credential_sql = f\"\"\"\n        CREATE DATABASE SCOPED CREDENTIAL {credential_name}\n        WITH IDENTITY = 'SHARED ACCESS SIGNATURE'\n        SECRET = '{sas_token}';\n        \"\"\"\n\n        create_external_db_sql = f\"\"\"\n        CREATE EXTERNAL DATA SOURCE {external_database_name} WITH (\n            LOCATION = f'https://{storage_account_name}.blob.core.windows.net/' \\\n            f'{container_name}',\n            CREDENTIAL = {credential_name}\n        );\n        \"\"\"\n\n        self.run(create_master_key_sql)\n        self.run(create_external_db_credential_sql)\n        self.run(create_external_db_sql)\n\n    def to_df(\n        self,\n        query: str,\n        if_empty: Literal[\"warn\", \"skip\", \"fail\"] = \"warn\",\n        convert_bytes: bool = False,\n        remove_special_characters: bool | None = None,\n        columns_to_clean: list[str] | None = None,\n    ) -&gt; pd.DataFrame:\n        \"\"\"Execute a query and return the result as a pandas DataFrame.\n\n        Args:\n            query (str): The query to execute.\n            con (pyodbc.Connection, optional): The connection to use to pull the data.\n            if_empty (Literal[\"warn\", \"skip\", \"fail\"], optional): What to do if the\n                query returns no data. Defaults to None.\n            convert_bytes (bool). A boolean value to trigger method\n                df_converts_bytes_to_int. It is used to convert bytes data type into\n                int, as pulling data with bytes can lead to malformed data in dataframe.\n                Defaults to False.\n            remove_special_characters (str, optional): Call a function that remove\n                special characters like escape symbols. Defaults to None.\n            columns_to_clean (List(str), optional): Select columns to clean, used with\n                remove_special_characters. If None whole data frame will be processed.\n                Defaults to None.\n        \"\"\"\n        df = super().to_df(query=query, if_empty=if_empty)\n\n        if convert_bytes:\n            df = df_converts_bytes_to_int(df=df)\n\n        if remove_special_characters:\n            df = df_clean_column(df=df, columns_to_clean=columns_to_clean)\n\n        return df\n</code></pre>"},{"location":"references/sources/database/#viadot.sources.azure_sql.AzureSQL.__init__","title":"<code>__init__(*args, config_key='AZURE_SQL', **kwargs)</code>","text":"<p>Initialize the AzureSQL connector.</p> <p>This constructor sets up the Azure SQL connector with the specified configuration key. It allows for additional positional and keyword arguments to be passed to the parent SQLServer class.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <p>Variable length argument list passed to the parent class.</p> <code>()</code> <code>config_key</code> <code>str</code> <p>The configuration key used to retrieve connection settings. Defaults to \"AZURE_SQL\".</p> <code>'AZURE_SQL'</code> <code>**kwargs</code> <p>Additional keyword arguments passed to the parent class.</p> <code>{}</code> Source code in <code>src/viadot/sources/azure_sql.py</code> <pre><code>def __init__(self, *args, config_key: str = \"AZURE_SQL\", **kwargs):\n    \"\"\"Initialize the AzureSQL connector.\n\n    This constructor sets up the Azure SQL connector with the specified\n    configuration key. It allows for additional positional and keyword arguments\n    to be passed to the parent SQLServer class.\n\n    Args:\n        *args: Variable length argument list passed to the parent class.\n        config_key (str, optional): The configuration key used to retrieve\n            connection settings. Defaults to \"AZURE_SQL\".\n        **kwargs: Additional keyword arguments passed to the parent class.\n    \"\"\"\n    super().__init__(*args, config_key=config_key, **kwargs)\n</code></pre>"},{"location":"references/sources/database/#viadot.sources.azure_sql.AzureSQL.bulk_insert","title":"<code>bulk_insert(table, schema=None, source_path=None, sep='\\t', if_exists='append')</code>","text":"<p>Function to bulk insert.</p> <p>Parameters:</p> Name Type Description Default <code>table</code> <code>str</code> <p>Table name.</p> required <code>schema</code> <code>str</code> <p>Schema name. Defaults to None.</p> <code>None</code> <code>source_path</code> <code>str</code> <p>Full path to a data file. Defaults to one.</p> <code>None</code> <code>sep</code> <code>str</code> <p>field terminator to be used for char and widechar data files. Defaults to \"\\t\".</p> <code>'\\t'</code> <code>if_exists</code> <code>Literal['append', 'replace']</code> <p>What to do if the table already exists. Defaults to \"append\".</p> <code>'append'</code> Source code in <code>src/viadot/sources/azure_sql.py</code> <pre><code>def bulk_insert(\n    self,\n    table: str,\n    schema: str | None = None,\n    source_path: str | None = None,\n    sep: str | None = \"\\t\",\n    if_exists: Literal[\"append\", \"replace\"] = \"append\",\n) -&gt; bool:\n    r\"\"\"Function to bulk insert.\n\n    Args:\n        table (str): Table name.\n        schema (str, optional): Schema name. Defaults to None.\n        source_path (str, optional): Full path to a data file. Defaults to one.\n        sep (str, optional):  field terminator to be used for char and\n            widechar data files. Defaults to \"\\t\".\n        if_exists (Literal[\"append\", \"replace\"] , optional): What to do if the table\n            already exists. Defaults to \"append\".\n    \"\"\"\n    if schema is None:\n        schema = self.DEFAULT_SCHEMA\n    fqn = f\"{schema}.{table}\"\n    insert_sql = f\"\"\"\n        BULK INSERT {fqn} FROM '{source_path}'\n        WITH (\n            CHECK_CONSTRAINTS,\n            DATA_SOURCE='{self.credentials['data_source']}',\n            DATAFILETYPE='char',\n            FIELDTERMINATOR='{sep}',\n            ROWTERMINATOR='0x0a',\n            FIRSTROW=2,\n            KEEPIDENTITY,\n            TABLOCK,\n            CODEPAGE='65001'\n        );\n    \"\"\"\n    if if_exists == \"replace\":\n        self.run(f\"DELETE FROM {schema}.{table}\")  # noqa: S608\n    self.run(insert_sql)\n    return True\n</code></pre>"},{"location":"references/sources/database/#viadot.sources.azure_sql.AzureSQL.create_external_database","title":"<code>create_external_database(external_database_name, storage_account_name, container_name, sas_token, master_key_password, credential_name=None)</code>","text":"<p>Create an external database.</p> <p>Used to eg. execute BULK INSERT or OPENROWSET queries.</p> <p>Parameters:</p> Name Type Description Default <code>external_database_name</code> <code>str</code> <p>The name of the extrnal source (db) to be created.</p> required <code>storage_account_name</code> <code>str</code> <p>The name of the Azure storage account.</p> required <code>container_name</code> <code>str</code> <p>The name of the container which should become the \"database\".</p> required <code>sas_token</code> <code>str</code> <p>The SAS token to be used as the credential. Note that the auth system in Azure is pretty broken and you might need to paste here your storage account's account key instead.</p> required <code>master_key_password</code> <code>str</code> <p>The password for the database master key of your Azure SQL Database.</p> required <code>credential_name</code> <code>str</code> <p>How to name the SAS credential. This is really an Azure internal thing and can be anything. By default '{external_database_name}_credential`.</p> <code>None</code> Source code in <code>src/viadot/sources/azure_sql.py</code> <pre><code>def create_external_database(\n    self,\n    external_database_name: str,\n    storage_account_name: str,\n    container_name: str,\n    sas_token: str,\n    master_key_password: str,\n    credential_name: str | None = None,\n) -&gt; None:\n    \"\"\"Create an external database.\n\n    Used to eg. execute BULK INSERT or OPENROWSET queries.\n\n    Args:\n        external_database_name (str): The name of the extrnal source (db)\n            to be created.\n        storage_account_name (str): The name of the Azure storage account.\n        container_name (str): The name of the container which should\n            become the \"database\".\n        sas_token (str): The SAS token to be used as the credential.\n            Note that the auth system in Azure is pretty broken and you might need\n            to paste here your storage account's account key instead.\n        master_key_password (str): The password for the database master key of your\n            Azure SQL Database.\n        credential_name (str): How to name the SAS credential. This is really\n            an Azure internal thing and can be anything.\n            By default '{external_database_name}_credential`.\n    \"\"\"\n    # stupid Microsoft thing\n    if sas_token.startswith(\"?\"):\n        sas_token = sas_token[1:]\n\n    if credential_name is None:\n        credential_name = f\"{external_database_name}_credential\"\n\n    create_master_key_sql = (\n        f\"CREATE MASTER KEY ENCRYPTION BY PASSWORD = {master_key_password}\"\n    )\n\n    create_external_db_credential_sql = f\"\"\"\n    CREATE DATABASE SCOPED CREDENTIAL {credential_name}\n    WITH IDENTITY = 'SHARED ACCESS SIGNATURE'\n    SECRET = '{sas_token}';\n    \"\"\"\n\n    create_external_db_sql = f\"\"\"\n    CREATE EXTERNAL DATA SOURCE {external_database_name} WITH (\n        LOCATION = f'https://{storage_account_name}.blob.core.windows.net/' \\\n        f'{container_name}',\n        CREDENTIAL = {credential_name}\n    );\n    \"\"\"\n\n    self.run(create_master_key_sql)\n    self.run(create_external_db_credential_sql)\n    self.run(create_external_db_sql)\n</code></pre>"},{"location":"references/sources/database/#viadot.sources.azure_sql.AzureSQL.to_df","title":"<code>to_df(query, if_empty='warn', convert_bytes=False, remove_special_characters=None, columns_to_clean=None)</code>","text":"<p>Execute a query and return the result as a pandas DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>The query to execute.</p> required <code>con</code> <code>Connection</code> <p>The connection to use to pull the data.</p> required <code>if_empty</code> <code>Literal['warn', 'skip', 'fail']</code> <p>What to do if the query returns no data. Defaults to None.</p> <code>'warn'</code> <code>remove_special_characters</code> <code>str</code> <p>Call a function that remove special characters like escape symbols. Defaults to None.</p> <code>None</code> <code>columns_to_clean</code> <code>List(str)</code> <p>Select columns to clean, used with remove_special_characters. If None whole data frame will be processed. Defaults to None.</p> <code>None</code> Source code in <code>src/viadot/sources/azure_sql.py</code> <pre><code>def to_df(\n    self,\n    query: str,\n    if_empty: Literal[\"warn\", \"skip\", \"fail\"] = \"warn\",\n    convert_bytes: bool = False,\n    remove_special_characters: bool | None = None,\n    columns_to_clean: list[str] | None = None,\n) -&gt; pd.DataFrame:\n    \"\"\"Execute a query and return the result as a pandas DataFrame.\n\n    Args:\n        query (str): The query to execute.\n        con (pyodbc.Connection, optional): The connection to use to pull the data.\n        if_empty (Literal[\"warn\", \"skip\", \"fail\"], optional): What to do if the\n            query returns no data. Defaults to None.\n        convert_bytes (bool). A boolean value to trigger method\n            df_converts_bytes_to_int. It is used to convert bytes data type into\n            int, as pulling data with bytes can lead to malformed data in dataframe.\n            Defaults to False.\n        remove_special_characters (str, optional): Call a function that remove\n            special characters like escape symbols. Defaults to None.\n        columns_to_clean (List(str), optional): Select columns to clean, used with\n            remove_special_characters. If None whole data frame will be processed.\n            Defaults to None.\n    \"\"\"\n    df = super().to_df(query=query, if_empty=if_empty)\n\n    if convert_bytes:\n        df = df_converts_bytes_to_int(df=df)\n\n    if remove_special_characters:\n        df = df_clean_column(df=df, columns_to_clean=columns_to_clean)\n\n    return df\n</code></pre>"},{"location":"references/sources/database/#viadot.sources.databricks.Databricks","title":"<code>viadot.sources.databricks.Databricks</code>","text":"<p>               Bases: <code>Source</code></p> Source code in <code>src/viadot/sources/databricks.py</code> <pre><code>class Databricks(Source):\n    DEFAULT_SCHEMA = \"default\"\n\n    def __init__(\n        self,\n        credentials: DatabricksCredentials | None = None,\n        config_key: str | None = None,\n        *args,\n        **kwargs,\n    ):\n        \"\"\"A class for pulling and manipulating data on Databricks.\n\n        Documentation for Databricks is located at:\n        https://docs.microsoft.com/en-us/azure/databricks/\n\n        Parameters\n        ----------\n        credentials : DatabricksCredentials, optional\n        Databricks connection configuration.\n        config_key (str, optional): The key in the viadot config holding relevant\n        credentials.\n        \"\"\"\n        raw_creds = credentials or get_source_credentials(config_key)\n        validated_creds = dict(\n            DatabricksCredentials(**raw_creds)\n        )  # validate the credentials\n\n        super().__init__(*args, credentials=validated_creds, **kwargs)\n\n        self._session = None\n\n    def __enter__(self):  # noqa: D105\n        return self\n\n    def __exit__(self, exc_type, exc_value, traceback):  # noqa: D105, ANN001\n        if self._session:\n            self._session.stop()\n            self._session = None\n\n    @property\n    def session(self) -&gt; Union[SparkSession, \"DatabricksSession\"]:  # noqa\n        if self._session is None:\n            session = self._create_spark_session()\n            self._session = session\n            return session\n        return self._session\n\n    def _create_spark_session(self):\n        \"\"\"Establish a connection to the Databricks cluster.\n\n        Returns:\n            SparkSession: A configured SparkSession object.\n        \"\"\"\n        db_connect_config = {\n            \"host\": self.credentials.get(\"host\"),\n            \"token\": self.credentials.get(\"token\"),\n            \"cluster_id\": self.credentials.get(\"cluster_id\"),\n            \"org_id\": self.credentials.get(\"org_id\"),\n            \"port\": self.credentials.get(\"port\"),\n        }\n\n        with Path.open(Path.expanduser(\"~/.databricks-connect\"), \"w\") as f:\n            json.dump(db_connect_config, f)\n\n        return SparkSession.builder.getOrCreate()  # noqa\n\n    def _create_spark_connect_session(self):\n        \"\"\"Establish a connection to a Databricks cluster.\n\n        Returns:\n            SparkSession: A configured SparkSession object.\n        \"\"\"\n        from databricks.connect import DatabricksSession\n\n        workspace_instance_name = self.credentials.get(\"host\").split(\"/\")[2]\n        port = self.credentials.get(\"port\")\n        token = self.credentials.get(\"token\")\n        cluster_id = self.credentials.get(\"cluster_id\")\n\n        conn_str = f\"sc://{workspace_instance_name}:{port}/;token={token};x-databricks-cluster-id={cluster_id}\"\n\n        return DatabricksSession.builder.remote(conn_str).getOrCreate()\n\n    @add_viadot_metadata_columns\n    def to_df(\n        self,\n        query: str,\n        if_empty: Literal[\"warn\", \"skip\", \"fail\"] = \"warn\",\n    ) -&gt; pd.DataFrame:\n        \"\"\"Execute a query and return a Pandas DataFrame.\n\n        Args:\n            query (str): The query to execute\n            if_empty (str, optional): What to do if the query returns no data.\n                Defaults to 'warn'.\n\n        Example:\n        ```python\n        from viadot.sources import Databricks\n\n        databricks = Databricks()\n        table_data = databricks.to_df(\"SELECT * FROM schema.table_1\")\n        ```\n        Returns:\n            pd.DataFrame: A Pandas DataFrame containing the requested table's data.\n        \"\"\"\n        if query.upper().startswith(\"SELECT\"):\n            df = self.run(query, fetch_type=\"pandas\")\n            if df.empty:\n                self._handle_if_empty(if_empty=if_empty)\n        else:\n            df = pd.DataFrame()\n        return df\n\n    def _pandas_df_to_spark_df(self, df: pd.DataFrame) -&gt; spark.DataFrame:\n        \"\"\"Convert a Pandas DataFrame to a Spark DataFrame.\n\n        Args:\n            df (pd.DataFrame): The Pandas DataFrame to be converted to a Spark\n                DataFrame.\n\n        Example:\n        ```python\n        from viadot.sources import Databricks\n        import pandas as pd\n\n        databricks = Databricks()\n        list = [{\"id\":\"1\", \"name\":\"Joe\"}]\n        df = pd.DataFrame(list)\n\n        spark_df = databricks._pandas_df_to_spark_df(df)\n        ```\n        Returns:\n            spark.DataFrame: The resulting Spark DataFrame.\n        \"\"\"\n        return self.session.createDataFrame(df)\n\n    def _spark_df_to_pandas_df(self, spark_df: spark.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"Convert a Spark DataFrame to a Pandas DataFrame.\n\n        Args:\n            df (spark.DataFrame): The Spark DataFrame to be converted to a Pandas\n                DataFrame.\n\n        Example:\n        ```python\n        from viadot.sources import Databricks\n\n        databricks = Databricks()\n        list = [{\"id\":\"1\", \"name\":\"Joe\"}]\n        df = pd.DataFrame(list)\n        spark_df = databricks._pandas_df_to_spark_df(df)\n\n        pandas_df = databricks._spark_df_to_pandas_df(spark_df)\n        ```\n        Returns:\n            pd.DataFrame: The resulting Pandas DataFrame.\n        \"\"\"\n        return spark_df.toPandas()\n\n    def run(\n        self, query: str, fetch_type: Literal[\"spark\", \"pandas\"] = \"spark\"\n    ) -&gt; spark.DataFrame | pd.DataFrame | bool:\n        \"\"\"Execute an SQL query.\n\n        Args:\n            query (str): The query to execute.\n            fetch_type (Literal, optional): How to return the data: either\n                in the default Spark DataFrame format or as a Pandas DataFrame. Defaults\n                to \"spark\".\n\n        Example:\n        ```python\n        from viadot.sources import Databricks\n\n        databricks = Databricks()\n        query_result = databricks.run(\"SELECT * FROM schema.table_1\")\n        ```\n        Returns:\n            Union[spark.DataFrame, pd.DataFrame, bool]: Either the result set of a query\n            or, in case of DDL/DML queries, a boolean describing whether the query was\n            executed successfully.\n        \"\"\"\n        if fetch_type not in [\"spark\", \"pandas\"]:\n            msg = \"Only the values 'spark', 'pandas' are allowed for 'fetch_type'\"\n            raise ValueError(msg)\n\n        query_clean = query.upper().strip()\n        query_keywords = [\"SELECT\", \"SHOW\", \"PRAGMA\", \"DESCRIBE\"]\n\n        query_result = self.session.sql(query)\n\n        if any(query_clean.startswith(word) for word in query_keywords):\n            if fetch_type == \"spark\":\n                result = query_result\n            else:\n                result = self._spark_df_to_pandas_df(query_result)\n        else:\n            result = True\n\n        return result\n\n    def _check_if_table_exists(self, table: str, schema: str | None = None) -&gt; bool:\n        if schema is None:\n            schema = Databricks.DEFAULT_SCHEMA\n        return self.session.catalog.tableExists(dbName=schema, tableName=table)\n\n    def _check_if_schema_exists(self, schema: str) -&gt; bool:\n        return self.session.catalog.databaseExists(schema)\n\n    def create_table_from_pandas(\n        self,\n        df: pd.DataFrame,\n        table: str,\n        schema: str | None = None,\n        if_empty: Literal[\"warn\", \"skip\", \"fail\"] = \"warn\",\n        if_exists: Literal[\"replace\", \"skip\", \"fail\"] = \"fail\",\n        snakecase_column_names: bool = True,\n        cast_df_columns: bool = True,\n    ) -&gt; bool:\n        \"\"\"Create a table using a pandas `DataFrame`.\n\n        Args:\n            df (pd.DataFrame): The `DataFrame` to be written as a table.\n            table (str): Name of the table to be created.\n            schema (str, optional): Name of the schema.\n            if_empty (str, optional): What to do if the input `DataFrame` is empty.\n                Defaults to 'warn'.\n            if_exists (Literal, optional): What to do if the table already exists.\n                Defaults to 'fail'.\n            snakecase_column_names (bool, optional): Whether to convert column names to\n                snake case. Defaults to True.\n            cast_df_columns (bool, optional): Converts column types in DataFrame using\n                utils._cast_df_cols(). This param exists because of possible errors with\n                object cols. Defaults to True.\n\n        Example:\n        ```python\n        from viadot.sources import Databricks\n\n        databricks = Databricks()\n        list = [{\"id\":\"1\", \"name\":\"Joe\"}]\n        df = pd.DataFrame(list)\n\n        new_table = databricks.create_table_from_pandas(\n            df=df, schema=\"viadot_test\", table=\"test\"\n        )\n        ```\n        Returns:\n            bool: True if the table was created successfully, False otherwise.\n        \"\"\"\n        if df.empty:\n            self._handle_if_empty(if_empty)\n\n        if schema is None:\n            schema = Databricks.DEFAULT_SCHEMA\n\n        if snakecase_column_names:\n            df = df_snakecase_column_names(df)\n\n        if cast_df_columns:\n            df = _cast_df_cols(df, types_to_convert=[\"object\"])\n\n        fqn = f\"{schema}.{table}\"\n        success_message = f\"Table {fqn} has been created successfully.\"\n\n        if self._check_if_table_exists(schema=schema, table=table):\n            if if_exists == \"skip\":\n                self.logger.warning(f\"Table {fqn} already exists.\")\n                result = False\n            elif if_exists == \"fail\":\n                raise TableAlreadyExistsError(fqn)\n            elif if_exists == \"replace\":\n                result = self._full_refresh(schema=schema, table=table, df=df)\n            else:\n                success_message = f\"Table {fqn} has been overwritten successfully.\"\n                result = True\n        else:\n            sdf = self._pandas_df_to_spark_df(df)\n            sdf.createOrReplaceTempView(\"tmp_view\")\n\n            result = self.run(\n                f\"CREATE TABLE {fqn} USING DELTA AS SELECT * FROM tmp_view;\"  # noqa: S608\n            )\n\n        if result:\n            self.logger.info(success_message)\n\n        return result\n\n    def drop_table(self, table: str, schema: str | None = None) -&gt; bool:\n        \"\"\"Delete an existing table.\n\n        Args:\n            schema (str): Name of the schema.\n            table (str): Name of the new table to be created.\n\n        Raises:\n            TableDoesNotExistError: If the table does not exist.\n\n        Example:\n        ```python\n        from viadot.sources import Databricks\n\n        databricks = Databricks()\n\n        databricks.drop_table(schema=\"viadot_test\", table=\"test\")\n        ```\n        \"\"\"\n        if schema is None:\n            schema = Databricks.DEFAULT_SCHEMA\n\n        fqn = f\"{schema}.{table}\"\n\n        if self._check_if_table_exists(schema=schema, table=table):\n            result = self.run(f\"DROP TABLE {fqn}\")\n            self.logger.info(f\"Table {fqn} has been deleted successfully.\")\n        else:\n            raise TableDoesNotExistError(fqn=fqn)\n\n        return result\n\n    def _append(self, schema: str, table: str, df: pd.DataFrame):\n        fqn = f\"{schema}.{table}\"\n        spark_df = self._pandas_df_to_spark_df(df)\n        spark_df.write.format(\"delta\").mode(\"append\").saveAsTable(fqn)\n\n        self.logger.info(f\"Table {fqn} has been appended successfully.\")\n\n    def _full_refresh(self, schema: str, table: str, df: pd.DataFrame) -&gt; bool:\n        \"\"\"Overwrite an existing table with data from a Pandas DataFrame.\n\n        Args:\n            schema (str): Name of the schema.\n            table (str): Name of the new table to be created.\n            df (pd.DataFrame): DataFrame to be used to overwrite the table.\n\n        Example:\n        ```python\n        from viadot.sources import Databricks\n\n        databricks = Databricks()\n        list = [{\"id\":\"1\", \"name\":\"Joe\"}]\n        df = pd.DataFrame(list)\n\n        databricks.insert_into(\n            df=df, schema=\"viadot_test\", table=\"test\", mode=\"replace\"\n        )\n        ```\n        Returns:\n            bool: True if the table has been refreshed successfully, False otherwise.\n        \"\"\"\n        fqn = f\"{schema}.{table}\"\n        data = self._pandas_df_to_spark_df(df)\n        data.write.format(\"delta\").mode(\"overwrite\").option(\n            \"overwriteSchema\", \"true\"\n        ).saveAsTable(fqn)\n\n        self.logger.info(f\"Table {fqn} has been refreshed successfully.\")\n\n        return True\n\n    def _upsert(\n        self,\n        df: pd.DataFrame,\n        table: str,\n        primary_key: str,\n        schema: str | None = None,\n    ):\n        spark_df = self._pandas_df_to_spark_df(df)\n        merge_query = build_merge_query(\n            df=spark_df,\n            schema=schema,\n            table=table,\n            primary_key=primary_key,\n            source=self,\n        )\n        result = self.run(merge_query)\n\n        self.logger.info(\"Data has been upserted successfully.\")\n\n        return result\n\n    def insert_into(\n        self,\n        df: pd.DataFrame,\n        table: str,\n        schema: str | None = None,\n        primary_key: str | None = None,\n        mode: Literal[\"replace\", \"append\", \"update\"] = \"append\",\n    ) -&gt; None:\n        \"\"\"Insert data from a pandas `DataFrame` into a Delta table.\n\n        Args:\n            df (pd.DataFrame): DataFrame with the data to be inserted into the table.\n            table (str): Name of the new table to be created.\n            schema (str, Optional): Name of the schema.\n            primary_key (str, Optional): The primary key on which the data will be\n                joined.\n                Required only when updating existing data.\n            mode (str, Optional): Which operation to run with the data. Allowed\n                operations are: 'replace', 'append', and 'update'. By default, 'append'.\n\n        Example:\n        ```python\n        from viadot.sources import Databricks\n\n        databricks = Databricks()\n        list = [{\"id\":\"1\", \"name\":\"Joe\"}]\n        df = pd.DataFrame(list)\n\n        databricks.insert_into(\n            df=df, schema=\"viadot_test\", table=\"test\", primary_key=\"pk\", mode=\"update\"\n        )\n        ```\n        \"\"\"\n        if schema is None:\n            schema = Databricks.DEFAULT_SCHEMA\n\n        fqn = f\"{schema}.{table}\" if schema else table\n\n        exists = self._check_if_table_exists(schema=schema, table=table)\n        if exists:\n            if mode == \"replace\":\n                self._full_refresh(df=df, schema=schema, table=table)\n            elif mode == \"append\":\n                self._append(df=df, schema=schema, table=table)\n            elif mode == \"update\":\n                self._upsert(df=df, schema=schema, table=table, primary_key=primary_key)\n            else:\n                msg = \"`mode` must be one of: 'replace', 'append', or 'update'.\"\n                raise ValueError(msg)\n        else:\n            msg = f\"Table {fqn} does not exist.\"\n            raise ValueError(msg)\n\n    def create_schema(self, schema_name: str) -&gt; bool:\n        \"\"\"Create a schema for storing tables.\n\n        Args:\n            schema_name (str): Name of the new schema to be created.\n\n        Example:\n        ```python\n        from viadot.sources import Databricks\n\n        databricks = Databricks()\n\n        databricks.create_schema(\"schema_1\")\n        ```\n        \"\"\"\n        result = self.run(f\"CREATE SCHEMA IF NOT EXISTS {schema_name}\")\n        self.logger.info(f\"Schema {schema_name} has been created successfully.\")\n        return result\n\n    def drop_schema(self, schema_name: str) -&gt; bool:\n        \"\"\"Delete a schema.\n\n        Args:\n            schema_name (str): Name of the schema to be deleted.\n\n        Example:\n        ```python\n        from viadot.sources import Databricks\n\n        databricks = Databricks()\n\n        databricks.drop_schema(\"schema_1\")\n        ```\n        \"\"\"\n        result = self.run(f\"DROP SCHEMA {schema_name}\")\n        self.logger.info(f\"Schema {schema_name} deleted.\")\n        return result\n\n    def discover_schema(self, table: str, schema: str | None = None) -&gt; dict:\n        \"\"\"Return a table's schema.\n\n        Args:\n            schema (str): Name of the schema.\n            table (str): Name of the new table to be created.\n\n        Example:\n        ```python\n        from viadot.sources import Databricks\n\n        databricks = Databricks()\n\n        databricks.discover_schema(schema=\"viadot_test\", table=\"test\")\n        ```\n        Returns:\n            schema (dict): A dictionary containing the schema details of the table.\n        \"\"\"\n        if schema is None:\n            schema = Databricks.DEFAULT_SCHEMA\n\n        fqn = f\"{schema}.{table}\"\n        result = self.run(f\"DESCRIBE {fqn}\", fetch_type=\"pandas\")\n\n        # Delete the last 3 records, which are boilerplate for Delta Tables\n        col_names = result[\"col_name\"].values.tolist()\n        col_names = col_names[:-3]\n\n        data_types = result[\"data_type\"].values.tolist()\n        data_types = data_types[:-3]\n\n        return dict(zip(col_names, data_types, strict=False))\n\n    def get_table_version(self, table: str, schema: str | None = None) -&gt; int:\n        \"\"\"Get the provided table's version number.\n\n        Args:\n            schema (str): Name of the schema.\n            table (str): Name of the table to rollback.\n\n        Returns:\n            version_number (int): The table's version number.\n        ```\n        \"\"\"\n        if schema is None:\n            schema = Databricks.DEFAULT_SCHEMA\n\n        fqn = f\"{schema}.{table}\"\n\n        # Get the info regarding the current version of the delta table\n        history = self.run(f\"DESCRIBE HISTORY {fqn}\", \"pandas\")\n\n        # Extract the current version number\n        version_number = history[\"version\"].iat[0]\n        return int(version_number)\n\n    def rollback(\n        self, table: str, version_number: int, schema: str | None = None\n    ) -&gt; bool:\n        \"\"\"Rollback a table to a previous version.\n\n        Args:\n            schema (str): Name of the schema.\n            table (str): Name of the table to rollback.\n            version_number (int): Number of the table's version to rollback to.\n\n        Example:\n        ```python\n        from viadot.sources import Databricks\n        databricks = Databricks()\n\n        schema = \"viadot_test\"\n        table = \"table_1\"\n\n        version_number = databricks.get_table_version(schema=schema, table=table)\n\n        # Perform changes on the table, in this example we are appending to the table\n        list = [{\"id\":\"1\", \"name\":\"Joe\"}]\n        df = pd.DataFrame(list)\n        databricks.insert_into(df=df, schema=schema, table=table)\n\n        databricks.rollback(schema=schema, table=table, version_number=version_number)\n        ```\n        Returns:\n            result (bool): A boolean indicating the success of the rollback.\n        \"\"\"\n        if schema is None:\n            schema = Databricks.DEFAULT_SCHEMA\n\n        fqn = f\"{schema}.{table}\"\n\n        # Retrieve the data from the previous table\n        old_table = self.to_df(f\"SELECT * FROM {fqn}@v{version_number}\")  # noqa: S608\n\n        # Perform full-refresh and overwrite the table with the new data\n        result = self.insert_into(\n            df=old_table, schema=schema, table=table, mode=\"replace\"\n        )\n\n        self.logger.info(\n            f\"Rollback for table {fqn} to version #{version_number} completed.\"\n        )\n\n        return result\n</code></pre>"},{"location":"references/sources/database/#viadot.sources.databricks.Databricks.__init__","title":"<code>__init__(credentials=None, config_key=None, *args, **kwargs)</code>","text":"<p>A class for pulling and manipulating data on Databricks.</p> <p>Documentation for Databricks is located at: https://docs.microsoft.com/en-us/azure/databricks/</p>"},{"location":"references/sources/database/#viadot.sources.databricks.Databricks.__init__--parameters","title":"Parameters","text":"<p>credentials : DatabricksCredentials, optional Databricks connection configuration. config_key (str, optional): The key in the viadot config holding relevant credentials.</p> Source code in <code>src/viadot/sources/databricks.py</code> <pre><code>def __init__(\n    self,\n    credentials: DatabricksCredentials | None = None,\n    config_key: str | None = None,\n    *args,\n    **kwargs,\n):\n    \"\"\"A class for pulling and manipulating data on Databricks.\n\n    Documentation for Databricks is located at:\n    https://docs.microsoft.com/en-us/azure/databricks/\n\n    Parameters\n    ----------\n    credentials : DatabricksCredentials, optional\n    Databricks connection configuration.\n    config_key (str, optional): The key in the viadot config holding relevant\n    credentials.\n    \"\"\"\n    raw_creds = credentials or get_source_credentials(config_key)\n    validated_creds = dict(\n        DatabricksCredentials(**raw_creds)\n    )  # validate the credentials\n\n    super().__init__(*args, credentials=validated_creds, **kwargs)\n\n    self._session = None\n</code></pre>"},{"location":"references/sources/database/#viadot.sources.databricks.Databricks.create_schema","title":"<code>create_schema(schema_name)</code>","text":"<p>Create a schema for storing tables.</p> <p>Parameters:</p> Name Type Description Default <code>schema_name</code> <code>str</code> <p>Name of the new schema to be created.</p> required <p>Example: <pre><code>from viadot.sources import Databricks\n\ndatabricks = Databricks()\n\ndatabricks.create_schema(\"schema_1\")\n</code></pre></p> Source code in <code>src/viadot/sources/databricks.py</code> <pre><code>def create_schema(self, schema_name: str) -&gt; bool:\n    \"\"\"Create a schema for storing tables.\n\n    Args:\n        schema_name (str): Name of the new schema to be created.\n\n    Example:\n    ```python\n    from viadot.sources import Databricks\n\n    databricks = Databricks()\n\n    databricks.create_schema(\"schema_1\")\n    ```\n    \"\"\"\n    result = self.run(f\"CREATE SCHEMA IF NOT EXISTS {schema_name}\")\n    self.logger.info(f\"Schema {schema_name} has been created successfully.\")\n    return result\n</code></pre>"},{"location":"references/sources/database/#viadot.sources.databricks.Databricks.create_table_from_pandas","title":"<code>create_table_from_pandas(df, table, schema=None, if_empty='warn', if_exists='fail', snakecase_column_names=True, cast_df_columns=True)</code>","text":"<p>Create a table using a pandas <code>DataFrame</code>.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The <code>DataFrame</code> to be written as a table.</p> required <code>table</code> <code>str</code> <p>Name of the table to be created.</p> required <code>schema</code> <code>str</code> <p>Name of the schema.</p> <code>None</code> <code>if_empty</code> <code>str</code> <p>What to do if the input <code>DataFrame</code> is empty. Defaults to 'warn'.</p> <code>'warn'</code> <code>if_exists</code> <code>Literal</code> <p>What to do if the table already exists. Defaults to 'fail'.</p> <code>'fail'</code> <code>snakecase_column_names</code> <code>bool</code> <p>Whether to convert column names to snake case. Defaults to True.</p> <code>True</code> <code>cast_df_columns</code> <code>bool</code> <p>Converts column types in DataFrame using utils._cast_df_cols(). This param exists because of possible errors with object cols. Defaults to True.</p> <code>True</code> <p>Example: <pre><code>from viadot.sources import Databricks\n\ndatabricks = Databricks()\nlist = [{\"id\":\"1\", \"name\":\"Joe\"}]\ndf = pd.DataFrame(list)\n\nnew_table = databricks.create_table_from_pandas(\n    df=df, schema=\"viadot_test\", table=\"test\"\n)\n</code></pre> Returns:     bool: True if the table was created successfully, False otherwise.</p> Source code in <code>src/viadot/sources/databricks.py</code> <pre><code>def create_table_from_pandas(\n    self,\n    df: pd.DataFrame,\n    table: str,\n    schema: str | None = None,\n    if_empty: Literal[\"warn\", \"skip\", \"fail\"] = \"warn\",\n    if_exists: Literal[\"replace\", \"skip\", \"fail\"] = \"fail\",\n    snakecase_column_names: bool = True,\n    cast_df_columns: bool = True,\n) -&gt; bool:\n    \"\"\"Create a table using a pandas `DataFrame`.\n\n    Args:\n        df (pd.DataFrame): The `DataFrame` to be written as a table.\n        table (str): Name of the table to be created.\n        schema (str, optional): Name of the schema.\n        if_empty (str, optional): What to do if the input `DataFrame` is empty.\n            Defaults to 'warn'.\n        if_exists (Literal, optional): What to do if the table already exists.\n            Defaults to 'fail'.\n        snakecase_column_names (bool, optional): Whether to convert column names to\n            snake case. Defaults to True.\n        cast_df_columns (bool, optional): Converts column types in DataFrame using\n            utils._cast_df_cols(). This param exists because of possible errors with\n            object cols. Defaults to True.\n\n    Example:\n    ```python\n    from viadot.sources import Databricks\n\n    databricks = Databricks()\n    list = [{\"id\":\"1\", \"name\":\"Joe\"}]\n    df = pd.DataFrame(list)\n\n    new_table = databricks.create_table_from_pandas(\n        df=df, schema=\"viadot_test\", table=\"test\"\n    )\n    ```\n    Returns:\n        bool: True if the table was created successfully, False otherwise.\n    \"\"\"\n    if df.empty:\n        self._handle_if_empty(if_empty)\n\n    if schema is None:\n        schema = Databricks.DEFAULT_SCHEMA\n\n    if snakecase_column_names:\n        df = df_snakecase_column_names(df)\n\n    if cast_df_columns:\n        df = _cast_df_cols(df, types_to_convert=[\"object\"])\n\n    fqn = f\"{schema}.{table}\"\n    success_message = f\"Table {fqn} has been created successfully.\"\n\n    if self._check_if_table_exists(schema=schema, table=table):\n        if if_exists == \"skip\":\n            self.logger.warning(f\"Table {fqn} already exists.\")\n            result = False\n        elif if_exists == \"fail\":\n            raise TableAlreadyExistsError(fqn)\n        elif if_exists == \"replace\":\n            result = self._full_refresh(schema=schema, table=table, df=df)\n        else:\n            success_message = f\"Table {fqn} has been overwritten successfully.\"\n            result = True\n    else:\n        sdf = self._pandas_df_to_spark_df(df)\n        sdf.createOrReplaceTempView(\"tmp_view\")\n\n        result = self.run(\n            f\"CREATE TABLE {fqn} USING DELTA AS SELECT * FROM tmp_view;\"  # noqa: S608\n        )\n\n    if result:\n        self.logger.info(success_message)\n\n    return result\n</code></pre>"},{"location":"references/sources/database/#viadot.sources.databricks.Databricks.discover_schema","title":"<code>discover_schema(table, schema=None)</code>","text":"<p>Return a table's schema.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>str</code> <p>Name of the schema.</p> <code>None</code> <code>table</code> <code>str</code> <p>Name of the new table to be created.</p> required <p>Example: <pre><code>from viadot.sources import Databricks\n\ndatabricks = Databricks()\n\ndatabricks.discover_schema(schema=\"viadot_test\", table=\"test\")\n</code></pre> Returns:     schema (dict): A dictionary containing the schema details of the table.</p> Source code in <code>src/viadot/sources/databricks.py</code> <pre><code>def discover_schema(self, table: str, schema: str | None = None) -&gt; dict:\n    \"\"\"Return a table's schema.\n\n    Args:\n        schema (str): Name of the schema.\n        table (str): Name of the new table to be created.\n\n    Example:\n    ```python\n    from viadot.sources import Databricks\n\n    databricks = Databricks()\n\n    databricks.discover_schema(schema=\"viadot_test\", table=\"test\")\n    ```\n    Returns:\n        schema (dict): A dictionary containing the schema details of the table.\n    \"\"\"\n    if schema is None:\n        schema = Databricks.DEFAULT_SCHEMA\n\n    fqn = f\"{schema}.{table}\"\n    result = self.run(f\"DESCRIBE {fqn}\", fetch_type=\"pandas\")\n\n    # Delete the last 3 records, which are boilerplate for Delta Tables\n    col_names = result[\"col_name\"].values.tolist()\n    col_names = col_names[:-3]\n\n    data_types = result[\"data_type\"].values.tolist()\n    data_types = data_types[:-3]\n\n    return dict(zip(col_names, data_types, strict=False))\n</code></pre>"},{"location":"references/sources/database/#viadot.sources.databricks.Databricks.drop_schema","title":"<code>drop_schema(schema_name)</code>","text":"<p>Delete a schema.</p> <p>Parameters:</p> Name Type Description Default <code>schema_name</code> <code>str</code> <p>Name of the schema to be deleted.</p> required <p>Example: <pre><code>from viadot.sources import Databricks\n\ndatabricks = Databricks()\n\ndatabricks.drop_schema(\"schema_1\")\n</code></pre></p> Source code in <code>src/viadot/sources/databricks.py</code> <pre><code>def drop_schema(self, schema_name: str) -&gt; bool:\n    \"\"\"Delete a schema.\n\n    Args:\n        schema_name (str): Name of the schema to be deleted.\n\n    Example:\n    ```python\n    from viadot.sources import Databricks\n\n    databricks = Databricks()\n\n    databricks.drop_schema(\"schema_1\")\n    ```\n    \"\"\"\n    result = self.run(f\"DROP SCHEMA {schema_name}\")\n    self.logger.info(f\"Schema {schema_name} deleted.\")\n    return result\n</code></pre>"},{"location":"references/sources/database/#viadot.sources.databricks.Databricks.drop_table","title":"<code>drop_table(table, schema=None)</code>","text":"<p>Delete an existing table.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>str</code> <p>Name of the schema.</p> <code>None</code> <code>table</code> <code>str</code> <p>Name of the new table to be created.</p> required <p>Raises:</p> Type Description <code>TableDoesNotExistError</code> <p>If the table does not exist.</p> <p>Example: <pre><code>from viadot.sources import Databricks\n\ndatabricks = Databricks()\n\ndatabricks.drop_table(schema=\"viadot_test\", table=\"test\")\n</code></pre></p> Source code in <code>src/viadot/sources/databricks.py</code> <pre><code>def drop_table(self, table: str, schema: str | None = None) -&gt; bool:\n    \"\"\"Delete an existing table.\n\n    Args:\n        schema (str): Name of the schema.\n        table (str): Name of the new table to be created.\n\n    Raises:\n        TableDoesNotExistError: If the table does not exist.\n\n    Example:\n    ```python\n    from viadot.sources import Databricks\n\n    databricks = Databricks()\n\n    databricks.drop_table(schema=\"viadot_test\", table=\"test\")\n    ```\n    \"\"\"\n    if schema is None:\n        schema = Databricks.DEFAULT_SCHEMA\n\n    fqn = f\"{schema}.{table}\"\n\n    if self._check_if_table_exists(schema=schema, table=table):\n        result = self.run(f\"DROP TABLE {fqn}\")\n        self.logger.info(f\"Table {fqn} has been deleted successfully.\")\n    else:\n        raise TableDoesNotExistError(fqn=fqn)\n\n    return result\n</code></pre>"},{"location":"references/sources/database/#viadot.sources.databricks.Databricks.get_table_version","title":"<code>get_table_version(table, schema=None)</code>","text":"<p>Get the provided table's version number.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>str</code> <p>Name of the schema.</p> <code>None</code> <code>table</code> <code>str</code> <p>Name of the table to rollback.</p> required <p>Returns:</p> Name Type Description <code>version_number</code> <code>int</code> <p>The table's version number.</p> <p>```</p> Source code in <code>src/viadot/sources/databricks.py</code> <pre><code>def get_table_version(self, table: str, schema: str | None = None) -&gt; int:\n    \"\"\"Get the provided table's version number.\n\n    Args:\n        schema (str): Name of the schema.\n        table (str): Name of the table to rollback.\n\n    Returns:\n        version_number (int): The table's version number.\n    ```\n    \"\"\"\n    if schema is None:\n        schema = Databricks.DEFAULT_SCHEMA\n\n    fqn = f\"{schema}.{table}\"\n\n    # Get the info regarding the current version of the delta table\n    history = self.run(f\"DESCRIBE HISTORY {fqn}\", \"pandas\")\n\n    # Extract the current version number\n    version_number = history[\"version\"].iat[0]\n    return int(version_number)\n</code></pre>"},{"location":"references/sources/database/#viadot.sources.databricks.Databricks.insert_into","title":"<code>insert_into(df, table, schema=None, primary_key=None, mode='append')</code>","text":"<p>Insert data from a pandas <code>DataFrame</code> into a Delta table.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame with the data to be inserted into the table.</p> required <code>table</code> <code>str</code> <p>Name of the new table to be created.</p> required <code>schema</code> <code>(str, Optional)</code> <p>Name of the schema.</p> <code>None</code> <code>primary_key</code> <code>(str, Optional)</code> <p>The primary key on which the data will be joined. Required only when updating existing data.</p> <code>None</code> <code>mode</code> <code>(str, Optional)</code> <p>Which operation to run with the data. Allowed operations are: 'replace', 'append', and 'update'. By default, 'append'.</p> <code>'append'</code> <p>Example: <pre><code>from viadot.sources import Databricks\n\ndatabricks = Databricks()\nlist = [{\"id\":\"1\", \"name\":\"Joe\"}]\ndf = pd.DataFrame(list)\n\ndatabricks.insert_into(\n    df=df, schema=\"viadot_test\", table=\"test\", primary_key=\"pk\", mode=\"update\"\n)\n</code></pre></p> Source code in <code>src/viadot/sources/databricks.py</code> <pre><code>def insert_into(\n    self,\n    df: pd.DataFrame,\n    table: str,\n    schema: str | None = None,\n    primary_key: str | None = None,\n    mode: Literal[\"replace\", \"append\", \"update\"] = \"append\",\n) -&gt; None:\n    \"\"\"Insert data from a pandas `DataFrame` into a Delta table.\n\n    Args:\n        df (pd.DataFrame): DataFrame with the data to be inserted into the table.\n        table (str): Name of the new table to be created.\n        schema (str, Optional): Name of the schema.\n        primary_key (str, Optional): The primary key on which the data will be\n            joined.\n            Required only when updating existing data.\n        mode (str, Optional): Which operation to run with the data. Allowed\n            operations are: 'replace', 'append', and 'update'. By default, 'append'.\n\n    Example:\n    ```python\n    from viadot.sources import Databricks\n\n    databricks = Databricks()\n    list = [{\"id\":\"1\", \"name\":\"Joe\"}]\n    df = pd.DataFrame(list)\n\n    databricks.insert_into(\n        df=df, schema=\"viadot_test\", table=\"test\", primary_key=\"pk\", mode=\"update\"\n    )\n    ```\n    \"\"\"\n    if schema is None:\n        schema = Databricks.DEFAULT_SCHEMA\n\n    fqn = f\"{schema}.{table}\" if schema else table\n\n    exists = self._check_if_table_exists(schema=schema, table=table)\n    if exists:\n        if mode == \"replace\":\n            self._full_refresh(df=df, schema=schema, table=table)\n        elif mode == \"append\":\n            self._append(df=df, schema=schema, table=table)\n        elif mode == \"update\":\n            self._upsert(df=df, schema=schema, table=table, primary_key=primary_key)\n        else:\n            msg = \"`mode` must be one of: 'replace', 'append', or 'update'.\"\n            raise ValueError(msg)\n    else:\n        msg = f\"Table {fqn} does not exist.\"\n        raise ValueError(msg)\n</code></pre>"},{"location":"references/sources/database/#viadot.sources.databricks.Databricks.rollback","title":"<code>rollback(table, version_number, schema=None)</code>","text":"<p>Rollback a table to a previous version.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>str</code> <p>Name of the schema.</p> <code>None</code> <code>table</code> <code>str</code> <p>Name of the table to rollback.</p> required <code>version_number</code> <code>int</code> <p>Number of the table's version to rollback to.</p> required <p>Example: <pre><code>from viadot.sources import Databricks\ndatabricks = Databricks()\n\nschema = \"viadot_test\"\ntable = \"table_1\"\n\nversion_number = databricks.get_table_version(schema=schema, table=table)\n\n# Perform changes on the table, in this example we are appending to the table\nlist = [{\"id\":\"1\", \"name\":\"Joe\"}]\ndf = pd.DataFrame(list)\ndatabricks.insert_into(df=df, schema=schema, table=table)\n\ndatabricks.rollback(schema=schema, table=table, version_number=version_number)\n</code></pre> Returns:     result (bool): A boolean indicating the success of the rollback.</p> Source code in <code>src/viadot/sources/databricks.py</code> <pre><code>def rollback(\n    self, table: str, version_number: int, schema: str | None = None\n) -&gt; bool:\n    \"\"\"Rollback a table to a previous version.\n\n    Args:\n        schema (str): Name of the schema.\n        table (str): Name of the table to rollback.\n        version_number (int): Number of the table's version to rollback to.\n\n    Example:\n    ```python\n    from viadot.sources import Databricks\n    databricks = Databricks()\n\n    schema = \"viadot_test\"\n    table = \"table_1\"\n\n    version_number = databricks.get_table_version(schema=schema, table=table)\n\n    # Perform changes on the table, in this example we are appending to the table\n    list = [{\"id\":\"1\", \"name\":\"Joe\"}]\n    df = pd.DataFrame(list)\n    databricks.insert_into(df=df, schema=schema, table=table)\n\n    databricks.rollback(schema=schema, table=table, version_number=version_number)\n    ```\n    Returns:\n        result (bool): A boolean indicating the success of the rollback.\n    \"\"\"\n    if schema is None:\n        schema = Databricks.DEFAULT_SCHEMA\n\n    fqn = f\"{schema}.{table}\"\n\n    # Retrieve the data from the previous table\n    old_table = self.to_df(f\"SELECT * FROM {fqn}@v{version_number}\")  # noqa: S608\n\n    # Perform full-refresh and overwrite the table with the new data\n    result = self.insert_into(\n        df=old_table, schema=schema, table=table, mode=\"replace\"\n    )\n\n    self.logger.info(\n        f\"Rollback for table {fqn} to version #{version_number} completed.\"\n    )\n\n    return result\n</code></pre>"},{"location":"references/sources/database/#viadot.sources.databricks.Databricks.run","title":"<code>run(query, fetch_type='spark')</code>","text":"<p>Execute an SQL query.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>The query to execute.</p> required <code>fetch_type</code> <code>Literal</code> <p>How to return the data: either in the default Spark DataFrame format or as a Pandas DataFrame. Defaults to \"spark\".</p> <code>'spark'</code> <p>Example: <pre><code>from viadot.sources import Databricks\n\ndatabricks = Databricks()\nquery_result = databricks.run(\"SELECT * FROM schema.table_1\")\n</code></pre> Returns:     Union[spark.DataFrame, pd.DataFrame, bool]: Either the result set of a query     or, in case of DDL/DML queries, a boolean describing whether the query was     executed successfully.</p> Source code in <code>src/viadot/sources/databricks.py</code> <pre><code>def run(\n    self, query: str, fetch_type: Literal[\"spark\", \"pandas\"] = \"spark\"\n) -&gt; spark.DataFrame | pd.DataFrame | bool:\n    \"\"\"Execute an SQL query.\n\n    Args:\n        query (str): The query to execute.\n        fetch_type (Literal, optional): How to return the data: either\n            in the default Spark DataFrame format or as a Pandas DataFrame. Defaults\n            to \"spark\".\n\n    Example:\n    ```python\n    from viadot.sources import Databricks\n\n    databricks = Databricks()\n    query_result = databricks.run(\"SELECT * FROM schema.table_1\")\n    ```\n    Returns:\n        Union[spark.DataFrame, pd.DataFrame, bool]: Either the result set of a query\n        or, in case of DDL/DML queries, a boolean describing whether the query was\n        executed successfully.\n    \"\"\"\n    if fetch_type not in [\"spark\", \"pandas\"]:\n        msg = \"Only the values 'spark', 'pandas' are allowed for 'fetch_type'\"\n        raise ValueError(msg)\n\n    query_clean = query.upper().strip()\n    query_keywords = [\"SELECT\", \"SHOW\", \"PRAGMA\", \"DESCRIBE\"]\n\n    query_result = self.session.sql(query)\n\n    if any(query_clean.startswith(word) for word in query_keywords):\n        if fetch_type == \"spark\":\n            result = query_result\n        else:\n            result = self._spark_df_to_pandas_df(query_result)\n    else:\n        result = True\n\n    return result\n</code></pre>"},{"location":"references/sources/database/#viadot.sources.databricks.Databricks.to_df","title":"<code>to_df(query, if_empty='warn')</code>","text":"<p>Execute a query and return a Pandas DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>The query to execute</p> required <code>if_empty</code> <code>str</code> <p>What to do if the query returns no data. Defaults to 'warn'.</p> <code>'warn'</code> <p>Example: <pre><code>from viadot.sources import Databricks\n\ndatabricks = Databricks()\ntable_data = databricks.to_df(\"SELECT * FROM schema.table_1\")\n</code></pre> Returns:     pd.DataFrame: A Pandas DataFrame containing the requested table's data.</p> Source code in <code>src/viadot/sources/databricks.py</code> <pre><code>@add_viadot_metadata_columns\ndef to_df(\n    self,\n    query: str,\n    if_empty: Literal[\"warn\", \"skip\", \"fail\"] = \"warn\",\n) -&gt; pd.DataFrame:\n    \"\"\"Execute a query and return a Pandas DataFrame.\n\n    Args:\n        query (str): The query to execute\n        if_empty (str, optional): What to do if the query returns no data.\n            Defaults to 'warn'.\n\n    Example:\n    ```python\n    from viadot.sources import Databricks\n\n    databricks = Databricks()\n    table_data = databricks.to_df(\"SELECT * FROM schema.table_1\")\n    ```\n    Returns:\n        pd.DataFrame: A Pandas DataFrame containing the requested table's data.\n    \"\"\"\n    if query.upper().startswith(\"SELECT\"):\n        df = self.run(query, fetch_type=\"pandas\")\n        if df.empty:\n            self._handle_if_empty(if_empty=if_empty)\n    else:\n        df = pd.DataFrame()\n    return df\n</code></pre>"},{"location":"references/sources/database/#viadot.sources._duckdb.DuckDB","title":"<code>viadot.sources._duckdb.DuckDB</code>","text":"<p>               Bases: <code>Source</code></p> Source code in <code>src/viadot/sources/_duckdb.py</code> <pre><code>class DuckDB(Source):\n    DEFAULT_SCHEMA = \"main\"\n\n    def __init__(\n        self,\n        config_key: str | None = None,\n        credentials: DuckDBCredentials | None = None,\n        *args,\n        **kwargs,\n    ):\n        \"\"\"A class for interacting with DuckDB.\n\n        Args:\n            config_key (str, optional): The key inside local config containing the\n                credentials.\n            credentials (DuckDBCredentials, optional): Credentials for the connection.\n                Defaults to None.\n            config_key (str, optional): The key in the viadot config holding relevant\n                credentials. Defaults to None.\n        \"\"\"\n        raw_creds = credentials or get_source_credentials(config_key) or {}\n        if credentials is None:\n            msg = \"Please specify the credentials.\"\n            raise CredentialError(msg)\n        validated_creds = dict(\n            DuckDBCredentials(**raw_creds)\n        )  # validate the credentials\n        super().__init__(*args, credentials=validated_creds, **kwargs)\n\n    @property\n    def con(self) -&gt; duckdb.DuckDBPyConnection:\n        \"\"\"Return a new connection to the database.\n\n        As the views are highly isolated, we need a new connection for each query in\n        order to see the changes from previous queries (eg. if we create a new table and\n        then we want to list tables from INFORMATION_SCHEMA, we need to create a new\n        DuckDB connection).\n\n        Returns:\n            duckdb.DuckDBPyConnection: database connection.\n        \"\"\"\n        return duckdb.connect(\n            database=self.credentials.get(\"database\"),\n            read_only=self.credentials.get(\"read_only\", False),\n        )\n\n    @property\n    def tables(self) -&gt; list[str]:\n        \"\"\"Show the list of fully qualified table names.\n\n        Returns:\n            list[str]: The list of tables in the format '{SCHEMA}.{TABLE}'.\n        \"\"\"\n        tables_meta: list[tuple] = self.run_query(\n            \"SELECT * FROM information_schema.tables\"\n        )\n        return [table_meta[1] + \".\" + table_meta[2] for table_meta in tables_meta]\n\n    @property\n    def schemas(self) -&gt; list[str]:\n        \"\"\"Show the list of schemas.\n\n        Returns:\n            list[str]: The list of schemas.\n        \"\"\"\n        self.logger.warning(\n            \"DuckDB does not expose a way to list schemas. `DuckDB.schemas` only contains schemas with tables.\"\n        )\n        tables_meta: list[tuple] = self.run_query(\n            \"SELECT * FROM information_schema.tables\"\n        )\n        return [table_meta[1] for table_meta in tables_meta]\n\n    def to_df(self, query: str, if_empty: str | None = None) -&gt; pd.DataFrame:\n        \"\"\"Run DuckDB query and save output to a pandas DataFrame.\n\n        Args:\n            query (str): The query to execute. If query doesn't start with SELECT or\n                WITH, empty DataFrame will be returned.\n            if_empty (str, optional): What to do if output DataFrame is empty. Defaults\n                to None.\n\n        Returns:\n            pd.DataFrame: DataFrame with query output\n        \"\"\"\n        if query.upper().startswith(\"SELECT\") or query.upper().startswith(\"WITH\"):\n            df = self.run_query(query, fetch_type=\"dataframe\")\n            if df.empty:\n                self._handle_if_empty(if_empty=if_empty)\n        else:\n            df = pd.DataFrame()\n        return df\n\n    def run_query(\n        self, query: str, fetch_type: Literal[\"record\", \"dataframe\"] = \"record\"\n    ) -&gt; list[Record] | bool:\n        \"\"\"Run a query on DuckDB.\n\n        Args:\n            query (str): The query to execute.\n            fetch_type (Literal[, optional): How to return the data: either in the\n                default record format or as a pandas DataFrame. Defaults to \"record\".\n\n        Returns:\n            Union[list[Record], bool]: Either the result set of a query or,\n                in case of DDL/DML queries, a boolean describing whether\n                the query was executed successfully.\n        \"\"\"\n        allowed_fetch_type_values = [\"record\", \"dataframe\"]\n        if fetch_type not in allowed_fetch_type_values:\n            msg = f\"Only the values {allowed_fetch_type_values} are allowed for 'fetch_type'\"\n            raise ValueError(msg)\n        cursor = self.con.cursor()\n        cursor.execute(query)\n\n        # Cleanup the query.\n        query_clean = query.upper().strip()\n        # Find comments.\n        regex = r\"^\\s*[--;].*\"\n        lines = query_clean.splitlines()\n        final_query = \"\"\n\n        for line_raw in lines:\n            line = line_raw.strip()\n            match_object = re.match(regex, line)\n            if not match_object:\n                final_query += \" \" + line\n        final_query = final_query.strip()\n        query_keywords = [\"SELECT\", \"SHOW\", \"PRAGMA\", \"WITH\"]\n        if any(final_query.startswith(word) for word in query_keywords):\n            result = cursor.fetchall() if fetch_type == \"record\" else cursor.fetchdf()\n        else:\n            result = True\n\n        cursor.close()\n        return result\n\n    def _handle_if_empty(self, if_empty: str = \"warn\") -&gt; None:\n        if if_empty == \"warn\":\n            self.logger.warning(\"The query produced no data.\")\n        elif if_empty == \"skip\":\n            msg = \"The query produced no data. Skipping...\"\n            raise SKIP(msg)\n        elif if_empty == \"fail\":\n            msg = \"The query produced no data.\"\n            raise ValueError(msg)\n\n    def create_table_from_parquet(\n        self,\n        table: str,\n        path: str,\n        schema: str | None = None,\n        if_exists: Literal[\"fail\", \"replace\", \"append\", \"skip\", \"delete\"] = \"fail\",\n    ) -&gt; bool:\n        \"\"\"Create a DuckDB table with a CTAS from Parquet file(s).\n\n        Args:\n            table (str): Destination table.\n            path (str): The path to the source Parquet file(s). Glob expressions are\n                also allowed here (eg. `my_folder/*.parquet`).\n            schema (str, optional): Destination schema. Defaults to None.\n            if_exists (Literal[, optional): What to do if the table already exists.\n            The 'delete' option deletes data and then inserts new one. Defaults to\n                \"fail\".\n\n        Raises:\n            ValueError: If the table exists and `if_exists` is set to `fail`.\n\n        Returns:\n            None: Does not return anything.\n        \"\"\"\n        schema = schema or DuckDB.DEFAULT_SCHEMA\n        fqn = schema + \".\" + table\n        exists = self._check_if_table_exists(schema=schema, table=table)\n\n        if exists:\n            if if_exists == \"replace\":\n                self.run_query(f\"DROP TABLE {fqn}\")\n            elif if_exists == \"append\":\n                self.logger.info(f\"Appending to table {fqn}...\")\n                create_table_query = f\"COPY {fqn} FROM '{path}' (FORMAT 'parquet')\"\n                self.run_query(create_table_query)\n                self.logger.info(f\"Successfully appended data to table '{fqn}'.\")\n                return True\n            elif if_exists == \"delete\":\n                self.run_query(f\"DELETE FROM {fqn}\")  # noqa: S608\n                self.logger.info(f\"Successfully deleted data from table '{fqn}'.\")\n                self.run_query(\n                    f\"INSERT INTO {fqn} SELECT * FROM read_parquet('{path}')\"  # noqa: S608\n                )\n                self.logger.info(f\"Successfully inserted data into table '{fqn}'.\")\n                return True\n            elif if_exists == \"fail\":\n                msg = \"The table already exists and 'if_exists' is set to 'fail'.\"\n                raise ValueError(msg)\n            elif if_exists == \"skip\":\n                return False\n        self.run_query(f\"CREATE SCHEMA IF NOT EXISTS {schema}\")\n        self.logger.info(f\"Creating table {fqn}...\")\n        create_table_query = f\"CREATE TABLE {fqn} AS SELECT * FROM '{path}';\"  # noqa: S608\n        self.run_query(create_table_query)\n        self.logger.info(f\"Table {fqn} has been created successfully.\")\n        return True\n\n    def drop_table(self, table: str, schema: str | None = None) -&gt; bool:\n        \"\"\"Drop a table.\n\n        A thin wrapper around DuckDB.run_query(), with additional logs.\n\n        Args:\n            table (str): The table to be dropped.\n            schema (str, optional): The schema where the table is located. Defaults to\n                None.\n\n        Returns:\n            bool: Whether the table was dropped.\n        \"\"\"\n        schema = schema or DuckDB.DEFAULT_SCHEMA\n        fqn = schema + \".\" + table\n\n        self.logger.info(f\"Dropping table {fqn}...\")\n        dropped = self.run_query(f\"DROP TABLE IF EXISTS {fqn}\")\n        if dropped:\n            self.logger.info(f\"Table {fqn} has been dropped successfully.\")\n        else:\n            self.logger.info(f\"Table {fqn} could not be dropped.\")\n        return dropped\n\n    def _check_if_table_exists(self, table: str, schema: str | None = None) -&gt; bool:\n        schema = schema or DuckDB.DEFAULT_SCHEMA\n        fqn = schema + \".\" + table\n        return fqn in self.tables\n\n    def _check_if_schema_exists(self, schema: str) -&gt; bool:\n        if schema == DuckDB.DEFAULT_SCHEMA:\n            return True\n        fqns = self.tables\n        return any(fqn.split(\".\")[0] == schema for fqn in fqns)\n</code></pre>"},{"location":"references/sources/database/#viadot.sources._duckdb.DuckDB.con","title":"<code>con</code>  <code>property</code>","text":"<p>Return a new connection to the database.</p> <p>As the views are highly isolated, we need a new connection for each query in order to see the changes from previous queries (eg. if we create a new table and then we want to list tables from INFORMATION_SCHEMA, we need to create a new DuckDB connection).</p> <p>Returns:</p> Type Description <code>DuckDBPyConnection</code> <p>duckdb.DuckDBPyConnection: database connection.</p>"},{"location":"references/sources/database/#viadot.sources._duckdb.DuckDB.schemas","title":"<code>schemas</code>  <code>property</code>","text":"<p>Show the list of schemas.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: The list of schemas.</p>"},{"location":"references/sources/database/#viadot.sources._duckdb.DuckDB.tables","title":"<code>tables</code>  <code>property</code>","text":"<p>Show the list of fully qualified table names.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: The list of tables in the format '{SCHEMA}.{TABLE}'.</p>"},{"location":"references/sources/database/#viadot.sources._duckdb.DuckDB.__init__","title":"<code>__init__(config_key=None, credentials=None, *args, **kwargs)</code>","text":"<p>A class for interacting with DuckDB.</p> <p>Parameters:</p> Name Type Description Default <code>config_key</code> <code>str</code> <p>The key inside local config containing the credentials.</p> <code>None</code> <code>credentials</code> <code>DuckDBCredentials</code> <p>Credentials for the connection. Defaults to None.</p> <code>None</code> <code>config_key</code> <code>str</code> <p>The key in the viadot config holding relevant credentials. Defaults to None.</p> <code>None</code> Source code in <code>src/viadot/sources/_duckdb.py</code> <pre><code>def __init__(\n    self,\n    config_key: str | None = None,\n    credentials: DuckDBCredentials | None = None,\n    *args,\n    **kwargs,\n):\n    \"\"\"A class for interacting with DuckDB.\n\n    Args:\n        config_key (str, optional): The key inside local config containing the\n            credentials.\n        credentials (DuckDBCredentials, optional): Credentials for the connection.\n            Defaults to None.\n        config_key (str, optional): The key in the viadot config holding relevant\n            credentials. Defaults to None.\n    \"\"\"\n    raw_creds = credentials or get_source_credentials(config_key) or {}\n    if credentials is None:\n        msg = \"Please specify the credentials.\"\n        raise CredentialError(msg)\n    validated_creds = dict(\n        DuckDBCredentials(**raw_creds)\n    )  # validate the credentials\n    super().__init__(*args, credentials=validated_creds, **kwargs)\n</code></pre>"},{"location":"references/sources/database/#viadot.sources._duckdb.DuckDB.create_table_from_parquet","title":"<code>create_table_from_parquet(table, path, schema=None, if_exists='fail')</code>","text":"<p>Create a DuckDB table with a CTAS from Parquet file(s).</p> <p>Parameters:</p> Name Type Description Default <code>table</code> <code>str</code> <p>Destination table.</p> required <code>path</code> <code>str</code> <p>The path to the source Parquet file(s). Glob expressions are also allowed here (eg. <code>my_folder/*.parquet</code>).</p> required <code>schema</code> <code>str</code> <p>Destination schema. Defaults to None.</p> <code>None</code> <code>if_exists</code> <code>Literal[</code> <p>What to do if the table already exists.</p> <code>'fail'</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the table exists and <code>if_exists</code> is set to <code>fail</code>.</p> <p>Returns:</p> Name Type Description <code>None</code> <code>bool</code> <p>Does not return anything.</p> Source code in <code>src/viadot/sources/_duckdb.py</code> <pre><code>def create_table_from_parquet(\n    self,\n    table: str,\n    path: str,\n    schema: str | None = None,\n    if_exists: Literal[\"fail\", \"replace\", \"append\", \"skip\", \"delete\"] = \"fail\",\n) -&gt; bool:\n    \"\"\"Create a DuckDB table with a CTAS from Parquet file(s).\n\n    Args:\n        table (str): Destination table.\n        path (str): The path to the source Parquet file(s). Glob expressions are\n            also allowed here (eg. `my_folder/*.parquet`).\n        schema (str, optional): Destination schema. Defaults to None.\n        if_exists (Literal[, optional): What to do if the table already exists.\n        The 'delete' option deletes data and then inserts new one. Defaults to\n            \"fail\".\n\n    Raises:\n        ValueError: If the table exists and `if_exists` is set to `fail`.\n\n    Returns:\n        None: Does not return anything.\n    \"\"\"\n    schema = schema or DuckDB.DEFAULT_SCHEMA\n    fqn = schema + \".\" + table\n    exists = self._check_if_table_exists(schema=schema, table=table)\n\n    if exists:\n        if if_exists == \"replace\":\n            self.run_query(f\"DROP TABLE {fqn}\")\n        elif if_exists == \"append\":\n            self.logger.info(f\"Appending to table {fqn}...\")\n            create_table_query = f\"COPY {fqn} FROM '{path}' (FORMAT 'parquet')\"\n            self.run_query(create_table_query)\n            self.logger.info(f\"Successfully appended data to table '{fqn}'.\")\n            return True\n        elif if_exists == \"delete\":\n            self.run_query(f\"DELETE FROM {fqn}\")  # noqa: S608\n            self.logger.info(f\"Successfully deleted data from table '{fqn}'.\")\n            self.run_query(\n                f\"INSERT INTO {fqn} SELECT * FROM read_parquet('{path}')\"  # noqa: S608\n            )\n            self.logger.info(f\"Successfully inserted data into table '{fqn}'.\")\n            return True\n        elif if_exists == \"fail\":\n            msg = \"The table already exists and 'if_exists' is set to 'fail'.\"\n            raise ValueError(msg)\n        elif if_exists == \"skip\":\n            return False\n    self.run_query(f\"CREATE SCHEMA IF NOT EXISTS {schema}\")\n    self.logger.info(f\"Creating table {fqn}...\")\n    create_table_query = f\"CREATE TABLE {fqn} AS SELECT * FROM '{path}';\"  # noqa: S608\n    self.run_query(create_table_query)\n    self.logger.info(f\"Table {fqn} has been created successfully.\")\n    return True\n</code></pre>"},{"location":"references/sources/database/#viadot.sources._duckdb.DuckDB.drop_table","title":"<code>drop_table(table, schema=None)</code>","text":"<p>Drop a table.</p> <p>A thin wrapper around DuckDB.run_query(), with additional logs.</p> <p>Parameters:</p> Name Type Description Default <code>table</code> <code>str</code> <p>The table to be dropped.</p> required <code>schema</code> <code>str</code> <p>The schema where the table is located. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>Whether the table was dropped.</p> Source code in <code>src/viadot/sources/_duckdb.py</code> <pre><code>def drop_table(self, table: str, schema: str | None = None) -&gt; bool:\n    \"\"\"Drop a table.\n\n    A thin wrapper around DuckDB.run_query(), with additional logs.\n\n    Args:\n        table (str): The table to be dropped.\n        schema (str, optional): The schema where the table is located. Defaults to\n            None.\n\n    Returns:\n        bool: Whether the table was dropped.\n    \"\"\"\n    schema = schema or DuckDB.DEFAULT_SCHEMA\n    fqn = schema + \".\" + table\n\n    self.logger.info(f\"Dropping table {fqn}...\")\n    dropped = self.run_query(f\"DROP TABLE IF EXISTS {fqn}\")\n    if dropped:\n        self.logger.info(f\"Table {fqn} has been dropped successfully.\")\n    else:\n        self.logger.info(f\"Table {fqn} could not be dropped.\")\n    return dropped\n</code></pre>"},{"location":"references/sources/database/#viadot.sources._duckdb.DuckDB.run_query","title":"<code>run_query(query, fetch_type='record')</code>","text":"<p>Run a query on DuckDB.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>The query to execute.</p> required <code>fetch_type</code> <code>Literal[</code> <p>How to return the data: either in the default record format or as a pandas DataFrame. Defaults to \"record\".</p> <code>'record'</code> <p>Returns:</p> Type Description <code>list[Record] | bool</code> <p>Union[list[Record], bool]: Either the result set of a query or, in case of DDL/DML queries, a boolean describing whether the query was executed successfully.</p> Source code in <code>src/viadot/sources/_duckdb.py</code> <pre><code>def run_query(\n    self, query: str, fetch_type: Literal[\"record\", \"dataframe\"] = \"record\"\n) -&gt; list[Record] | bool:\n    \"\"\"Run a query on DuckDB.\n\n    Args:\n        query (str): The query to execute.\n        fetch_type (Literal[, optional): How to return the data: either in the\n            default record format or as a pandas DataFrame. Defaults to \"record\".\n\n    Returns:\n        Union[list[Record], bool]: Either the result set of a query or,\n            in case of DDL/DML queries, a boolean describing whether\n            the query was executed successfully.\n    \"\"\"\n    allowed_fetch_type_values = [\"record\", \"dataframe\"]\n    if fetch_type not in allowed_fetch_type_values:\n        msg = f\"Only the values {allowed_fetch_type_values} are allowed for 'fetch_type'\"\n        raise ValueError(msg)\n    cursor = self.con.cursor()\n    cursor.execute(query)\n\n    # Cleanup the query.\n    query_clean = query.upper().strip()\n    # Find comments.\n    regex = r\"^\\s*[--;].*\"\n    lines = query_clean.splitlines()\n    final_query = \"\"\n\n    for line_raw in lines:\n        line = line_raw.strip()\n        match_object = re.match(regex, line)\n        if not match_object:\n            final_query += \" \" + line\n    final_query = final_query.strip()\n    query_keywords = [\"SELECT\", \"SHOW\", \"PRAGMA\", \"WITH\"]\n    if any(final_query.startswith(word) for word in query_keywords):\n        result = cursor.fetchall() if fetch_type == \"record\" else cursor.fetchdf()\n    else:\n        result = True\n\n    cursor.close()\n    return result\n</code></pre>"},{"location":"references/sources/database/#viadot.sources._duckdb.DuckDB.to_df","title":"<code>to_df(query, if_empty=None)</code>","text":"<p>Run DuckDB query and save output to a pandas DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>The query to execute. If query doesn't start with SELECT or WITH, empty DataFrame will be returned.</p> required <code>if_empty</code> <code>str</code> <p>What to do if output DataFrame is empty. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: DataFrame with query output</p> Source code in <code>src/viadot/sources/_duckdb.py</code> <pre><code>def to_df(self, query: str, if_empty: str | None = None) -&gt; pd.DataFrame:\n    \"\"\"Run DuckDB query and save output to a pandas DataFrame.\n\n    Args:\n        query (str): The query to execute. If query doesn't start with SELECT or\n            WITH, empty DataFrame will be returned.\n        if_empty (str, optional): What to do if output DataFrame is empty. Defaults\n            to None.\n\n    Returns:\n        pd.DataFrame: DataFrame with query output\n    \"\"\"\n    if query.upper().startswith(\"SELECT\") or query.upper().startswith(\"WITH\"):\n        df = self.run_query(query, fetch_type=\"dataframe\")\n        if df.empty:\n            self._handle_if_empty(if_empty=if_empty)\n    else:\n        df = pd.DataFrame()\n    return df\n</code></pre>"},{"location":"references/sources/database/#viadot.sources.redshift_spectrum.RedshiftSpectrum","title":"<code>viadot.sources.redshift_spectrum.RedshiftSpectrum</code>","text":"<p>               Bases: <code>Source</code></p> Source code in <code>src/viadot/sources/redshift_spectrum.py</code> <pre><code>class RedshiftSpectrum(Source):\n    def __init__(\n        self,\n        credentials: RedshiftSpectrumCredentials | None = None,\n        config_key: str | None = None,\n        *args,\n        **kwargs,\n    ):\n        \"\"\"A class for working with Amazon Redshift Spectrum.\n\n        Note that internally, AWS SDK refers to schemas as \"databases\", as external\n        schemas correspond to AWS Glue databases. However, to keep consistent naming\n        with all other viadot sources, we use the word \"schema\" instead.\n\n        Args:\n        credentials (RedshiftSpectrumCredentials, optional): RedshiftSpectrumCredentials\n            credentials. Defaults to None.\n        config_key (str, optional): The key in the viadot config holding relevant\n            credentials. Defaults to None.\n\n        Examples:\n        ```python\n        from viadot.sources import RedshiftSpectrum\n\n        with RedshiftSpectrum(config_key=\"redshift_spectrum\") as redshift:\n            redshift.get_schemas()\n        ```\n        \"\"\"\n        raw_creds = credentials or get_source_credentials(config_key)\n        validated_creds = dict(RedshiftSpectrumCredentials(**raw_creds))\n\n        super().__init__(*args, credentials=validated_creds, **kwargs)\n\n        if not self.credentials:\n            self.logger.debug(\n                \"Credentials not specified. Falling back to `boto3` default credentials.\"\n            )\n\n        if self.credentials:\n            endpoint_url = self.credentials.get(\"endpoint_url\")\n            if endpoint_url:\n                wr.config.s3_endpoint_url = endpoint_url\n\n        self._session = None\n        self._con = None\n\n    def __enter__(self):  # noqa: D105\n        return self\n\n    def __exit__(self, exc_type, exc_value, traceback):  # noqa: D105, ANN001\n        if self._con:\n            self._con.close()\n            self._con = None\n\n    @property\n    def session(self) -&gt; boto3.session.Session:\n        \"\"\"A singleton-like property for initiating an AWS session with boto3.\n\n        Note that this is not an actual session, so it does not need to be closed.\n        \"\"\"\n        if not self._session:\n            self._session = boto3.session.Session(\n                region_name=self.credentials.get(\"region_name\"),\n                profile_name=self.credentials.get(\"profile_name\"),\n                aws_access_key_id=self.credentials.get(\"aws_access_key_id\"),\n                aws_secret_access_key=self.credentials.get(\"aws_secret_access_key\"),\n            )\n        return self._session\n\n    @property\n    def con(self) -&gt; redshift_connector.Connection:\n        \"\"\"A singleton-like property for establishing a connection.\"\"\"\n        if not self._con:\n            if self.credentials.get(\"credentials_secret\"):\n                self._con = wr.redshift.connect(\n                    boto3_session=self.session,\n                    timeout=10,\n                    secret_id=self.credentials.get(\"credentials_secret\"),\n                )\n            else:\n                msg = \"The `credentials_secret` config is required to connect to Redshift.\"\n                raise ValueError(msg)\n        return self._con\n\n    def from_df(\n        self,\n        df: pd.DataFrame,\n        to_path: str,\n        schema: str,\n        table: str,\n        extension: Literal[\".parquet\", \".csv\"] = \".parquet\",\n        if_exists: Literal[\"overwrite\", \"overwrite_partitions\", \"append\"] = \"overwrite\",\n        partition_cols: list[str] | None = None,\n        sep: str = \",\",\n        description: str | None = None,\n        **kwargs,\n    ) -&gt; None:\n        \"\"\"Upload a pandas `DataFrame` into a CSV or Parquet file.\n\n        For a full list of available parameters, please refer to the official\n        documentation:\n        https://aws-sdk-pandas.readthedocs.io/en/3.0.0/stubs/awswrangler.s3.to_parquet.html\n        https://aws-sdk-pandas.readthedocs.io/en/3.0.0/stubs/awswrangler.s3.to_csv.html\n\n        Args:\n            df (pd.DataFrame): Pandas `DataFrame`.\n            to_path (str): Path to Amazon S3 folder where the table will be located. If\n                needed, a bottom-level directory named f\"{table}\" is automatically\n                created, so that files are always located in a folder named the same as\n                the table.\n            schema (str): The name of the schema.\n            table (str): The name of the table to load the data into.\n            extension (Literal[\".parquet\", \".csv\"], optional): Required file type.\n                Defaults to '.parquet'.\n            if_exists (Literal[\"overwrite\", \"overwrite_partitions\", \"append\"], optional\n                ): 'overwrite' to recreate the table, 'overwrite_partitions' to only\n                recreate the partitions, 'append' to append the data. Defaults to\n                'overwrite'.\n            partition_cols (List[str], optional): List of column names that will be used\n                to create partitions. Only takes effect if dataset=True. Defaults to\n                None.\n            sep (str, optional): Field delimiter for the output file. Defaults to ','.\n            description (str, optional): Amazon Redshift Spectrum table description.\n                Defaults to None.\n        \"\"\"\n        # Ensure files are in a directory named {table}.\n        if not to_path.rstrip(\"/\").endswith(table):\n            to_path = to_path.rstrip(\"/\") + \"/\" + table\n\n        if extension == \".parquet\":\n            wr.s3.to_parquet(\n                boto3_session=self.session,\n                df=df,\n                path=to_path,\n                mode=if_exists,\n                dataset=True,\n                database=schema,\n                table=table,\n                partition_cols=partition_cols,\n                description=description,\n                **kwargs,\n            )\n        elif extension == \".csv\":\n            wr.s3.to_csv(\n                boto3_session=self.session,\n                df=df,\n                path=to_path,\n                dataset=True,\n                database=schema,\n                table=table,\n                sep=sep,\n                **kwargs,\n            )\n        else:\n            msg = \"Only CSV and parquet formats are supported.\"\n            raise ValueError(msg)\n\n    def to_df(\n        self,\n        schema: str,\n        table: str,\n        **kwargs,\n    ) -&gt; pd.DataFrame:\n        \"\"\"Read a table from an external schema into a pandas `DataFrame`.\n\n        For a full list of available parameters, please refer to the official\n        documentation:\n        https://aws-sdk-pandas.readthedocs.io/en/3.0.0/stubs/awswrangler.s3.read_parquet_table.html\n\n        Args:\n            schema (str): The name of the schema.\n            table (str): The name of the table to load.\n        \"\"\"\n        return wr.s3.read_parquet_table(\n            boto3_session=self.session,\n            database=schema,\n            table=table,\n            **kwargs,\n        )\n\n    def drop_table(\n        self,\n        schema: str,\n        table: str,\n        remove_files: bool = True,\n    ) -&gt; None:\n        \"\"\"Drop a table from a specified external schema.\n\n        Drops a table, including related files from Amazon S3, if specified.\n\n        Args:\n            schema (str): The name of the schema.\n            table (str): The name of the table to drop.\n            remove_files (bool, optional): If True, Amazon S3 file related to the table\n                will be removed. Defaults to True.\n        \"\"\"\n        if remove_files:\n            table_location = wr.catalog.get_table_location(\n                boto3_session=self.session,\n                database=schema,\n                table=table,\n            )\n            wr.s3.delete_objects(boto3_session=self.session, path=table_location)\n\n        wr.catalog.delete_table_if_exists(\n            boto3_session=self.session, database=schema, table=table\n        )\n\n    def get_tables(\n        self,\n        schema: str,\n    ) -&gt; list[str]:\n        \"\"\"Returns a list of tables in a specified schema.\n\n        Args:\n            schema (str): The name of the schema.\n        \"\"\"\n        get_tables_query = f\"SELECT t.tablename FROM SVV_EXTERNAL_TABLES t WHERE t.schemaname = '{schema}'\"  # noqa: S608\n        with self.con.cursor() as cursor:\n            tables_info = cursor.execute(get_tables_query).fetchall()\n        return [table_info[0] for table_info in tables_info]\n\n    def _check_if_table_exists(self, schema: str, table: str) -&gt; bool:\n        \"\"\"Check if a table exists in a specified Redshift Spectrum external schema.\n\n        Args:\n            schema (str): The name of the schema.\n            table (str): The name of the table to verify.\n\n        Returns:\n            bool: Whether the table exists.\n        \"\"\"\n        return table in self.get_tables(schema=schema)\n\n    def create_schema(\n        self,\n        schema: str,\n        description: str | None = None,\n    ) -&gt; None:\n        \"\"\"Create an external schema in Amazon Redshift Spectrum.\n\n        This involves two steps:\n        - creating a Glue database\n        - creating an external schema in Redshift, pointing to above Glue database\n\n        Args:\n            schema (str): The name of the schema.\n            description (str, optional): The description of the schema. Defaults to\n                None.\n        \"\"\"\n        self._create_glue_database(\n            database=schema, description=description, exist_ok=True\n        )\n        create_schema_query = f\"\"\"\ncreate external schema if not exists \"{schema}\" from data catalog\ndatabase '{schema}'\niam_role '{self.credentials.get(\"iam_role\")}'\nregion '{self.credentials.get(\"region_name\")}'\nCREATE EXTERNAL DATABASE IF NOT EXISTS;\n\"\"\"\n        with self.con.cursor() as cursor:\n            cursor.execute(create_schema_query)\n            self.con.commit()\n\n    def _create_glue_database(\n        self,\n        database: str,\n        description: str | None = None,\n        exist_ok: bool = False,\n    ):\n        \"\"\"Create an AWS Glue database.\n\n        Args:\n            database (str): The name of the database.\n            description (Optional[str], optional): The description of the database.\n                Defaults to None.\n            exist_ok (bool, optional): Whether to skip if the database already exists.\n                If set to False, will throw `AlreadyExistsException`. Defaults to False.\n        \"\"\"\n        wr.catalog.create_database(\n            name=database,\n            description=description,\n            boto3_session=self.session,\n            exist_ok=exist_ok,\n        )\n\n    def get_schemas(self) -&gt; list[str]:\n        \"\"\"Returns a list of schemas in the current Redshift Spectrum database.\"\"\"\n        # External Redshift schemas\n        get_schemas_query = \"SELECT schemaname FROM SVV_EXTERNAL_SCHEMAS\"\n        with self.con.cursor() as cursor:\n            schema_names: tuple[list] = cursor.execute(get_schemas_query).fetchall()\n        external_schemas = [schema_name[0] for schema_name in schema_names]\n\n        # Glue databases.\n        schema_infos = wr.catalog.get_databases(boto3_session=self.session)\n        glue_schemas = [schema_info[\"Name\"] for schema_info in schema_infos]\n\n        # An external Redshift schema is a Spectrum schema only if it's also a Glue\n        # database.\n        return [schema for schema in external_schemas if schema in glue_schemas]\n\n    def _check_if_schema_exists(self, schema: str) -&gt; bool:\n        \"\"\"Check if a schema exists in Amazon Redshift Spectrum.\n\n        Args:\n            schema (str): The name of the schema.\n\n        Returns:\n            bool: Whether the schema exists.\n        \"\"\"\n        return schema in self.get_schemas()\n\n    def _is_spectrum_schema(self, schema: str) -&gt; bool:\n        \"\"\"Check if a Redshift schema is a Spectrum schema.\n\n        Args:\n            schema (str): The name of the schema.\n\n        Returns:\n            bool: Whether the schema is a Spectrum schema.\n        \"\"\"\n        return self._check_if_schema_exists(schema)\n\n    def drop_schema(self, schema: str, drop_glue_database: bool = False) -&gt; None:\n        \"\"\"Drop a Spectrum schema. If specified, also drop the underlying Glue database.\n\n        Args:\n            schema (str): The name of the schema.\n        \"\"\"\n        if not self._is_spectrum_schema(schema):\n            msg = f\"Schema {schema} is not a Spectrum schema.\"\n            raise ValueError(msg)\n\n        drop_external_schema_query = f\"DROP SCHEMA IF EXISTS {schema}\"\n        with self.con.cursor() as cursor:\n            cursor.execute(drop_external_schema_query)\n            self.con.commit()\n\n        if drop_glue_database:\n            wr.catalog.delete_database(\n                name=schema,\n                boto3_session=self.session,\n            )\n</code></pre>"},{"location":"references/sources/database/#viadot.sources.redshift_spectrum.RedshiftSpectrum.con","title":"<code>con</code>  <code>property</code>","text":"<p>A singleton-like property for establishing a connection.</p>"},{"location":"references/sources/database/#viadot.sources.redshift_spectrum.RedshiftSpectrum.session","title":"<code>session</code>  <code>property</code>","text":"<p>A singleton-like property for initiating an AWS session with boto3.</p> <p>Note that this is not an actual session, so it does not need to be closed.</p>"},{"location":"references/sources/database/#viadot.sources.redshift_spectrum.RedshiftSpectrum.__init__","title":"<code>__init__(credentials=None, config_key=None, *args, **kwargs)</code>","text":"<p>A class for working with Amazon Redshift Spectrum.</p> <p>Note that internally, AWS SDK refers to schemas as \"databases\", as external schemas correspond to AWS Glue databases. However, to keep consistent naming with all other viadot sources, we use the word \"schema\" instead.</p> <p>credentials (RedshiftSpectrumCredentials, optional): RedshiftSpectrumCredentials     credentials. Defaults to None. config_key (str, optional): The key in the viadot config holding relevant     credentials. Defaults to None.</p> <p>Examples: <pre><code>from viadot.sources import RedshiftSpectrum\n\nwith RedshiftSpectrum(config_key=\"redshift_spectrum\") as redshift:\n    redshift.get_schemas()\n</code></pre></p> Source code in <code>src/viadot/sources/redshift_spectrum.py</code> <pre><code>def __init__(\n    self,\n    credentials: RedshiftSpectrumCredentials | None = None,\n    config_key: str | None = None,\n    *args,\n    **kwargs,\n):\n    \"\"\"A class for working with Amazon Redshift Spectrum.\n\n    Note that internally, AWS SDK refers to schemas as \"databases\", as external\n    schemas correspond to AWS Glue databases. However, to keep consistent naming\n    with all other viadot sources, we use the word \"schema\" instead.\n\n    Args:\n    credentials (RedshiftSpectrumCredentials, optional): RedshiftSpectrumCredentials\n        credentials. Defaults to None.\n    config_key (str, optional): The key in the viadot config holding relevant\n        credentials. Defaults to None.\n\n    Examples:\n    ```python\n    from viadot.sources import RedshiftSpectrum\n\n    with RedshiftSpectrum(config_key=\"redshift_spectrum\") as redshift:\n        redshift.get_schemas()\n    ```\n    \"\"\"\n    raw_creds = credentials or get_source_credentials(config_key)\n    validated_creds = dict(RedshiftSpectrumCredentials(**raw_creds))\n\n    super().__init__(*args, credentials=validated_creds, **kwargs)\n\n    if not self.credentials:\n        self.logger.debug(\n            \"Credentials not specified. Falling back to `boto3` default credentials.\"\n        )\n\n    if self.credentials:\n        endpoint_url = self.credentials.get(\"endpoint_url\")\n        if endpoint_url:\n            wr.config.s3_endpoint_url = endpoint_url\n\n    self._session = None\n    self._con = None\n</code></pre>"},{"location":"references/sources/database/#viadot.sources.redshift_spectrum.RedshiftSpectrum.create_schema","title":"<code>create_schema(schema, description=None)</code>","text":"<p>Create an external schema in Amazon Redshift Spectrum.</p> <p>This involves two steps: - creating a Glue database - creating an external schema in Redshift, pointing to above Glue database</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>str</code> <p>The name of the schema.</p> required <code>description</code> <code>str</code> <p>The description of the schema. Defaults to None.</p> <code>None</code> Source code in <code>src/viadot/sources/redshift_spectrum.py</code> <pre><code>    def create_schema(\n        self,\n        schema: str,\n        description: str | None = None,\n    ) -&gt; None:\n        \"\"\"Create an external schema in Amazon Redshift Spectrum.\n\n        This involves two steps:\n        - creating a Glue database\n        - creating an external schema in Redshift, pointing to above Glue database\n\n        Args:\n            schema (str): The name of the schema.\n            description (str, optional): The description of the schema. Defaults to\n                None.\n        \"\"\"\n        self._create_glue_database(\n            database=schema, description=description, exist_ok=True\n        )\n        create_schema_query = f\"\"\"\ncreate external schema if not exists \"{schema}\" from data catalog\ndatabase '{schema}'\niam_role '{self.credentials.get(\"iam_role\")}'\nregion '{self.credentials.get(\"region_name\")}'\nCREATE EXTERNAL DATABASE IF NOT EXISTS;\n\"\"\"\n        with self.con.cursor() as cursor:\n            cursor.execute(create_schema_query)\n            self.con.commit()\n</code></pre>"},{"location":"references/sources/database/#viadot.sources.redshift_spectrum.RedshiftSpectrum.drop_schema","title":"<code>drop_schema(schema, drop_glue_database=False)</code>","text":"<p>Drop a Spectrum schema. If specified, also drop the underlying Glue database.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>str</code> <p>The name of the schema.</p> required Source code in <code>src/viadot/sources/redshift_spectrum.py</code> <pre><code>def drop_schema(self, schema: str, drop_glue_database: bool = False) -&gt; None:\n    \"\"\"Drop a Spectrum schema. If specified, also drop the underlying Glue database.\n\n    Args:\n        schema (str): The name of the schema.\n    \"\"\"\n    if not self._is_spectrum_schema(schema):\n        msg = f\"Schema {schema} is not a Spectrum schema.\"\n        raise ValueError(msg)\n\n    drop_external_schema_query = f\"DROP SCHEMA IF EXISTS {schema}\"\n    with self.con.cursor() as cursor:\n        cursor.execute(drop_external_schema_query)\n        self.con.commit()\n\n    if drop_glue_database:\n        wr.catalog.delete_database(\n            name=schema,\n            boto3_session=self.session,\n        )\n</code></pre>"},{"location":"references/sources/database/#viadot.sources.redshift_spectrum.RedshiftSpectrum.drop_table","title":"<code>drop_table(schema, table, remove_files=True)</code>","text":"<p>Drop a table from a specified external schema.</p> <p>Drops a table, including related files from Amazon S3, if specified.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>str</code> <p>The name of the schema.</p> required <code>table</code> <code>str</code> <p>The name of the table to drop.</p> required <code>remove_files</code> <code>bool</code> <p>If True, Amazon S3 file related to the table will be removed. Defaults to True.</p> <code>True</code> Source code in <code>src/viadot/sources/redshift_spectrum.py</code> <pre><code>def drop_table(\n    self,\n    schema: str,\n    table: str,\n    remove_files: bool = True,\n) -&gt; None:\n    \"\"\"Drop a table from a specified external schema.\n\n    Drops a table, including related files from Amazon S3, if specified.\n\n    Args:\n        schema (str): The name of the schema.\n        table (str): The name of the table to drop.\n        remove_files (bool, optional): If True, Amazon S3 file related to the table\n            will be removed. Defaults to True.\n    \"\"\"\n    if remove_files:\n        table_location = wr.catalog.get_table_location(\n            boto3_session=self.session,\n            database=schema,\n            table=table,\n        )\n        wr.s3.delete_objects(boto3_session=self.session, path=table_location)\n\n    wr.catalog.delete_table_if_exists(\n        boto3_session=self.session, database=schema, table=table\n    )\n</code></pre>"},{"location":"references/sources/database/#viadot.sources.redshift_spectrum.RedshiftSpectrum.from_df","title":"<code>from_df(df, to_path, schema, table, extension='.parquet', if_exists='overwrite', partition_cols=None, sep=',', description=None, **kwargs)</code>","text":"<p>Upload a pandas <code>DataFrame</code> into a CSV or Parquet file.</p> <p>For a full list of available parameters, please refer to the official documentation: https://aws-sdk-pandas.readthedocs.io/en/3.0.0/stubs/awswrangler.s3.to_parquet.html https://aws-sdk-pandas.readthedocs.io/en/3.0.0/stubs/awswrangler.s3.to_csv.html</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Pandas <code>DataFrame</code>.</p> required <code>to_path</code> <code>str</code> <p>Path to Amazon S3 folder where the table will be located. If needed, a bottom-level directory named f\"{table}\" is automatically created, so that files are always located in a folder named the same as the table.</p> required <code>schema</code> <code>str</code> <p>The name of the schema.</p> required <code>table</code> <code>str</code> <p>The name of the table to load the data into.</p> required <code>extension</code> <code>Literal['.parquet', '.csv']</code> <p>Required file type. Defaults to '.parquet'.</p> <code>'.parquet'</code> <code>partition_cols</code> <code>List[str]</code> <p>List of column names that will be used to create partitions. Only takes effect if dataset=True. Defaults to None.</p> <code>None</code> <code>sep</code> <code>str</code> <p>Field delimiter for the output file. Defaults to ','.</p> <code>','</code> <code>description</code> <code>str</code> <p>Amazon Redshift Spectrum table description. Defaults to None.</p> <code>None</code> Source code in <code>src/viadot/sources/redshift_spectrum.py</code> <pre><code>def from_df(\n    self,\n    df: pd.DataFrame,\n    to_path: str,\n    schema: str,\n    table: str,\n    extension: Literal[\".parquet\", \".csv\"] = \".parquet\",\n    if_exists: Literal[\"overwrite\", \"overwrite_partitions\", \"append\"] = \"overwrite\",\n    partition_cols: list[str] | None = None,\n    sep: str = \",\",\n    description: str | None = None,\n    **kwargs,\n) -&gt; None:\n    \"\"\"Upload a pandas `DataFrame` into a CSV or Parquet file.\n\n    For a full list of available parameters, please refer to the official\n    documentation:\n    https://aws-sdk-pandas.readthedocs.io/en/3.0.0/stubs/awswrangler.s3.to_parquet.html\n    https://aws-sdk-pandas.readthedocs.io/en/3.0.0/stubs/awswrangler.s3.to_csv.html\n\n    Args:\n        df (pd.DataFrame): Pandas `DataFrame`.\n        to_path (str): Path to Amazon S3 folder where the table will be located. If\n            needed, a bottom-level directory named f\"{table}\" is automatically\n            created, so that files are always located in a folder named the same as\n            the table.\n        schema (str): The name of the schema.\n        table (str): The name of the table to load the data into.\n        extension (Literal[\".parquet\", \".csv\"], optional): Required file type.\n            Defaults to '.parquet'.\n        if_exists (Literal[\"overwrite\", \"overwrite_partitions\", \"append\"], optional\n            ): 'overwrite' to recreate the table, 'overwrite_partitions' to only\n            recreate the partitions, 'append' to append the data. Defaults to\n            'overwrite'.\n        partition_cols (List[str], optional): List of column names that will be used\n            to create partitions. Only takes effect if dataset=True. Defaults to\n            None.\n        sep (str, optional): Field delimiter for the output file. Defaults to ','.\n        description (str, optional): Amazon Redshift Spectrum table description.\n            Defaults to None.\n    \"\"\"\n    # Ensure files are in a directory named {table}.\n    if not to_path.rstrip(\"/\").endswith(table):\n        to_path = to_path.rstrip(\"/\") + \"/\" + table\n\n    if extension == \".parquet\":\n        wr.s3.to_parquet(\n            boto3_session=self.session,\n            df=df,\n            path=to_path,\n            mode=if_exists,\n            dataset=True,\n            database=schema,\n            table=table,\n            partition_cols=partition_cols,\n            description=description,\n            **kwargs,\n        )\n    elif extension == \".csv\":\n        wr.s3.to_csv(\n            boto3_session=self.session,\n            df=df,\n            path=to_path,\n            dataset=True,\n            database=schema,\n            table=table,\n            sep=sep,\n            **kwargs,\n        )\n    else:\n        msg = \"Only CSV and parquet formats are supported.\"\n        raise ValueError(msg)\n</code></pre>"},{"location":"references/sources/database/#viadot.sources.redshift_spectrum.RedshiftSpectrum.get_schemas","title":"<code>get_schemas()</code>","text":"<p>Returns a list of schemas in the current Redshift Spectrum database.</p> Source code in <code>src/viadot/sources/redshift_spectrum.py</code> <pre><code>def get_schemas(self) -&gt; list[str]:\n    \"\"\"Returns a list of schemas in the current Redshift Spectrum database.\"\"\"\n    # External Redshift schemas\n    get_schemas_query = \"SELECT schemaname FROM SVV_EXTERNAL_SCHEMAS\"\n    with self.con.cursor() as cursor:\n        schema_names: tuple[list] = cursor.execute(get_schemas_query).fetchall()\n    external_schemas = [schema_name[0] for schema_name in schema_names]\n\n    # Glue databases.\n    schema_infos = wr.catalog.get_databases(boto3_session=self.session)\n    glue_schemas = [schema_info[\"Name\"] for schema_info in schema_infos]\n\n    # An external Redshift schema is a Spectrum schema only if it's also a Glue\n    # database.\n    return [schema for schema in external_schemas if schema in glue_schemas]\n</code></pre>"},{"location":"references/sources/database/#viadot.sources.redshift_spectrum.RedshiftSpectrum.get_tables","title":"<code>get_tables(schema)</code>","text":"<p>Returns a list of tables in a specified schema.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>str</code> <p>The name of the schema.</p> required Source code in <code>src/viadot/sources/redshift_spectrum.py</code> <pre><code>def get_tables(\n    self,\n    schema: str,\n) -&gt; list[str]:\n    \"\"\"Returns a list of tables in a specified schema.\n\n    Args:\n        schema (str): The name of the schema.\n    \"\"\"\n    get_tables_query = f\"SELECT t.tablename FROM SVV_EXTERNAL_TABLES t WHERE t.schemaname = '{schema}'\"  # noqa: S608\n    with self.con.cursor() as cursor:\n        tables_info = cursor.execute(get_tables_query).fetchall()\n    return [table_info[0] for table_info in tables_info]\n</code></pre>"},{"location":"references/sources/database/#viadot.sources.redshift_spectrum.RedshiftSpectrum.to_df","title":"<code>to_df(schema, table, **kwargs)</code>","text":"<p>Read a table from an external schema into a pandas <code>DataFrame</code>.</p> <p>For a full list of available parameters, please refer to the official documentation: https://aws-sdk-pandas.readthedocs.io/en/3.0.0/stubs/awswrangler.s3.read_parquet_table.html</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>str</code> <p>The name of the schema.</p> required <code>table</code> <code>str</code> <p>The name of the table to load.</p> required Source code in <code>src/viadot/sources/redshift_spectrum.py</code> <pre><code>def to_df(\n    self,\n    schema: str,\n    table: str,\n    **kwargs,\n) -&gt; pd.DataFrame:\n    \"\"\"Read a table from an external schema into a pandas `DataFrame`.\n\n    For a full list of available parameters, please refer to the official\n    documentation:\n    https://aws-sdk-pandas.readthedocs.io/en/3.0.0/stubs/awswrangler.s3.read_parquet_table.html\n\n    Args:\n        schema (str): The name of the schema.\n        table (str): The name of the table to load.\n    \"\"\"\n    return wr.s3.read_parquet_table(\n        boto3_session=self.session,\n        database=schema,\n        table=table,\n        **kwargs,\n    )\n</code></pre>"},{"location":"references/sources/database/#viadot.sources.s3.S3","title":"<code>viadot.sources.s3.S3</code>","text":"<p>               Bases: <code>Source</code></p> Source code in <code>src/viadot/sources/s3.py</code> <pre><code>class S3(Source):\n    def __init__(\n        self,\n        credentials: S3Credentials | None = None,\n        config_key: str | None = None,\n        *args,\n        **kwargs,\n    ):\n        \"\"\"A class for pulling data from and uploading to the Amazon S3.\n\n        Args:\n        credentials (S3Credentials, optional): Amazon S3 credentials.\n            Defaults to None.\n        config_key (str, optional): The key in the viadot config holding relevant\n            credentials. Defaults to None.\n        \"\"\"\n        raw_creds = (\n            credentials\n            or get_source_credentials(config_key)\n            or self._get_env_credentials()\n        )\n        validated_creds = dict(S3Credentials(**raw_creds))  # validate the credentials\n\n        super().__init__(*args, credentials=validated_creds, **kwargs)\n\n        if not self.credentials:\n            self.logger.debug(\n                \"Credentials not specified. Falling back to `boto3` default credentials.\"\n            )\n\n        self.fs = s3fs.S3FileSystem(\n            profile=self.credentials.get(\"profile_name\"),\n            region_name=self.credentials.get(\"region_name\"),\n            key=self.credentials.get(\"aws_access_key_id\"),\n            secret=self.credentials.get(\"aws_secret_access_key\"),\n        )\n\n        self._session = None\n\n    @property\n    def session(self) -&gt; boto3.session.Session:\n        \"\"\"A singleton-like property for initiating a session to the AWS.\"\"\"\n        if not self._session:\n            self._session = boto3.session.Session(\n                region_name=self.credentials.get(\"region_name\"),\n                profile_name=self.credentials.get(\"profile_name\"),\n                aws_access_key_id=self.credentials.get(\"aws_access_key_id\"),\n                aws_secret_access_key=self.credentials.get(\"aws_secret_access_key\"),\n            )\n        return self._session\n\n    def _get_env_credentials(self):\n        return {\n            \"region_name\": os.environ.get(\"AWS_DEFAULT_REGION\"),\n            \"aws_access_key_id\": os.environ.get(\"AWS_ACCESS_KEY_ID\"),\n            \"aws_secret_access_key\": os.environ.get(\"AWS_SECRET_ACCESS_KEY\"),\n        }\n\n    def ls(self, path: str, suffix: str | None = None) -&gt; list[str]:\n        \"\"\"Returns a list of objects in a provided path.\n\n        Args:\n            path (str): Path to a folder.\n            suffix (Union[str, List[str], None]): Suffix or list of suffixes for\n                filtering Amazon S3 keys. Defaults to None.\n        \"\"\"\n        return wr.s3.list_objects(boto3_session=self.session, path=path, suffix=suffix)\n\n    def exists(self, path: str) -&gt; bool:\n        \"\"\"Check if an object exists in the Amazon S3.\n\n        Args:\n            path (str): The path to an object to check.\n\n        Returns:\n            bool: Whether the object exists.\n        \"\"\"\n        if not path.startswith(\"s3://\"):\n            msg = \"Path must be an AWS S3 URL ('s3://my/path').\"\n            raise ValueError(msg)\n\n        # Note this only checks for files.\n        file_exists = wr.s3.does_object_exist(boto3_session=self.session, path=path)\n\n        if file_exists:\n            return True\n\n        # Use another method in case the path is a folder.\n        client = self.session.client(\"s3\")\n        bucket = path.split(\"/\")[2]\n        path = str(Path(*path.rstrip(\"/\").split(\"/\")[3:]))\n\n        response = client.list_objects_v2(Bucket=bucket, Prefix=path, Delimiter=\"/\")\n\n        folders_with_prefix: list[dict] = response.get(\"CommonPrefixes\")\n        if folders_with_prefix is None:\n            folder_exists = False\n        else:\n            # This is because list_objects takes in `Prefix`, so eg. if there exists\n            #  a path `a/b/abc` and we run `list_objects_v2(path=`a/b/a`)`,\n            #  it would enlist `a/b/abc` as well.\n            paths = [path[\"Prefix\"].rstrip(\"/\") for path in folders_with_prefix]\n            folder_exists = path in paths\n        return folder_exists\n\n    def cp(self, from_path: str, to_path: str, recursive: bool = False) -&gt; None:\n        \"\"\"Copies the contents of `from_path` to `to_path`.\n\n        Args:\n            from_path (str): The path (S3 URL) of the source directory.\n            to_path (str): The path (S3 URL) of the target directory.\n            recursive (bool, optional): Set this to true if working with directories.\n                Defaults to False.\n\n        Example:\n            Copy files within two S3 locations:\n\n            ```python\n            from viadot.sources.s3 import S3\n\n            s3 = S3()\n            s3.cp(\n                from_path=\"s3://bucket-name/folder_a/\",\n                to_path=\"s3://bucket-name/folder_b/\",\n                recursive=True\n            )\n        \"\"\"\n        self.fs.copy(path1=from_path, path2=to_path, recursive=recursive)\n\n    def rm(self, path: str | list[str]) -&gt; None:\n        \"\"\"Delete files under `path`.\n\n        Args:\n            path (list[str]): Path to a list of files or a directory\n                to be removed. If the path is a directory, it will\n                be removed recursively.\n\n        Example:\n        ```python\n        from viadot.sources import S3\n\n        s3 = S3()\n        s3.rm(path=[\"file1.parquet\"])\n        ```\n        \"\"\"\n        wr.s3.delete_objects(boto3_session=self.session, path=path)\n\n    def from_df(\n        self,\n        df: pd.DataFrame,\n        path: str,\n        extension: Literal[\".csv\", \".parquet\"] = \".parquet\",\n        **kwargs,\n    ) -&gt; None:\n        \"\"\"Upload a pandas `DataFrame` into Amazon S3 as a CSV or Parquet file.\n\n        For a full list of available parameters, please refer to the official\n        documentation:\n        https://aws-sdk-pandas.readthedocs.io/en/3.0.0/stubs/awswrangler.s3.to_parquet.html\n        https://aws-sdk-pandas.readthedocs.io/en/3.0.0/stubs/awswrangler.s3.to_csv.html\n\n        Args:\n            df (pd.DataFrame): The pandas DataFrame to upload.\n            path (str): The destination path.\n            extension (Literal[\".csv\", \".parquet\"], optional): The file extension.\n                Defaults to \".parquet\".\n        \"\"\"\n        if extension == \".parquet\":\n            wr.s3.to_parquet(\n                boto3_session=self.session,\n                df=df,\n                path=path,\n                **kwargs,\n            )\n        elif extension == \".csv\":\n            wr.s3.to_csv(\n                boto3_session=self.session,\n                df=df,\n                path=path,\n                **kwargs,\n            )\n        else:\n            msg = \"Only CSV and Parquet formats are supported.\"\n            raise ValueError(msg)\n\n    def to_df(\n        self,\n        paths: list[str],\n        chunk_size: int | None = None,\n        **kwargs,\n    ) -&gt; pd.DataFrame | Iterable[pd.DataFrame]:\n        \"\"\"Read a CSV or Parquet file into a pandas `DataFrame`.\n\n        Args:\n            paths (list[str]): A list of paths to Amazon S3 files. All files under the\n                path must be of the same type.\n            chunk_size (int, optional): Number of rows to include in each chunk.\n                Defaults to None, ie. return all data as a single `DataFrame`.\n\n        Returns:\n            Union[pd.DataFrame, Iterable[pd.DataFrame]]: A pandas DataFrame\n                or an iterable of pandas DataFrames.\n\n        Example 1:\n        ```python\n        from viadot.sources import S3\n\n        s3 = S3()\n        paths = [\"s3://{bucket}/file1.parquet\", \"s3://{bucket}/file2.parquet\"]\n        s3.to_df(paths=paths)\n        ```\n\n        Example 2:\n        ```python\n        from viadot.sources import S3\n\n        s3 = S3()\n        paths = [\"s3://{bucket}/file1.parquet\", \"s3://{bucket}/file2.parquet\"]\n        dfs = s3.to_df(paths=paths, chunk_size=2)\n\n        for df in dfs:\n            print(df)\n        ```\n        \"\"\"\n        if chunk_size is None:\n            # `chunked` expects either an integer or a boolean.\n            chunk_size = False\n\n        if paths[0].endswith(\".csv\"):\n            df = wr.s3.read_csv(\n                boto3_session=self.session, path=paths, chunksize=chunk_size, **kwargs\n            )\n        elif paths[0].endswith(\".parquet\"):\n            df = wr.s3.read_parquet(\n                boto3_session=self.session, path=paths, chunked=chunk_size, **kwargs\n            )\n        else:\n            msg = \"Only CSV and Parquet formats are supported.\"\n            raise ValueError(msg)\n        return df\n\n    def upload(self, from_path: str, to_path: str) -&gt; None:\n        \"\"\"Upload file(s) to S3.\n\n        Args:\n            from_path (str): Path to local file(s) to be uploaded.\n            to_path (str): Path to the destination file/folder.\n        \"\"\"\n        wr.s3.upload(boto3_session=self.session, local_file=from_path, path=to_path)\n\n    def download(self, from_path: str, to_path: str) -&gt; None:\n        \"\"\"Download file(s) from Amazon S3.\n\n        Args:\n            from_path (str): Path to file in Amazon S3.\n            to_path (str): Path to local file(s) to be stored.\n        \"\"\"\n        wr.s3.download(boto3_session=self.session, path=from_path, local_file=to_path)\n\n    def get_page_iterator(\n        self,\n        bucket_name: str,\n        directory_path: str,\n        operation_name: str = \"list_objects_v2\",\n        **kwargs,\n    ) -&gt; Iterator[dict[str, Any]]:\n        \"\"\"Returns an iterator to paginate through the objects in S3 bucket directory.\n\n        This method uses the S3 paginator to list objects under a specified directory\n        path in a given S3 bucket. It can accept additional optional parameters\n        through **kwargs, which will be passed to the paginator.\n\n        Args:\n            bucket_name (str): The name of the S3 bucket.\n            directory_path (str): The directory path (prefix) in the bucket to list\n                objects from.\n            operation_name (str): The operation name. This is the same name as\n                the method name on the client. Defaults as \"list_objects_v2\".\n            **kwargs: Additional arguments to pass to the paginator (optional).\n\n        Returns:\n            Iterator: An iterator to paginate through the S3 objects.\n        \"\"\"\n        client = self.session.client(\"s3\")\n        paginator = client.get_paginator(operation_name=operation_name)\n\n        return paginator.paginate(Bucket=bucket_name, Prefix=directory_path, **kwargs)\n\n    def get_object_sizes(self, file_paths: str | list[str]) -&gt; dict[str, int | None]:\n        \"\"\"Retrieve the sizes of specified S3 objects.\n\n        Args:\n            file_paths (str | list[str]): A single file path or a list of file paths\n                in S3 bucket.\n\n        Returns:\n            dict[str, int]: A dictionary where the keys are file paths and the values\n                are their corresponding sizes in bytes.\n        \"\"\"\n        return wr.s3.size_objects(boto3_session=self.session, path=file_paths)\n\n    def describe_objects(\n        self,\n        file_paths: str | list[str],\n        last_modified_begin: datetime | None = None,\n        last_modified_end: datetime | None = None,\n    ) -&gt; dict[str, dict[str, Any]]:\n        \"\"\"Describe S3 objects from a received S3 prefix or list of S3 objects paths.\n\n        Args:\n            file_paths (str | list[str]): S3 prefix (accepts Unix shell-style wildcards)\n                (e.g. s3://bucket/prefix) or list of S3 objects paths\n                (e.g. [s3://bucket/key0, s3://bucket/key1]).\n            last_modified_begin (datetime | None, optional): Filter the S3 files by\n                the Last modified date of the object. The filter is applied only after\n                list all s3 files. Defaults to None.\n            last_modified_end (datetime | None, optional): Filter the s3 files by\n                the Last modified date of the object. The filter is applied only after\n                list all s3 files. Defaults to None.\n\n        Returns:\n            dict[str, dict[str, Any]]: Return a dictionary of objects returned from\n            head_objects where the key is the object path.\n\n        Examples:\n            &gt;&gt;&gt; s3 = S3(credentials=aws_credentials)\n                # Describe both objects\n            &gt;&gt;&gt; descs0 = s3.describe_objects(['s3://bucket/key0', 's3://bucket/key1'])\n                # Describe all objects under the prefix\n            &gt;&gt;&gt; descs1 = s3.describe_objects('s3://bucket/prefix')\n        \"\"\"\n        return wr.s3.describe_objects(\n            boto3_session=self.session,\n            path=file_paths,\n            last_modified_begin=last_modified_begin,\n            last_modified_end=last_modified_end,\n        )\n</code></pre>"},{"location":"references/sources/database/#viadot.sources.s3.S3.session","title":"<code>session</code>  <code>property</code>","text":"<p>A singleton-like property for initiating a session to the AWS.</p>"},{"location":"references/sources/database/#viadot.sources.s3.S3.__init__","title":"<code>__init__(credentials=None, config_key=None, *args, **kwargs)</code>","text":"<p>A class for pulling data from and uploading to the Amazon S3.</p> <p>credentials (S3Credentials, optional): Amazon S3 credentials.     Defaults to None. config_key (str, optional): The key in the viadot config holding relevant     credentials. Defaults to None.</p> Source code in <code>src/viadot/sources/s3.py</code> <pre><code>def __init__(\n    self,\n    credentials: S3Credentials | None = None,\n    config_key: str | None = None,\n    *args,\n    **kwargs,\n):\n    \"\"\"A class for pulling data from and uploading to the Amazon S3.\n\n    Args:\n    credentials (S3Credentials, optional): Amazon S3 credentials.\n        Defaults to None.\n    config_key (str, optional): The key in the viadot config holding relevant\n        credentials. Defaults to None.\n    \"\"\"\n    raw_creds = (\n        credentials\n        or get_source_credentials(config_key)\n        or self._get_env_credentials()\n    )\n    validated_creds = dict(S3Credentials(**raw_creds))  # validate the credentials\n\n    super().__init__(*args, credentials=validated_creds, **kwargs)\n\n    if not self.credentials:\n        self.logger.debug(\n            \"Credentials not specified. Falling back to `boto3` default credentials.\"\n        )\n\n    self.fs = s3fs.S3FileSystem(\n        profile=self.credentials.get(\"profile_name\"),\n        region_name=self.credentials.get(\"region_name\"),\n        key=self.credentials.get(\"aws_access_key_id\"),\n        secret=self.credentials.get(\"aws_secret_access_key\"),\n    )\n\n    self._session = None\n</code></pre>"},{"location":"references/sources/database/#viadot.sources.s3.S3.cp","title":"<code>cp(from_path, to_path, recursive=False)</code>","text":"<p>Copies the contents of <code>from_path</code> to <code>to_path</code>.</p> <p>Parameters:</p> Name Type Description Default <code>from_path</code> <code>str</code> <p>The path (S3 URL) of the source directory.</p> required <code>to_path</code> <code>str</code> <p>The path (S3 URL) of the target directory.</p> required <code>recursive</code> <code>bool</code> <p>Set this to true if working with directories. Defaults to False.</p> <code>False</code> Example <p>Copy files within two S3 locations:</p> <p>```python from viadot.sources.s3 import S3</p> <p>s3 = S3() s3.cp(     from_path=\"s3://bucket-name/folder_a/\",     to_path=\"s3://bucket-name/folder_b/\",     recursive=True )</p> Source code in <code>src/viadot/sources/s3.py</code> <pre><code>def cp(self, from_path: str, to_path: str, recursive: bool = False) -&gt; None:\n    \"\"\"Copies the contents of `from_path` to `to_path`.\n\n    Args:\n        from_path (str): The path (S3 URL) of the source directory.\n        to_path (str): The path (S3 URL) of the target directory.\n        recursive (bool, optional): Set this to true if working with directories.\n            Defaults to False.\n\n    Example:\n        Copy files within two S3 locations:\n\n        ```python\n        from viadot.sources.s3 import S3\n\n        s3 = S3()\n        s3.cp(\n            from_path=\"s3://bucket-name/folder_a/\",\n            to_path=\"s3://bucket-name/folder_b/\",\n            recursive=True\n        )\n    \"\"\"\n    self.fs.copy(path1=from_path, path2=to_path, recursive=recursive)\n</code></pre>"},{"location":"references/sources/database/#viadot.sources.s3.S3.describe_objects","title":"<code>describe_objects(file_paths, last_modified_begin=None, last_modified_end=None)</code>","text":"<p>Describe S3 objects from a received S3 prefix or list of S3 objects paths.</p> <p>Parameters:</p> Name Type Description Default <code>file_paths</code> <code>str | list[str]</code> <p>S3 prefix (accepts Unix shell-style wildcards) (e.g. s3://bucket/prefix) or list of S3 objects paths (e.g. [s3://bucket/key0, s3://bucket/key1]).</p> required <code>last_modified_begin</code> <code>datetime | None</code> <p>Filter the S3 files by the Last modified date of the object. The filter is applied only after list all s3 files. Defaults to None.</p> <code>None</code> <code>last_modified_end</code> <code>datetime | None</code> <p>Filter the s3 files by the Last modified date of the object. The filter is applied only after list all s3 files. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, dict[str, Any]]</code> <p>dict[str, dict[str, Any]]: Return a dictionary of objects returned from</p> <code>dict[str, dict[str, Any]]</code> <p>head_objects where the key is the object path.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; s3 = S3(credentials=aws_credentials)\n    # Describe both objects\n&gt;&gt;&gt; descs0 = s3.describe_objects(['s3://bucket/key0', 's3://bucket/key1'])\n    # Describe all objects under the prefix\n&gt;&gt;&gt; descs1 = s3.describe_objects('s3://bucket/prefix')\n</code></pre> Source code in <code>src/viadot/sources/s3.py</code> <pre><code>def describe_objects(\n    self,\n    file_paths: str | list[str],\n    last_modified_begin: datetime | None = None,\n    last_modified_end: datetime | None = None,\n) -&gt; dict[str, dict[str, Any]]:\n    \"\"\"Describe S3 objects from a received S3 prefix or list of S3 objects paths.\n\n    Args:\n        file_paths (str | list[str]): S3 prefix (accepts Unix shell-style wildcards)\n            (e.g. s3://bucket/prefix) or list of S3 objects paths\n            (e.g. [s3://bucket/key0, s3://bucket/key1]).\n        last_modified_begin (datetime | None, optional): Filter the S3 files by\n            the Last modified date of the object. The filter is applied only after\n            list all s3 files. Defaults to None.\n        last_modified_end (datetime | None, optional): Filter the s3 files by\n            the Last modified date of the object. The filter is applied only after\n            list all s3 files. Defaults to None.\n\n    Returns:\n        dict[str, dict[str, Any]]: Return a dictionary of objects returned from\n        head_objects where the key is the object path.\n\n    Examples:\n        &gt;&gt;&gt; s3 = S3(credentials=aws_credentials)\n            # Describe both objects\n        &gt;&gt;&gt; descs0 = s3.describe_objects(['s3://bucket/key0', 's3://bucket/key1'])\n            # Describe all objects under the prefix\n        &gt;&gt;&gt; descs1 = s3.describe_objects('s3://bucket/prefix')\n    \"\"\"\n    return wr.s3.describe_objects(\n        boto3_session=self.session,\n        path=file_paths,\n        last_modified_begin=last_modified_begin,\n        last_modified_end=last_modified_end,\n    )\n</code></pre>"},{"location":"references/sources/database/#viadot.sources.s3.S3.download","title":"<code>download(from_path, to_path)</code>","text":"<p>Download file(s) from Amazon S3.</p> <p>Parameters:</p> Name Type Description Default <code>from_path</code> <code>str</code> <p>Path to file in Amazon S3.</p> required <code>to_path</code> <code>str</code> <p>Path to local file(s) to be stored.</p> required Source code in <code>src/viadot/sources/s3.py</code> <pre><code>def download(self, from_path: str, to_path: str) -&gt; None:\n    \"\"\"Download file(s) from Amazon S3.\n\n    Args:\n        from_path (str): Path to file in Amazon S3.\n        to_path (str): Path to local file(s) to be stored.\n    \"\"\"\n    wr.s3.download(boto3_session=self.session, path=from_path, local_file=to_path)\n</code></pre>"},{"location":"references/sources/database/#viadot.sources.s3.S3.exists","title":"<code>exists(path)</code>","text":"<p>Check if an object exists in the Amazon S3.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to an object to check.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>Whether the object exists.</p> Source code in <code>src/viadot/sources/s3.py</code> <pre><code>def exists(self, path: str) -&gt; bool:\n    \"\"\"Check if an object exists in the Amazon S3.\n\n    Args:\n        path (str): The path to an object to check.\n\n    Returns:\n        bool: Whether the object exists.\n    \"\"\"\n    if not path.startswith(\"s3://\"):\n        msg = \"Path must be an AWS S3 URL ('s3://my/path').\"\n        raise ValueError(msg)\n\n    # Note this only checks for files.\n    file_exists = wr.s3.does_object_exist(boto3_session=self.session, path=path)\n\n    if file_exists:\n        return True\n\n    # Use another method in case the path is a folder.\n    client = self.session.client(\"s3\")\n    bucket = path.split(\"/\")[2]\n    path = str(Path(*path.rstrip(\"/\").split(\"/\")[3:]))\n\n    response = client.list_objects_v2(Bucket=bucket, Prefix=path, Delimiter=\"/\")\n\n    folders_with_prefix: list[dict] = response.get(\"CommonPrefixes\")\n    if folders_with_prefix is None:\n        folder_exists = False\n    else:\n        # This is because list_objects takes in `Prefix`, so eg. if there exists\n        #  a path `a/b/abc` and we run `list_objects_v2(path=`a/b/a`)`,\n        #  it would enlist `a/b/abc` as well.\n        paths = [path[\"Prefix\"].rstrip(\"/\") for path in folders_with_prefix]\n        folder_exists = path in paths\n    return folder_exists\n</code></pre>"},{"location":"references/sources/database/#viadot.sources.s3.S3.from_df","title":"<code>from_df(df, path, extension='.parquet', **kwargs)</code>","text":"<p>Upload a pandas <code>DataFrame</code> into Amazon S3 as a CSV or Parquet file.</p> <p>For a full list of available parameters, please refer to the official documentation: https://aws-sdk-pandas.readthedocs.io/en/3.0.0/stubs/awswrangler.s3.to_parquet.html https://aws-sdk-pandas.readthedocs.io/en/3.0.0/stubs/awswrangler.s3.to_csv.html</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The pandas DataFrame to upload.</p> required <code>path</code> <code>str</code> <p>The destination path.</p> required <code>extension</code> <code>Literal['.csv', '.parquet']</code> <p>The file extension. Defaults to \".parquet\".</p> <code>'.parquet'</code> Source code in <code>src/viadot/sources/s3.py</code> <pre><code>def from_df(\n    self,\n    df: pd.DataFrame,\n    path: str,\n    extension: Literal[\".csv\", \".parquet\"] = \".parquet\",\n    **kwargs,\n) -&gt; None:\n    \"\"\"Upload a pandas `DataFrame` into Amazon S3 as a CSV or Parquet file.\n\n    For a full list of available parameters, please refer to the official\n    documentation:\n    https://aws-sdk-pandas.readthedocs.io/en/3.0.0/stubs/awswrangler.s3.to_parquet.html\n    https://aws-sdk-pandas.readthedocs.io/en/3.0.0/stubs/awswrangler.s3.to_csv.html\n\n    Args:\n        df (pd.DataFrame): The pandas DataFrame to upload.\n        path (str): The destination path.\n        extension (Literal[\".csv\", \".parquet\"], optional): The file extension.\n            Defaults to \".parquet\".\n    \"\"\"\n    if extension == \".parquet\":\n        wr.s3.to_parquet(\n            boto3_session=self.session,\n            df=df,\n            path=path,\n            **kwargs,\n        )\n    elif extension == \".csv\":\n        wr.s3.to_csv(\n            boto3_session=self.session,\n            df=df,\n            path=path,\n            **kwargs,\n        )\n    else:\n        msg = \"Only CSV and Parquet formats are supported.\"\n        raise ValueError(msg)\n</code></pre>"},{"location":"references/sources/database/#viadot.sources.s3.S3.get_object_sizes","title":"<code>get_object_sizes(file_paths)</code>","text":"<p>Retrieve the sizes of specified S3 objects.</p> <p>Parameters:</p> Name Type Description Default <code>file_paths</code> <code>str | list[str]</code> <p>A single file path or a list of file paths in S3 bucket.</p> required <p>Returns:</p> Type Description <code>dict[str, int | None]</code> <p>dict[str, int]: A dictionary where the keys are file paths and the values are their corresponding sizes in bytes.</p> Source code in <code>src/viadot/sources/s3.py</code> <pre><code>def get_object_sizes(self, file_paths: str | list[str]) -&gt; dict[str, int | None]:\n    \"\"\"Retrieve the sizes of specified S3 objects.\n\n    Args:\n        file_paths (str | list[str]): A single file path or a list of file paths\n            in S3 bucket.\n\n    Returns:\n        dict[str, int]: A dictionary where the keys are file paths and the values\n            are their corresponding sizes in bytes.\n    \"\"\"\n    return wr.s3.size_objects(boto3_session=self.session, path=file_paths)\n</code></pre>"},{"location":"references/sources/database/#viadot.sources.s3.S3.get_page_iterator","title":"<code>get_page_iterator(bucket_name, directory_path, operation_name='list_objects_v2', **kwargs)</code>","text":"<p>Returns an iterator to paginate through the objects in S3 bucket directory.</p> <p>This method uses the S3 paginator to list objects under a specified directory path in a given S3 bucket. It can accept additional optional parameters through **kwargs, which will be passed to the paginator.</p> <p>Parameters:</p> Name Type Description Default <code>bucket_name</code> <code>str</code> <p>The name of the S3 bucket.</p> required <code>directory_path</code> <code>str</code> <p>The directory path (prefix) in the bucket to list objects from.</p> required <code>operation_name</code> <code>str</code> <p>The operation name. This is the same name as the method name on the client. Defaults as \"list_objects_v2\".</p> <code>'list_objects_v2'</code> <code>**kwargs</code> <p>Additional arguments to pass to the paginator (optional).</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Iterator</code> <code>Iterator[dict[str, Any]]</code> <p>An iterator to paginate through the S3 objects.</p> Source code in <code>src/viadot/sources/s3.py</code> <pre><code>def get_page_iterator(\n    self,\n    bucket_name: str,\n    directory_path: str,\n    operation_name: str = \"list_objects_v2\",\n    **kwargs,\n) -&gt; Iterator[dict[str, Any]]:\n    \"\"\"Returns an iterator to paginate through the objects in S3 bucket directory.\n\n    This method uses the S3 paginator to list objects under a specified directory\n    path in a given S3 bucket. It can accept additional optional parameters\n    through **kwargs, which will be passed to the paginator.\n\n    Args:\n        bucket_name (str): The name of the S3 bucket.\n        directory_path (str): The directory path (prefix) in the bucket to list\n            objects from.\n        operation_name (str): The operation name. This is the same name as\n            the method name on the client. Defaults as \"list_objects_v2\".\n        **kwargs: Additional arguments to pass to the paginator (optional).\n\n    Returns:\n        Iterator: An iterator to paginate through the S3 objects.\n    \"\"\"\n    client = self.session.client(\"s3\")\n    paginator = client.get_paginator(operation_name=operation_name)\n\n    return paginator.paginate(Bucket=bucket_name, Prefix=directory_path, **kwargs)\n</code></pre>"},{"location":"references/sources/database/#viadot.sources.s3.S3.ls","title":"<code>ls(path, suffix=None)</code>","text":"<p>Returns a list of objects in a provided path.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to a folder.</p> required <code>suffix</code> <code>Union[str, List[str], None]</code> <p>Suffix or list of suffixes for filtering Amazon S3 keys. Defaults to None.</p> <code>None</code> Source code in <code>src/viadot/sources/s3.py</code> <pre><code>def ls(self, path: str, suffix: str | None = None) -&gt; list[str]:\n    \"\"\"Returns a list of objects in a provided path.\n\n    Args:\n        path (str): Path to a folder.\n        suffix (Union[str, List[str], None]): Suffix or list of suffixes for\n            filtering Amazon S3 keys. Defaults to None.\n    \"\"\"\n    return wr.s3.list_objects(boto3_session=self.session, path=path, suffix=suffix)\n</code></pre>"},{"location":"references/sources/database/#viadot.sources.s3.S3.rm","title":"<code>rm(path)</code>","text":"<p>Delete files under <code>path</code>.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>list[str]</code> <p>Path to a list of files or a directory to be removed. If the path is a directory, it will be removed recursively.</p> required <p>Example: <pre><code>from viadot.sources import S3\n\ns3 = S3()\ns3.rm(path=[\"file1.parquet\"])\n</code></pre></p> Source code in <code>src/viadot/sources/s3.py</code> <pre><code>def rm(self, path: str | list[str]) -&gt; None:\n    \"\"\"Delete files under `path`.\n\n    Args:\n        path (list[str]): Path to a list of files or a directory\n            to be removed. If the path is a directory, it will\n            be removed recursively.\n\n    Example:\n    ```python\n    from viadot.sources import S3\n\n    s3 = S3()\n    s3.rm(path=[\"file1.parquet\"])\n    ```\n    \"\"\"\n    wr.s3.delete_objects(boto3_session=self.session, path=path)\n</code></pre>"},{"location":"references/sources/database/#viadot.sources.s3.S3.to_df","title":"<code>to_df(paths, chunk_size=None, **kwargs)</code>","text":"<p>Read a CSV or Parquet file into a pandas <code>DataFrame</code>.</p> <p>Parameters:</p> Name Type Description Default <code>paths</code> <code>list[str]</code> <p>A list of paths to Amazon S3 files. All files under the path must be of the same type.</p> required <code>chunk_size</code> <code>int</code> <p>Number of rows to include in each chunk. Defaults to None, ie. return all data as a single <code>DataFrame</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame | Iterable[DataFrame]</code> <p>Union[pd.DataFrame, Iterable[pd.DataFrame]]: A pandas DataFrame or an iterable of pandas DataFrames.</p> <p>Example 1: <pre><code>from viadot.sources import S3\n\ns3 = S3()\npaths = [\"s3://{bucket}/file1.parquet\", \"s3://{bucket}/file2.parquet\"]\ns3.to_df(paths=paths)\n</code></pre></p> <p>Example 2: <pre><code>from viadot.sources import S3\n\ns3 = S3()\npaths = [\"s3://{bucket}/file1.parquet\", \"s3://{bucket}/file2.parquet\"]\ndfs = s3.to_df(paths=paths, chunk_size=2)\n\nfor df in dfs:\n    print(df)\n</code></pre></p> Source code in <code>src/viadot/sources/s3.py</code> <pre><code>def to_df(\n    self,\n    paths: list[str],\n    chunk_size: int | None = None,\n    **kwargs,\n) -&gt; pd.DataFrame | Iterable[pd.DataFrame]:\n    \"\"\"Read a CSV or Parquet file into a pandas `DataFrame`.\n\n    Args:\n        paths (list[str]): A list of paths to Amazon S3 files. All files under the\n            path must be of the same type.\n        chunk_size (int, optional): Number of rows to include in each chunk.\n            Defaults to None, ie. return all data as a single `DataFrame`.\n\n    Returns:\n        Union[pd.DataFrame, Iterable[pd.DataFrame]]: A pandas DataFrame\n            or an iterable of pandas DataFrames.\n\n    Example 1:\n    ```python\n    from viadot.sources import S3\n\n    s3 = S3()\n    paths = [\"s3://{bucket}/file1.parquet\", \"s3://{bucket}/file2.parquet\"]\n    s3.to_df(paths=paths)\n    ```\n\n    Example 2:\n    ```python\n    from viadot.sources import S3\n\n    s3 = S3()\n    paths = [\"s3://{bucket}/file1.parquet\", \"s3://{bucket}/file2.parquet\"]\n    dfs = s3.to_df(paths=paths, chunk_size=2)\n\n    for df in dfs:\n        print(df)\n    ```\n    \"\"\"\n    if chunk_size is None:\n        # `chunked` expects either an integer or a boolean.\n        chunk_size = False\n\n    if paths[0].endswith(\".csv\"):\n        df = wr.s3.read_csv(\n            boto3_session=self.session, path=paths, chunksize=chunk_size, **kwargs\n        )\n    elif paths[0].endswith(\".parquet\"):\n        df = wr.s3.read_parquet(\n            boto3_session=self.session, path=paths, chunked=chunk_size, **kwargs\n        )\n    else:\n        msg = \"Only CSV and Parquet formats are supported.\"\n        raise ValueError(msg)\n    return df\n</code></pre>"},{"location":"references/sources/database/#viadot.sources.s3.S3.upload","title":"<code>upload(from_path, to_path)</code>","text":"<p>Upload file(s) to S3.</p> <p>Parameters:</p> Name Type Description Default <code>from_path</code> <code>str</code> <p>Path to local file(s) to be uploaded.</p> required <code>to_path</code> <code>str</code> <p>Path to the destination file/folder.</p> required Source code in <code>src/viadot/sources/s3.py</code> <pre><code>def upload(self, from_path: str, to_path: str) -&gt; None:\n    \"\"\"Upload file(s) to S3.\n\n    Args:\n        from_path (str): Path to local file(s) to be uploaded.\n        to_path (str): Path to the destination file/folder.\n    \"\"\"\n    wr.s3.upload(boto3_session=self.session, local_file=from_path, path=to_path)\n</code></pre>"},{"location":"references/sources/database/#viadot.sources.sap_bw.SAPBW","title":"<code>viadot.sources.sap_bw.SAPBW</code>","text":"<p>               Bases: <code>Source</code></p> <p>Quering the SAP BW (SAP Business Warehouse) source using pyrfc library.</p> Documentation to pyrfc can be found under <p>https://sap.github.io/PyRFC/pyrfc.html</p> <p>Documentation for SAP connection modules under:     https://www.se80.co.uk/sap-function-modules/list/?index=rsr_mdx</p> Source code in <code>src/viadot/sources/sap_bw.py</code> <pre><code>class SAPBW(Source):\n    \"\"\"Quering the SAP BW (SAP Business Warehouse) source using pyrfc library.\n\n    Documentation to pyrfc can be found under:\n        https://sap.github.io/PyRFC/pyrfc.html\n    Documentation for SAP connection modules under:\n        https://www.se80.co.uk/sap-function-modules/list/?index=rsr_mdx\n    \"\"\"\n\n    def __init__(\n        self,\n        *args,\n        credentials: SAPBWCredentials | None = None,\n        config_key: str = \"sap_bw\",\n        **kwargs,\n    ):\n        \"\"\"Create an instance of SAP BW.\n\n        Args:\n            credentials (Optional[SAPBWCredentials], optional): SAP BW credentials.\n                Defaults to None.\n            config_key (str, optional): The key in the viadot config holding relevant\n                credentials. Defaults to \"sap_bw\".\n\n        Examples:\n            sap_bw = SAPBW(\n                credentials=credentials,\n                config_key=config_key,\n            )\n            sap_bw.api_connection(\n                ...\n            )\n            data_frame = sap_bw.to_df()\n\n        Raises:\n            CredentialError: If credentials are not provided in local_config or\n                directly as a parameter.\n        \"\"\"\n        raw_creds = credentials or get_source_credentials(config_key)\n        validated_creds = dict(SAPBWCredentials(**raw_creds))\n\n        super().__init__(*args, credentials=validated_creds, **kwargs)\n\n        self.query_output = None\n\n    def _create_connection(self):\n        \"\"\"Create the connection with SAP BW.\n\n        Returns:\n            Connection: Connection to SAP.\n        \"\"\"\n        return pyrfc.Connection(\n            ashost=self.credentials.get(\"ashost\"),\n            sysnr=self.credentials.get(\"sysnr\"),\n            user=self.credentials.get(\"user\"),\n            passwd=self.credentials.get(\"passwd\"),\n            client=self.credentials.get(\"client\"),\n        )\n\n    def api_connection(self, mdx_query: str) -&gt; None:\n        \"\"\"Generate the SAP BW output dataset from MDX query.\n\n        Args:\n            mdx_query (str): The MDX query to be passed to connection.\n        \"\"\"\n        conn = self._create_connection()\n\n        query = textwrap.wrap(mdx_query, 75)\n        properties = conn.call(\"RSR_MDX_CREATE_OBJECT\", COMMAND_TEXT=query)\n\n        datasetid = properties[\"DATASETID\"]\n        self.query_output = conn.call(\"RSR_MDX_GET_FLAT_DATA\", DATASETID=datasetid)\n        conn.close()\n\n    def _apply_user_mapping(\n        self,\n        df: pd.DataFrame,\n        mapping_dict: dict[str, Any] | None = None,\n    ) -&gt; pd.DataFrame:\n        \"\"\"Apply the column mapping defined by user for the output dataframe.\n\n            DataFrame will be cut to selected columns - if any other columns need to be\n        included in the output file, please add them to the mapping dictionary with\n        original names.\n\n        Args:\n            df (pd.DataFrame): Input dataframe for the column mapping task.\n            mapping_dict (dict[str, Any], optional): Dictionary with original and new\n                column names. Defaults to None.\n\n        Returns:\n            pd.DataFrame: Output DataFrame with mapped columns.\n        \"\"\"\n        self.logger.info(\"Applying user defined mapping for columns...\")\n        df = df[mapping_dict.keys()]\n        df.columns = mapping_dict.values()\n\n        self.logger.info(\"Successfully applied user mapping.\")\n\n        return df\n\n    @add_viadot_metadata_columns\n    def to_df(\n        self,\n        if_empty: str = \"warn\",\n        mapping_dict: dict[str, Any] | None = None,\n    ) -&gt; pd.DataFrame:\n        \"\"\"Convert the SAP BW output JSON data into a dataframe.\n\n        Args:\n            if_empty (str, optional): What to do if a fetch produce no data.\n                Defaults to \"warn\".\n            mapping_dict (dict[str, Any], optional): Dictionary with original and new\n                column names. Defaults to None.\n\n        Raises:\n            ValidationError: Prints the original SAP error message in case of issues\n                with MDX execution.\n\n        Returns:\n            pd.Dataframe: The response data as a pandas DataFrame, enriched\n                with viadot metadata columns.\n        \"\"\"\n        raw_data = {}\n\n        if self.query_output[\"RETURN\"][\"MESSAGE\"] == \"\":\n            results = self.query_output[\"DATA\"]\n            for cell in results:\n                if cell[\"ROW\"] not in raw_data:\n                    raw_data[cell[\"ROW\"]] = {}\n                if \"].[\" not in cell[\"DATA\"]:\n                    raw_data[cell[\"ROW\"]][cell[\"COLUMN\"]] = cell[\"DATA\"]\n            rows = [raw_data[row] for row in raw_data]\n            cols = [x[\"DATA\"] for x in self.query_output[\"HEADER\"]]\n\n            data_frame = pd.DataFrame(data=rows)\n            data_frame.columns = cols\n\n        else:\n            data_frame = pd.DataFrame()\n            raise ValidationError(self.query_output[\"RETURN\"][\"MESSAGE\"])\n\n        if mapping_dict:\n            data_frame = self._apply_user_mapping(data_frame, mapping_dict)\n\n        if data_frame.empty:\n            self._handle_if_empty(\n                if_empty=if_empty,\n                message=\"The response does not contain any data.\",\n            )\n        else:\n            self.logger.info(\"Successfully downloaded data from the Mindful API.\")\n\n        return data_frame\n\n    def get_available_columns(self, mdx_query: str) -&gt; list[str]:\n        \"\"\"Generate list of all available columns in a SAP BW table.\n\n        Args:\n            mdx_query (str): The MDX query to be passed to connection.\n\n        Returns:\n            list[str]: List of all available columns in the source table.\n        \"\"\"\n        conn = self._create_connection()\n        query = textwrap.wrap(mdx_query, width=75)\n\n        properties = conn.call(\"RSR_MDX_CREATE_STORED_OBJECT\", COMMAND_TEXT=query)\n        datasetid = properties[\"DATASETID\"]\n\n        if properties[\"RETURN\"][\"MESSAGE\"] == \"\":\n            get_axis_info = conn.call(\"RSR_MDX_GET_AXIS_INFO\", DATASETID=datasetid)\n            cols = get_axis_info[\"AXIS_DIMENSIONS\"]\n\n            all_available_columns = [x[\"DIM_UNAM\"] for x in cols]\n        else:\n            all_available_columns = []\n            self.logger.error(properties[\"RETURN\"][\"MESSAGE\"])\n\n        return all_available_columns\n</code></pre>"},{"location":"references/sources/database/#viadot.sources.sap_bw.SAPBW.__init__","title":"<code>__init__(*args, credentials=None, config_key='sap_bw', **kwargs)</code>","text":"<p>Create an instance of SAP BW.</p> <p>Parameters:</p> Name Type Description Default <code>credentials</code> <code>Optional[SAPBWCredentials]</code> <p>SAP BW credentials. Defaults to None.</p> <code>None</code> <code>config_key</code> <code>str</code> <p>The key in the viadot config holding relevant credentials. Defaults to \"sap_bw\".</p> <code>'sap_bw'</code> <p>Examples:</p> <p>sap_bw = SAPBW(     credentials=credentials,     config_key=config_key, ) sap_bw.api_connection(     ... ) data_frame = sap_bw.to_df()</p> <p>Raises:</p> Type Description <code>CredentialError</code> <p>If credentials are not provided in local_config or directly as a parameter.</p> Source code in <code>src/viadot/sources/sap_bw.py</code> <pre><code>def __init__(\n    self,\n    *args,\n    credentials: SAPBWCredentials | None = None,\n    config_key: str = \"sap_bw\",\n    **kwargs,\n):\n    \"\"\"Create an instance of SAP BW.\n\n    Args:\n        credentials (Optional[SAPBWCredentials], optional): SAP BW credentials.\n            Defaults to None.\n        config_key (str, optional): The key in the viadot config holding relevant\n            credentials. Defaults to \"sap_bw\".\n\n    Examples:\n        sap_bw = SAPBW(\n            credentials=credentials,\n            config_key=config_key,\n        )\n        sap_bw.api_connection(\n            ...\n        )\n        data_frame = sap_bw.to_df()\n\n    Raises:\n        CredentialError: If credentials are not provided in local_config or\n            directly as a parameter.\n    \"\"\"\n    raw_creds = credentials or get_source_credentials(config_key)\n    validated_creds = dict(SAPBWCredentials(**raw_creds))\n\n    super().__init__(*args, credentials=validated_creds, **kwargs)\n\n    self.query_output = None\n</code></pre>"},{"location":"references/sources/database/#viadot.sources.sap_bw.SAPBW.api_connection","title":"<code>api_connection(mdx_query)</code>","text":"<p>Generate the SAP BW output dataset from MDX query.</p> <p>Parameters:</p> Name Type Description Default <code>mdx_query</code> <code>str</code> <p>The MDX query to be passed to connection.</p> required Source code in <code>src/viadot/sources/sap_bw.py</code> <pre><code>def api_connection(self, mdx_query: str) -&gt; None:\n    \"\"\"Generate the SAP BW output dataset from MDX query.\n\n    Args:\n        mdx_query (str): The MDX query to be passed to connection.\n    \"\"\"\n    conn = self._create_connection()\n\n    query = textwrap.wrap(mdx_query, 75)\n    properties = conn.call(\"RSR_MDX_CREATE_OBJECT\", COMMAND_TEXT=query)\n\n    datasetid = properties[\"DATASETID\"]\n    self.query_output = conn.call(\"RSR_MDX_GET_FLAT_DATA\", DATASETID=datasetid)\n    conn.close()\n</code></pre>"},{"location":"references/sources/database/#viadot.sources.sap_bw.SAPBW.get_available_columns","title":"<code>get_available_columns(mdx_query)</code>","text":"<p>Generate list of all available columns in a SAP BW table.</p> <p>Parameters:</p> Name Type Description Default <code>mdx_query</code> <code>str</code> <p>The MDX query to be passed to connection.</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: List of all available columns in the source table.</p> Source code in <code>src/viadot/sources/sap_bw.py</code> <pre><code>def get_available_columns(self, mdx_query: str) -&gt; list[str]:\n    \"\"\"Generate list of all available columns in a SAP BW table.\n\n    Args:\n        mdx_query (str): The MDX query to be passed to connection.\n\n    Returns:\n        list[str]: List of all available columns in the source table.\n    \"\"\"\n    conn = self._create_connection()\n    query = textwrap.wrap(mdx_query, width=75)\n\n    properties = conn.call(\"RSR_MDX_CREATE_STORED_OBJECT\", COMMAND_TEXT=query)\n    datasetid = properties[\"DATASETID\"]\n\n    if properties[\"RETURN\"][\"MESSAGE\"] == \"\":\n        get_axis_info = conn.call(\"RSR_MDX_GET_AXIS_INFO\", DATASETID=datasetid)\n        cols = get_axis_info[\"AXIS_DIMENSIONS\"]\n\n        all_available_columns = [x[\"DIM_UNAM\"] for x in cols]\n    else:\n        all_available_columns = []\n        self.logger.error(properties[\"RETURN\"][\"MESSAGE\"])\n\n    return all_available_columns\n</code></pre>"},{"location":"references/sources/database/#viadot.sources.sap_bw.SAPBW.to_df","title":"<code>to_df(if_empty='warn', mapping_dict=None)</code>","text":"<p>Convert the SAP BW output JSON data into a dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>if_empty</code> <code>str</code> <p>What to do if a fetch produce no data. Defaults to \"warn\".</p> <code>'warn'</code> <code>mapping_dict</code> <code>dict[str, Any]</code> <p>Dictionary with original and new column names. Defaults to None.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValidationError</code> <p>Prints the original SAP error message in case of issues with MDX execution.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.Dataframe: The response data as a pandas DataFrame, enriched with viadot metadata columns.</p> Source code in <code>src/viadot/sources/sap_bw.py</code> <pre><code>@add_viadot_metadata_columns\ndef to_df(\n    self,\n    if_empty: str = \"warn\",\n    mapping_dict: dict[str, Any] | None = None,\n) -&gt; pd.DataFrame:\n    \"\"\"Convert the SAP BW output JSON data into a dataframe.\n\n    Args:\n        if_empty (str, optional): What to do if a fetch produce no data.\n            Defaults to \"warn\".\n        mapping_dict (dict[str, Any], optional): Dictionary with original and new\n            column names. Defaults to None.\n\n    Raises:\n        ValidationError: Prints the original SAP error message in case of issues\n            with MDX execution.\n\n    Returns:\n        pd.Dataframe: The response data as a pandas DataFrame, enriched\n            with viadot metadata columns.\n    \"\"\"\n    raw_data = {}\n\n    if self.query_output[\"RETURN\"][\"MESSAGE\"] == \"\":\n        results = self.query_output[\"DATA\"]\n        for cell in results:\n            if cell[\"ROW\"] not in raw_data:\n                raw_data[cell[\"ROW\"]] = {}\n            if \"].[\" not in cell[\"DATA\"]:\n                raw_data[cell[\"ROW\"]][cell[\"COLUMN\"]] = cell[\"DATA\"]\n        rows = [raw_data[row] for row in raw_data]\n        cols = [x[\"DATA\"] for x in self.query_output[\"HEADER\"]]\n\n        data_frame = pd.DataFrame(data=rows)\n        data_frame.columns = cols\n\n    else:\n        data_frame = pd.DataFrame()\n        raise ValidationError(self.query_output[\"RETURN\"][\"MESSAGE\"])\n\n    if mapping_dict:\n        data_frame = self._apply_user_mapping(data_frame, mapping_dict)\n\n    if data_frame.empty:\n        self._handle_if_empty(\n            if_empty=if_empty,\n            message=\"The response does not contain any data.\",\n        )\n    else:\n        self.logger.info(\"Successfully downloaded data from the Mindful API.\")\n\n    return data_frame\n</code></pre>"},{"location":"references/sources/database/#viadot.sources.sap_rfc.SAPRFC","title":"<code>viadot.sources.sap_rfc.SAPRFC</code>","text":"<p>               Bases: <code>Source</code></p> <p>A class for querying SAP with SQL using the RFC protocol.</p> <p>Note that only a very limited subset of SQL is supported: - aliases - where clauses combined using the AND operator - limit &amp; offset</p> <p>Unsupported: - aggregations - joins - subqueries - etc.</p> Source code in <code>src/viadot/sources/sap_rfc.py</code> <pre><code>class SAPRFC(Source):\n    \"\"\"A class for querying SAP with SQL using the RFC protocol.\n\n    Note that only a very limited subset of SQL is supported:\n    - aliases\n    - where clauses combined using the AND operator\n    - limit &amp; offset\n\n    Unsupported:\n    - aggregations\n    - joins\n    - subqueries\n    - etc.\n    \"\"\"\n\n    COL_CHARACTER_WIDTH_LIMIT = 75\n\n    def __init__(\n        self,\n        sep: str | None = None,\n        replacement: str = \"-\",\n        func: str = \"RFC_READ_TABLE\",\n        rfc_total_col_width_character_limit: int = 400,\n        rfc_unique_id: list[str] | None = None,\n        credentials: dict[str, Any] | None = None,\n        config_key: str | None = None,\n        *args,\n        **kwargs,\n    ):\n        \"\"\"Create an instance of the SAPRFC class.\n\n        Args:\n            sep (str, optional): Which separator to use when querying SAP. If not\n                provided, multiple options are automatically tried.\n            replacement (str, optional): In case of separator is on a columns, set up a\n                new character to replace inside the string to avoid flow breakdowns.\n                Defaults to \"-\".\n            func (str, optional): SAP RFC function to use. Defaults to \"RFC_READ_TABLE\".\n            rfc_total_col_width_character_limit (int, optional): Number of characters by\n                which query will be split in chunks in case of too many columns for RFC\n                function. According to SAP documentation, the limit is 512 characters.\n                However, we observed SAP raising an exception even on a slightly lower\n                number of characters, so we add a safety margin. Defaults to 400.\n            rfc_unique_id  (List[str], optional): Reference columns to merge chunks\n                DataFrames. These columns must to be unique. Defaults to None.\n            credentials (Dict[str, Any], optional): 'api_key'. Defaults to None.\n            config_key (str, optional): The key in the viadot config holding relevant\n                credentials.\n\n        Raises:\n            CredentialError: If provided credentials are incorrect.\n        \"\"\"\n        self._con = None\n\n        credentials = credentials or get_source_credentials(config_key)\n        if credentials is None:\n            msg = \"Please specify the credentials.\"\n            raise CredentialError(msg)\n\n        super().__init__(*args, credentials=credentials, **kwargs)\n\n        self.sep = sep\n        self.replacement = replacement\n        self.client_side_filters = None\n        self.func = func\n        self.rfc_total_col_width_character_limit = rfc_total_col_width_character_limit\n\n        if rfc_unique_id is not None:\n            self.rfc_unique_id = list(set(rfc_unique_id))\n            self._rfc_unique_id_len = {}\n        else:\n            self.rfc_unique_id = rfc_unique_id\n\n    @property\n    def con(self) -&gt; pyrfc.Connection:\n        \"\"\"The pyRFC connection to SAP.\"\"\"\n        if self._con is not None:\n            return self._con\n        con = pyrfc.Connection(**self.credentials)\n        self._con = con\n        return con\n\n    def check_connection(self) -&gt; None:\n        \"\"\"Check the connection to SAP.\"\"\"\n        self.logger.info(\"Checking the connection...\")\n        self.con.ping()\n        self.logger.info(\"Connection has been validated successfully.\")\n\n    def close_connection(self) -&gt; None:\n        \"\"\"Close the SAP RFC connection.\"\"\"\n        self.con.close()\n        self.logger.info(\"Connection has been closed successfully.\")\n\n    def get_function_parameters(\n        self,\n        function_name: str,\n        description: None | Literal[\"short\", \"long\"] = \"short\",\n        *args,\n    ) -&gt; list[str] | pd.DataFrame:\n        \"\"\"Get the description for a SAP RFC function.\n\n        Args:\n            function_name (str): The name of the function to detail.\n            description (Union[None, Literal[, optional): Whether to display\n            a short or a long description. Defaults to \"short\".\n\n        Raises:\n            ValueError: If the argument for description is incorrect.\n\n        Returns:\n            Union[List[str], pd.DataFrame]: Either a list of the function's\n            parameter names (if 'description' is set to None),\n            or a short or long description.\n        \"\"\"\n        if description not in [\"short\", \"long\"]:\n            msg = \"Incorrect value for 'description'. Correct values: None, 'short', 'long'.\"\n            raise ValueError(msg)\n\n        descr = self.con.get_function_description(function_name, *args)\n        param_names = [param[\"name\"] for param in descr.parameters]\n        detailed_params = descr.parameters\n        filtered_detailed_params = [\n            {\n                \"name\": param[\"name\"],\n                \"parameter_type\": param[\"parameter_type\"],\n                \"default_value\": param[\"default_value\"],\n                \"optional\": param[\"optional\"],\n                \"parameter_text\": param[\"parameter_text\"],\n            }\n            for param in descr.parameters\n        ]\n\n        if description is not None:\n            if description == \"long\":\n                params = detailed_params\n            else:\n                params = filtered_detailed_params\n            params = pd.DataFrame.from_records(params)\n        else:\n            params = param_names\n\n        return params\n\n    def _get_where_condition(self, sql: str) -&gt; str:\n        \"\"\"Retrieve the WHERE conditions from a SQL query.\n\n        Args:\n            sql (str): The input SQL query.\n\n        Raises:\n            ValueError: Raised if the WHERE clause is longer than\n            75 characters (SAP's limitation) and the condition for the\n            extra clause(s) is OR.\n\n        Returns:\n            str: The where clause trimmed to &lt;= 75 characters.\n        \"\"\"\n        where_match = re.search(\"\\\\sWHERE \", sql.upper())\n        if not where_match:\n            return None\n\n        limit_match = re.search(\"\\\\sLIMIT \", sql.upper())\n        limit_pos = limit_match.span()[0] if limit_match else len(sql)\n\n        where = sql[where_match.span()[1] : limit_pos]\n        where_sanitized = _remove_whitespaces(where)\n        where_trimmed, client_side_filters = _trim_where(where_sanitized)\n        if client_side_filters:\n            self.logger.warning(\n                \"A WHERE clause longer than 75 character limit detected.\"\n            )\n            if \"OR\" in [key.upper() for key in client_side_filters]:\n                msg = \"WHERE conditions after the 75 character limit can only be combined with the AND keyword.\"\n                raise ValueError(msg)\n            for val in client_side_filters.values():\n                if \")\" in val:\n                    msg = \"Nested conditions eg. AND (col_1 = 'a' AND col_2 = 'b') found between or after 75 characters in WHERE condition!\"\n                    msg += \" Please change nested conditions part of query separated with 'AND' keywords,\"\n                    msg += \" or place nested conditions part at the beginning of the where statement.\"\n                    raise ValueError(msg)\n            filters_pretty = list(client_side_filters.items())\n            self.logger.warning(\n                f\"Trimmed conditions ({filters_pretty}) will be applied client-side.\"\n            )\n            self.logger.warning(\"See the documentation for caveats.\")\n\n        self.client_side_filters = client_side_filters\n        return where_trimmed\n\n    @staticmethod\n    def _get_table_name(sql: str) -&gt; str:\n        parsed = Parser(sql)\n        if len(parsed.tables) &gt; 1:\n            msg = \"Querying more than one table is not supported.\"\n            raise ValueError(msg)\n        return parsed.tables[0]\n\n    def _build_pandas_filter_query(\n        self, client_side_filters: OrderedDictType[str, str]\n    ) -&gt; str:\n        \"\"\"Build a WHERE clause that will be applied client-side.\n\n        This is required if the WHERE clause passed to query() is\n        longer than 75 characters.\n\n        Args:\n            client_side_filters (OrderedDictType[str, str]): The\n            client-side filters to apply.\n\n        Returns:\n            str: the WHERE clause reformatted to fit the format\n            required by DataFrame.query().\n        \"\"\"\n        for i, f in enumerate(client_side_filters.items()):\n            if i == 0:\n                # skip the first keyword; we assume it's \"AND\"\n                query = f[1]\n            else:\n                query += \" \" + f[0] + \" \" + f[1]\n\n            filter_column_name = f[1].split()[0]\n            resolved_column_name = self._resolve_col_name(filter_column_name)\n        return re.sub(\"\\\\s?=\\\\s?\", \" == \", query).replace(\n            filter_column_name, resolved_column_name\n        )\n\n    def extract_values(self, sql: str) -&gt; None:\n        \"\"\"TODO: This should cover all values, not just columns.\"\"\"\n        self.where = self._get_where_condition(sql)\n        self.select_columns = self._get_columns(sql, aliased=False)\n        self.select_columns_aliased = self._get_columns(sql, aliased=True)\n\n    def _resolve_col_name(self, column: str) -&gt; str:\n        \"\"\"Get aliased column name if it exists, otherwise return column name.\"\"\"\n        return self.aliases_keyed_by_columns.get(column, column)\n\n    def _get_columns(self, sql: str, aliased: bool = False) -&gt; list[str]:\n        \"\"\"Retrieve column names from a SQL query.\n\n        Args:\n            sql (str): The SQL query to parse.\n            aliased (bool, optional): Whether to returned aliased\n            names. Defaults to False.\n\n        Returns:\n            List[str]: A list of column names.\n        \"\"\"\n        parsed = Parser(sql)\n        columns = list(parsed.columns_dict[\"select\"])\n        if aliased:\n            aliases_keyed_by_alias = parsed.columns_aliases\n            aliases_keyed_by_columns = OrderedDict(\n                {val: key for key, val in aliases_keyed_by_alias.items()}\n            )\n\n            self.aliases_keyed_by_columns = aliases_keyed_by_columns\n\n            columns = [aliases_keyed_by_columns.get(col, col) for col in columns]\n\n        if self.client_side_filters:\n            # In case the WHERE clause is &gt; 75 characters long, we execute the rest of\n            # the filters client-side. To do this, we need to pull all fields in the\n            # client-side WHERE conditions. Below code adds these columns to the list of\n            # SELECTed fields.\n            cols_to_add = [v.split()[0] for v in self.client_side_filters.values()]\n            if aliased:\n                cols_to_add = [aliases_keyed_by_columns[col] for col in cols_to_add]\n            columns.extend(cols_to_add)\n            columns = list(dict.fromkeys(columns))  # remove duplicates\n\n        return columns\n\n    @staticmethod\n    def _get_limit(sql: str) -&gt; int | None:\n        \"\"\"Get limit from the query.\"\"\"\n        limit_match = re.search(\"\\\\sLIMIT \", sql.upper())\n        if not limit_match:\n            return None\n\n        return int(sql[limit_match.span()[1] :].split()[0])\n\n    @staticmethod\n    def _get_offset(sql: str) -&gt; int | None:\n        \"\"\"Get offset from the query.\"\"\"\n        offset_match = re.search(\"\\\\sOFFSET \", sql.upper())\n        if not offset_match:\n            return None\n\n        return int(sql[offset_match.span()[1] :].split()[0])\n\n    def _parse_dates(\n        self,\n        query: str,\n        dynamic_date_symbols: list[str] = [\"&lt;&lt;\", \"&gt;&gt;\"],  # noqa: B006\n        dynamic_date_format: str = \"%Y%m%d\",\n        dynamic_date_timezone: str = \"UTC\",\n    ) -&gt; str:\n        \"\"\"Process dynamic dates inside the query and validate used patterns type.\n\n        Args:\n            query (str): The SQL query to be processed.\n            dynamic_date_symbols (list[str], optional): Symbols used for dynamic date\n                handling. Defaults to [\"&lt;&lt;\", \"&gt;&gt;\"].\n            dynamic_date_format (str, optional): Format used for dynamic date parsing.\n                Defaults to \"%Y%m%d\".\n            dynamic_date_timezone (str, optional): Timezone used for dynamic date\n                processing. Defaults to \"UTC\".\n\n        Returns:\n            str: The processed SQL query with dynamic dates replaced.\n\n        Raises:\n            TypeError: If the query contains dynamic date patterns that generate\n        a range of dates.\n        \"\"\"\n        ddh = DynamicDateHandler(\n            dynamic_date_symbols=dynamic_date_symbols,\n            dynamic_date_format=dynamic_date_format,\n            dynamic_date_timezone=dynamic_date_timezone,\n        )\n        processed_sql_or_list = ddh.process_dates(query)\n\n        if isinstance(processed_sql_or_list, list):\n            msg = (\n                f\"This query contains {ddh._find_dynamic_date_patterns(query)} dynamic date(s) \"\n                \"that generate a range of dates, which is currently not supported\"\n                \"in query generation. Please use one of the singular pattern dynamic date symbols.\"\n            )\n            raise TypeError(msg)\n\n        return processed_sql_or_list\n\n    # Holy crap what a mess. TODO: refactor this so it can be even remotely tested...\n    def query(self, sql: str, sep: str | None = None) -&gt; None:  # noqa: C901, PLR0912\n        \"\"\"Parse an SQL query into pyRFC commands and save it into an internal dict.\n\n        Args:\n            sql (str): The SQL query to be ran.\n            sep (str, optional): The separator to be used\n            to split columns in the result blob. Defaults to self.sep.\n\n        Raises:\n            ValueError: If the query is not a SELECT query.\n        \"\"\"\n        if not sql.strip().upper().startswith(\"SELECT\"):\n            msg = \"Only SELECT queries are supported.\"\n            raise ValueError(msg)\n\n        sep = sep if sep is not None else self.sep\n\n        sql = adjust_where_condition_by_adding_missing_spaces(sql=sql)\n        self.sql = sql\n\n        self.extract_values(sql)\n\n        table_name = self._get_table_name(sql)\n        # this has to be called before checking client_side_filters\n        where = self.where\n        columns = self.select_columns\n        lists_of_columns = []\n        cols = []\n        col_length_total = 0\n        if isinstance(self.rfc_unique_id[0], str):\n            character_limit = self.rfc_total_col_width_character_limit\n            for rfc_unique_col in self.rfc_unique_id:\n                rfc_unique_col_len = int(\n                    self.call(\n                        \"DDIF_FIELDINFO_GET\",\n                        TABNAME=table_name,\n                        FIELDNAME=rfc_unique_col,\n                    )[\"DFIES_TAB\"][0][\"LENG\"]\n                )\n                if rfc_unique_col_len &gt; int(\n                    self.rfc_total_col_width_character_limit / 4\n                ):\n                    msg = f\"{rfc_unique_col} can't be used as unique column, too large.\"\n                    raise ValueError(msg)\n                local_limit = (\n                    self.rfc_total_col_width_character_limit - rfc_unique_col_len\n                )\n                character_limit = min(local_limit, character_limit)\n                self._rfc_unique_id_len[rfc_unique_col] = rfc_unique_col_len\n        else:\n            character_limit = self.rfc_total_col_width_character_limit\n\n        for col in columns:\n            info = self.call(\"DDIF_FIELDINFO_GET\", TABNAME=table_name, FIELDNAME=col)\n            col_length = info[\"DFIES_TAB\"][0][\"LENG\"]\n            col_length_total += int(col_length)\n            if col_length_total &lt;= character_limit:\n                cols.append(col)\n            else:\n                if isinstance(self.rfc_unique_id[0], str) and all(\n                    rfc_unique_col not in cols for rfc_unique_col in self.rfc_unique_id\n                ):\n                    for rfc_unique_col in self.rfc_unique_id:\n                        if rfc_unique_col not in cols:\n                            cols.append(rfc_unique_col)\n                lists_of_columns.append(cols)\n                cols = [col]\n                col_length_total = int(col_length)\n\n        if isinstance(self.rfc_unique_id[0], str) and all(\n            rfc_unique_col not in cols for rfc_col in self.rfc_unique_id\n        ):\n            for rfc_unique_col in self.rfc_unique_id:\n                if rfc_unique_col not in cols:\n                    cols.append(rfc_unique_col)\n        lists_of_columns.append(cols)\n\n        columns = lists_of_columns\n        options = [{\"TEXT\": where}] if where else None\n        limit = self._get_limit(sql)\n        offset = self._get_offset(sql)\n        query_json = {\n            \"QUERY_TABLE\": table_name,\n            \"FIELDS\": columns,\n            \"OPTIONS\": options,\n            \"ROWCOUNT\": limit,\n            \"ROWSKIPS\": offset,\n            \"DELIMITER\": sep,\n        }\n        # SAP doesn't understand None, so we filter out non-specified parameters\n        query_json_filtered = {\n            key: query_json[key] for key in query_json if query_json[key] is not None\n        }\n        self._query = query_json_filtered\n\n    def call(self, func: str, *args, **kwargs) -&gt; dict[str, Any]:\n        \"\"\"Call a SAP RFC function.\"\"\"\n        return self.con.call(func, *args, **kwargs)\n\n    def _get_alias(self, column: str) -&gt; str:\n        return self.aliases_keyed_by_columns.get(column, column)\n\n    def _get_client_side_filter_cols(self):\n        return [f[1].split()[0] for f in self.client_side_filters.items()]\n\n    def _adjust_whitespaces(self, df: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"Adjust the number of whitespaces.\n\n        Add whitespace characters in each row of each unique column to achieve\n        equal length of values in these columns, ensuring proper merging of subqueries.\n\n        \"\"\"\n        for rfc_unique_col in self.rfc_unique_id:\n            # Check in SAP metadata what is the declared\n            # dtype characters amount\n            rfc_unique_column_len = self._rfc_unique_id_len[rfc_unique_col]\n            actual_length_of_field = df[rfc_unique_col].str.len()\n            # Check which rows have fewer characters\n            # than specified in the column data type.\n            rows_missing_whitespaces = actual_length_of_field &lt; rfc_unique_column_len\n            if any(rows_missing_whitespaces):\n                # Check how many whitespaces are missing in each row.\n                logger.info(f\"Adding whitespaces for {rfc_unique_col} column\")\n                n_missing_whitespaces = rfc_unique_column_len - actual_length_of_field\n                df.loc[rows_missing_whitespaces, rfc_unique_col] += np.char.multiply(\n                    \" \", n_missing_whitespaces[rows_missing_whitespaces]\n                )\n        return df\n\n    # TODO: refactor to remove linter warnings and so this can be tested.\n    @add_viadot_metadata_columns\n    def to_df(self, tests: dict | None = None) -&gt; pd.DataFrame:  # noqa: C901, PLR0912, PLR0915\n        \"\"\"Load the results of a query into a pandas DataFrame.\n\n        Due to SAP limitations, if the length of the WHERE clause is longer than 75\n        characters, we trim whe WHERE clause and perform the rest of the filtering\n        on the resulting DataFrame. Eg. if the WHERE clause contains 4 conditions\n        and has 80 characters, we only perform 3 filters in the query, and perform\n        the last filter on the DataFrame. If characters per row limit will be exceeded,\n        data will be downloaded in chunks.\n\n        Source: https://success.jitterbit.com/display/DOC/Guide+to+Using+RFC_READ_TABLE+to+Query+SAP+Tables#GuidetoUsingRFC_READ_TABLEtoQuerySAPTables-create-the-operation\n        - WHERE clause: 75 character limit\n        - SELECT: 512 character row limit\n\n        Args:\n            tests (Dict[str], optional): A dictionary with optional list of tests\n                to verify the output dataframe. If defined, triggers the `validate`\n                function from utils. Defaults to None.\n\n        Returns:\n            pd.DataFrame: A DataFrame representing the result of the query provided in\n                `PyRFC.query()`.\n        \"\"\"\n        params = self._query\n        columns = self.select_columns_aliased\n        sep = self._query.get(\"DELIMITER\")\n        fields_lists = self._query.get(\"FIELDS\")\n        if len(fields_lists) &gt; 1:\n            logger.info(f\"Data will be downloaded in {len(fields_lists)} chunks.\")\n        func = self.func\n        if sep is None:\n            # Automatically find a working separator.\n            separators = [\n                \"|\",\n                \"/t\",\n                \"#\",\n                \";\",\n                \"@\",\n                \"%\",\n                \"^\",\n                \"`\",\n                \"~\",\n                \"{\",\n                \"}\",\n                \"$\",\n            ]\n        else:\n            separators = [sep]\n\n        for sep in separators:\n            logger.info(f\"Checking if separator '{sep}' works.\")\n            if isinstance(self.rfc_unique_id[0], str):\n                # Columns only for the first chunk. We add the rest later to avoid name\n                # conflicts.\n                df = pd.DataFrame(columns=fields_lists[0])\n            else:\n                df = pd.DataFrame()\n            self._query[\"DELIMITER\"] = sep\n            chunk = 1\n            row_index = 0\n            for fields in fields_lists:\n                logger.info(f\"Downloading {chunk} data chunk...\")\n                self._query[\"FIELDS\"] = fields\n                try:\n                    response = self.call(func, **params)\n                except ABAPApplicationError as e:\n                    if e.key == \"DATA_BUFFER_EXCEEDED\":\n                        msg = \"Character limit per row exceeded. Please select fewer columns.\"\n                        raise DataBufferExceededError(msg) from e\n                    raise\n                # Check and skip if there is no data returned.\n                if response[\"DATA\"]:\n                    record_key = \"WA\"\n                    data_raw = np.array(response[\"DATA\"])\n                    del response\n                    # If reference columns are provided, it's not necessary to remove\n                    # any extra row.\n                    if not isinstance(self.rfc_unique_id[0], str):\n                        row_index, data_raw, start = _detect_extra_rows(\n                            row_index, data_raw, chunk, fields\n                        )\n                    else:\n                        start = False\n                    records = list(_gen_split(data_raw, sep, record_key))\n                    del data_raw\n                    if (\n                        isinstance(self.rfc_unique_id[0], str)\n                        and list(df.columns) != fields\n                    ):\n                        df_tmp = pd.DataFrame(columns=fields)\n                        df_tmp[fields] = records\n                        df_tmp = self._adjust_whitespaces(df_tmp)\n                        df = pd.merge(df, df_tmp, on=self.rfc_unique_id, how=\"outer\")\n                    elif not start:\n                        df[fields] = records\n                    else:\n                        df[fields] = np.nan\n                    chunk += 1\n                elif not response[\"DATA\"]:\n                    logger.warning(\"No data returned from SAP.\")\n        if not df.empty:\n            # It is used to filter out columns which are not in select query\n            # for example columns passed only as unique column\n            df = df.loc[:, columns]\n\n        if self.client_side_filters:\n            filter_query = self._build_pandas_filter_query(self.client_side_filters)\n            df.query(filter_query, inplace=True)\n            client_side_filter_cols_aliased = [\n                self._get_alias(col) for col in self._get_client_side_filter_cols()\n            ]\n            cols_to_drop = [\n                col\n                for col in client_side_filter_cols_aliased\n                if col not in self.select_columns_aliased\n            ]\n            df.drop(cols_to_drop, axis=1, inplace=True)\n        self.close_connection()\n\n        if tests:\n            validate(df=df, tests=tests)\n\n        return df\n</code></pre>"},{"location":"references/sources/database/#viadot.sources.sap_rfc.SAPRFC.con","title":"<code>con</code>  <code>property</code>","text":"<p>The pyRFC connection to SAP.</p>"},{"location":"references/sources/database/#viadot.sources.sap_rfc.SAPRFC.__init__","title":"<code>__init__(sep=None, replacement='-', func='RFC_READ_TABLE', rfc_total_col_width_character_limit=400, rfc_unique_id=None, credentials=None, config_key=None, *args, **kwargs)</code>","text":"<p>Create an instance of the SAPRFC class.</p> <p>Parameters:</p> Name Type Description Default <code>sep</code> <code>str</code> <p>Which separator to use when querying SAP. If not provided, multiple options are automatically tried.</p> <code>None</code> <code>replacement</code> <code>str</code> <p>In case of separator is on a columns, set up a new character to replace inside the string to avoid flow breakdowns. Defaults to \"-\".</p> <code>'-'</code> <code>func</code> <code>str</code> <p>SAP RFC function to use. Defaults to \"RFC_READ_TABLE\".</p> <code>'RFC_READ_TABLE'</code> <code>rfc_total_col_width_character_limit</code> <code>int</code> <p>Number of characters by which query will be split in chunks in case of too many columns for RFC function. According to SAP documentation, the limit is 512 characters. However, we observed SAP raising an exception even on a slightly lower number of characters, so we add a safety margin. Defaults to 400.</p> <code>400</code> <code>rfc_unique_id</code> <code> (List[str]</code> <p>Reference columns to merge chunks DataFrames. These columns must to be unique. Defaults to None.</p> <code>None</code> <code>credentials</code> <code>Dict[str, Any]</code> <p>'api_key'. Defaults to None.</p> <code>None</code> <code>config_key</code> <code>str</code> <p>The key in the viadot config holding relevant credentials.</p> <code>None</code> <p>Raises:</p> Type Description <code>CredentialError</code> <p>If provided credentials are incorrect.</p> Source code in <code>src/viadot/sources/sap_rfc.py</code> <pre><code>def __init__(\n    self,\n    sep: str | None = None,\n    replacement: str = \"-\",\n    func: str = \"RFC_READ_TABLE\",\n    rfc_total_col_width_character_limit: int = 400,\n    rfc_unique_id: list[str] | None = None,\n    credentials: dict[str, Any] | None = None,\n    config_key: str | None = None,\n    *args,\n    **kwargs,\n):\n    \"\"\"Create an instance of the SAPRFC class.\n\n    Args:\n        sep (str, optional): Which separator to use when querying SAP. If not\n            provided, multiple options are automatically tried.\n        replacement (str, optional): In case of separator is on a columns, set up a\n            new character to replace inside the string to avoid flow breakdowns.\n            Defaults to \"-\".\n        func (str, optional): SAP RFC function to use. Defaults to \"RFC_READ_TABLE\".\n        rfc_total_col_width_character_limit (int, optional): Number of characters by\n            which query will be split in chunks in case of too many columns for RFC\n            function. According to SAP documentation, the limit is 512 characters.\n            However, we observed SAP raising an exception even on a slightly lower\n            number of characters, so we add a safety margin. Defaults to 400.\n        rfc_unique_id  (List[str], optional): Reference columns to merge chunks\n            DataFrames. These columns must to be unique. Defaults to None.\n        credentials (Dict[str, Any], optional): 'api_key'. Defaults to None.\n        config_key (str, optional): The key in the viadot config holding relevant\n            credentials.\n\n    Raises:\n        CredentialError: If provided credentials are incorrect.\n    \"\"\"\n    self._con = None\n\n    credentials = credentials or get_source_credentials(config_key)\n    if credentials is None:\n        msg = \"Please specify the credentials.\"\n        raise CredentialError(msg)\n\n    super().__init__(*args, credentials=credentials, **kwargs)\n\n    self.sep = sep\n    self.replacement = replacement\n    self.client_side_filters = None\n    self.func = func\n    self.rfc_total_col_width_character_limit = rfc_total_col_width_character_limit\n\n    if rfc_unique_id is not None:\n        self.rfc_unique_id = list(set(rfc_unique_id))\n        self._rfc_unique_id_len = {}\n    else:\n        self.rfc_unique_id = rfc_unique_id\n</code></pre>"},{"location":"references/sources/database/#viadot.sources.sap_rfc.SAPRFC.call","title":"<code>call(func, *args, **kwargs)</code>","text":"<p>Call a SAP RFC function.</p> Source code in <code>src/viadot/sources/sap_rfc.py</code> <pre><code>def call(self, func: str, *args, **kwargs) -&gt; dict[str, Any]:\n    \"\"\"Call a SAP RFC function.\"\"\"\n    return self.con.call(func, *args, **kwargs)\n</code></pre>"},{"location":"references/sources/database/#viadot.sources.sap_rfc.SAPRFC.check_connection","title":"<code>check_connection()</code>","text":"<p>Check the connection to SAP.</p> Source code in <code>src/viadot/sources/sap_rfc.py</code> <pre><code>def check_connection(self) -&gt; None:\n    \"\"\"Check the connection to SAP.\"\"\"\n    self.logger.info(\"Checking the connection...\")\n    self.con.ping()\n    self.logger.info(\"Connection has been validated successfully.\")\n</code></pre>"},{"location":"references/sources/database/#viadot.sources.sap_rfc.SAPRFC.close_connection","title":"<code>close_connection()</code>","text":"<p>Close the SAP RFC connection.</p> Source code in <code>src/viadot/sources/sap_rfc.py</code> <pre><code>def close_connection(self) -&gt; None:\n    \"\"\"Close the SAP RFC connection.\"\"\"\n    self.con.close()\n    self.logger.info(\"Connection has been closed successfully.\")\n</code></pre>"},{"location":"references/sources/database/#viadot.sources.sap_rfc.SAPRFC.extract_values","title":"<code>extract_values(sql)</code>","text":"<p>TODO: This should cover all values, not just columns.</p> Source code in <code>src/viadot/sources/sap_rfc.py</code> <pre><code>def extract_values(self, sql: str) -&gt; None:\n    \"\"\"TODO: This should cover all values, not just columns.\"\"\"\n    self.where = self._get_where_condition(sql)\n    self.select_columns = self._get_columns(sql, aliased=False)\n    self.select_columns_aliased = self._get_columns(sql, aliased=True)\n</code></pre>"},{"location":"references/sources/database/#viadot.sources.sap_rfc.SAPRFC.get_function_parameters","title":"<code>get_function_parameters(function_name, description='short', *args)</code>","text":"<p>Get the description for a SAP RFC function.</p> <p>Parameters:</p> Name Type Description Default <code>function_name</code> <code>str</code> <p>The name of the function to detail.</p> required <code>description</code> <code>Union[None, Literal[</code> <p>Whether to display</p> <code>'short'</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the argument for description is incorrect.</p> <p>Returns:</p> Type Description <code>list[str] | DataFrame</code> <p>Union[List[str], pd.DataFrame]: Either a list of the function's</p> <code>list[str] | DataFrame</code> <p>parameter names (if 'description' is set to None),</p> <code>list[str] | DataFrame</code> <p>or a short or long description.</p> Source code in <code>src/viadot/sources/sap_rfc.py</code> <pre><code>def get_function_parameters(\n    self,\n    function_name: str,\n    description: None | Literal[\"short\", \"long\"] = \"short\",\n    *args,\n) -&gt; list[str] | pd.DataFrame:\n    \"\"\"Get the description for a SAP RFC function.\n\n    Args:\n        function_name (str): The name of the function to detail.\n        description (Union[None, Literal[, optional): Whether to display\n        a short or a long description. Defaults to \"short\".\n\n    Raises:\n        ValueError: If the argument for description is incorrect.\n\n    Returns:\n        Union[List[str], pd.DataFrame]: Either a list of the function's\n        parameter names (if 'description' is set to None),\n        or a short or long description.\n    \"\"\"\n    if description not in [\"short\", \"long\"]:\n        msg = \"Incorrect value for 'description'. Correct values: None, 'short', 'long'.\"\n        raise ValueError(msg)\n\n    descr = self.con.get_function_description(function_name, *args)\n    param_names = [param[\"name\"] for param in descr.parameters]\n    detailed_params = descr.parameters\n    filtered_detailed_params = [\n        {\n            \"name\": param[\"name\"],\n            \"parameter_type\": param[\"parameter_type\"],\n            \"default_value\": param[\"default_value\"],\n            \"optional\": param[\"optional\"],\n            \"parameter_text\": param[\"parameter_text\"],\n        }\n        for param in descr.parameters\n    ]\n\n    if description is not None:\n        if description == \"long\":\n            params = detailed_params\n        else:\n            params = filtered_detailed_params\n        params = pd.DataFrame.from_records(params)\n    else:\n        params = param_names\n\n    return params\n</code></pre>"},{"location":"references/sources/database/#viadot.sources.sap_rfc.SAPRFC.query","title":"<code>query(sql, sep=None)</code>","text":"<p>Parse an SQL query into pyRFC commands and save it into an internal dict.</p> <p>Parameters:</p> Name Type Description Default <code>sql</code> <code>str</code> <p>The SQL query to be ran.</p> required <code>sep</code> <code>str</code> <p>The separator to be used</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the query is not a SELECT query.</p> Source code in <code>src/viadot/sources/sap_rfc.py</code> <pre><code>def query(self, sql: str, sep: str | None = None) -&gt; None:  # noqa: C901, PLR0912\n    \"\"\"Parse an SQL query into pyRFC commands and save it into an internal dict.\n\n    Args:\n        sql (str): The SQL query to be ran.\n        sep (str, optional): The separator to be used\n        to split columns in the result blob. Defaults to self.sep.\n\n    Raises:\n        ValueError: If the query is not a SELECT query.\n    \"\"\"\n    if not sql.strip().upper().startswith(\"SELECT\"):\n        msg = \"Only SELECT queries are supported.\"\n        raise ValueError(msg)\n\n    sep = sep if sep is not None else self.sep\n\n    sql = adjust_where_condition_by_adding_missing_spaces(sql=sql)\n    self.sql = sql\n\n    self.extract_values(sql)\n\n    table_name = self._get_table_name(sql)\n    # this has to be called before checking client_side_filters\n    where = self.where\n    columns = self.select_columns\n    lists_of_columns = []\n    cols = []\n    col_length_total = 0\n    if isinstance(self.rfc_unique_id[0], str):\n        character_limit = self.rfc_total_col_width_character_limit\n        for rfc_unique_col in self.rfc_unique_id:\n            rfc_unique_col_len = int(\n                self.call(\n                    \"DDIF_FIELDINFO_GET\",\n                    TABNAME=table_name,\n                    FIELDNAME=rfc_unique_col,\n                )[\"DFIES_TAB\"][0][\"LENG\"]\n            )\n            if rfc_unique_col_len &gt; int(\n                self.rfc_total_col_width_character_limit / 4\n            ):\n                msg = f\"{rfc_unique_col} can't be used as unique column, too large.\"\n                raise ValueError(msg)\n            local_limit = (\n                self.rfc_total_col_width_character_limit - rfc_unique_col_len\n            )\n            character_limit = min(local_limit, character_limit)\n            self._rfc_unique_id_len[rfc_unique_col] = rfc_unique_col_len\n    else:\n        character_limit = self.rfc_total_col_width_character_limit\n\n    for col in columns:\n        info = self.call(\"DDIF_FIELDINFO_GET\", TABNAME=table_name, FIELDNAME=col)\n        col_length = info[\"DFIES_TAB\"][0][\"LENG\"]\n        col_length_total += int(col_length)\n        if col_length_total &lt;= character_limit:\n            cols.append(col)\n        else:\n            if isinstance(self.rfc_unique_id[0], str) and all(\n                rfc_unique_col not in cols for rfc_unique_col in self.rfc_unique_id\n            ):\n                for rfc_unique_col in self.rfc_unique_id:\n                    if rfc_unique_col not in cols:\n                        cols.append(rfc_unique_col)\n            lists_of_columns.append(cols)\n            cols = [col]\n            col_length_total = int(col_length)\n\n    if isinstance(self.rfc_unique_id[0], str) and all(\n        rfc_unique_col not in cols for rfc_col in self.rfc_unique_id\n    ):\n        for rfc_unique_col in self.rfc_unique_id:\n            if rfc_unique_col not in cols:\n                cols.append(rfc_unique_col)\n    lists_of_columns.append(cols)\n\n    columns = lists_of_columns\n    options = [{\"TEXT\": where}] if where else None\n    limit = self._get_limit(sql)\n    offset = self._get_offset(sql)\n    query_json = {\n        \"QUERY_TABLE\": table_name,\n        \"FIELDS\": columns,\n        \"OPTIONS\": options,\n        \"ROWCOUNT\": limit,\n        \"ROWSKIPS\": offset,\n        \"DELIMITER\": sep,\n    }\n    # SAP doesn't understand None, so we filter out non-specified parameters\n    query_json_filtered = {\n        key: query_json[key] for key in query_json if query_json[key] is not None\n    }\n    self._query = query_json_filtered\n</code></pre>"},{"location":"references/sources/database/#viadot.sources.sap_rfc.SAPRFC.to_df","title":"<code>to_df(tests=None)</code>","text":"<p>Load the results of a query into a pandas DataFrame.</p> <p>Due to SAP limitations, if the length of the WHERE clause is longer than 75 characters, we trim whe WHERE clause and perform the rest of the filtering on the resulting DataFrame. Eg. if the WHERE clause contains 4 conditions and has 80 characters, we only perform 3 filters in the query, and perform the last filter on the DataFrame. If characters per row limit will be exceeded, data will be downloaded in chunks.</p> <p>Source: https://success.jitterbit.com/display/DOC/Guide+to+Using+RFC_READ_TABLE+to+Query+SAP+Tables#GuidetoUsingRFC_READ_TABLEtoQuerySAPTables-create-the-operation - WHERE clause: 75 character limit - SELECT: 512 character row limit</p> <p>Parameters:</p> Name Type Description Default <code>tests</code> <code>Dict[str]</code> <p>A dictionary with optional list of tests to verify the output dataframe. If defined, triggers the <code>validate</code> function from utils. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A DataFrame representing the result of the query provided in <code>PyRFC.query()</code>.</p> Source code in <code>src/viadot/sources/sap_rfc.py</code> <pre><code>@add_viadot_metadata_columns\ndef to_df(self, tests: dict | None = None) -&gt; pd.DataFrame:  # noqa: C901, PLR0912, PLR0915\n    \"\"\"Load the results of a query into a pandas DataFrame.\n\n    Due to SAP limitations, if the length of the WHERE clause is longer than 75\n    characters, we trim whe WHERE clause and perform the rest of the filtering\n    on the resulting DataFrame. Eg. if the WHERE clause contains 4 conditions\n    and has 80 characters, we only perform 3 filters in the query, and perform\n    the last filter on the DataFrame. If characters per row limit will be exceeded,\n    data will be downloaded in chunks.\n\n    Source: https://success.jitterbit.com/display/DOC/Guide+to+Using+RFC_READ_TABLE+to+Query+SAP+Tables#GuidetoUsingRFC_READ_TABLEtoQuerySAPTables-create-the-operation\n    - WHERE clause: 75 character limit\n    - SELECT: 512 character row limit\n\n    Args:\n        tests (Dict[str], optional): A dictionary with optional list of tests\n            to verify the output dataframe. If defined, triggers the `validate`\n            function from utils. Defaults to None.\n\n    Returns:\n        pd.DataFrame: A DataFrame representing the result of the query provided in\n            `PyRFC.query()`.\n    \"\"\"\n    params = self._query\n    columns = self.select_columns_aliased\n    sep = self._query.get(\"DELIMITER\")\n    fields_lists = self._query.get(\"FIELDS\")\n    if len(fields_lists) &gt; 1:\n        logger.info(f\"Data will be downloaded in {len(fields_lists)} chunks.\")\n    func = self.func\n    if sep is None:\n        # Automatically find a working separator.\n        separators = [\n            \"|\",\n            \"/t\",\n            \"#\",\n            \";\",\n            \"@\",\n            \"%\",\n            \"^\",\n            \"`\",\n            \"~\",\n            \"{\",\n            \"}\",\n            \"$\",\n        ]\n    else:\n        separators = [sep]\n\n    for sep in separators:\n        logger.info(f\"Checking if separator '{sep}' works.\")\n        if isinstance(self.rfc_unique_id[0], str):\n            # Columns only for the first chunk. We add the rest later to avoid name\n            # conflicts.\n            df = pd.DataFrame(columns=fields_lists[0])\n        else:\n            df = pd.DataFrame()\n        self._query[\"DELIMITER\"] = sep\n        chunk = 1\n        row_index = 0\n        for fields in fields_lists:\n            logger.info(f\"Downloading {chunk} data chunk...\")\n            self._query[\"FIELDS\"] = fields\n            try:\n                response = self.call(func, **params)\n            except ABAPApplicationError as e:\n                if e.key == \"DATA_BUFFER_EXCEEDED\":\n                    msg = \"Character limit per row exceeded. Please select fewer columns.\"\n                    raise DataBufferExceededError(msg) from e\n                raise\n            # Check and skip if there is no data returned.\n            if response[\"DATA\"]:\n                record_key = \"WA\"\n                data_raw = np.array(response[\"DATA\"])\n                del response\n                # If reference columns are provided, it's not necessary to remove\n                # any extra row.\n                if not isinstance(self.rfc_unique_id[0], str):\n                    row_index, data_raw, start = _detect_extra_rows(\n                        row_index, data_raw, chunk, fields\n                    )\n                else:\n                    start = False\n                records = list(_gen_split(data_raw, sep, record_key))\n                del data_raw\n                if (\n                    isinstance(self.rfc_unique_id[0], str)\n                    and list(df.columns) != fields\n                ):\n                    df_tmp = pd.DataFrame(columns=fields)\n                    df_tmp[fields] = records\n                    df_tmp = self._adjust_whitespaces(df_tmp)\n                    df = pd.merge(df, df_tmp, on=self.rfc_unique_id, how=\"outer\")\n                elif not start:\n                    df[fields] = records\n                else:\n                    df[fields] = np.nan\n                chunk += 1\n            elif not response[\"DATA\"]:\n                logger.warning(\"No data returned from SAP.\")\n    if not df.empty:\n        # It is used to filter out columns which are not in select query\n        # for example columns passed only as unique column\n        df = df.loc[:, columns]\n\n    if self.client_side_filters:\n        filter_query = self._build_pandas_filter_query(self.client_side_filters)\n        df.query(filter_query, inplace=True)\n        client_side_filter_cols_aliased = [\n            self._get_alias(col) for col in self._get_client_side_filter_cols()\n        ]\n        cols_to_drop = [\n            col\n            for col in client_side_filter_cols_aliased\n            if col not in self.select_columns_aliased\n        ]\n        df.drop(cols_to_drop, axis=1, inplace=True)\n    self.close_connection()\n\n    if tests:\n        validate(df=df, tests=tests)\n\n    return df\n</code></pre>"},{"location":"references/sources/database/#viadot.sources.base.Source","title":"<code>viadot.sources.base.Source</code>","text":"Source code in <code>src/viadot/sources/base.py</code> <pre><code>class Source:\n    def __init__(self, *args, credentials: dict[str, Any] | None = None, **kwargs):  # noqa: ARG002\n        \"\"\"Base class for data sources.\n\n        Args:\n            credentials (dict[str, Any] | None, optional): The credentials for the\n                source. Defaults to None.\n        \"\"\"\n        self.credentials = credentials\n        self.data: pa.Table = None\n        self.logger = logger\n\n    @abstractmethod\n    def to_json(self) -&gt; dict:\n        \"\"\"Download data from source to a dictionary.\"\"\"\n\n    @abstractmethod\n    def to_df(self, if_empty: Literal[\"warn\", \"skip\", \"fail\"] = \"warn\") -&gt; pd.DataFrame:\n        \"\"\"Download data from source to a pandas DataFrame.\n\n        Args:\n            if_empty (Literal[warn, skip, fail], optional): What to do if there is no\n                data. Defaults to \"warn\".\n\n        Returns:\n            pd.DataFrame: The data from the source as a pandas DataFrame.\n        \"\"\"\n\n    @abstractmethod\n    def query(self) -&gt; list[Record] | bool:\n        \"\"\"Run a query and possibly return the results.\"\"\"\n        pass\n\n    def to_arrow(self, if_empty: Literal[\"warn\", \"skip\", \"fail\"] = \"warn\") -&gt; pa.Table:\n        \"\"\"Creates a pyarrow table from source.\n\n        Args:\n            if_empty (Literal[\"warn\", \"skip\", \"fail\"], optional): : What to do if data\n                source contains no data. Defaults to \"warn\".\n        \"\"\"\n        try:\n            df = self.to_df(if_empty=if_empty)\n        except SKIP:\n            return False\n\n        return pa.Table.from_pandas(df)\n\n    def to_csv(\n        self,\n        path: str,\n        if_exists: Literal[\"append\", \"replace\"] = \"replace\",\n        if_empty: Literal[\"warn\", \"skip\", \"fail\"] = \"warn\",\n        sep: str = \"\\t\",\n        **kwargs,\n    ) -&gt; bool:\n        r\"\"\"Write from source to a CSV file.\n\n        Note that the source can be a particular file or table,\n        but also a database in general. Therefore, some sources may require\n        additional parameters to pull the right resource. Hence this method\n        passes kwargs to the `to_df()` method implemented by the concrete source.\n\n        Args:\n            path (str): The destination path.\n            if_exists (Literal[, optional): What to do if the file exists. Defaults to\n                \"replace\".\n            if_empty (Literal[\"warn\", \"skip\", \"fail\"], optional): What to do if the\n                source contains no data. Defaults to \"warn\".\n            sep (str, optional): The separator to use in the CSV. Defaults to \"\\t\".\n\n        Raises:\n            ValueError: If the `if_exists` argument is incorrect.\n\n        Returns:\n            bool: Whether the operation was successful.\n        \"\"\"\n        try:\n            df = self.to_df(if_empty=if_empty, **kwargs)\n        except SKIP:\n            return False\n\n        if if_exists == \"append\":\n            mode = \"a\"\n        elif if_exists == \"replace\":\n            mode = \"w\"\n        else:\n            msg = \"'if_exists' must be one of ['append', 'replace']\"\n            raise ValueError(msg)\n\n        df.to_csv(path, sep=sep, mode=mode, index=False, header=not Path(path).exists())\n\n        return True\n\n    def to_excel(\n        self,\n        path: str,\n        if_exists: str = \"replace\",\n        if_empty: Literal[\"warn\", \"skip\", \"fail\"] = \"warn\",\n    ) -&gt; bool:\n        \"\"\"Write from source to a excel file.\n\n        Args:\n            path (str): The destination path.\n            if_exists (str, optional): What to do if the file exists. Defaults to\n                \"replace\".\n            if_empty (Literal[\"warn\", \"skip\", \"fail\"], optional): What to do if the\n                source contains no data.\n\n        \"\"\"\n        try:\n            df = self.to_df(if_empty=if_empty)\n        except SKIP:\n            return False\n\n        if if_exists == \"append\":\n            if Path(path).is_file():\n                excel_df = pd.read_excel(path)\n                out_df = pd.concat([excel_df, df])\n            else:\n                out_df = df\n        elif if_exists == \"replace\":\n            out_df = df\n        out_df.to_excel(path, index=False, encoding=\"utf8\")\n        return True\n\n    def _handle_if_empty(\n        self,\n        if_empty: Literal[\"warn\", \"skip\", \"fail\"] = \"warn\",\n        message: str = \"The query produced no data.\",\n    ) -&gt; None:\n        \"\"\"What to do if a fetch (database query, API request) produced no data.\"\"\"\n        if if_empty == \"warn\":\n            self.logger.warning(message)\n        elif if_empty == \"skip\":\n            raise SKIP(message)\n        elif if_empty == \"fail\":\n            raise ValueError(message)\n</code></pre>"},{"location":"references/sources/database/#viadot.sources.base.Source.__init__","title":"<code>__init__(*args, credentials=None, **kwargs)</code>","text":"<p>Base class for data sources.</p> <p>Parameters:</p> Name Type Description Default <code>credentials</code> <code>dict[str, Any] | None</code> <p>The credentials for the source. Defaults to None.</p> <code>None</code> Source code in <code>src/viadot/sources/base.py</code> <pre><code>def __init__(self, *args, credentials: dict[str, Any] | None = None, **kwargs):  # noqa: ARG002\n    \"\"\"Base class for data sources.\n\n    Args:\n        credentials (dict[str, Any] | None, optional): The credentials for the\n            source. Defaults to None.\n    \"\"\"\n    self.credentials = credentials\n    self.data: pa.Table = None\n    self.logger = logger\n</code></pre>"},{"location":"references/sources/database/#viadot.sources.base.Source.query","title":"<code>query()</code>  <code>abstractmethod</code>","text":"<p>Run a query and possibly return the results.</p> Source code in <code>src/viadot/sources/base.py</code> <pre><code>@abstractmethod\ndef query(self) -&gt; list[Record] | bool:\n    \"\"\"Run a query and possibly return the results.\"\"\"\n    pass\n</code></pre>"},{"location":"references/sources/database/#viadot.sources.base.Source.to_arrow","title":"<code>to_arrow(if_empty='warn')</code>","text":"<p>Creates a pyarrow table from source.</p> <p>Parameters:</p> Name Type Description Default <code>if_empty</code> <code>Literal['warn', 'skip', 'fail']</code> <p>: What to do if data source contains no data. Defaults to \"warn\".</p> <code>'warn'</code> Source code in <code>src/viadot/sources/base.py</code> <pre><code>def to_arrow(self, if_empty: Literal[\"warn\", \"skip\", \"fail\"] = \"warn\") -&gt; pa.Table:\n    \"\"\"Creates a pyarrow table from source.\n\n    Args:\n        if_empty (Literal[\"warn\", \"skip\", \"fail\"], optional): : What to do if data\n            source contains no data. Defaults to \"warn\".\n    \"\"\"\n    try:\n        df = self.to_df(if_empty=if_empty)\n    except SKIP:\n        return False\n\n    return pa.Table.from_pandas(df)\n</code></pre>"},{"location":"references/sources/database/#viadot.sources.base.Source.to_csv","title":"<code>to_csv(path, if_exists='replace', if_empty='warn', sep='\\t', **kwargs)</code>","text":"<p>Write from source to a CSV file.</p> <p>Note that the source can be a particular file or table, but also a database in general. Therefore, some sources may require additional parameters to pull the right resource. Hence this method passes kwargs to the <code>to_df()</code> method implemented by the concrete source.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The destination path.</p> required <code>if_exists</code> <code>Literal[</code> <p>What to do if the file exists. Defaults to \"replace\".</p> <code>'replace'</code> <code>if_empty</code> <code>Literal['warn', 'skip', 'fail']</code> <p>What to do if the source contains no data. Defaults to \"warn\".</p> <code>'warn'</code> <code>sep</code> <code>str</code> <p>The separator to use in the CSV. Defaults to \"\\t\".</p> <code>'\\t'</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the <code>if_exists</code> argument is incorrect.</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>Whether the operation was successful.</p> Source code in <code>src/viadot/sources/base.py</code> <pre><code>def to_csv(\n    self,\n    path: str,\n    if_exists: Literal[\"append\", \"replace\"] = \"replace\",\n    if_empty: Literal[\"warn\", \"skip\", \"fail\"] = \"warn\",\n    sep: str = \"\\t\",\n    **kwargs,\n) -&gt; bool:\n    r\"\"\"Write from source to a CSV file.\n\n    Note that the source can be a particular file or table,\n    but also a database in general. Therefore, some sources may require\n    additional parameters to pull the right resource. Hence this method\n    passes kwargs to the `to_df()` method implemented by the concrete source.\n\n    Args:\n        path (str): The destination path.\n        if_exists (Literal[, optional): What to do if the file exists. Defaults to\n            \"replace\".\n        if_empty (Literal[\"warn\", \"skip\", \"fail\"], optional): What to do if the\n            source contains no data. Defaults to \"warn\".\n        sep (str, optional): The separator to use in the CSV. Defaults to \"\\t\".\n\n    Raises:\n        ValueError: If the `if_exists` argument is incorrect.\n\n    Returns:\n        bool: Whether the operation was successful.\n    \"\"\"\n    try:\n        df = self.to_df(if_empty=if_empty, **kwargs)\n    except SKIP:\n        return False\n\n    if if_exists == \"append\":\n        mode = \"a\"\n    elif if_exists == \"replace\":\n        mode = \"w\"\n    else:\n        msg = \"'if_exists' must be one of ['append', 'replace']\"\n        raise ValueError(msg)\n\n    df.to_csv(path, sep=sep, mode=mode, index=False, header=not Path(path).exists())\n\n    return True\n</code></pre>"},{"location":"references/sources/database/#viadot.sources.base.Source.to_df","title":"<code>to_df(if_empty='warn')</code>  <code>abstractmethod</code>","text":"<p>Download data from source to a pandas DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>if_empty</code> <code>Literal[warn, skip, fail]</code> <p>What to do if there is no data. Defaults to \"warn\".</p> <code>'warn'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: The data from the source as a pandas DataFrame.</p> Source code in <code>src/viadot/sources/base.py</code> <pre><code>@abstractmethod\ndef to_df(self, if_empty: Literal[\"warn\", \"skip\", \"fail\"] = \"warn\") -&gt; pd.DataFrame:\n    \"\"\"Download data from source to a pandas DataFrame.\n\n    Args:\n        if_empty (Literal[warn, skip, fail], optional): What to do if there is no\n            data. Defaults to \"warn\".\n\n    Returns:\n        pd.DataFrame: The data from the source as a pandas DataFrame.\n    \"\"\"\n</code></pre>"},{"location":"references/sources/database/#viadot.sources.base.Source.to_excel","title":"<code>to_excel(path, if_exists='replace', if_empty='warn')</code>","text":"<p>Write from source to a excel file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The destination path.</p> required <code>if_exists</code> <code>str</code> <p>What to do if the file exists. Defaults to \"replace\".</p> <code>'replace'</code> <code>if_empty</code> <code>Literal['warn', 'skip', 'fail']</code> <p>What to do if the source contains no data.</p> <code>'warn'</code> Source code in <code>src/viadot/sources/base.py</code> <pre><code>def to_excel(\n    self,\n    path: str,\n    if_exists: str = \"replace\",\n    if_empty: Literal[\"warn\", \"skip\", \"fail\"] = \"warn\",\n) -&gt; bool:\n    \"\"\"Write from source to a excel file.\n\n    Args:\n        path (str): The destination path.\n        if_exists (str, optional): What to do if the file exists. Defaults to\n            \"replace\".\n        if_empty (Literal[\"warn\", \"skip\", \"fail\"], optional): What to do if the\n            source contains no data.\n\n    \"\"\"\n    try:\n        df = self.to_df(if_empty=if_empty)\n    except SKIP:\n        return False\n\n    if if_exists == \"append\":\n        if Path(path).is_file():\n            excel_df = pd.read_excel(path)\n            out_df = pd.concat([excel_df, df])\n        else:\n            out_df = df\n    elif if_exists == \"replace\":\n        out_df = df\n    out_df.to_excel(path, index=False, encoding=\"utf8\")\n    return True\n</code></pre>"},{"location":"references/sources/database/#viadot.sources.base.Source.to_json","title":"<code>to_json()</code>  <code>abstractmethod</code>","text":"<p>Download data from source to a dictionary.</p> Source code in <code>src/viadot/sources/base.py</code> <pre><code>@abstractmethod\ndef to_json(self) -&gt; dict:\n    \"\"\"Download data from source to a dictionary.\"\"\"\n</code></pre>"},{"location":"references/sources/database/#viadot.sources.base.SQL","title":"<code>viadot.sources.base.SQL</code>","text":"<p>               Bases: <code>Source</code></p> Source code in <code>src/viadot/sources/base.py</code> <pre><code>class SQL(Source):\n    def __init__(\n        self,\n        driver: str | None = None,\n        config_key: str | None = None,\n        credentials: str | None = None,\n        query_timeout: int = 60 * 60,\n        *args,\n        **kwargs,\n    ):\n        \"\"\"A base SQL source class.\n\n        Args:\n            driver (str, optional): The SQL driver to use. Defaults to None.\n            config_key (str, optional): The key inside local config containing the\n                config. User can choose to use this or pass credentials directly to the\n                `credentials` parameter. Defaults to None.\n            credentials (str, optional): Credentials for the connection. Defaults to\n                None.\n            query_timeout (int, optional): The timeout for executed queries. Defaults to\n                1 hour.\n        \"\"\"\n        self.query_timeout = query_timeout\n\n        config_credentials = get_source_credentials(config_key) if config_key else None\n\n        credentials = credentials or config_credentials or {}\n\n        if driver:\n            credentials[\"driver\"] = driver\n\n        super().__init__(*args, credentials=credentials, **kwargs)\n\n        self._con = None\n\n    @property\n    def conn_str(self) -&gt; str:\n        \"\"\"Generate a connection string from params or config.\n\n        Note that the user and password are escaped with '{}' characters.\n\n        Returns:\n            str: The ODBC connection string.\n        \"\"\"\n        driver = self.credentials[\"driver\"]\n        server = self.credentials[\"server\"]\n        db_name = self.credentials[\"db_name\"]\n        uid = self.credentials.get(\"user\") or \"\"\n        pwd = self.credentials.get(\"password\") or \"\"\n\n        conn_str = f\"DRIVER={{{driver}}};SERVER={server};DATABASE={db_name};UID={uid};PWD={pwd};\"\n\n        if \"authentication\" in self.credentials:\n            conn_str += \"Authentication=\" + self.credentials[\"authentication\"] + \";\"\n\n        if \"trust_server_certificate\" in self.credentials:\n            conn_str += (\n                \"TrustServerCertificate=\"\n                + self.credentials[\"trust_server_certificate\"]\n                + \";\"\n            )\n        if \"encrypt\" in self.credentials:\n            conn_str += \"Encrypt=\" + self.credentials[\"encrypt\"] + \";\"\n        return conn_str\n\n    @property\n    def con(self) -&gt; pyodbc.Connection:\n        \"\"\"A singleton-like property for initiating a connection to the database.\n\n        Returns:\n            pyodbc.Connection: database connection.\n        \"\"\"\n        if not self._con:\n            self._con = pyodbc.connect(self.conn_str, timeout=5)\n            self._con.timeout = self.query_timeout\n        return self._con\n\n    def run(self, query: str) -&gt; list[Record] | bool:\n        \"\"\"Execute a query and return the result.\n\n        Args:\n            query (str): The query to execute.\n\n        Returns:\n            list[Record] | bool: If the query is a SELECT, return the result as a list\n                of records.\n        \"\"\"\n        cursor = self.con.cursor()\n        cursor.execute(query)\n\n        query_sanitized = query.strip().upper()\n        if query_sanitized.startswith(\"SELECT\") or query_sanitized.startswith(\"WITH\"):\n            result = cursor.fetchall()\n        else:\n            result = True\n\n        self.con.commit()\n        cursor.close()\n\n        return result\n\n    def to_df(\n        self,\n        query: str,\n        con: pyodbc.Connection | None = None,\n        if_empty: Literal[\"warn\", \"skip\", \"fail\"] = \"warn\",\n    ) -&gt; pd.DataFrame:\n        \"\"\"Execute a query and return the result as a pandas DataFrame.\n\n        Args:\n            query (str): The query to execute.\n            con (pyodbc.Connection, optional): The connection to use to pull the data.\n            if_empty (Literal[\"warn\", \"skip\", \"fail\"], optional): What to do if the\n                query returns no data. Defaults to None.\n        \"\"\"\n        conn = con or self.con\n\n        query_sanitized = query.strip().upper()\n        if query_sanitized.startswith(\"SELECT\") or query_sanitized.startswith(\"WITH\"):\n            df = pd.read_sql_query(query, conn)\n            if df.empty:\n                self._handle_if_empty(if_empty=if_empty)\n        else:\n            df = pd.DataFrame()\n        return df\n\n    def _check_if_table_exists(self, table: str, schema: str | None = None) -&gt; bool:\n        \"\"\"Check if table exists in a specified schema.\n\n        Args:\n            table (str): Table name.\n            schema (str, optional): Schema name. Defaults to None.\n        \"\"\"\n        exists_query = f\"SELECT * FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_SCHEMA = '{schema}' AND TABLE_NAME='{table}'\"  # noqa: S608\n        return bool(self.run(exists_query))\n\n    def create_table(\n        self,\n        table: str,\n        schema: str | None = None,\n        dtypes: dict[str, Any] | None = None,\n        if_exists: Literal[\"fail\", \"replace\", \"skip\", \"delete\"] = \"fail\",\n    ) -&gt; bool:\n        \"\"\"Create a table.\n\n        Args:\n            table (str): The destination table. Defaults to None.\n            schema (str, optional): The destination schema. Defaults to None.\n            dtypes (Dict[str, Any], optional): The data types to use for the table.\n                Defaults to None.\n            if_exists (Literal, optional): What to do if the table already exists.\n                Defaults to \"fail\".\n\n        Returns:\n            bool: Whether the operation was successful.\n        \"\"\"\n        fqn = f\"{schema}.{table}\" if schema is not None else table\n        exists = self._check_if_table_exists(schema=schema, table=table)\n\n        if exists:\n            if if_exists == \"replace\":\n                self.run(f\"DROP TABLE {fqn}\")\n            elif if_exists == \"delete\":\n                self.run(f\"DELETE FROM {fqn}\")  # noqa: S608\n                return True\n            elif if_exists == \"fail\":\n                msg = (\n                    f\"The table {fqn} already exists and 'if_exists' is set to 'fail'.\"\n                )\n                raise ValueError(msg)\n            elif if_exists == \"skip\":\n                return False\n\n        indent = \"  \"\n        dtypes_rows = [\n            indent + f'\"{col}\"' + \" \" + dtype for col, dtype in dtypes.items()\n        ]\n        dtypes_formatted = \",\\n\".join(dtypes_rows)\n        create_table_sql = f\"CREATE TABLE {fqn}(\\n{dtypes_formatted}\\n)\"\n        self.run(create_table_sql)\n        return True\n\n    def insert_into(self, table: str, df: pd.DataFrame) -&gt; str:\n        \"\"\"Insert values from a pandas DataFrame into an existing database table.\n\n        Args:\n            table (str): table name\n            df (pd.DataFrame): pandas dataframe\n\n        Returns:\n            str: The executed SQL insert query.\n        \"\"\"\n        values = \"\"\n        rows_count = df.shape[0]\n        counter = 0\n        for row in df.values:\n            counter += 1  # noqa: SIM113\n            out_row = \", \".join(map(self._escape_column_name, row))\n            comma = \",\\n\"\n            if counter == rows_count:\n                comma = \";\"\n            out_row = f\"({out_row}){comma}\"\n            values += out_row\n\n        columns = \", \".join(df.columns)\n\n        sql = f\"INSERT INTO {table} ({columns})\\n VALUES {values}\"  # noqa: S608\n        self.run(sql)\n\n        return sql\n\n    def _escape_column_name(self, column_name: str) -&gt; str:\n        \"\"\"Return an escaped column name.\"\"\"\n        return f\"'{column_name}'\"\n</code></pre>"},{"location":"references/sources/database/#viadot.sources.base.SQL.con","title":"<code>con</code>  <code>property</code>","text":"<p>A singleton-like property for initiating a connection to the database.</p> <p>Returns:</p> Type Description <code>Connection</code> <p>pyodbc.Connection: database connection.</p>"},{"location":"references/sources/database/#viadot.sources.base.SQL.conn_str","title":"<code>conn_str</code>  <code>property</code>","text":"<p>Generate a connection string from params or config.</p> <p>Note that the user and password are escaped with '{}' characters.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The ODBC connection string.</p>"},{"location":"references/sources/database/#viadot.sources.base.SQL.__init__","title":"<code>__init__(driver=None, config_key=None, credentials=None, query_timeout=60 * 60, *args, **kwargs)</code>","text":"<p>A base SQL source class.</p> <p>Parameters:</p> Name Type Description Default <code>driver</code> <code>str</code> <p>The SQL driver to use. Defaults to None.</p> <code>None</code> <code>config_key</code> <code>str</code> <p>The key inside local config containing the config. User can choose to use this or pass credentials directly to the <code>credentials</code> parameter. Defaults to None.</p> <code>None</code> <code>credentials</code> <code>str</code> <p>Credentials for the connection. Defaults to None.</p> <code>None</code> <code>query_timeout</code> <code>int</code> <p>The timeout for executed queries. Defaults to 1 hour.</p> <code>60 * 60</code> Source code in <code>src/viadot/sources/base.py</code> <pre><code>def __init__(\n    self,\n    driver: str | None = None,\n    config_key: str | None = None,\n    credentials: str | None = None,\n    query_timeout: int = 60 * 60,\n    *args,\n    **kwargs,\n):\n    \"\"\"A base SQL source class.\n\n    Args:\n        driver (str, optional): The SQL driver to use. Defaults to None.\n        config_key (str, optional): The key inside local config containing the\n            config. User can choose to use this or pass credentials directly to the\n            `credentials` parameter. Defaults to None.\n        credentials (str, optional): Credentials for the connection. Defaults to\n            None.\n        query_timeout (int, optional): The timeout for executed queries. Defaults to\n            1 hour.\n    \"\"\"\n    self.query_timeout = query_timeout\n\n    config_credentials = get_source_credentials(config_key) if config_key else None\n\n    credentials = credentials or config_credentials or {}\n\n    if driver:\n        credentials[\"driver\"] = driver\n\n    super().__init__(*args, credentials=credentials, **kwargs)\n\n    self._con = None\n</code></pre>"},{"location":"references/sources/database/#viadot.sources.base.SQL.create_table","title":"<code>create_table(table, schema=None, dtypes=None, if_exists='fail')</code>","text":"<p>Create a table.</p> <p>Parameters:</p> Name Type Description Default <code>table</code> <code>str</code> <p>The destination table. Defaults to None.</p> required <code>schema</code> <code>str</code> <p>The destination schema. Defaults to None.</p> <code>None</code> <code>dtypes</code> <code>Dict[str, Any]</code> <p>The data types to use for the table. Defaults to None.</p> <code>None</code> <code>if_exists</code> <code>Literal</code> <p>What to do if the table already exists. Defaults to \"fail\".</p> <code>'fail'</code> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>Whether the operation was successful.</p> Source code in <code>src/viadot/sources/base.py</code> <pre><code>def create_table(\n    self,\n    table: str,\n    schema: str | None = None,\n    dtypes: dict[str, Any] | None = None,\n    if_exists: Literal[\"fail\", \"replace\", \"skip\", \"delete\"] = \"fail\",\n) -&gt; bool:\n    \"\"\"Create a table.\n\n    Args:\n        table (str): The destination table. Defaults to None.\n        schema (str, optional): The destination schema. Defaults to None.\n        dtypes (Dict[str, Any], optional): The data types to use for the table.\n            Defaults to None.\n        if_exists (Literal, optional): What to do if the table already exists.\n            Defaults to \"fail\".\n\n    Returns:\n        bool: Whether the operation was successful.\n    \"\"\"\n    fqn = f\"{schema}.{table}\" if schema is not None else table\n    exists = self._check_if_table_exists(schema=schema, table=table)\n\n    if exists:\n        if if_exists == \"replace\":\n            self.run(f\"DROP TABLE {fqn}\")\n        elif if_exists == \"delete\":\n            self.run(f\"DELETE FROM {fqn}\")  # noqa: S608\n            return True\n        elif if_exists == \"fail\":\n            msg = (\n                f\"The table {fqn} already exists and 'if_exists' is set to 'fail'.\"\n            )\n            raise ValueError(msg)\n        elif if_exists == \"skip\":\n            return False\n\n    indent = \"  \"\n    dtypes_rows = [\n        indent + f'\"{col}\"' + \" \" + dtype for col, dtype in dtypes.items()\n    ]\n    dtypes_formatted = \",\\n\".join(dtypes_rows)\n    create_table_sql = f\"CREATE TABLE {fqn}(\\n{dtypes_formatted}\\n)\"\n    self.run(create_table_sql)\n    return True\n</code></pre>"},{"location":"references/sources/database/#viadot.sources.base.SQL.insert_into","title":"<code>insert_into(table, df)</code>","text":"<p>Insert values from a pandas DataFrame into an existing database table.</p> <p>Parameters:</p> Name Type Description Default <code>table</code> <code>str</code> <p>table name</p> required <code>df</code> <code>DataFrame</code> <p>pandas dataframe</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The executed SQL insert query.</p> Source code in <code>src/viadot/sources/base.py</code> <pre><code>def insert_into(self, table: str, df: pd.DataFrame) -&gt; str:\n    \"\"\"Insert values from a pandas DataFrame into an existing database table.\n\n    Args:\n        table (str): table name\n        df (pd.DataFrame): pandas dataframe\n\n    Returns:\n        str: The executed SQL insert query.\n    \"\"\"\n    values = \"\"\n    rows_count = df.shape[0]\n    counter = 0\n    for row in df.values:\n        counter += 1  # noqa: SIM113\n        out_row = \", \".join(map(self._escape_column_name, row))\n        comma = \",\\n\"\n        if counter == rows_count:\n            comma = \";\"\n        out_row = f\"({out_row}){comma}\"\n        values += out_row\n\n    columns = \", \".join(df.columns)\n\n    sql = f\"INSERT INTO {table} ({columns})\\n VALUES {values}\"  # noqa: S608\n    self.run(sql)\n\n    return sql\n</code></pre>"},{"location":"references/sources/database/#viadot.sources.base.SQL.run","title":"<code>run(query)</code>","text":"<p>Execute a query and return the result.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>The query to execute.</p> required <p>Returns:</p> Type Description <code>list[Record] | bool</code> <p>list[Record] | bool: If the query is a SELECT, return the result as a list of records.</p> Source code in <code>src/viadot/sources/base.py</code> <pre><code>def run(self, query: str) -&gt; list[Record] | bool:\n    \"\"\"Execute a query and return the result.\n\n    Args:\n        query (str): The query to execute.\n\n    Returns:\n        list[Record] | bool: If the query is a SELECT, return the result as a list\n            of records.\n    \"\"\"\n    cursor = self.con.cursor()\n    cursor.execute(query)\n\n    query_sanitized = query.strip().upper()\n    if query_sanitized.startswith(\"SELECT\") or query_sanitized.startswith(\"WITH\"):\n        result = cursor.fetchall()\n    else:\n        result = True\n\n    self.con.commit()\n    cursor.close()\n\n    return result\n</code></pre>"},{"location":"references/sources/database/#viadot.sources.base.SQL.to_df","title":"<code>to_df(query, con=None, if_empty='warn')</code>","text":"<p>Execute a query and return the result as a pandas DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>The query to execute.</p> required <code>con</code> <code>Connection</code> <p>The connection to use to pull the data.</p> <code>None</code> <code>if_empty</code> <code>Literal['warn', 'skip', 'fail']</code> <p>What to do if the query returns no data. Defaults to None.</p> <code>'warn'</code> Source code in <code>src/viadot/sources/base.py</code> <pre><code>def to_df(\n    self,\n    query: str,\n    con: pyodbc.Connection | None = None,\n    if_empty: Literal[\"warn\", \"skip\", \"fail\"] = \"warn\",\n) -&gt; pd.DataFrame:\n    \"\"\"Execute a query and return the result as a pandas DataFrame.\n\n    Args:\n        query (str): The query to execute.\n        con (pyodbc.Connection, optional): The connection to use to pull the data.\n        if_empty (Literal[\"warn\", \"skip\", \"fail\"], optional): What to do if the\n            query returns no data. Defaults to None.\n    \"\"\"\n    conn = con or self.con\n\n    query_sanitized = query.strip().upper()\n    if query_sanitized.startswith(\"SELECT\") or query_sanitized.startswith(\"WITH\"):\n        df = pd.read_sql_query(query, conn)\n        if df.empty:\n            self._handle_if_empty(if_empty=if_empty)\n    else:\n        df = pd.DataFrame()\n    return df\n</code></pre>"},{"location":"references/sources/database/#viadot.sources.sqlite.SQLite","title":"<code>viadot.sources.sqlite.SQLite</code>","text":"<p>               Bases: <code>SQL</code></p> Source code in <code>src/viadot/sources/sqlite.py</code> <pre><code>class SQLite(SQL):\n    def __init__(\n        self,\n        query_timeout: int = 60,\n        *args,\n        **kwargs,\n    ):\n        \"\"\"SQLite connector.\"\"\"\n        super().__init__(\n            *args,\n            driver=\"/usr/lib/x86_64-linux-gnu/odbc/libsqlite3odbc.so\",\n            query_timeout=query_timeout,\n            **kwargs,\n        )\n        self.credentials[\"server\"] = \"localhost\"\n\n    @property\n    def conn_str(self) -&gt; str:\n        \"\"\"Generate a connection string from params or config.\n\n        Note that the user and password are escaped with '{}' characters.\n\n        Returns:\n            str: The ODBC connection string.\n        \"\"\"\n        driver = self.credentials[\"driver\"]\n        server = self.credentials[\"server\"]\n        db_name = self.credentials[\"db_name\"]\n\n        return f\"DRIVER={{{driver}}};SERVER={server};DATABASE={db_name};\"\n\n    def _check_if_table_exists(self, table: str, schema: str | None = None) -&gt; bool:\n        \"\"\"Checks if table exists.\n\n        Args:\n            table (str): Table name.\n            schema (str, optional): Schema name. Defaults to None.\n        \"\"\"\n        fqn = f\"{schema}.{table}\" if schema is not None else table\n        exists_query = (\n            f\"SELECT name FROM sqlite_master WHERE type='table' AND name='{fqn}'\"  # noqa: S608\n        )\n        return bool(self.run(exists_query))\n</code></pre>"},{"location":"references/sources/database/#viadot.sources.sqlite.SQLite.conn_str","title":"<code>conn_str</code>  <code>property</code>","text":"<p>Generate a connection string from params or config.</p> <p>Note that the user and password are escaped with '{}' characters.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The ODBC connection string.</p>"},{"location":"references/sources/database/#viadot.sources.sqlite.SQLite.__init__","title":"<code>__init__(query_timeout=60, *args, **kwargs)</code>","text":"<p>SQLite connector.</p> Source code in <code>src/viadot/sources/sqlite.py</code> <pre><code>def __init__(\n    self,\n    query_timeout: int = 60,\n    *args,\n    **kwargs,\n):\n    \"\"\"SQLite connector.\"\"\"\n    super().__init__(\n        *args,\n        driver=\"/usr/lib/x86_64-linux-gnu/odbc/libsqlite3odbc.so\",\n        query_timeout=query_timeout,\n        **kwargs,\n    )\n    self.credentials[\"server\"] = \"localhost\"\n</code></pre>"},{"location":"references/sources/database/#viadot.sources.sql_server.SQLServer","title":"<code>viadot.sources.sql_server.SQLServer</code>","text":"<p>               Bases: <code>SQL</code></p> Source code in <code>src/viadot/sources/sql_server.py</code> <pre><code>class SQLServer(SQL):\n    DEFAULT_SCHEMA = \"dbo\"\n\n    def __init__(\n        self,\n        credentials: SQLServerCredentials | None = None,\n        config_key: str | None = None,\n        *args,\n        **kwargs,\n    ):\n        \"\"\"Connector for SQL Server.\n\n        Args:\n            credentials (SQLServerCredentials | None, optional): The credentials to use.\n                Defaults to None.\n            config_key (str | None, optional): The viadot config key from which to read\n                the credentials. Defaults to None.\n        \"\"\"\n        raw_creds = credentials or get_source_credentials(config_key) or {}\n        validated_creds = SQLServerCredentials(**raw_creds).dict(\n            by_alias=True\n        )  # validate the credentials\n\n        super().__init__(*args, credentials=validated_creds, **kwargs)\n        self.server = self.credentials.get(\"server\")\n        self.username = self.credentials.get(\"username\")\n        self.password = self.credentials.get(\"password\")\n        self.driver = self.credentials.get(\"driver\")\n        self.db_name = self.credentials.get(\"db_name\")\n\n        self.con.add_output_converter(-155, self._handle_datetimeoffset)\n\n    @property\n    def schemas(self) -&gt; list[str]:\n        \"\"\"Return a list of all schemas.\"\"\"\n        schemas_tuples = self.run(\"SELECT s.name as schema_name from sys.schemas s\")\n        return [schema_tuple[0] for schema_tuple in schemas_tuples]\n\n    @property\n    def tables(self) -&gt; list[str]:\n        \"\"\"Return a list of all tables in the database.\"\"\"\n        tables_tuples = self.run(\n            \"SELECT schema_name(t.schema_id), t.name FROM sys.tables t\"\n        )\n        return [\".\".join(row) for row in tables_tuples]\n\n    @staticmethod\n    def _handle_datetimeoffset(dto_value: str) -&gt; datetime:\n        \"\"\"Adds support for SQL Server's custom `datetimeoffset` type.\n\n        This type is not handled natively by ODBC/pyodbc.\n\n        See: https://github.com/mkleehammer/pyodbc/issues/134#issuecomment-281739794\n        \"\"\"\n        (\n            year,\n            month,\n            day,\n            hour,\n            minute,\n            second,\n            nanoseconds,\n            offset_hours,\n            offset_minutes,\n        ) = struct.unpack(\"&lt;6hI2h\", dto_value)\n        return datetime(\n            year,\n            month,\n            day,\n            hour,\n            minute,\n            second,\n            nanoseconds // 1000,\n            tzinfo=timezone(timedelta(hours=offset_hours, minutes=offset_minutes)),\n        )\n\n    def exists(self, table: str, schema: str | None = None) -&gt; bool:\n        \"\"\"Check whether a table exists.\n\n        Args:\n            table (str): The table to be checked.\n            schema (str, optional): The schema where the table is located.\n                Defaults to 'dbo'.\n\n        Returns:\n            bool: Whether the table exists.\n        \"\"\"\n        if not schema:\n            schema = self.DEFAULT_SCHEMA\n\n        list_table_info_query = f\"\"\"\n            SELECT *\n            FROM sys.tables t\n            JOIN sys.schemas s\n                ON t.schema_id = s.schema_id\n            WHERE s.name = '{schema}' AND t.name = '{table}'\n        \"\"\"  # noqa: S608\n        return bool(self.run(list_table_info_query))\n</code></pre>"},{"location":"references/sources/database/#viadot.sources.sql_server.SQLServer.schemas","title":"<code>schemas</code>  <code>property</code>","text":"<p>Return a list of all schemas.</p>"},{"location":"references/sources/database/#viadot.sources.sql_server.SQLServer.tables","title":"<code>tables</code>  <code>property</code>","text":"<p>Return a list of all tables in the database.</p>"},{"location":"references/sources/database/#viadot.sources.sql_server.SQLServer.__init__","title":"<code>__init__(credentials=None, config_key=None, *args, **kwargs)</code>","text":"<p>Connector for SQL Server.</p> <p>Parameters:</p> Name Type Description Default <code>credentials</code> <code>SQLServerCredentials | None</code> <p>The credentials to use. Defaults to None.</p> <code>None</code> <code>config_key</code> <code>str | None</code> <p>The viadot config key from which to read the credentials. Defaults to None.</p> <code>None</code> Source code in <code>src/viadot/sources/sql_server.py</code> <pre><code>def __init__(\n    self,\n    credentials: SQLServerCredentials | None = None,\n    config_key: str | None = None,\n    *args,\n    **kwargs,\n):\n    \"\"\"Connector for SQL Server.\n\n    Args:\n        credentials (SQLServerCredentials | None, optional): The credentials to use.\n            Defaults to None.\n        config_key (str | None, optional): The viadot config key from which to read\n            the credentials. Defaults to None.\n    \"\"\"\n    raw_creds = credentials or get_source_credentials(config_key) or {}\n    validated_creds = SQLServerCredentials(**raw_creds).dict(\n        by_alias=True\n    )  # validate the credentials\n\n    super().__init__(*args, credentials=validated_creds, **kwargs)\n    self.server = self.credentials.get(\"server\")\n    self.username = self.credentials.get(\"username\")\n    self.password = self.credentials.get(\"password\")\n    self.driver = self.credentials.get(\"driver\")\n    self.db_name = self.credentials.get(\"db_name\")\n\n    self.con.add_output_converter(-155, self._handle_datetimeoffset)\n</code></pre>"},{"location":"references/sources/database/#viadot.sources.sql_server.SQLServer.exists","title":"<code>exists(table, schema=None)</code>","text":"<p>Check whether a table exists.</p> <p>Parameters:</p> Name Type Description Default <code>table</code> <code>str</code> <p>The table to be checked.</p> required <code>schema</code> <code>str</code> <p>The schema where the table is located. Defaults to 'dbo'.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>Whether the table exists.</p> Source code in <code>src/viadot/sources/sql_server.py</code> <pre><code>def exists(self, table: str, schema: str | None = None) -&gt; bool:\n    \"\"\"Check whether a table exists.\n\n    Args:\n        table (str): The table to be checked.\n        schema (str, optional): The schema where the table is located.\n            Defaults to 'dbo'.\n\n    Returns:\n        bool: Whether the table exists.\n    \"\"\"\n    if not schema:\n        schema = self.DEFAULT_SCHEMA\n\n    list_table_info_query = f\"\"\"\n        SELECT *\n        FROM sys.tables t\n        JOIN sys.schemas s\n            ON t.schema_id = s.schema_id\n        WHERE s.name = '{schema}' AND t.name = '{table}'\n    \"\"\"  # noqa: S608\n    return bool(self.run(list_table_info_query))\n</code></pre>"},{"location":"references/sources/database/#viadot.sources._trino.Trino","title":"<code>viadot.sources._trino.Trino</code>","text":"<p>               Bases: <code>Source</code></p> Source code in <code>src/viadot/sources/_trino.py</code> <pre><code>class Trino(Source):\n    def __init__(\n        self,\n        credentials: TrinoCredentials | None = None,\n        config_key: str | None = None,\n        *args,\n        **kwargs,\n    ):\n        \"\"\"A class for interacting with Trino as a database.\n\n        Currently supports only generic and Iceberg operations.\n\n        Args:\n            credentials (TrinoCredentials): Trino credentials.\n            config_key (str, optional): The key in the viadot config holding relevant\n                credentials.\n        \"\"\"\n        raw_creds = credentials or get_source_credentials(config_key) or {}\n        validated_creds = TrinoCredentials(**raw_creds).dict(\n            by_alias=True\n        )  # validate the credentials\n\n        super().__init__(*args, credentials=validated_creds, **kwargs)\n\n        self.http_scheme = self.credentials.get(\"http_scheme\")\n        self.host = self.credentials.get(\"host\")\n        self.port = self.credentials.get(\"port\")\n        self.username = self.credentials.get(\"user\")\n        self.password = self.credentials.get(\"password\")\n        self.catalog = self.credentials.get(\"catalog\")\n        self.schema = self.credentials.get(\"schema\")\n        self.verify = self.credentials.get(\"verify\")\n\n        self.connection_string = (\n            f\"trino://{self.username}@{self.host}:{self.port}/{self.catalog}\"\n        )\n        self.connect_args = {\n            \"verify\": self.verify,\n            \"auth\": BasicAuthentication(self.username, self.password),\n            \"http_scheme\": self.http_scheme,\n        }\n        self.engine = create_engine(\n            self.connection_string, connect_args=self.connect_args, future=True\n        )\n\n    @contextmanager\n    def get_connection(self) -&gt; Generator[Connection, None, None]:\n        \"\"\"Provide a transactional scope around a series of operations.\n\n        Examples:\n        &gt;&gt;&gt; trino = Trino()\n        &gt;&gt;&gt; with trino.get_connection() as connection:\n        &gt;&gt;&gt;    trino.run(query1, connection=connection)\n        &gt;&gt;&gt;    trino.run(query2, connection=connection)\n        \"\"\"\n        connection = self.engine.connect()\n        try:\n            yield connection\n            connection.commit()\n        except:\n            connection.rollback()\n            raise\n        finally:\n            connection.close()\n\n    def get_tables(self, schema_name: str) -&gt; list[str]:\n        \"\"\"List all tables in a schema.\n\n        Args:\n            schema_name (str): _description_\n\n        Returns:\n            list[str]: _description_\n        \"\"\"\n        query = f\"SHOW TABLES FROM {schema_name}\"\n        with self.get_connection() as connection:\n            return list(self.run(query, connection=connection))\n\n    def drop_table(self, table_name: str, schema_name: str | None = None) -&gt; None:\n        \"\"\"Drop a table.\n\n        Args:\n            table_name (str): _description_\n            schema_name (str | None, optional): _description_. Defaults to None.\n        \"\"\"\n        fqn = get_fqn(schema_name=schema_name, table_name=table_name)\n        query = f\"DROP TABLE IF EXISTS {fqn}\"\n\n        self.logger.info(f\"Dropping table '{fqn}'...\")\n        with self.get_connection() as connection:\n            self.run(query, connection=connection)\n        self.logger.info(f\"Table '{fqn}' has been successfully dropped.\")\n\n    def delete_table(self, table_name: str, schema_name: str | None = None) -&gt; None:\n        \"\"\"Delete all data from a table.\n\n        Args:\n            table_name (str): _description_\n            schema_name (str | None, optional): _description_. Defaults to None.\n        \"\"\"\n        fqn = get_fqn(schema_name=schema_name, table_name=table_name)\n        query = f\"DELETE FROM {fqn}\"  # noqa: S608\n        self.logger.info(f\"Removing all data from table '{fqn}'...\")\n        with self.get_connection() as connection:\n            self.run(query, connection=connection)\n        self.logger.info(f\"Data from table '{fqn}' has been successfully removed.\")\n\n    def _check_if_table_exists(self, table_name: str, schema_name: str) -&gt; None:\n        query = f\"\"\"\nSELECT *\nFROM INFORMATION_SCHEMA.TABLES\nWHERE TABLE_SCHEMA = '{schema_name}'\nAND TABLE_NAME = '{table_name}'\"\"\"\n        with self.get_connection() as connection:\n            results = list(self.run(query, connection=connection))\n        return len(results) &gt; 0\n\n    def get_schemas(self) -&gt; list[str]:\n        \"\"\"List all schemas in the database.\n\n        Returns:\n            list[str]: _description_\n        \"\"\"\n        query = \"SHOW SCHEMAS\"\n        with self.get_connection() as connection:\n            return list(self.run(query, connection=connection))\n\n    def _check_if_schema_exists(self, schema_name: str) -&gt; None:\n        query = f\"SELECT * FROM INFORMATION_SCHEMA.SCHEMATA WHERE SCHEMA_NAME = '{schema_name}'\"  # noqa: S608\n        with self.get_connection() as connection:\n            results = list(self.run(query, connection=connection))\n        return bool(results)\n\n    def drop_schema(self, schema_name: str, cascade: bool = False) -&gt; None:\n        \"\"\"Drop a schema.\n\n        Args:\n            schema_name (str): _description_\n            cascade (bool, optional): _description_. Defaults to False.\n        \"\"\"\n        if not self._check_if_schema_exists(schema_name):\n            return\n\n        if cascade:\n            tables = self.get_tables(schema_name)\n            [self.drop_table(schema_name=schema_name, table_name=t) for t in tables]\n\n        self.logger.info(f\"Dropping schema {schema_name}...\")\n        with self.get_connection() as connection:\n            self.run(f\"DROP SCHEMA {schema_name}\", connection=connection)\n        self.logger.info(f\"Schema {schema_name} has been successfully dropped.\")\n\n    def create_iceberg_schema(\n        self,\n        schema_name: str,\n        location: str,\n        if_exists: Literal[\"fail\", \"skip\"] = \"fail\",\n    ) -&gt; None:\n        \"\"\"Create an Iceberg schema.\n\n        Args:\n            schema_name (str): _description_\n            location (str): _description_\n            if_exists (Literal[&amp;quot;fail&amp;quot;, &amp;quot;skip&amp;quot;], optional): What\n                to do if the schema already exists. Defaults to \"fail\".\n\n        Raises:\n            ValueError: _description_\n        \"\"\"\n        exists = self._check_if_schema_exists(schema_name)\n\n        if exists:\n            if if_exists == \"fail\":\n                msg = f\"Schema '{schema_name}' already exists.\"\n                raise ValueError(msg)\n\n            self.logger.info(f\"Schema '{schema_name}' already exists. Skipping...\")\n            return\n\n        query = f\"\"\"\nCREATE SCHEMA {schema_name}\nWITH (location = '{location}')\n        \"\"\"\n        self.logger.info(f\"Creating schema '{schema_name}'...\")\n        with self.get_connection() as connection:\n            self.run(query, connection=connection)\n        self.logger.info(f\"Schema '{schema_name}' has been successfully created.\")\n\n    def create_iceberg_table_from_arrow(\n        self,\n        table: pa.Table,\n        table_name: str,\n        schema_name: str | None = None,\n        location: str | None = None,\n        file_format: Literal[\"PARQUET\", \"ORC\"] = \"PARQUET\",\n        partition_cols: list[str] | None = None,\n        sort_by: list[str] | None = None,\n    ) -&gt; None:\n        \"\"\"Create an Iceberg table from a pyarrow Table.\n\n        Args:\n            table (pa.Table): _description_\n            table_name (str): _description_\n            schema_name (str | None, optional): _description_. Defaults to None.\n            location (str | None, optional): _description_. Defaults to None.\n            file_format (Literal[&amp;quot;PARQUET&amp;quot;, &amp;quot;ORC&amp;quot;], optional): The\n                file format to use. Defaults to \"PARQUET\".\n            partition_cols (list[str] | None, optional): The partition columns to use.\n                Defaults to None.\n            sort_by (list[str] | None, optional): _description_. Defaults to None.\n        \"\"\"\n        columns = table.schema.names\n        types = [self.pyarrow_to_trino_type(str(typ)) for typ in table.schema.types]\n        create_table_query = self._create_table_query(\n            schema_name=schema_name,\n            table_name=table_name,\n            columns=columns,\n            types=types,\n            location=location,\n            file_format=file_format,\n            partition_cols=partition_cols,\n            sort_by_cols=sort_by,\n        )\n\n        fqn = get_fqn(schema_name=schema_name, table_name=table_name)\n        self.logger.info(f\"Creating table '{fqn}'...\")\n        with self.get_connection() as connection:\n            self.run(create_table_query, connection=connection)\n        self.logger.info(f\"Table '{fqn}' has been successfully created.\")\n\n    def create_iceberg_table_from_pandas(\n        self,\n        df: pd.DataFrame,\n        table_name: str,\n        schema_name: str | None = None,\n        location: str | None = None,\n        file_format: Literal[\"PARQUET\", \"ORC\"] = \"PARQUET\",\n        partition_cols: list[str] | None = None,\n        sort_by: list[str] | None = None,\n    ) -&gt; None:\n        \"\"\"Create an Iceberg table from a pandas DataFrame.\n\n        Args:\n            df (pd.DataFrame): _description_\n            table_name (str): _description_\n            schema_name (str | None, optional): _description_. Defaults to None.\n            location (str | None, optional): _description_. Defaults to None.\n            file_format (Literal[&amp;quot;PARQUET&amp;quot;, &amp;quot;ORC&amp;quot;], optional): The\n                file format to use. Defaults to \"PARQUET\".\n            partition_cols (list[str] | None, optional): The partition columns to use.\n                Defaults to None.\n            sort_by (list[str] | None, optional): _description_. Defaults to None.\n        \"\"\"\n        pa_table = pa.Table.from_pandas(df)\n        self.create_iceberg_table_from_arrow(\n            table=pa_table,\n            schema_name=schema_name,\n            table_name=table_name,\n            location=location,\n            file_format=file_format,\n            partition_cols=partition_cols,\n            sort_by=sort_by,\n        )\n\n    def _create_table_query(\n        self,\n        table_name: str,\n        columns: list[str],\n        types: list[str],\n        schema_name: str | None = None,\n        location: str | None = None,\n        file_format: Literal[\"PARQUET\", \"ORC\"] = \"PARQUET\",\n        partition_cols: list[str] | None = None,\n        sort_by_cols: list[str] | None = None,\n    ) -&gt; str:\n        cols_and_dtypes = \",\\n\\t\".join(\n            col + \" \" + dtype for col, dtype in zip(columns, types, strict=False)\n        )\n        fqn = get_fqn(schema_name=schema_name, table_name=table_name)\n        with_clause = f\"format = '{file_format}'\"\n\n        if partition_cols:\n            with_clause += \",\\n\\tpartitioning = ARRAY\" + str(partition_cols)\n\n        if sort_by_cols:\n            with_clause += \",\\n\\tsorted_by = ARRAY\" + str(sort_by_cols)\n\n        if location:\n            with_clause += f\",\\n\\tlocation = '{location}'\"\n\n        return f\"\"\"\nCREATE TABLE IF NOT EXISTS {fqn} (\n    {cols_and_dtypes}\n)\nWITH (\n    {with_clause}\n)\"\"\"\n\n    def run(\n        self, sql: str, connection: Connection\n    ) -&gt; Generator[tuple, None, None] | None:\n        \"\"\"Run a SQL query.\n\n        Args:\n            sql (str): _description_\n            connection (Connection): _description_\n\n        Yields:\n            Generator[tuple, None, None] | None: _description_\n        \"\"\"\n\n        def row_generator(result: CursorResult):\n            # Fetch rows in chunks of size `yield_per`.\n            # This has to be inside a function due to how Python generators work.\n            for partition in result.partitions():\n                yield from partition\n\n        self.logger.debug(\"Executing SQL:\\n\" + sql)\n\n        try:\n            # Execute with server-side cursor of size 5000.\n            result = connection.execution_options(yield_per=5000).execute(text(sql))\n        except Exception as e:\n            msg = f\"Failed executing SQL:\\n{sql}\"\n            raise ValueError(msg) from e\n\n        query_keywords = [\"SELECT\", \"SHOW\", \"PRAGMA\", \"WITH\"]\n        is_query = any(sql.strip().upper().startswith(word) for word in query_keywords)\n\n        return row_generator(result) if is_query else None\n\n    @staticmethod\n    def pyarrow_to_trino_type(pyarrow_type: str) -&gt; str:\n        \"\"\"Convert a pyarrow data type to a Trino type.\n\n        Args:\n            pyarrow_type (str): The Pyarrow type to convert.\n\n        Returns:\n            str: The Trino type.\n        \"\"\"\n        mapping = {\n            \"string\": \"VARCHAR\",\n            \"large_string\": \"VARCHAR\",\n            \"int8\": \"TINYINT\",\n            \"int16\": \"SMALLINT\",\n            \"int32\": \"INTEGER\",\n            \"int64\": \"BIGINT\",\n            \"float\": \"REAL\",\n            \"double\": \"DOUBLE\",\n            \"bool\": \"BOOLEAN\",\n            \"date32[day]\": \"DATE\",\n            \"timestamp[ns]\": \"TIMESTAMP(6)\",\n            \"decimal\": \"DECIMAL\",\n            \"decimal128\": \"DECIMAL\",\n            \"decimal256\": \"DECIMAL\",\n        }\n        precision, scale = None, None\n        decimal_match = re.match(r\"(\\w+)\\((\\d+), (\\d+)\\)\", pyarrow_type)\n        if decimal_match:\n            pyarrow_type = decimal_match.group(1)\n            precision = int(decimal_match.group(2))\n            scale = int(decimal_match.group(3))\n\n        mapped_type = mapping.get(pyarrow_type) or \"VARCHAR\"\n\n        if precision and scale:\n            mapped_type += f\"({precision}, {scale})\"\n\n        return mapped_type\n\n    def _check_connection(self) -&gt; None:\n        try:\n            with self.get_connection() as connection:\n                self.run(\"select 1\", connection=connection)\n        except Exception as e:\n            msg = f\"Failed to connect to Trino server at {self.host}\"\n            raise ValueError(msg) from e\n</code></pre>"},{"location":"references/sources/database/#viadot.sources._trino.Trino.__init__","title":"<code>__init__(credentials=None, config_key=None, *args, **kwargs)</code>","text":"<p>A class for interacting with Trino as a database.</p> <p>Currently supports only generic and Iceberg operations.</p> <p>Parameters:</p> Name Type Description Default <code>credentials</code> <code>TrinoCredentials</code> <p>Trino credentials.</p> <code>None</code> <code>config_key</code> <code>str</code> <p>The key in the viadot config holding relevant credentials.</p> <code>None</code> Source code in <code>src/viadot/sources/_trino.py</code> <pre><code>def __init__(\n    self,\n    credentials: TrinoCredentials | None = None,\n    config_key: str | None = None,\n    *args,\n    **kwargs,\n):\n    \"\"\"A class for interacting with Trino as a database.\n\n    Currently supports only generic and Iceberg operations.\n\n    Args:\n        credentials (TrinoCredentials): Trino credentials.\n        config_key (str, optional): The key in the viadot config holding relevant\n            credentials.\n    \"\"\"\n    raw_creds = credentials or get_source_credentials(config_key) or {}\n    validated_creds = TrinoCredentials(**raw_creds).dict(\n        by_alias=True\n    )  # validate the credentials\n\n    super().__init__(*args, credentials=validated_creds, **kwargs)\n\n    self.http_scheme = self.credentials.get(\"http_scheme\")\n    self.host = self.credentials.get(\"host\")\n    self.port = self.credentials.get(\"port\")\n    self.username = self.credentials.get(\"user\")\n    self.password = self.credentials.get(\"password\")\n    self.catalog = self.credentials.get(\"catalog\")\n    self.schema = self.credentials.get(\"schema\")\n    self.verify = self.credentials.get(\"verify\")\n\n    self.connection_string = (\n        f\"trino://{self.username}@{self.host}:{self.port}/{self.catalog}\"\n    )\n    self.connect_args = {\n        \"verify\": self.verify,\n        \"auth\": BasicAuthentication(self.username, self.password),\n        \"http_scheme\": self.http_scheme,\n    }\n    self.engine = create_engine(\n        self.connection_string, connect_args=self.connect_args, future=True\n    )\n</code></pre>"},{"location":"references/sources/database/#viadot.sources._trino.Trino.create_iceberg_schema","title":"<code>create_iceberg_schema(schema_name, location, if_exists='fail')</code>","text":"<p>Create an Iceberg schema.</p> <p>Parameters:</p> Name Type Description Default <code>schema_name</code> <code>str</code> <p>description</p> required <code>location</code> <code>str</code> <p>description</p> required <code>if_exists</code> <code>Literal[&amp;quot;fail&amp;quot;, &amp;quot;skip&amp;quot;]</code> <p>What to do if the schema already exists. Defaults to \"fail\".</p> <code>'fail'</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>description</p> Source code in <code>src/viadot/sources/_trino.py</code> <pre><code>    def create_iceberg_schema(\n        self,\n        schema_name: str,\n        location: str,\n        if_exists: Literal[\"fail\", \"skip\"] = \"fail\",\n    ) -&gt; None:\n        \"\"\"Create an Iceberg schema.\n\n        Args:\n            schema_name (str): _description_\n            location (str): _description_\n            if_exists (Literal[&amp;quot;fail&amp;quot;, &amp;quot;skip&amp;quot;], optional): What\n                to do if the schema already exists. Defaults to \"fail\".\n\n        Raises:\n            ValueError: _description_\n        \"\"\"\n        exists = self._check_if_schema_exists(schema_name)\n\n        if exists:\n            if if_exists == \"fail\":\n                msg = f\"Schema '{schema_name}' already exists.\"\n                raise ValueError(msg)\n\n            self.logger.info(f\"Schema '{schema_name}' already exists. Skipping...\")\n            return\n\n        query = f\"\"\"\nCREATE SCHEMA {schema_name}\nWITH (location = '{location}')\n        \"\"\"\n        self.logger.info(f\"Creating schema '{schema_name}'...\")\n        with self.get_connection() as connection:\n            self.run(query, connection=connection)\n        self.logger.info(f\"Schema '{schema_name}' has been successfully created.\")\n</code></pre>"},{"location":"references/sources/database/#viadot.sources._trino.Trino.create_iceberg_table_from_arrow","title":"<code>create_iceberg_table_from_arrow(table, table_name, schema_name=None, location=None, file_format='PARQUET', partition_cols=None, sort_by=None)</code>","text":"<p>Create an Iceberg table from a pyarrow Table.</p> <p>Parameters:</p> Name Type Description Default <code>table</code> <code>Table</code> <p>description</p> required <code>table_name</code> <code>str</code> <p>description</p> required <code>schema_name</code> <code>str | None</code> <p>description. Defaults to None.</p> <code>None</code> <code>location</code> <code>str | None</code> <p>description. Defaults to None.</p> <code>None</code> <code>file_format</code> <code>Literal[&amp;quot;PARQUET&amp;quot;, &amp;quot;ORC&amp;quot;]</code> <p>The file format to use. Defaults to \"PARQUET\".</p> <code>'PARQUET'</code> <code>partition_cols</code> <code>list[str] | None</code> <p>The partition columns to use. Defaults to None.</p> <code>None</code> <code>sort_by</code> <code>list[str] | None</code> <p>description. Defaults to None.</p> <code>None</code> Source code in <code>src/viadot/sources/_trino.py</code> <pre><code>def create_iceberg_table_from_arrow(\n    self,\n    table: pa.Table,\n    table_name: str,\n    schema_name: str | None = None,\n    location: str | None = None,\n    file_format: Literal[\"PARQUET\", \"ORC\"] = \"PARQUET\",\n    partition_cols: list[str] | None = None,\n    sort_by: list[str] | None = None,\n) -&gt; None:\n    \"\"\"Create an Iceberg table from a pyarrow Table.\n\n    Args:\n        table (pa.Table): _description_\n        table_name (str): _description_\n        schema_name (str | None, optional): _description_. Defaults to None.\n        location (str | None, optional): _description_. Defaults to None.\n        file_format (Literal[&amp;quot;PARQUET&amp;quot;, &amp;quot;ORC&amp;quot;], optional): The\n            file format to use. Defaults to \"PARQUET\".\n        partition_cols (list[str] | None, optional): The partition columns to use.\n            Defaults to None.\n        sort_by (list[str] | None, optional): _description_. Defaults to None.\n    \"\"\"\n    columns = table.schema.names\n    types = [self.pyarrow_to_trino_type(str(typ)) for typ in table.schema.types]\n    create_table_query = self._create_table_query(\n        schema_name=schema_name,\n        table_name=table_name,\n        columns=columns,\n        types=types,\n        location=location,\n        file_format=file_format,\n        partition_cols=partition_cols,\n        sort_by_cols=sort_by,\n    )\n\n    fqn = get_fqn(schema_name=schema_name, table_name=table_name)\n    self.logger.info(f\"Creating table '{fqn}'...\")\n    with self.get_connection() as connection:\n        self.run(create_table_query, connection=connection)\n    self.logger.info(f\"Table '{fqn}' has been successfully created.\")\n</code></pre>"},{"location":"references/sources/database/#viadot.sources._trino.Trino.create_iceberg_table_from_pandas","title":"<code>create_iceberg_table_from_pandas(df, table_name, schema_name=None, location=None, file_format='PARQUET', partition_cols=None, sort_by=None)</code>","text":"<p>Create an Iceberg table from a pandas DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>description</p> required <code>table_name</code> <code>str</code> <p>description</p> required <code>schema_name</code> <code>str | None</code> <p>description. Defaults to None.</p> <code>None</code> <code>location</code> <code>str | None</code> <p>description. Defaults to None.</p> <code>None</code> <code>file_format</code> <code>Literal[&amp;quot;PARQUET&amp;quot;, &amp;quot;ORC&amp;quot;]</code> <p>The file format to use. Defaults to \"PARQUET\".</p> <code>'PARQUET'</code> <code>partition_cols</code> <code>list[str] | None</code> <p>The partition columns to use. Defaults to None.</p> <code>None</code> <code>sort_by</code> <code>list[str] | None</code> <p>description. Defaults to None.</p> <code>None</code> Source code in <code>src/viadot/sources/_trino.py</code> <pre><code>def create_iceberg_table_from_pandas(\n    self,\n    df: pd.DataFrame,\n    table_name: str,\n    schema_name: str | None = None,\n    location: str | None = None,\n    file_format: Literal[\"PARQUET\", \"ORC\"] = \"PARQUET\",\n    partition_cols: list[str] | None = None,\n    sort_by: list[str] | None = None,\n) -&gt; None:\n    \"\"\"Create an Iceberg table from a pandas DataFrame.\n\n    Args:\n        df (pd.DataFrame): _description_\n        table_name (str): _description_\n        schema_name (str | None, optional): _description_. Defaults to None.\n        location (str | None, optional): _description_. Defaults to None.\n        file_format (Literal[&amp;quot;PARQUET&amp;quot;, &amp;quot;ORC&amp;quot;], optional): The\n            file format to use. Defaults to \"PARQUET\".\n        partition_cols (list[str] | None, optional): The partition columns to use.\n            Defaults to None.\n        sort_by (list[str] | None, optional): _description_. Defaults to None.\n    \"\"\"\n    pa_table = pa.Table.from_pandas(df)\n    self.create_iceberg_table_from_arrow(\n        table=pa_table,\n        schema_name=schema_name,\n        table_name=table_name,\n        location=location,\n        file_format=file_format,\n        partition_cols=partition_cols,\n        sort_by=sort_by,\n    )\n</code></pre>"},{"location":"references/sources/database/#viadot.sources._trino.Trino.delete_table","title":"<code>delete_table(table_name, schema_name=None)</code>","text":"<p>Delete all data from a table.</p> <p>Parameters:</p> Name Type Description Default <code>table_name</code> <code>str</code> <p>description</p> required <code>schema_name</code> <code>str | None</code> <p>description. Defaults to None.</p> <code>None</code> Source code in <code>src/viadot/sources/_trino.py</code> <pre><code>def delete_table(self, table_name: str, schema_name: str | None = None) -&gt; None:\n    \"\"\"Delete all data from a table.\n\n    Args:\n        table_name (str): _description_\n        schema_name (str | None, optional): _description_. Defaults to None.\n    \"\"\"\n    fqn = get_fqn(schema_name=schema_name, table_name=table_name)\n    query = f\"DELETE FROM {fqn}\"  # noqa: S608\n    self.logger.info(f\"Removing all data from table '{fqn}'...\")\n    with self.get_connection() as connection:\n        self.run(query, connection=connection)\n    self.logger.info(f\"Data from table '{fqn}' has been successfully removed.\")\n</code></pre>"},{"location":"references/sources/database/#viadot.sources._trino.Trino.drop_schema","title":"<code>drop_schema(schema_name, cascade=False)</code>","text":"<p>Drop a schema.</p> <p>Parameters:</p> Name Type Description Default <code>schema_name</code> <code>str</code> <p>description</p> required <code>cascade</code> <code>bool</code> <p>description. Defaults to False.</p> <code>False</code> Source code in <code>src/viadot/sources/_trino.py</code> <pre><code>def drop_schema(self, schema_name: str, cascade: bool = False) -&gt; None:\n    \"\"\"Drop a schema.\n\n    Args:\n        schema_name (str): _description_\n        cascade (bool, optional): _description_. Defaults to False.\n    \"\"\"\n    if not self._check_if_schema_exists(schema_name):\n        return\n\n    if cascade:\n        tables = self.get_tables(schema_name)\n        [self.drop_table(schema_name=schema_name, table_name=t) for t in tables]\n\n    self.logger.info(f\"Dropping schema {schema_name}...\")\n    with self.get_connection() as connection:\n        self.run(f\"DROP SCHEMA {schema_name}\", connection=connection)\n    self.logger.info(f\"Schema {schema_name} has been successfully dropped.\")\n</code></pre>"},{"location":"references/sources/database/#viadot.sources._trino.Trino.drop_table","title":"<code>drop_table(table_name, schema_name=None)</code>","text":"<p>Drop a table.</p> <p>Parameters:</p> Name Type Description Default <code>table_name</code> <code>str</code> <p>description</p> required <code>schema_name</code> <code>str | None</code> <p>description. Defaults to None.</p> <code>None</code> Source code in <code>src/viadot/sources/_trino.py</code> <pre><code>def drop_table(self, table_name: str, schema_name: str | None = None) -&gt; None:\n    \"\"\"Drop a table.\n\n    Args:\n        table_name (str): _description_\n        schema_name (str | None, optional): _description_. Defaults to None.\n    \"\"\"\n    fqn = get_fqn(schema_name=schema_name, table_name=table_name)\n    query = f\"DROP TABLE IF EXISTS {fqn}\"\n\n    self.logger.info(f\"Dropping table '{fqn}'...\")\n    with self.get_connection() as connection:\n        self.run(query, connection=connection)\n    self.logger.info(f\"Table '{fqn}' has been successfully dropped.\")\n</code></pre>"},{"location":"references/sources/database/#viadot.sources._trino.Trino.get_connection","title":"<code>get_connection()</code>","text":"<p>Provide a transactional scope around a series of operations.</p> <p>Examples:</p> <p>trino = Trino() with trino.get_connection() as connection:    trino.run(query1, connection=connection)    trino.run(query2, connection=connection)</p> Source code in <code>src/viadot/sources/_trino.py</code> <pre><code>@contextmanager\ndef get_connection(self) -&gt; Generator[Connection, None, None]:\n    \"\"\"Provide a transactional scope around a series of operations.\n\n    Examples:\n    &gt;&gt;&gt; trino = Trino()\n    &gt;&gt;&gt; with trino.get_connection() as connection:\n    &gt;&gt;&gt;    trino.run(query1, connection=connection)\n    &gt;&gt;&gt;    trino.run(query2, connection=connection)\n    \"\"\"\n    connection = self.engine.connect()\n    try:\n        yield connection\n        connection.commit()\n    except:\n        connection.rollback()\n        raise\n    finally:\n        connection.close()\n</code></pre>"},{"location":"references/sources/database/#viadot.sources._trino.Trino.get_schemas","title":"<code>get_schemas()</code>","text":"<p>List all schemas in the database.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: description</p> Source code in <code>src/viadot/sources/_trino.py</code> <pre><code>def get_schemas(self) -&gt; list[str]:\n    \"\"\"List all schemas in the database.\n\n    Returns:\n        list[str]: _description_\n    \"\"\"\n    query = \"SHOW SCHEMAS\"\n    with self.get_connection() as connection:\n        return list(self.run(query, connection=connection))\n</code></pre>"},{"location":"references/sources/database/#viadot.sources._trino.Trino.get_tables","title":"<code>get_tables(schema_name)</code>","text":"<p>List all tables in a schema.</p> <p>Parameters:</p> Name Type Description Default <code>schema_name</code> <code>str</code> <p>description</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: description</p> Source code in <code>src/viadot/sources/_trino.py</code> <pre><code>def get_tables(self, schema_name: str) -&gt; list[str]:\n    \"\"\"List all tables in a schema.\n\n    Args:\n        schema_name (str): _description_\n\n    Returns:\n        list[str]: _description_\n    \"\"\"\n    query = f\"SHOW TABLES FROM {schema_name}\"\n    with self.get_connection() as connection:\n        return list(self.run(query, connection=connection))\n</code></pre>"},{"location":"references/sources/database/#viadot.sources._trino.Trino.pyarrow_to_trino_type","title":"<code>pyarrow_to_trino_type(pyarrow_type)</code>  <code>staticmethod</code>","text":"<p>Convert a pyarrow data type to a Trino type.</p> <p>Parameters:</p> Name Type Description Default <code>pyarrow_type</code> <code>str</code> <p>The Pyarrow type to convert.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The Trino type.</p> Source code in <code>src/viadot/sources/_trino.py</code> <pre><code>@staticmethod\ndef pyarrow_to_trino_type(pyarrow_type: str) -&gt; str:\n    \"\"\"Convert a pyarrow data type to a Trino type.\n\n    Args:\n        pyarrow_type (str): The Pyarrow type to convert.\n\n    Returns:\n        str: The Trino type.\n    \"\"\"\n    mapping = {\n        \"string\": \"VARCHAR\",\n        \"large_string\": \"VARCHAR\",\n        \"int8\": \"TINYINT\",\n        \"int16\": \"SMALLINT\",\n        \"int32\": \"INTEGER\",\n        \"int64\": \"BIGINT\",\n        \"float\": \"REAL\",\n        \"double\": \"DOUBLE\",\n        \"bool\": \"BOOLEAN\",\n        \"date32[day]\": \"DATE\",\n        \"timestamp[ns]\": \"TIMESTAMP(6)\",\n        \"decimal\": \"DECIMAL\",\n        \"decimal128\": \"DECIMAL\",\n        \"decimal256\": \"DECIMAL\",\n    }\n    precision, scale = None, None\n    decimal_match = re.match(r\"(\\w+)\\((\\d+), (\\d+)\\)\", pyarrow_type)\n    if decimal_match:\n        pyarrow_type = decimal_match.group(1)\n        precision = int(decimal_match.group(2))\n        scale = int(decimal_match.group(3))\n\n    mapped_type = mapping.get(pyarrow_type) or \"VARCHAR\"\n\n    if precision and scale:\n        mapped_type += f\"({precision}, {scale})\"\n\n    return mapped_type\n</code></pre>"},{"location":"references/sources/database/#viadot.sources._trino.Trino.run","title":"<code>run(sql, connection)</code>","text":"<p>Run a SQL query.</p> <p>Parameters:</p> Name Type Description Default <code>sql</code> <code>str</code> <p>description</p> required <code>connection</code> <code>Connection</code> <p>description</p> required <p>Yields:</p> Type Description <code>Generator[tuple, None, None] | None</code> <p>Generator[tuple, None, None] | None: description</p> Source code in <code>src/viadot/sources/_trino.py</code> <pre><code>def run(\n    self, sql: str, connection: Connection\n) -&gt; Generator[tuple, None, None] | None:\n    \"\"\"Run a SQL query.\n\n    Args:\n        sql (str): _description_\n        connection (Connection): _description_\n\n    Yields:\n        Generator[tuple, None, None] | None: _description_\n    \"\"\"\n\n    def row_generator(result: CursorResult):\n        # Fetch rows in chunks of size `yield_per`.\n        # This has to be inside a function due to how Python generators work.\n        for partition in result.partitions():\n            yield from partition\n\n    self.logger.debug(\"Executing SQL:\\n\" + sql)\n\n    try:\n        # Execute with server-side cursor of size 5000.\n        result = connection.execution_options(yield_per=5000).execute(text(sql))\n    except Exception as e:\n        msg = f\"Failed executing SQL:\\n{sql}\"\n        raise ValueError(msg) from e\n\n    query_keywords = [\"SELECT\", \"SHOW\", \"PRAGMA\", \"WITH\"]\n    is_query = any(sql.strip().upper().startswith(word) for word in query_keywords)\n\n    return row_generator(result) if is_query else None\n</code></pre>"},{"location":"references/sources/other/","title":"Other sources","text":""},{"location":"references/sources/other/#viadot.sources.sftp.Sftp","title":"<code>viadot.sources.sftp.Sftp</code>","text":"<p>               Bases: <code>Source</code></p> <p>Class implementing a SFTP server connection.</p> Source code in <code>src/viadot/sources/sftp.py</code> <pre><code>class Sftp(Source):\n    \"\"\"Class implementing a SFTP server connection.\"\"\"\n\n    def __init__(\n        self,\n        *args,\n        credentials: SftpCredentials | None = None,\n        config_key: str = \"sftp\",\n        **kwargs,\n    ):\n        \"\"\"Create an instance of SFTP.\n\n        Args:\n            credentials (SftpCredentials, optional): SFTP credentials. Defaults to None.\n            config_key (str, optional): The key in the viadot config holding relevant\n                credentials. Defaults to \"sftp\".\n\n        Notes:\n            self.conn is paramiko.SFTPClient.from_transport method that contains\n            additional methods like get, put, open etc. Some of them were not\n            implemented in that class. For more check documentation\n            (https://docs.paramiko.org/en/stable/api/sftp.html).\n\n            sftp = Sftp()\n            sftp.conn.open(filename='folder_a/my_file.zip', mode='r')\n\n        Raises:\n            CredentialError: If credentials are not provided in viadot config or\n                directly as a parameter.\n        \"\"\"\n        credentials = credentials or get_source_credentials(config_key)\n\n        if credentials is None:\n            message = \"Missing credentials.\"\n            raise CredentialError(message)\n\n        validated_creds = dict(SftpCredentials(**credentials))\n        super().__init__(*args, credentials=validated_creds, **kwargs)\n\n        self.conn = None\n        self.hostname = validated_creds.get(\"hostname\")\n        self.username = validated_creds.get(\"username\")\n        self.password = validated_creds.get(\"password\")\n        self.port = validated_creds.get(\"port\")\n        self.rsa_key = validated_creds.get(\"rsa_key\")\n\n    def _get_file_object(self, file_name: str) -&gt; BytesIO:\n        \"\"\"Copy a remote file from the SFTP server and write to a file-like object.\n\n        Args:\n            file_name (str, optional): File name to copy.\n\n        Returns:\n            BytesIO: file-like object.\n        \"\"\"\n        file_object = BytesIO()\n        try:\n            self.conn.getfo(file_name, file_object)\n\n        except FileNotFoundError as error:\n            raise SFTPError from error\n\n        else:\n            return file_object\n\n    def get_connection(self) -&gt; paramiko.SFTPClient:\n        \"\"\"Returns a SFTP connection object.\n\n        Returns: paramiko.SFTPClient.\n        \"\"\"\n        ssh = paramiko.SSHClient()\n\n        if not self.rsa_key:\n            transport = paramiko.Transport((self.hostname, self.port))\n            transport.connect(None, self.username, self.password)\n\n            self.conn = paramiko.SFTPClient.from_transport(transport)\n\n        else:\n            mykey = paramiko.RSAKey.from_private_key(StringIO(self.rsa_key))\n            ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())  # noqa: S507\n            ssh.connect(self.hostname, username=self.username, pkey=mykey)\n            time.sleep(1)\n            self.conn = ssh.open_sftp()\n\n        self.logger.info(\"Connected to the SFTP server.\")\n\n    @add_viadot_metadata_columns\n    def to_df(\n        self,\n        if_empty: Literal[\"warn\", \"skip\", \"fail\"] = \"warn\",\n        file_name: str | None = None,\n        sep: str = \"\\t\",\n        columns: list[str] | None = None,\n    ) -&gt; pd.DataFrame:\n        r\"\"\"Copy a remote file from the SFTP server and write it to Pandas dataframe.\n\n        Args:\n            if_empty (Literal[\"warn\", \"skip\", \"fail\"], optional): What to do if\n                the fetch produces no data. Defaults to \"warn\".\n            file_name (str, optional): The name of the file to download.\n            sep (str, optional): The delimiter for the source file. Defaults to \"\\t\".\n            columns (list[str], optional): List of columns to select from file.\n                Defaults to None.\n\n        Returns:\n            pd.DataFrame: The response data as a Pandas Data Frame plus viadot metadata.\n        \"\"\"\n        byte_file = self._get_file_object(file_name=file_name)\n        byte_file.seek(0)\n\n        self._close_conn()\n\n        suffix = Path(file_name).suffix\n        if suffix == \".csv\":\n            df = pd.read_csv(byte_file, sep=sep, usecols=columns)\n\n        elif suffix == \".parquet\":\n            df = pd.read_parquet(byte_file, usecols=columns)\n\n        elif suffix == \".tsv\":\n            df = pd.read_csv(byte_file, sep=sep, usecols=columns)\n\n        elif suffix in [\".xls\", \".xlsx\", \".xlsm\"]:\n            df = pd.read_excel(byte_file, usecols=columns)\n\n        elif suffix == \".json\":\n            df = pd.read_json(byte_file)\n\n        elif suffix == \".pkl\":\n            df = pd.read_pickle(byte_file)  # noqa: S301\n\n        elif suffix == \".sql\":\n            df = pd.read_sql(byte_file)\n\n        elif suffix == \".hdf\":\n            df = pd.read_hdf(byte_file)\n\n        else:\n            message = (\n                f\"Unable to read file '{Path(file_name).name}', \"\n                + f\"unsupported filetype: {suffix}\"\n            )\n            raise ValueError(message)\n\n        if df.empty:\n            self._handle_if_empty(\n                if_empty=if_empty,\n                message=\"The response does not contain any data.\",\n            )\n        else:\n            self.logger.info(\"Successfully downloaded data from the SFTP server.\")\n\n        return df\n\n    def _ls(self, path: str | None = \".\", recursive: bool = False) -&gt; list[str]:\n        \"\"\"List files in specified directory, with optional recursion.\n\n        Args:\n            path (str | None): Full path to the remote directory to list.\n                Defaults to \".\".\n            recursive (bool): Whether to list files recursively. Defaults to False.\n\n        Returns:\n            list[str]: List of files in the specified directory.\n        \"\"\"\n        files_list = []\n\n        path = \".\" if path is None else path\n        try:\n            if not recursive:\n                return [\n                    str(Path(path) / attr.filename)\n                    for attr in self.conn.listdir_attr(path)\n                    if S_ISREG(attr.st_mode)\n                ]\n\n            for attr in self.conn.listdir_attr(path):\n                full_path = str(Path(path) / attr.filename)\n                if S_ISDIR(attr.st_mode):\n                    files_list.extend(self._ls(full_path, recursive=True))\n                else:\n                    files_list.append(full_path)\n        except FileNotFoundError as e:\n            self.logger.info(f\"Directory not found: {path}. Error: {e}\")\n        except Exception as e:\n            self.logger.info(f\"Error accessing {path}: {e}\")\n\n        return files_list\n\n    def get_files_list(\n        self,\n        path: str | None = None,\n        recursive: bool = False,\n        matching_path: str | None = None,\n    ) -&gt; list[str]:\n        \"\"\"List files in `path`.\n\n        Args:\n            path (str | None): Destination path from where to get the structure.\n                Defaults to None.\n            recursive (bool): Get the structure in deeper folders.\n                Defaults to False.\n            matching_path (str | None): Filtering folders to return by a regex\n                pattern. Defaults to None.\n\n        Returns:\n            list[str]: List of files in the specified path.\n        \"\"\"\n        files_list = self._ls(path=path, recursive=recursive)\n\n        if matching_path is not None:\n            files_list = [f for f in files_list if re.match(matching_path, f)]\n\n        self._close_conn()\n\n        self.logger.info(\"Successfully loaded file list from SFTP server.\")\n\n        return files_list\n\n    def _close_conn(self) -&gt; None:\n        \"\"\"Close the SFTP server connection.\"\"\"\n        if self.conn is not None:\n            self.conn.close()\n            self.conn = None\n</code></pre>"},{"location":"references/sources/other/#viadot.sources.sftp.Sftp.__init__","title":"<code>__init__(*args, credentials=None, config_key='sftp', **kwargs)</code>","text":"<p>Create an instance of SFTP.</p> <p>Parameters:</p> Name Type Description Default <code>credentials</code> <code>SftpCredentials</code> <p>SFTP credentials. Defaults to None.</p> <code>None</code> <code>config_key</code> <code>str</code> <p>The key in the viadot config holding relevant credentials. Defaults to \"sftp\".</p> <code>'sftp'</code> Notes <p>self.conn is paramiko.SFTPClient.from_transport method that contains additional methods like get, put, open etc. Some of them were not implemented in that class. For more check documentation (https://docs.paramiko.org/en/stable/api/sftp.html).</p> <p>sftp = Sftp() sftp.conn.open(filename='folder_a/my_file.zip', mode='r')</p> <p>Raises:</p> Type Description <code>CredentialError</code> <p>If credentials are not provided in viadot config or directly as a parameter.</p> Source code in <code>src/viadot/sources/sftp.py</code> <pre><code>def __init__(\n    self,\n    *args,\n    credentials: SftpCredentials | None = None,\n    config_key: str = \"sftp\",\n    **kwargs,\n):\n    \"\"\"Create an instance of SFTP.\n\n    Args:\n        credentials (SftpCredentials, optional): SFTP credentials. Defaults to None.\n        config_key (str, optional): The key in the viadot config holding relevant\n            credentials. Defaults to \"sftp\".\n\n    Notes:\n        self.conn is paramiko.SFTPClient.from_transport method that contains\n        additional methods like get, put, open etc. Some of them were not\n        implemented in that class. For more check documentation\n        (https://docs.paramiko.org/en/stable/api/sftp.html).\n\n        sftp = Sftp()\n        sftp.conn.open(filename='folder_a/my_file.zip', mode='r')\n\n    Raises:\n        CredentialError: If credentials are not provided in viadot config or\n            directly as a parameter.\n    \"\"\"\n    credentials = credentials or get_source_credentials(config_key)\n\n    if credentials is None:\n        message = \"Missing credentials.\"\n        raise CredentialError(message)\n\n    validated_creds = dict(SftpCredentials(**credentials))\n    super().__init__(*args, credentials=validated_creds, **kwargs)\n\n    self.conn = None\n    self.hostname = validated_creds.get(\"hostname\")\n    self.username = validated_creds.get(\"username\")\n    self.password = validated_creds.get(\"password\")\n    self.port = validated_creds.get(\"port\")\n    self.rsa_key = validated_creds.get(\"rsa_key\")\n</code></pre>"},{"location":"references/sources/other/#viadot.sources.sftp.Sftp.get_connection","title":"<code>get_connection()</code>","text":"<p>Returns a SFTP connection object.</p> <p>Returns: paramiko.SFTPClient.</p> Source code in <code>src/viadot/sources/sftp.py</code> <pre><code>def get_connection(self) -&gt; paramiko.SFTPClient:\n    \"\"\"Returns a SFTP connection object.\n\n    Returns: paramiko.SFTPClient.\n    \"\"\"\n    ssh = paramiko.SSHClient()\n\n    if not self.rsa_key:\n        transport = paramiko.Transport((self.hostname, self.port))\n        transport.connect(None, self.username, self.password)\n\n        self.conn = paramiko.SFTPClient.from_transport(transport)\n\n    else:\n        mykey = paramiko.RSAKey.from_private_key(StringIO(self.rsa_key))\n        ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())  # noqa: S507\n        ssh.connect(self.hostname, username=self.username, pkey=mykey)\n        time.sleep(1)\n        self.conn = ssh.open_sftp()\n\n    self.logger.info(\"Connected to the SFTP server.\")\n</code></pre>"},{"location":"references/sources/other/#viadot.sources.sftp.Sftp.get_files_list","title":"<code>get_files_list(path=None, recursive=False, matching_path=None)</code>","text":"<p>List files in <code>path</code>.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | None</code> <p>Destination path from where to get the structure. Defaults to None.</p> <code>None</code> <code>recursive</code> <code>bool</code> <p>Get the structure in deeper folders. Defaults to False.</p> <code>False</code> <code>matching_path</code> <code>str | None</code> <p>Filtering folders to return by a regex pattern. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: List of files in the specified path.</p> Source code in <code>src/viadot/sources/sftp.py</code> <pre><code>def get_files_list(\n    self,\n    path: str | None = None,\n    recursive: bool = False,\n    matching_path: str | None = None,\n) -&gt; list[str]:\n    \"\"\"List files in `path`.\n\n    Args:\n        path (str | None): Destination path from where to get the structure.\n            Defaults to None.\n        recursive (bool): Get the structure in deeper folders.\n            Defaults to False.\n        matching_path (str | None): Filtering folders to return by a regex\n            pattern. Defaults to None.\n\n    Returns:\n        list[str]: List of files in the specified path.\n    \"\"\"\n    files_list = self._ls(path=path, recursive=recursive)\n\n    if matching_path is not None:\n        files_list = [f for f in files_list if re.match(matching_path, f)]\n\n    self._close_conn()\n\n    self.logger.info(\"Successfully loaded file list from SFTP server.\")\n\n    return files_list\n</code></pre>"},{"location":"references/sources/other/#viadot.sources.sftp.Sftp.to_df","title":"<code>to_df(if_empty='warn', file_name=None, sep='\\t', columns=None)</code>","text":"<p>Copy a remote file from the SFTP server and write it to Pandas dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>if_empty</code> <code>Literal['warn', 'skip', 'fail']</code> <p>What to do if the fetch produces no data. Defaults to \"warn\".</p> <code>'warn'</code> <code>file_name</code> <code>str</code> <p>The name of the file to download.</p> <code>None</code> <code>sep</code> <code>str</code> <p>The delimiter for the source file. Defaults to \"\\t\".</p> <code>'\\t'</code> <code>columns</code> <code>list[str]</code> <p>List of columns to select from file. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: The response data as a Pandas Data Frame plus viadot metadata.</p> Source code in <code>src/viadot/sources/sftp.py</code> <pre><code>@add_viadot_metadata_columns\ndef to_df(\n    self,\n    if_empty: Literal[\"warn\", \"skip\", \"fail\"] = \"warn\",\n    file_name: str | None = None,\n    sep: str = \"\\t\",\n    columns: list[str] | None = None,\n) -&gt; pd.DataFrame:\n    r\"\"\"Copy a remote file from the SFTP server and write it to Pandas dataframe.\n\n    Args:\n        if_empty (Literal[\"warn\", \"skip\", \"fail\"], optional): What to do if\n            the fetch produces no data. Defaults to \"warn\".\n        file_name (str, optional): The name of the file to download.\n        sep (str, optional): The delimiter for the source file. Defaults to \"\\t\".\n        columns (list[str], optional): List of columns to select from file.\n            Defaults to None.\n\n    Returns:\n        pd.DataFrame: The response data as a Pandas Data Frame plus viadot metadata.\n    \"\"\"\n    byte_file = self._get_file_object(file_name=file_name)\n    byte_file.seek(0)\n\n    self._close_conn()\n\n    suffix = Path(file_name).suffix\n    if suffix == \".csv\":\n        df = pd.read_csv(byte_file, sep=sep, usecols=columns)\n\n    elif suffix == \".parquet\":\n        df = pd.read_parquet(byte_file, usecols=columns)\n\n    elif suffix == \".tsv\":\n        df = pd.read_csv(byte_file, sep=sep, usecols=columns)\n\n    elif suffix in [\".xls\", \".xlsx\", \".xlsm\"]:\n        df = pd.read_excel(byte_file, usecols=columns)\n\n    elif suffix == \".json\":\n        df = pd.read_json(byte_file)\n\n    elif suffix == \".pkl\":\n        df = pd.read_pickle(byte_file)  # noqa: S301\n\n    elif suffix == \".sql\":\n        df = pd.read_sql(byte_file)\n\n    elif suffix == \".hdf\":\n        df = pd.read_hdf(byte_file)\n\n    else:\n        message = (\n            f\"Unable to read file '{Path(file_name).name}', \"\n            + f\"unsupported filetype: {suffix}\"\n        )\n        raise ValueError(message)\n\n    if df.empty:\n        self._handle_if_empty(\n            if_empty=if_empty,\n            message=\"The response does not contain any data.\",\n        )\n    else:\n        self.logger.info(\"Successfully downloaded data from the SFTP server.\")\n\n    return df\n</code></pre>"}]}