{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Viadot Documentation : https://dyvenia.github.io/viadot/ Source Code : https://github.com/dyvenia/viadot A simple data ingestion library to guide data flows from some places to other places. Structure This documentation is following the di\u00e1taxis framework. Getting Data from a Source Viadot supports several API and RDBMS sources, private and public. Currently, we support the UK Carbon Intensity public API and base the examples on it. from viadot.sources.uk_carbon_intensity import UKCarbonIntensity ukci = UKCarbonIntensity () ukci . query ( \"/intensity\" ) df = ukci . to_df () df Output: from to forecast actual index 0 2021-08-10T11:00Z 2021-08-10T11:30Z 211 216 moderate The above df is a python pandas DataFrame object. The above df contains data downloaded from viadot from the Carbon Intensity UK API. Loading Data to a Source Depending on the source, viadot provides different methods of uploading data. For instance, for SQL sources, this would be bulk inserts. For data lake sources, it would be a file upload. We also provide ready-made pipelines including data validation steps using Great Expectations. An example of loading data into SQLite from a pandas DataFrame using the SQLiteInsert Prefect task: from viadot.tasks import SQLiteInsert insert_task = SQLiteInsert () insert_task . run ( table_name = TABLE_NAME , dtypes = dtypes , db_path = database_path , df = df , if_exists = \"replace\" ) Running tests To run tests, log into the container and run pytest: cd viadot/docker run.sh docker exec -it viadot_testing bash pytest Running flows locally You can run the example flows from the terminal: run.sh docker exec -it viadot_testing bash FLOW_NAME=hello_world; python -m viadot.examples.$FLOW_NAME However, when developing, the easiest way is to use the provided Jupyter Lab container available at http://localhost:9000/ . How to contribute Clone the release branch Pull the docker env by running viadot/docker/update.sh -t dev Run the env with viadot/docker/run.sh Log into the dev container and install in development mode so that viadot will auto-install at each code change: docker exec -it viadot_testing bash pip install -e . Edit and test your changes with pytest Submit a PR. The PR should contain the following: new/changed functionality tests for the changes changes added to CHANGELOG.md any other relevant resources updated (esp. viadot/docs ) Please follow the standards and best practices used within the library (eg. when adding tasks, see how other tasks are constructed, etc.). For any questions, please reach out to us here on GitHub.","title":"Home"},{"location":"#viadot","text":"Documentation : https://dyvenia.github.io/viadot/ Source Code : https://github.com/dyvenia/viadot A simple data ingestion library to guide data flows from some places to other places.","title":"Viadot"},{"location":"#structure","text":"This documentation is following the di\u00e1taxis framework.","title":"Structure"},{"location":"#getting-data-from-a-source","text":"Viadot supports several API and RDBMS sources, private and public. Currently, we support the UK Carbon Intensity public API and base the examples on it. from viadot.sources.uk_carbon_intensity import UKCarbonIntensity ukci = UKCarbonIntensity () ukci . query ( \"/intensity\" ) df = ukci . to_df () df Output: from to forecast actual index 0 2021-08-10T11:00Z 2021-08-10T11:30Z 211 216 moderate The above df is a python pandas DataFrame object. The above df contains data downloaded from viadot from the Carbon Intensity UK API.","title":"Getting Data from a Source"},{"location":"#loading-data-to-a-source","text":"Depending on the source, viadot provides different methods of uploading data. For instance, for SQL sources, this would be bulk inserts. For data lake sources, it would be a file upload. We also provide ready-made pipelines including data validation steps using Great Expectations. An example of loading data into SQLite from a pandas DataFrame using the SQLiteInsert Prefect task: from viadot.tasks import SQLiteInsert insert_task = SQLiteInsert () insert_task . run ( table_name = TABLE_NAME , dtypes = dtypes , db_path = database_path , df = df , if_exists = \"replace\" )","title":"Loading Data to a Source"},{"location":"#running-tests","text":"To run tests, log into the container and run pytest: cd viadot/docker run.sh docker exec -it viadot_testing bash pytest","title":"Running tests"},{"location":"#running-flows-locally","text":"You can run the example flows from the terminal: run.sh docker exec -it viadot_testing bash FLOW_NAME=hello_world; python -m viadot.examples.$FLOW_NAME However, when developing, the easiest way is to use the provided Jupyter Lab container available at http://localhost:9000/ .","title":"Running flows locally"},{"location":"#how-to-contribute","text":"Clone the release branch Pull the docker env by running viadot/docker/update.sh -t dev Run the env with viadot/docker/run.sh Log into the dev container and install in development mode so that viadot will auto-install at each code change: docker exec -it viadot_testing bash pip install -e . Edit and test your changes with pytest Submit a PR. The PR should contain the following: new/changed functionality tests for the changes changes added to CHANGELOG.md any other relevant resources updated (esp. viadot/docs ) Please follow the standards and best practices used within the library (eg. when adding tasks, see how other tasks are constructed, etc.). For any questions, please reach out to us here on GitHub.","title":"How to contribute"},{"location":"howtos/config_file/","text":"Config File Credentials and other settings for various sources are stored in a file named credentials.json . A credential file needs to be written in json format. A typical credentials file looks like so: { \"SUPERMETRICS\" : { \"API_KEY\" : \"apikey from supermetrics\" , \"USER\" : \"user@gmail.com\" , \"SOURCES\" : { \"Google Ads\" : { \"Accounts\" : [ \"456\" ] } } }, \"AZURE_SQL\" : { \"server\" : \"server url\" , \"db_name\" : \"db name\" , \"user\" : \"user\" , \"password\" : \"db password\" , \"driver\" : \"driver\" } } In the above SUPERMETRICS and AZURE_SQL are config_keys. These config settings are fed to the Supermetrics() or AzureSQL() Sources. For example, this is how to use the AZURE_SQL configuration stanza from the credentials file. # initiates the AzureSQL class with the AZURE_SQL configs azure_sql = AzureSQL ( config_key = \"AZURE_SQL\" ) The above will pass all the configurations, including secrets like passwords, to the class. This avoids having to write secrets or configs in the code. Storing the file locally Currently only local files are supported. Make sure to store the file in the correct path. On Linux the path is /home/viadot/.config/credentials.json On Windows you need to create a .config folder with credentials.json inside the User folder C:\\Users\\<user>","title":"Config File"},{"location":"howtos/config_file/#config-file","text":"Credentials and other settings for various sources are stored in a file named credentials.json . A credential file needs to be written in json format. A typical credentials file looks like so: { \"SUPERMETRICS\" : { \"API_KEY\" : \"apikey from supermetrics\" , \"USER\" : \"user@gmail.com\" , \"SOURCES\" : { \"Google Ads\" : { \"Accounts\" : [ \"456\" ] } } }, \"AZURE_SQL\" : { \"server\" : \"server url\" , \"db_name\" : \"db name\" , \"user\" : \"user\" , \"password\" : \"db password\" , \"driver\" : \"driver\" } } In the above SUPERMETRICS and AZURE_SQL are config_keys. These config settings are fed to the Supermetrics() or AzureSQL() Sources. For example, this is how to use the AZURE_SQL configuration stanza from the credentials file. # initiates the AzureSQL class with the AZURE_SQL configs azure_sql = AzureSQL ( config_key = \"AZURE_SQL\" ) The above will pass all the configurations, including secrets like passwords, to the class. This avoids having to write secrets or configs in the code.","title":"Config File"},{"location":"howtos/config_file/#storing-the-file-locally","text":"Currently only local files are supported. Make sure to store the file in the correct path. On Linux the path is /home/viadot/.config/credentials.json On Windows you need to create a .config folder with credentials.json inside the User folder C:\\Users\\<user>","title":"Storing the file locally"},{"location":"howtos/flows/","text":"Using flows Viadot flows subclass Prefect Flow class. We take this class, specify the tasks, and build the flow using the parameters provided at initialization time. See Prefect Flow documentation for more information. For instance, a S3 to Redshift flow would include the tasks necessary to insert S3 files into Redshift, and automate the generation of the flow. You only need to pass the required parameters. For examples, check out the examples folder. Writing flows For now, see the existing flows in viadot/flows .","title":"Flows"},{"location":"howtos/flows/#using-flows","text":"Viadot flows subclass Prefect Flow class. We take this class, specify the tasks, and build the flow using the parameters provided at initialization time. See Prefect Flow documentation for more information. For instance, a S3 to Redshift flow would include the tasks necessary to insert S3 files into Redshift, and automate the generation of the flow. You only need to pass the required parameters. For examples, check out the examples folder.","title":"Using flows"},{"location":"howtos/flows/#writing-flows","text":"For now, see the existing flows in viadot/flows .","title":"Writing flows"},{"location":"howtos/running_tasks/","text":"Running tasks To run a task, first initialize it, and then use the run() method to execute it: from viadot.tasks import SupermetricsToDF task = SupermetricsToDF () # initialize task_params = {} # parameters to be passed to the task task . run ( ** task_params ) # run All tasks have inline docstrings, so you can use the standard shortcut alt + tab to see the docstring in your IDE (eg. Visual Studio) -- both for the initialization, as well as the run() method (although we recommend to only pass parameters inside the run() method and so only check its docstring).","title":"Running a task"},{"location":"howtos/running_tasks/#running-tasks","text":"To run a task, first initialize it, and then use the run() method to execute it: from viadot.tasks import SupermetricsToDF task = SupermetricsToDF () # initialize task_params = {} # parameters to be passed to the task task . run ( ** task_params ) # run All tasks have inline docstrings, so you can use the standard shortcut alt + tab to see the docstring in your IDE (eg. Visual Studio) -- both for the initialization, as well as the run() method (although we recommend to only pass parameters inside the run() method and so only check its docstring).","title":"Running tasks"},{"location":"references/api_sources/","text":"API Sources viadot.sources.uk_carbon_intensity.UKCarbonIntensity ( Source ) Fetches data of Carbon Intensity of the UK Power Grid. Documentation for this source API is located at: https://carbon-intensity.github.io/api-definitions/#carbon-intensity-api-v2-0-0 Parameters api_url : str, optional The URL endpoint to call, by default None to_df ( self , if_empty = 'warn' ) Returns a pandas DataFrame with flattened data Returns: Type Description pandas.DataFrame A Pandas DataFrame Source code in viadot/sources/uk_carbon_intensity.py def to_df ( self , if_empty : str = \"warn\" ): \"\"\"Returns a pandas DataFrame with flattened data Returns: pandas.DataFrame: A Pandas DataFrame \"\"\" from_ = [] to = [] forecast = [] actual = [] max_ = [] average = [] min_ = [] index = [] json_data = self . to_json () if not json_data : self . _handle_if_empty ( if_empty ) for row in json_data [ \"data\" ]: from_ . append ( row [ \"from\" ]) to . append ( row [ \"to\" ]) index . append ( row [ \"intensity\" ][ \"index\" ]) try : forecast . append ( row [ \"intensity\" ][ \"forecast\" ]) actual . append ( row [ \"intensity\" ][ \"actual\" ]) df = pd . DataFrame ( { \"from\" : from_ , \"to\" : to , \"forecast\" : forecast , \"actual\" : actual , \"index\" : index , } ) except KeyError : max_ . append ( row [ \"intensity\" ][ \"max\" ]) average . append ( row [ \"intensity\" ][ \"average\" ]) min_ . append ( row [ \"intensity\" ][ \"min\" ]) df = pd . DataFrame ( { \"from\" : from_ , \"to\" : to , \"max\" : max_ , \"average\" : average , \"min\" : min_ , } ) return df to_json ( self ) Creates json file Source code in viadot/sources/uk_carbon_intensity.py def to_json ( self ): \"\"\"Creates json file\"\"\" url = f \" { self . API_ENDPOINT }{ self . api_url } \" headers = { \"Accept\" : \"application/json\" } response = requests . get ( url , params = {}, headers = headers ) if response . ok : return response . json () else : raise f \"Error { response . json () } \" viadot.sources.supermetrics.Supermetrics ( Source ) A class implementing the Supermetrics API. Documentation for this API is located at: https://supermetrics.com/docs/product-api-getting-started/ Usage limits: https://supermetrics.com/docs/product-api-usage-limits/ Parameters query_params : Dict[str, Any], optional The parameters to pass to the GET query. See https://supermetrics.com/docs/product-api-get-data/ for full specification, by default None get_params_from_api_query ( url ) classmethod Returns parmeters from API query in a dictionary Source code in viadot/sources/supermetrics.py @classmethod def get_params_from_api_query ( cls , url : str ) -> Dict [ str , Any ]: \"\"\"Returns parmeters from API query in a dictionary\"\"\" url_unquoted = urllib . parse . unquote ( url ) s = urllib . parse . parse_qs ( url_unquoted ) endpoint = list ( s . keys ())[ 0 ] params = s [ endpoint ][ 0 ] params_d = json . loads ( params ) return params_d to_df ( self , if_empty = 'warn' ) Download data into a pandas DataFrame. Note that Supermetric can calculate some fields on the fly and alias them in the returned result. For example, if the query requests the position field, Supermetric may return an Average position caclulated field. For this reason we take columns names from the actual results rather than from input fields. Parameters: Name Type Description Default if_empty str What to do if query returned no data. Defaults to \"warn\". 'warn' Returns: Type Description pd.DataFrame the DataFrame containing query results Source code in viadot/sources/supermetrics.py def to_df ( self , if_empty : str = \"warn\" ) -> pd . DataFrame : \"\"\"Download data into a pandas DataFrame. Note that Supermetric can calculate some fields on the fly and alias them in the returned result. For example, if the query requests the `position` field, Supermetric may return an `Average position` caclulated field. For this reason we take columns names from the actual results rather than from input fields. Args: if_empty (str, optional): What to do if query returned no data. Defaults to \"warn\". Returns: pd.DataFrame: the DataFrame containing query results \"\"\" try : columns = self . _get_col_names () except ValueError : columns = None data = self . to_json ()[ \"data\" ] if data : df = pd . DataFrame ( data [ 1 :], columns = columns ) . replace ( \"\" , np . nan ) else : df = pd . DataFrame ( columns = columns ) if df . empty : self . _handle_if_empty ( if_empty ) return df to_json ( self , timeout = ( 3.05 , 1800 )) Download query results to a dictionary. Note that Supermetrics API will sometimes hang and not return any error message, so we're adding a timeout to GET. See requests docs for an explanation of why this timeout value will work on long-running queries but fail fast on connection issues. Source code in viadot/sources/supermetrics.py def to_json ( self , timeout = ( 3.05 , 60 * 30 )) -> Dict [ str , Any ]: \"\"\"Download query results to a dictionary. Note that Supermetrics API will sometimes hang and not return any error message, so we're adding a timeout to GET. See [requests docs](https://docs.python-requests.org/en/master/user/advanced/#timeouts) for an explanation of why this timeout value will work on long-running queries but fail fast on connection issues. \"\"\" if not self . query_params : raise ValueError ( \"Please build the query first\" ) params = { \"json\" : json . dumps ( self . query_params )} headers = { \"Authorization\" : f 'Bearer { self . credentials [ \"API_KEY\" ] } ' } response = handle_api_response ( url = self . API_ENDPOINT , params = params , headers = headers , timeout = timeout ) return response . json () viadot.sources.cloud_for_customers.CloudForCustomers ( Source ) __init__ ( self , * args , * , report_url = None , url = None , endpoint = None , params = None , env = 'QA' , credentials = None , ** kwargs ) special Cloud for Customers connector build for fetching Odata source. See pyodata docs for an explanation how Odata works. Parameters report_url (str, optional): The url to the API in case of prepared report. Defaults to None. url (str, optional): The url to the API. Defaults to None. endpoint (str, optional): The endpoint of the API. Defaults to None. params (Dict[str, Any]): The query parameters like filter by creation date time. Defaults to json format. env (str, optional): The development environments. Defaults to 'QA'. credentials (Dict[str, Any], optional): The credentials are populated with values from config file or this parameter. Defaults to None than use credentials from local_config. Source code in viadot/sources/cloud_for_customers.py def __init__ ( self , * args , report_url : str = None , url : str = None , endpoint : str = None , params : Dict [ str , Any ] = None , env : str = \"QA\" , credentials : Dict [ str , Any ] = None , ** kwargs , ): \"\"\"Cloud for Customers connector build for fetching Odata source. See [pyodata docs](https://pyodata.readthedocs.io/en/latest/index.html) for an explanation how Odata works. Parameters ---------- report_url (str, optional): The url to the API in case of prepared report. Defaults to None. url (str, optional): The url to the API. Defaults to None. endpoint (str, optional): The endpoint of the API. Defaults to None. params (Dict[str, Any]): The query parameters like filter by creation date time. Defaults to json format. env (str, optional): The development environments. Defaults to 'QA'. credentials (Dict[str, Any], optional): The credentials are populated with values from config file or this parameter. Defaults to None than use credentials from local_config. \"\"\" super () . __init__ ( * args , ** kwargs ) try : DEFAULT_CREDENTIALS = local_config [ \"CLOUD_FOR_CUSTOMERS\" ] . get ( env ) except KeyError : DEFAULT_CREDENTIALS = None self . credentials = credentials or DEFAULT_CREDENTIALS or {} self . url = url or self . credentials . get ( \"server\" ) self . report_url = report_url if self . url is None and report_url is None : raise CredentialError ( \"One of: ('url', 'report_url') is required.\" ) self . is_report = bool ( report_url ) self . query_endpoint = endpoint if params : params_merged = self . DEFAULT_PARAMS . copy () params_merged . update ( params ) self . params = params_merged else : self . params = self . DEFAULT_PARAMS if self . url : self . full_url = urljoin ( self . url , self . query_endpoint ) super () . __init__ ( * args , credentials = self . credentials , ** kwargs ) get_response ( self , url , params = None , timeout = ( 3.05 , 1800 )) Handle and raise Python exceptions during request. Using of url and service endpoint needs additional parameters stores in params. report_url contain additional params in their structure. In report_url scenario it can not contain params parameter. Parameters: Name Type Description Default url str the URL which trying to connect. required params Dict[str, Any] Additional parameters like filter, used in case of normal url. None timeout tuple the request times out. Defaults to (3.05, 60 * 30). (3.05, 1800) Returns: Type Description Response requests.models.Response Source code in viadot/sources/cloud_for_customers.py def get_response ( self , url : str , params : Dict [ str , Any ] = None , timeout : tuple = ( 3.05 , 60 * 30 ) ) -> requests . models . Response : \"\"\"Handle and raise Python exceptions during request. Using of url and service endpoint needs additional parameters stores in params. report_url contain additional params in their structure. In report_url scenario it can not contain params parameter. Args: url (str): the URL which trying to connect. params (Dict[str, Any], optional): Additional parameters like filter, used in case of normal url. Defaults to None used in case of report_url, which can not contain params. timeout (tuple, optional): the request times out. Defaults to (3.05, 60 * 30). Returns: requests.models.Response \"\"\" username = self . credentials . get ( \"username\" ) pw = self . credentials . get ( \"password\" ) response = handle_api_response ( url = url , params = params , auth = ( username , pw ), timeout = timeout , ) return response map_columns ( self , url = None ) Fetch metadata from url used to column name map. Parameters: Name Type Description Default url str the URL which trying to fetch metadata. Defaults to None. None Returns: Type Description Dict[str, str] Property Name as key mapped to the value of sap label. Source code in viadot/sources/cloud_for_customers.py def map_columns ( self , url : str = None ) -> Dict [ str , str ]: \"\"\"Fetch metadata from url used to column name map. Args: url (str, optional): the URL which trying to fetch metadata. Defaults to None. Returns: Dict[str, str]: Property Name as key mapped to the value of sap label. \"\"\" column_mapping = {} if url : username = self . credentials . get ( \"username\" ) pw = self . credentials . get ( \"password\" ) response = requests . get ( url , auth = ( username , pw )) for sentence in response . text . split ( \"/>\" ): result = re . search ( r '(?<=Name=\")([^\"]+).+(sap:label=\")([^\"]+)+' , sentence ) if result : key = result . groups ( 0 )[ 0 ] val = result . groups ( 0 )[ 2 ] column_mapping [ key ] = val return column_mapping response_to_entity_list ( self , dirty_json , url ) Changing request json response to list. Parameters: Name Type Description Default dirty_json Dict[str, Any] json from response. required url str the URL which trying to fetch metadata. required Returns: Type Description List List of dictionaries. Source code in viadot/sources/cloud_for_customers.py def response_to_entity_list ( self , dirty_json : Dict [ str , Any ], url : str ) -> List : \"\"\"Changing request json response to list. Args: dirty_json (Dict[str, Any]): json from response. url (str): the URL which trying to fetch metadata. Returns: List: List of dictionaries. \"\"\" metadata_url = self . change_to_meta_url ( url ) column_maper_dict = self . map_columns ( metadata_url ) entity_list = [] for element in dirty_json [ \"d\" ][ \"results\" ]: new_entity = {} for key , object_of_interest in element . items (): if key not in [ \"__metadata\" , \"Photo\" , \"\" , \"Picture\" ]: if \"{\" not in str ( object_of_interest ): new_key = column_maper_dict . get ( key ) if new_key : new_entity [ new_key ] = object_of_interest else : new_entity [ key ] = object_of_interest entity_list . append ( new_entity ) return entity_list to_df ( self , fields = None , if_empty = 'warn' , dtype = None , ** kwargs ) Returns records in a pandas DataFrame. Parameters: Name Type Description Default fields List[str] List of fields to put in DataFrame. Defaults to None. None dtype dict The dtypes to use in the DataFrame. We catch this parameter here since None kwargs The parameters to pass to DataFrame constructor. {} Source code in viadot/sources/cloud_for_customers.py def to_df ( self , fields : List [ str ] = None , if_empty : str = \"warn\" , dtype : dict = None , ** kwargs , ) -> pd . DataFrame : \"\"\"Returns records in a pandas DataFrame. Args: fields (List[str], optional): List of fields to put in DataFrame. Defaults to None. dtype (dict, optional): The dtypes to use in the DataFrame. We catch this parameter here since pandas doesn't support passing dtypes (eg. as a dict) to the constructor. kwargs: The parameters to pass to DataFrame constructor. \"\"\" records = self . to_records () df = pd . DataFrame ( data = records , ** kwargs ) if dtype : df = df . astype ( dtype ) if fields : return df [ fields ] return df to_records ( self ) Download a list of entities in the records format Source code in viadot/sources/cloud_for_customers.py def to_records ( self ) -> List [ Dict [ str , Any ]]: \"\"\" Download a list of entities in the records format \"\"\" if self . is_report : url = self . report_url return self . _to_records_report ( url = url ) else : url = self . full_url return self . _to_records_other ( url = url )","title":"API Sources"},{"location":"references/api_sources/#api-sources","text":"","title":"API Sources"},{"location":"references/api_sources/#viadot.sources.uk_carbon_intensity.UKCarbonIntensity","text":"Fetches data of Carbon Intensity of the UK Power Grid. Documentation for this source API is located at: https://carbon-intensity.github.io/api-definitions/#carbon-intensity-api-v2-0-0","title":"UKCarbonIntensity"},{"location":"references/api_sources/#viadot.sources.uk_carbon_intensity.UKCarbonIntensity--parameters","text":"api_url : str, optional The URL endpoint to call, by default None","title":"Parameters"},{"location":"references/api_sources/#viadot.sources.uk_carbon_intensity.UKCarbonIntensity.to_df","text":"Returns a pandas DataFrame with flattened data Returns: Type Description pandas.DataFrame A Pandas DataFrame Source code in viadot/sources/uk_carbon_intensity.py def to_df ( self , if_empty : str = \"warn\" ): \"\"\"Returns a pandas DataFrame with flattened data Returns: pandas.DataFrame: A Pandas DataFrame \"\"\" from_ = [] to = [] forecast = [] actual = [] max_ = [] average = [] min_ = [] index = [] json_data = self . to_json () if not json_data : self . _handle_if_empty ( if_empty ) for row in json_data [ \"data\" ]: from_ . append ( row [ \"from\" ]) to . append ( row [ \"to\" ]) index . append ( row [ \"intensity\" ][ \"index\" ]) try : forecast . append ( row [ \"intensity\" ][ \"forecast\" ]) actual . append ( row [ \"intensity\" ][ \"actual\" ]) df = pd . DataFrame ( { \"from\" : from_ , \"to\" : to , \"forecast\" : forecast , \"actual\" : actual , \"index\" : index , } ) except KeyError : max_ . append ( row [ \"intensity\" ][ \"max\" ]) average . append ( row [ \"intensity\" ][ \"average\" ]) min_ . append ( row [ \"intensity\" ][ \"min\" ]) df = pd . DataFrame ( { \"from\" : from_ , \"to\" : to , \"max\" : max_ , \"average\" : average , \"min\" : min_ , } ) return df","title":"to_df()"},{"location":"references/api_sources/#viadot.sources.uk_carbon_intensity.UKCarbonIntensity.to_json","text":"Creates json file Source code in viadot/sources/uk_carbon_intensity.py def to_json ( self ): \"\"\"Creates json file\"\"\" url = f \" { self . API_ENDPOINT }{ self . api_url } \" headers = { \"Accept\" : \"application/json\" } response = requests . get ( url , params = {}, headers = headers ) if response . ok : return response . json () else : raise f \"Error { response . json () } \"","title":"to_json()"},{"location":"references/api_sources/#viadot.sources.supermetrics.Supermetrics","text":"A class implementing the Supermetrics API. Documentation for this API is located at: https://supermetrics.com/docs/product-api-getting-started/ Usage limits: https://supermetrics.com/docs/product-api-usage-limits/","title":"Supermetrics"},{"location":"references/api_sources/#viadot.sources.supermetrics.Supermetrics--parameters","text":"query_params : Dict[str, Any], optional The parameters to pass to the GET query. See https://supermetrics.com/docs/product-api-get-data/ for full specification, by default None","title":"Parameters"},{"location":"references/api_sources/#viadot.sources.supermetrics.Supermetrics.get_params_from_api_query","text":"Returns parmeters from API query in a dictionary Source code in viadot/sources/supermetrics.py @classmethod def get_params_from_api_query ( cls , url : str ) -> Dict [ str , Any ]: \"\"\"Returns parmeters from API query in a dictionary\"\"\" url_unquoted = urllib . parse . unquote ( url ) s = urllib . parse . parse_qs ( url_unquoted ) endpoint = list ( s . keys ())[ 0 ] params = s [ endpoint ][ 0 ] params_d = json . loads ( params ) return params_d","title":"get_params_from_api_query()"},{"location":"references/api_sources/#viadot.sources.supermetrics.Supermetrics.to_df","text":"Download data into a pandas DataFrame. Note that Supermetric can calculate some fields on the fly and alias them in the returned result. For example, if the query requests the position field, Supermetric may return an Average position caclulated field. For this reason we take columns names from the actual results rather than from input fields. Parameters: Name Type Description Default if_empty str What to do if query returned no data. Defaults to \"warn\". 'warn' Returns: Type Description pd.DataFrame the DataFrame containing query results Source code in viadot/sources/supermetrics.py def to_df ( self , if_empty : str = \"warn\" ) -> pd . DataFrame : \"\"\"Download data into a pandas DataFrame. Note that Supermetric can calculate some fields on the fly and alias them in the returned result. For example, if the query requests the `position` field, Supermetric may return an `Average position` caclulated field. For this reason we take columns names from the actual results rather than from input fields. Args: if_empty (str, optional): What to do if query returned no data. Defaults to \"warn\". Returns: pd.DataFrame: the DataFrame containing query results \"\"\" try : columns = self . _get_col_names () except ValueError : columns = None data = self . to_json ()[ \"data\" ] if data : df = pd . DataFrame ( data [ 1 :], columns = columns ) . replace ( \"\" , np . nan ) else : df = pd . DataFrame ( columns = columns ) if df . empty : self . _handle_if_empty ( if_empty ) return df","title":"to_df()"},{"location":"references/api_sources/#viadot.sources.supermetrics.Supermetrics.to_json","text":"Download query results to a dictionary. Note that Supermetrics API will sometimes hang and not return any error message, so we're adding a timeout to GET. See requests docs for an explanation of why this timeout value will work on long-running queries but fail fast on connection issues. Source code in viadot/sources/supermetrics.py def to_json ( self , timeout = ( 3.05 , 60 * 30 )) -> Dict [ str , Any ]: \"\"\"Download query results to a dictionary. Note that Supermetrics API will sometimes hang and not return any error message, so we're adding a timeout to GET. See [requests docs](https://docs.python-requests.org/en/master/user/advanced/#timeouts) for an explanation of why this timeout value will work on long-running queries but fail fast on connection issues. \"\"\" if not self . query_params : raise ValueError ( \"Please build the query first\" ) params = { \"json\" : json . dumps ( self . query_params )} headers = { \"Authorization\" : f 'Bearer { self . credentials [ \"API_KEY\" ] } ' } response = handle_api_response ( url = self . API_ENDPOINT , params = params , headers = headers , timeout = timeout ) return response . json ()","title":"to_json()"},{"location":"references/api_sources/#viadot.sources.cloud_for_customers.CloudForCustomers","text":"","title":"CloudForCustomers"},{"location":"references/api_sources/#viadot.sources.cloud_for_customers.CloudForCustomers.__init__","text":"Cloud for Customers connector build for fetching Odata source. See pyodata docs for an explanation how Odata works.","title":"__init__()"},{"location":"references/api_sources/#viadot.sources.cloud_for_customers.CloudForCustomers.__init__--parameters","text":"report_url (str, optional): The url to the API in case of prepared report. Defaults to None. url (str, optional): The url to the API. Defaults to None. endpoint (str, optional): The endpoint of the API. Defaults to None. params (Dict[str, Any]): The query parameters like filter by creation date time. Defaults to json format. env (str, optional): The development environments. Defaults to 'QA'. credentials (Dict[str, Any], optional): The credentials are populated with values from config file or this parameter. Defaults to None than use credentials from local_config. Source code in viadot/sources/cloud_for_customers.py def __init__ ( self , * args , report_url : str = None , url : str = None , endpoint : str = None , params : Dict [ str , Any ] = None , env : str = \"QA\" , credentials : Dict [ str , Any ] = None , ** kwargs , ): \"\"\"Cloud for Customers connector build for fetching Odata source. See [pyodata docs](https://pyodata.readthedocs.io/en/latest/index.html) for an explanation how Odata works. Parameters ---------- report_url (str, optional): The url to the API in case of prepared report. Defaults to None. url (str, optional): The url to the API. Defaults to None. endpoint (str, optional): The endpoint of the API. Defaults to None. params (Dict[str, Any]): The query parameters like filter by creation date time. Defaults to json format. env (str, optional): The development environments. Defaults to 'QA'. credentials (Dict[str, Any], optional): The credentials are populated with values from config file or this parameter. Defaults to None than use credentials from local_config. \"\"\" super () . __init__ ( * args , ** kwargs ) try : DEFAULT_CREDENTIALS = local_config [ \"CLOUD_FOR_CUSTOMERS\" ] . get ( env ) except KeyError : DEFAULT_CREDENTIALS = None self . credentials = credentials or DEFAULT_CREDENTIALS or {} self . url = url or self . credentials . get ( \"server\" ) self . report_url = report_url if self . url is None and report_url is None : raise CredentialError ( \"One of: ('url', 'report_url') is required.\" ) self . is_report = bool ( report_url ) self . query_endpoint = endpoint if params : params_merged = self . DEFAULT_PARAMS . copy () params_merged . update ( params ) self . params = params_merged else : self . params = self . DEFAULT_PARAMS if self . url : self . full_url = urljoin ( self . url , self . query_endpoint ) super () . __init__ ( * args , credentials = self . credentials , ** kwargs )","title":"Parameters"},{"location":"references/api_sources/#viadot.sources.cloud_for_customers.CloudForCustomers.get_response","text":"Handle and raise Python exceptions during request. Using of url and service endpoint needs additional parameters stores in params. report_url contain additional params in their structure. In report_url scenario it can not contain params parameter. Parameters: Name Type Description Default url str the URL which trying to connect. required params Dict[str, Any] Additional parameters like filter, used in case of normal url. None timeout tuple the request times out. Defaults to (3.05, 60 * 30). (3.05, 1800) Returns: Type Description Response requests.models.Response Source code in viadot/sources/cloud_for_customers.py def get_response ( self , url : str , params : Dict [ str , Any ] = None , timeout : tuple = ( 3.05 , 60 * 30 ) ) -> requests . models . Response : \"\"\"Handle and raise Python exceptions during request. Using of url and service endpoint needs additional parameters stores in params. report_url contain additional params in their structure. In report_url scenario it can not contain params parameter. Args: url (str): the URL which trying to connect. params (Dict[str, Any], optional): Additional parameters like filter, used in case of normal url. Defaults to None used in case of report_url, which can not contain params. timeout (tuple, optional): the request times out. Defaults to (3.05, 60 * 30). Returns: requests.models.Response \"\"\" username = self . credentials . get ( \"username\" ) pw = self . credentials . get ( \"password\" ) response = handle_api_response ( url = url , params = params , auth = ( username , pw ), timeout = timeout , ) return response","title":"get_response()"},{"location":"references/api_sources/#viadot.sources.cloud_for_customers.CloudForCustomers.map_columns","text":"Fetch metadata from url used to column name map. Parameters: Name Type Description Default url str the URL which trying to fetch metadata. Defaults to None. None Returns: Type Description Dict[str, str] Property Name as key mapped to the value of sap label. Source code in viadot/sources/cloud_for_customers.py def map_columns ( self , url : str = None ) -> Dict [ str , str ]: \"\"\"Fetch metadata from url used to column name map. Args: url (str, optional): the URL which trying to fetch metadata. Defaults to None. Returns: Dict[str, str]: Property Name as key mapped to the value of sap label. \"\"\" column_mapping = {} if url : username = self . credentials . get ( \"username\" ) pw = self . credentials . get ( \"password\" ) response = requests . get ( url , auth = ( username , pw )) for sentence in response . text . split ( \"/>\" ): result = re . search ( r '(?<=Name=\")([^\"]+).+(sap:label=\")([^\"]+)+' , sentence ) if result : key = result . groups ( 0 )[ 0 ] val = result . groups ( 0 )[ 2 ] column_mapping [ key ] = val return column_mapping","title":"map_columns()"},{"location":"references/api_sources/#viadot.sources.cloud_for_customers.CloudForCustomers.response_to_entity_list","text":"Changing request json response to list. Parameters: Name Type Description Default dirty_json Dict[str, Any] json from response. required url str the URL which trying to fetch metadata. required Returns: Type Description List List of dictionaries. Source code in viadot/sources/cloud_for_customers.py def response_to_entity_list ( self , dirty_json : Dict [ str , Any ], url : str ) -> List : \"\"\"Changing request json response to list. Args: dirty_json (Dict[str, Any]): json from response. url (str): the URL which trying to fetch metadata. Returns: List: List of dictionaries. \"\"\" metadata_url = self . change_to_meta_url ( url ) column_maper_dict = self . map_columns ( metadata_url ) entity_list = [] for element in dirty_json [ \"d\" ][ \"results\" ]: new_entity = {} for key , object_of_interest in element . items (): if key not in [ \"__metadata\" , \"Photo\" , \"\" , \"Picture\" ]: if \"{\" not in str ( object_of_interest ): new_key = column_maper_dict . get ( key ) if new_key : new_entity [ new_key ] = object_of_interest else : new_entity [ key ] = object_of_interest entity_list . append ( new_entity ) return entity_list","title":"response_to_entity_list()"},{"location":"references/api_sources/#viadot.sources.cloud_for_customers.CloudForCustomers.to_df","text":"Returns records in a pandas DataFrame. Parameters: Name Type Description Default fields List[str] List of fields to put in DataFrame. Defaults to None. None dtype dict The dtypes to use in the DataFrame. We catch this parameter here since None kwargs The parameters to pass to DataFrame constructor. {} Source code in viadot/sources/cloud_for_customers.py def to_df ( self , fields : List [ str ] = None , if_empty : str = \"warn\" , dtype : dict = None , ** kwargs , ) -> pd . DataFrame : \"\"\"Returns records in a pandas DataFrame. Args: fields (List[str], optional): List of fields to put in DataFrame. Defaults to None. dtype (dict, optional): The dtypes to use in the DataFrame. We catch this parameter here since pandas doesn't support passing dtypes (eg. as a dict) to the constructor. kwargs: The parameters to pass to DataFrame constructor. \"\"\" records = self . to_records () df = pd . DataFrame ( data = records , ** kwargs ) if dtype : df = df . astype ( dtype ) if fields : return df [ fields ] return df","title":"to_df()"},{"location":"references/api_sources/#viadot.sources.cloud_for_customers.CloudForCustomers.to_records","text":"Download a list of entities in the records format Source code in viadot/sources/cloud_for_customers.py def to_records ( self ) -> List [ Dict [ str , Any ]]: \"\"\" Download a list of entities in the records format \"\"\" if self . is_report : url = self . report_url return self . _to_records_report ( url = url ) else : url = self . full_url return self . _to_records_other ( url = url )","title":"to_records()"},{"location":"references/flows_library/","text":"Flows library viadot.flows.adls_container_to_container.ADLSContainerToContainer ( Flow ) Copy file(s) between containers. Parameters: Name Type Description Default name str The name of the flow. required from_path str The path to the Data Lake file. required to_path str The path of the final file location a/a/filename.extension. required adls_sp_credentials_secret str The name of the Azure Key Vault secret containing a dictionary with ACCOUNT_NAME and Service Principal credentials (TENANT_ID, CLIENT_ID, CLIENT_SECRET) for the Azure Data Lake. Defaults to None. required vault_name str The name of the vault from which to retrieve the secrets. required timeout(int, optional The amount of time (in seconds) to wait while running this task before a timeout occurs. Defaults to 3600. required viadot.flows.adls_gen1_to_azure_sql_new.ADLSGen1ToAzureSQLNew ( Flow ) Move file(s) from Azure Data Lake gen1 to gen2. Parameters: Name Type Description Default name str The name of the flow. required gen1_path str The path to the gen1 Data Lake file/folder. required gen2_path str The path of the final gen2 file/folder. required local_file_path str Where the gen1 file should be downloaded. required overwrite str Whether to overwrite the destination file(s). required read_sep str The delimiter for the gen1 file. required write_sep str The delimiter for the output file. required read_quoting str The quoting option for the input file. required read_lineterminator str The line terminator for the input file. required read_error_bad_lines bool Whether to raise an exception on bad lines. required gen1_sp_credentials_secret str The Key Vault secret holding Service Pricipal credentials for gen1 lake required gen2_sp_credentials_secret str The Key Vault secret holding Service Pricipal credentials for gen2 lake required sqldb_credentials_secret str The Key Vault secret holding Azure SQL Database credentials required vault_name str The name of the vault from which to retrieve sp_credentials_secret required timeout(int, optional The amount of time (in seconds) to wait while running this task before a timeout occurs. Defaults to 3600. required viadot.flows.adls_gen1_to_azure_sql.ADLSGen1ToAzureSQL ( Flow ) Bulk insert a file from an Azure Data Lake gen1 to Azure SQL Database. Parameters: Name Type Description Default name str The name of the flow. required path str The path to the Data Lake file/folder. required blob_path str The path of the generated blob. required dtypes dict Which dtypes to use when creating the table in Azure SQL Database. required local_file_path str Where the gen1 file should be downloaded. required sp_credentials_secret str The Key Vault secret holding Service Pricipal credentials required vault_name str The name of the vault from which to retrieve sp_credentials_secret required timeout(int, optional The amount of time (in seconds) to wait while running this task before a timeout occurs. Defaults to 3600. required viadot.flows.adls_gen1_to_gen2.ADLSGen1ToGen2 ( Flow ) Move file(s) from Azure Data Lake gen1 to gen2. Parameters: Name Type Description Default name str The name of the flow. required gen1_path str The path to the gen1 Data Lake file/folder. required gen2_path str The path of the final gen2 file/folder. required local_file_path str Where the gen1 file should be downloaded. required overwrite str Whether to overwrite the destination file(s). required gen1_sp_credentials_secret str The Key Vault secret holding Service Pricipal credentials for gen1 lake required gen2_sp_credentials_secret str The Key Vault secret holding Service Pricipal credentials for gen2 lake required vault_name str The name of the vault from which to retrieve the secrets. required timeout(int, optional The amount of time (in seconds) to wait while running this task before a timeout occurs. Defaults to 3600. required viadot.flows.adls_to_azure_sql.ADLSToAzureSQL ( Flow ) __init__ ( self , name , local_file_path = None , adls_path = None , read_sep = ' \\t ' , write_sep = ' \\t ' , remove_tab = False , overwrite_adls = True , if_empty = 'warn' , adls_sp_credentials_secret = None , dtypes = None , check_dtypes_order = True , table = None , schema = None , if_exists = 'replace' , check_col_order = True , sqldb_credentials_secret = None , on_bcp_error = 'skip' , max_download_retries = 5 , tags = [ 'promotion' ], vault_name = None , timeout = 3600 , * args , ** kwargs ) special Flow for downloading data from different marketing APIs to a local CSV using Supermetrics API, then uploading it to Azure Data Lake, and finally inserting into Azure SQL Database. Parameters: Name Type Description Default name str The name of the flow. required local_file_path str Local destination path. Defaults to None. None adls_path str The path to an ADLS folder or file. If you pass a path to a directory, None read_sep str The delimiter for the source file. Defaults to \" \". '\\t' write_sep str The delimiter for the output CSV file. Defaults to \" \". '\\t' remove_tab bool Whether to remove tab delimiters from the data. Defaults to False. False overwrite_adls bool Whether to overwrite the file in ADLS. Defaults to True. True if_empty str What to do if the Supermetrics query returns no data. Defaults to \"warn\". 'warn' adls_sp_credentials_secret str The name of the Azure Key Vault secret containing a dictionary with None dtypes dict Which custom data types should be used for SQL table creation task. None check_dtypes_order bool, optiona By default, this task will be used by all the flows. It can be set up to False for avoiding its application. Defaults to True. True table str Destination table. Defaults to None. None schema str Destination schema. Defaults to None. None if_exists Literal What to do if the table exists. Defaults to \"replace\". 'replace' check_col_order bool Whether to check column order. Defaults to True. True sqldb_credentials_secret str The name of the Azure Key Vault secret containing a dictionary with None on_bcp_error Literal[\"skip\", \"fail\"] What to do if error occurs. Defaults to \"skip\". 'skip' max_download_retries int How many times to retry the download. Defaults to 5. 5 tags List[str] Flow tags to use, eg. to control flow concurrency. Defaults to [\"promotion\"]. ['promotion'] vault_name str The name of the vault from which to obtain the secrets. Defaults to None. None timeout(int, optional The amount of time (in seconds) to wait while running this task before a timeout occurs. Defaults to 3600. required Source code in viadot/flows/adls_to_azure_sql.py def __init__ ( self , name : str , local_file_path : str = None , adls_path : str = None , read_sep : str = \" \\t \" , write_sep : str = \" \\t \" , remove_tab : bool = False , overwrite_adls : bool = True , if_empty : str = \"warn\" , adls_sp_credentials_secret : str = None , dtypes : Dict [ str , Any ] = None , check_dtypes_order : bool = True , table : str = None , schema : str = None , if_exists : Literal [ \"fail\" , \"replace\" , \"append\" , \"delete\" ] = \"replace\" , check_col_order : bool = True , sqldb_credentials_secret : str = None , on_bcp_error : Literal [ \"skip\" , \"fail\" ] = \"skip\" , max_download_retries : int = 5 , tags : List [ str ] = [ \"promotion\" ], vault_name : str = None , timeout : int = 3600 , * args : List [ any ], ** kwargs : Dict [ str , Any ], ): \"\"\" Flow for downloading data from different marketing APIs to a local CSV using Supermetrics API, then uploading it to Azure Data Lake, and finally inserting into Azure SQL Database. Args: name (str): The name of the flow. local_file_path (str, optional): Local destination path. Defaults to None. adls_path (str): The path to an ADLS folder or file. If you pass a path to a directory, the latest file from that directory will be loaded. We assume that the files are named using timestamps. read_sep (str, optional): The delimiter for the source file. Defaults to \"\\t\". write_sep (str, optional): The delimiter for the output CSV file. Defaults to \"\\t\". remove_tab (bool, optional): Whether to remove tab delimiters from the data. Defaults to False. overwrite_adls (bool, optional): Whether to overwrite the file in ADLS. Defaults to True. if_empty (str, optional): What to do if the Supermetrics query returns no data. Defaults to \"warn\". adls_sp_credentials_secret (str, optional): The name of the Azure Key Vault secret containing a dictionary with ACCOUNT_NAME and Service Principal credentials (TENANT_ID, CLIENT_ID, CLIENT_SECRET) for the Azure Data Lake. Defaults to None. dtypes (dict, optional): Which custom data types should be used for SQL table creation task. To be used only in case that dtypes need to be manually mapped - dtypes from raw schema file in use by default. Defaults to None. check_dtypes_order (bool, optiona): By default, this task will be used by all the flows. It can be set up to False for avoiding its application. Defaults to True. table (str, optional): Destination table. Defaults to None. schema (str, optional): Destination schema. Defaults to None. if_exists (Literal, optional): What to do if the table exists. Defaults to \"replace\". check_col_order (bool, optional): Whether to check column order. Defaults to True. sqldb_credentials_secret (str, optional): The name of the Azure Key Vault secret containing a dictionary with Azure SQL Database credentials. Defaults to None. on_bcp_error (Literal[\"skip\", \"fail\"], optional): What to do if error occurs. Defaults to \"skip\". max_download_retries (int, optional): How many times to retry the download. Defaults to 5. tags (List[str], optional): Flow tags to use, eg. to control flow concurrency. Defaults to [\"promotion\"]. vault_name (str, optional): The name of the vault from which to obtain the secrets. Defaults to None. timeout(int, optional): The amount of time (in seconds) to wait while running this task before a timeout occurs. Defaults to 3600. \"\"\" adls_path = adls_path . strip ( \"/\" ) # Read parquet if adls_path . split ( \".\" )[ - 1 ] in [ \"csv\" , \"parquet\" ]: self . adls_path = adls_path else : self . adls_path = get_key_value ( key = adls_path ) # Read schema self . dtypes = dtypes self . check_dtypes_order = check_dtypes_order self . adls_root_dir_path = os . path . split ( self . adls_path )[ 0 ] self . adls_file_name = os . path . split ( self . adls_path )[ - 1 ] extension = os . path . splitext ( self . adls_path )[ - 1 ] json_schema_file_name = self . adls_file_name . replace ( extension , \".json\" ) self . json_shema_path = os . path . join ( self . adls_root_dir_path , \"schema\" , json_schema_file_name , ) # AzureDataLakeUpload self . local_file_path = local_file_path or self . slugify ( name ) + \".csv\" self . local_json_path = self . slugify ( name ) + \".json\" self . read_sep = read_sep self . write_sep = write_sep self . overwrite_adls = overwrite_adls self . if_empty = if_empty self . adls_sp_credentials_secret = adls_sp_credentials_secret self . adls_path_conformed = self . get_promoted_path ( env = \"conformed\" ) self . adls_path_operations = self . get_promoted_path ( env = \"operations\" ) # AzureSQLCreateTable self . table = table self . schema = schema self . if_exists = if_exists self . check_col_order = check_col_order # Generate CSV self . remove_tab = remove_tab # BCPTask self . sqldb_credentials_secret = sqldb_credentials_secret self . on_bcp_error = on_bcp_error # Global self . max_download_retries = max_download_retries self . tags = tags self . vault_name = vault_name self . timeout = timeout super () . __init__ ( * args , name = name , ** kwargs ) # self.dtypes.update(METADATA_COLUMNS) self . gen_flow () viadot.flows.azure_sql_transform.AzureSQLTransform ( Flow ) __init__ ( self , name , query , sqldb_credentials_secret = None , vault_name = None , tags = [ 'transform' ], timeout = 3600 , * args , ** kwargs ) special Flow for running SQL queries on top of Azure SQL Database. Parameters: Name Type Description Default name str The name of the flow. required query str, required The query to execute on the database. required credentials_secret str The name of the Azure Key Vault secret containing a dictionary required vault_name str The name of the vault from which to obtain the secret. Defaults to None. None tags list Tag for marking flow. Defaults to \"transform\". ['transform'] timeout(int, optional The amount of time (in seconds) to wait while running this task before a timeout occurs. Defaults to 3600. required Source code in viadot/flows/azure_sql_transform.py def __init__ ( self , name : str , query : str , sqldb_credentials_secret : str = None , vault_name : str = None , tags : List [ str ] = [ \"transform\" ], timeout : int = 3600 , * args : List [ any ], ** kwargs : Dict [ str , Any ] ): \"\"\" Flow for running SQL queries on top of Azure SQL Database. Args: name (str): The name of the flow. query (str, required): The query to execute on the database. credentials_secret (str, optional): The name of the Azure Key Vault secret containing a dictionary with SQL db credentials (server, db_name, user, and password). vault_name (str, optional): The name of the vault from which to obtain the secret. Defaults to None. tags (list, optional): Tag for marking flow. Defaults to \"transform\". timeout(int, optional): The amount of time (in seconds) to wait while running this task before a timeout occurs. Defaults to 3600. \"\"\" self . query = query self . tags = tags self . sqldb_credentials_secret = sqldb_credentials_secret self . vault_name = vault_name self . timeout = timeout super () . __init__ ( * args , name = name , ** kwargs ) self . gen_flow () viadot.flows.supermetrics_to_adls.SupermetricsToADLS ( Flow ) __init__ ( self , name , ds_id , ds_accounts , fields , ds_user = None , ds_segments = None , date_range_type = None , start_date = None , end_date = None , settings = None , filter = None , max_rows = 1000000 , max_columns = None , order_columns = None , expectation_suite = None , evaluation_parameters = None , keep_validation_output = False , output_file_extension = '.parquet' , local_file_path = None , adls_file_name = None , adls_dir_path = None , overwrite_adls = True , if_empty = 'warn' , if_exists = 'replace' , adls_sp_credentials_secret = None , max_download_retries = 5 , supermetrics_task_timeout = 1800 , parallel = True , tags = [ 'extract' ], vault_name = None , check_missing_data = True , timeout = 3600 , * args , ** kwargs ) special Flow for downloading data from different marketing APIs to a local CSV using Supermetrics API, then uploading it to Azure Data Lake. Parameters: Name Type Description Default name str The name of the flow. required ds_id str A query parameter passed to the SupermetricsToCSV task required ds_accounts List[str] A query parameter passed to the SupermetricsToCSV task required ds_user str A query parameter passed to the SupermetricsToCSV task None fields List[str] A query parameter passed to the SupermetricsToCSV task required ds_segments List[str] A query parameter passed to the SupermetricsToCSV task. Defaults to None. None date_range_type str A query parameter passed to the SupermetricsToCSV task. Defaults to None. None start_date str A query parameter to pass start date to the date range filter. Defaults to None. None end_date str A query parameter to pass end date to the date range filter. Defaults to None. None settings Dict[str, Any] A query parameter passed to the SupermetricsToCSV task. Defaults to None. None filter str A query parameter passed to the SupermetricsToCSV task. Defaults to None. None max_rows int A query parameter passed to the SupermetricsToCSV task. Defaults to 1000000. 1000000 max_columns int A query parameter passed to the SupermetricsToCSV task. Defaults to None. None order_columns str A query parameter passed to the SupermetricsToCSV task. Defaults to None. None expectation_suite dict The Great Expectations suite used to valiaate the data. Defaults to None. None evaluation_parameters str A dictionary containing evaluation parameters for the validation. Defaults to None. None keep_validation_output bool Whether to keep the output files generated by the Great Expectations task. Defaults to False. False local_file_path str Local destination path. Defaults to None. None adls_file_name str Name of file in ADLS. Defaults to None. None output_file_extension str Output file extension - to allow selection of .csv for data which is not easy to handle with parquet. Defaults to \".parquet\".. '.parquet' adls_dir_path str Azure Data Lake destination folder/catalog path. Defaults to None. None sep str The separator to use in the CSV. Defaults to \" \". required overwrite_adls bool Whether to overwrite the file in ADLS. Defaults to True. True if_empty str What to do if the Supermetrics query returns no data. Defaults to \"warn\". 'warn' adls_sp_credentials_secret str The name of the Azure Key Vault secret containing a dictionary with None max_download_retries int How many times to retry the download. Defaults to 5. 5 supermetrics_task_timeout int The timeout for the download task. Defaults to 60*30. 1800 parallel bool Whether to parallelize the downloads. Defaults to True. True tags List[str] Flow tags to use, eg. to control flow concurrency. Defaults to [\"extract\"]. ['extract'] vault_name str The name of the vault from which to obtain the secrets. Defaults to None. None check_missing_data bool Whether to check missing data. Defaults to True. True timeout(int, optional The amount of time (in seconds) to wait while running this task before a timeout occurs. Defaults to 3600. required Source code in viadot/flows/supermetrics_to_adls.py def __init__ ( self , name : str , ds_id : str , ds_accounts : List [ str ], fields : List [ str ], ds_user : str = None , ds_segments : List [ str ] = None , date_range_type : str = None , start_date : str = None , end_date : str = None , settings : Dict [ str , Any ] = None , filter : str = None , max_rows : int = 1000000 , max_columns : int = None , order_columns : str = None , expectation_suite : dict = None , evaluation_parameters : dict = None , keep_validation_output : bool = False , output_file_extension : str = \".parquet\" , local_file_path : str = None , adls_file_name : str = None , adls_dir_path : str = None , overwrite_adls : bool = True , if_empty : str = \"warn\" , if_exists : str = \"replace\" , adls_sp_credentials_secret : str = None , max_download_retries : int = 5 , supermetrics_task_timeout : int = 60 * 30 , parallel : bool = True , tags : List [ str ] = [ \"extract\" ], vault_name : str = None , check_missing_data : bool = True , timeout : int = 3600 , * args : List [ any ], ** kwargs : Dict [ str , Any ], ): \"\"\" Flow for downloading data from different marketing APIs to a local CSV using Supermetrics API, then uploading it to Azure Data Lake. Args: name (str): The name of the flow. ds_id (str): A query parameter passed to the SupermetricsToCSV task ds_accounts (List[str]): A query parameter passed to the SupermetricsToCSV task ds_user (str): A query parameter passed to the SupermetricsToCSV task fields (List[str]): A query parameter passed to the SupermetricsToCSV task ds_segments (List[str], optional): A query parameter passed to the SupermetricsToCSV task. Defaults to None. date_range_type (str, optional): A query parameter passed to the SupermetricsToCSV task. Defaults to None. start_date (str, optional): A query parameter to pass start date to the date range filter. Defaults to None. end_date (str, optional): A query parameter to pass end date to the date range filter. Defaults to None. settings (Dict[str, Any], optional): A query parameter passed to the SupermetricsToCSV task. Defaults to None. filter (str, optional): A query parameter passed to the SupermetricsToCSV task. Defaults to None. max_rows (int, optional): A query parameter passed to the SupermetricsToCSV task. Defaults to 1000000. max_columns (int, optional): A query parameter passed to the SupermetricsToCSV task. Defaults to None. order_columns (str, optional): A query parameter passed to the SupermetricsToCSV task. Defaults to None. expectation_suite (dict, optional): The Great Expectations suite used to valiaate the data. Defaults to None. evaluation_parameters (str, optional): A dictionary containing evaluation parameters for the validation. Defaults to None. keep_validation_output (bool, optional): Whether to keep the output files generated by the Great Expectations task. Defaults to False. Currently, only GitHub URLs are supported. Defaults to None. local_file_path (str, optional): Local destination path. Defaults to None. adls_file_name (str, optional): Name of file in ADLS. Defaults to None. output_file_extension (str, optional): Output file extension - to allow selection of .csv for data which is not easy to handle with parquet. Defaults to \".parquet\".. adls_dir_path (str, optional): Azure Data Lake destination folder/catalog path. Defaults to None. sep (str, optional): The separator to use in the CSV. Defaults to \"\\t\". overwrite_adls (bool, optional): Whether to overwrite the file in ADLS. Defaults to True. if_empty (str, optional): What to do if the Supermetrics query returns no data. Defaults to \"warn\". adls_sp_credentials_secret (str, optional): The name of the Azure Key Vault secret containing a dictionary with ACCOUNT_NAME and Service Principal credentials (TENANT_ID, CLIENT_ID, CLIENT_SECRET) for the Azure Data Lake. Defaults to None. max_download_retries (int, optional): How many times to retry the download. Defaults to 5. supermetrics_task_timeout (int, optional): The timeout for the download task. Defaults to 60*30. parallel (bool, optional): Whether to parallelize the downloads. Defaults to True. tags (List[str], optional): Flow tags to use, eg. to control flow concurrency. Defaults to [\"extract\"]. vault_name (str, optional): The name of the vault from which to obtain the secrets. Defaults to None. check_missing_data (bool, optional): Whether to check missing data. Defaults to True. timeout(int, optional): The amount of time (in seconds) to wait while running this task before a timeout occurs. Defaults to 3600. \"\"\" if not ds_user : try : ds_user = PrefectSecret ( \"SUPERMETRICS_DEFAULT_USER\" ) . run () except ValueError as e : msg = \"Neither 'ds_user' parameter nor 'SUPERMETRICS_DEFAULT_USER' secret were not specified\" raise ValueError ( msg ) from e self . flow_name = name self . check_missing_data = check_missing_data self . timeout = timeout # SupermetricsToDF self . ds_id = ds_id self . ds_accounts = ds_accounts self . ds_segments = ds_segments self . ds_user = ds_user self . fields = fields self . date_range_type = date_range_type self . start_date = start_date self . end_date = end_date self . settings = settings self . filter = filter self . max_rows = max_rows self . max_columns = max_columns self . order_columns = order_columns self . if_exists = if_exists self . output_file_extension = output_file_extension # RunGreatExpectationsValidation self . expectation_suite = expectation_suite self . expectations_path = \"/home/viadot/tmp/expectations\" self . expectation_suite_name = expectation_suite [ \"expectation_suite_name\" ] self . evaluation_parameters = evaluation_parameters self . keep_validation_output = keep_validation_output # AzureDataLakeUpload self . local_file_path = ( local_file_path or self . slugify ( name ) + self . output_file_extension ) self . local_json_path = self . slugify ( name ) + \".json\" self . now = str ( pendulum . now ( \"utc\" )) self . adls_dir_path = adls_dir_path if adls_file_name is not None : self . adls_file_path = os . path . join ( adls_dir_path , adls_file_name ) self . adls_schema_file_dir_file = os . path . join ( adls_dir_path , \"schema\" , Path ( adls_file_name ) . stem + \".json\" ) else : self . adls_file_path = os . path . join ( adls_dir_path , self . now + self . output_file_extension ) self . adls_schema_file_dir_file = os . path . join ( adls_dir_path , \"schema\" , self . now + \".json\" ) self . overwrite_adls = overwrite_adls self . if_empty = if_empty self . adls_sp_credentials_secret = adls_sp_credentials_secret # Global self . max_download_retries = max_download_retries self . supermetrics_task_timeout = supermetrics_task_timeout self . parallel = parallel self . tags = tags self . vault_name = vault_name super () . __init__ ( * args , name = name , ** kwargs ) self . gen_flow () viadot.flows.supermetrics_to_azure_sql.SupermetricsToAzureSQL ( Flow ) viadot.flows.cloud_for_customers_report_to_adls.CloudForCustomersReportToADLS ( Flow ) __init__ ( self , name = None , report_url = None , url = None , endpoint = None , params = {}, fields = None , skip = 0 , top = 1000 , channels = None , months = None , years = None , env = 'QA' , c4c_credentials_secret = None , local_file_path = None , output_file_extension = '.csv' , adls_dir_path = None , adls_file_path = None , overwrite_adls = False , adls_sp_credentials_secret = None , if_empty = 'warn' , if_exists = 'replace' , timeout = 3600 , * args , ** kwargs ) special Flow for downloading data from different marketing APIs to a local CSV using Cloud for Customers API, then uploading it to Azure Data Lake. Parameters: Name Type Description Default name str The name of the flow. None report_url str The url to the API. Defaults to None. None url str ??? None endpoint str ??? None params dict ??? {} fields list ??? None skip int Initial index value of reading row. Defaults to 0. 0 top int The value of top reading row. Defaults to 1000. 1000 channels List[str] Filtering parameters passed to the url. Defaults to None. None months List[str] Filtering parameters passed to the url. Defaults to None. None years List[str] Filtering parameters passed to the url. Defaults to None. None env str ??? 'QA' c4c_credentials_secret str The name of the Azure Key Vault secret containing a dictionary with None local_file_path str Local destination path. Defaults to None. None output_file_extension str Output file extension - to allow selection of .csv for data which is not easy '.csv' adls_dir_path str Azure Data Lake destination folder/catalog path. Defaults to None. None adls_file_path str Azure Data Lake destination file path. Defaults to None. None overwrite_adls bool Whether to overwrite the file in ADLS. Defaults to False. False adls_sp_credentials_secret str The name of the Azure Key Vault secret containing a dictionary with None if_empty str What to do if the Supermetrics query returns no data. Defaults to \"warn\". 'warn' if_exists str What to do if the local file already exists. Defaults to \"replace\". 'replace' timeout(int, optional The amount of time (in seconds) to wait while running this task before a timeout occurs. Defaults to 3600. required Source code in viadot/flows/cloud_for_customers_report_to_adls.py def __init__ ( self , name : str = None , report_url : str = None , url : str = None , endpoint : str = None , params : Dict [ str , Any ] = {}, fields : List [ str ] = None , skip : int = 0 , top : int = 1000 , channels : List [ str ] = None , months : List [ str ] = None , years : List [ str ] = None , env : str = \"QA\" , c4c_credentials_secret : str = None , local_file_path : str = None , output_file_extension : str = \".csv\" , adls_dir_path : str = None , adls_file_path : str = None , overwrite_adls : bool = False , adls_sp_credentials_secret : str = None , if_empty : str = \"warn\" , if_exists : str = \"replace\" , timeout : int = 3600 , * args : List [ any ], ** kwargs : Dict [ str , Any ], ): \"\"\" Flow for downloading data from different marketing APIs to a local CSV using Cloud for Customers API, then uploading it to Azure Data Lake. Args: name (str): The name of the flow. report_url (str, optional): The url to the API. Defaults to None. url (str, optional): ??? endpoint (str, optional): ??? params (dict, optional): ??? fields (list, optional): ??? skip (int, optional): Initial index value of reading row. Defaults to 0. top (int, optional): The value of top reading row. Defaults to 1000. channels (List[str], optional): Filtering parameters passed to the url. Defaults to None. months (List[str], optional): Filtering parameters passed to the url. Defaults to None. years (List[str], optional): Filtering parameters passed to the url. Defaults to None. env (str, optional): ??? c4c_credentials_secret (str, optional): The name of the Azure Key Vault secret containing a dictionary with username and password for the Cloud for Customers instance. local_file_path (str, optional): Local destination path. Defaults to None. output_file_extension (str, optional): Output file extension - to allow selection of .csv for data which is not easy to handle with parquet. Defaults to \".csv\". adls_dir_path (str, optional): Azure Data Lake destination folder/catalog path. Defaults to None. adls_file_path (str, optional): Azure Data Lake destination file path. Defaults to None. overwrite_adls (bool, optional): Whether to overwrite the file in ADLS. Defaults to False. adls_sp_credentials_secret (str, optional): The name of the Azure Key Vault secret containing a dictionary with ACCOUNT_NAME and Service Principal credentials (TENANT_ID, CLIENT_ID, CLIENT_SECRET) for the Azure Data Lake. Defaults to None. if_empty (str, optional): What to do if the Supermetrics query returns no data. Defaults to \"warn\". if_exists (str, optional): What to do if the local file already exists. Defaults to \"replace\". timeout(int, optional): The amount of time (in seconds) to wait while running this task before a timeout occurs. Defaults to 3600. \"\"\" self . report_url = report_url self . skip = skip self . top = top self . if_empty = if_empty self . env = env self . c4c_credentials_secret = c4c_credentials_secret self . timeout = timeout # AzureDataLakeUpload self . adls_sp_credentials_secret = adls_sp_credentials_secret self . if_exists = if_exists self . overwrite_adls = overwrite_adls self . output_file_extension = output_file_extension self . local_file_path = ( local_file_path or slugify ( name ) + self . output_file_extension ) self . now = str ( pendulum . now ( \"utc\" )) self . adls_dir_path = adls_dir_path self . adls_file_path = adls_file_path or os . path . join ( adls_dir_path , self . now + self . output_file_extension ) # in case of non-report invoking self . url = url self . endpoint = endpoint self . params = params self . fields = fields # filtering report_url for reports self . channels = channels self . months = months self . years = years self . report_urls_with_filters = [ self . report_url ] self . report_urls_with_filters = self . create_url_with_fields ( fields_list = self . channels , filter_code = \"CCHANNETZTEXT12CE6C2FA0D77995\" ) self . report_urls_with_filters = self . create_url_with_fields ( fields_list = self . months , filter_code = \"CMONTH_ID\" ) self . report_urls_with_filters = self . create_url_with_fields ( fields_list = self . years , filter_code = \"CYEAR_ID\" ) super () . __init__ ( * args , name = name , ** kwargs ) self . gen_flow ()","title":"Flows library"},{"location":"references/flows_library/#flows-library","text":"","title":"Flows library"},{"location":"references/flows_library/#viadot.flows.adls_container_to_container.ADLSContainerToContainer","text":"Copy file(s) between containers. Parameters: Name Type Description Default name str The name of the flow. required from_path str The path to the Data Lake file. required to_path str The path of the final file location a/a/filename.extension. required adls_sp_credentials_secret str The name of the Azure Key Vault secret containing a dictionary with ACCOUNT_NAME and Service Principal credentials (TENANT_ID, CLIENT_ID, CLIENT_SECRET) for the Azure Data Lake. Defaults to None. required vault_name str The name of the vault from which to retrieve the secrets. required timeout(int, optional The amount of time (in seconds) to wait while running this task before a timeout occurs. Defaults to 3600. required","title":"ADLSContainerToContainer"},{"location":"references/flows_library/#viadot.flows.adls_gen1_to_azure_sql_new.ADLSGen1ToAzureSQLNew","text":"Move file(s) from Azure Data Lake gen1 to gen2. Parameters: Name Type Description Default name str The name of the flow. required gen1_path str The path to the gen1 Data Lake file/folder. required gen2_path str The path of the final gen2 file/folder. required local_file_path str Where the gen1 file should be downloaded. required overwrite str Whether to overwrite the destination file(s). required read_sep str The delimiter for the gen1 file. required write_sep str The delimiter for the output file. required read_quoting str The quoting option for the input file. required read_lineterminator str The line terminator for the input file. required read_error_bad_lines bool Whether to raise an exception on bad lines. required gen1_sp_credentials_secret str The Key Vault secret holding Service Pricipal credentials for gen1 lake required gen2_sp_credentials_secret str The Key Vault secret holding Service Pricipal credentials for gen2 lake required sqldb_credentials_secret str The Key Vault secret holding Azure SQL Database credentials required vault_name str The name of the vault from which to retrieve sp_credentials_secret required timeout(int, optional The amount of time (in seconds) to wait while running this task before a timeout occurs. Defaults to 3600. required","title":"ADLSGen1ToAzureSQLNew"},{"location":"references/flows_library/#viadot.flows.adls_gen1_to_azure_sql.ADLSGen1ToAzureSQL","text":"Bulk insert a file from an Azure Data Lake gen1 to Azure SQL Database. Parameters: Name Type Description Default name str The name of the flow. required path str The path to the Data Lake file/folder. required blob_path str The path of the generated blob. required dtypes dict Which dtypes to use when creating the table in Azure SQL Database. required local_file_path str Where the gen1 file should be downloaded. required sp_credentials_secret str The Key Vault secret holding Service Pricipal credentials required vault_name str The name of the vault from which to retrieve sp_credentials_secret required timeout(int, optional The amount of time (in seconds) to wait while running this task before a timeout occurs. Defaults to 3600. required","title":"ADLSGen1ToAzureSQL"},{"location":"references/flows_library/#viadot.flows.adls_gen1_to_gen2.ADLSGen1ToGen2","text":"Move file(s) from Azure Data Lake gen1 to gen2. Parameters: Name Type Description Default name str The name of the flow. required gen1_path str The path to the gen1 Data Lake file/folder. required gen2_path str The path of the final gen2 file/folder. required local_file_path str Where the gen1 file should be downloaded. required overwrite str Whether to overwrite the destination file(s). required gen1_sp_credentials_secret str The Key Vault secret holding Service Pricipal credentials for gen1 lake required gen2_sp_credentials_secret str The Key Vault secret holding Service Pricipal credentials for gen2 lake required vault_name str The name of the vault from which to retrieve the secrets. required timeout(int, optional The amount of time (in seconds) to wait while running this task before a timeout occurs. Defaults to 3600. required","title":"ADLSGen1ToGen2"},{"location":"references/flows_library/#viadot.flows.adls_to_azure_sql.ADLSToAzureSQL","text":"","title":"ADLSToAzureSQL"},{"location":"references/flows_library/#viadot.flows.adls_to_azure_sql.ADLSToAzureSQL.__init__","text":"Flow for downloading data from different marketing APIs to a local CSV using Supermetrics API, then uploading it to Azure Data Lake, and finally inserting into Azure SQL Database. Parameters: Name Type Description Default name str The name of the flow. required local_file_path str Local destination path. Defaults to None. None adls_path str The path to an ADLS folder or file. If you pass a path to a directory, None read_sep str The delimiter for the source file. Defaults to \" \". '\\t' write_sep str The delimiter for the output CSV file. Defaults to \" \". '\\t' remove_tab bool Whether to remove tab delimiters from the data. Defaults to False. False overwrite_adls bool Whether to overwrite the file in ADLS. Defaults to True. True if_empty str What to do if the Supermetrics query returns no data. Defaults to \"warn\". 'warn' adls_sp_credentials_secret str The name of the Azure Key Vault secret containing a dictionary with None dtypes dict Which custom data types should be used for SQL table creation task. None check_dtypes_order bool, optiona By default, this task will be used by all the flows. It can be set up to False for avoiding its application. Defaults to True. True table str Destination table. Defaults to None. None schema str Destination schema. Defaults to None. None if_exists Literal What to do if the table exists. Defaults to \"replace\". 'replace' check_col_order bool Whether to check column order. Defaults to True. True sqldb_credentials_secret str The name of the Azure Key Vault secret containing a dictionary with None on_bcp_error Literal[\"skip\", \"fail\"] What to do if error occurs. Defaults to \"skip\". 'skip' max_download_retries int How many times to retry the download. Defaults to 5. 5 tags List[str] Flow tags to use, eg. to control flow concurrency. Defaults to [\"promotion\"]. ['promotion'] vault_name str The name of the vault from which to obtain the secrets. Defaults to None. None timeout(int, optional The amount of time (in seconds) to wait while running this task before a timeout occurs. Defaults to 3600. required Source code in viadot/flows/adls_to_azure_sql.py def __init__ ( self , name : str , local_file_path : str = None , adls_path : str = None , read_sep : str = \" \\t \" , write_sep : str = \" \\t \" , remove_tab : bool = False , overwrite_adls : bool = True , if_empty : str = \"warn\" , adls_sp_credentials_secret : str = None , dtypes : Dict [ str , Any ] = None , check_dtypes_order : bool = True , table : str = None , schema : str = None , if_exists : Literal [ \"fail\" , \"replace\" , \"append\" , \"delete\" ] = \"replace\" , check_col_order : bool = True , sqldb_credentials_secret : str = None , on_bcp_error : Literal [ \"skip\" , \"fail\" ] = \"skip\" , max_download_retries : int = 5 , tags : List [ str ] = [ \"promotion\" ], vault_name : str = None , timeout : int = 3600 , * args : List [ any ], ** kwargs : Dict [ str , Any ], ): \"\"\" Flow for downloading data from different marketing APIs to a local CSV using Supermetrics API, then uploading it to Azure Data Lake, and finally inserting into Azure SQL Database. Args: name (str): The name of the flow. local_file_path (str, optional): Local destination path. Defaults to None. adls_path (str): The path to an ADLS folder or file. If you pass a path to a directory, the latest file from that directory will be loaded. We assume that the files are named using timestamps. read_sep (str, optional): The delimiter for the source file. Defaults to \"\\t\". write_sep (str, optional): The delimiter for the output CSV file. Defaults to \"\\t\". remove_tab (bool, optional): Whether to remove tab delimiters from the data. Defaults to False. overwrite_adls (bool, optional): Whether to overwrite the file in ADLS. Defaults to True. if_empty (str, optional): What to do if the Supermetrics query returns no data. Defaults to \"warn\". adls_sp_credentials_secret (str, optional): The name of the Azure Key Vault secret containing a dictionary with ACCOUNT_NAME and Service Principal credentials (TENANT_ID, CLIENT_ID, CLIENT_SECRET) for the Azure Data Lake. Defaults to None. dtypes (dict, optional): Which custom data types should be used for SQL table creation task. To be used only in case that dtypes need to be manually mapped - dtypes from raw schema file in use by default. Defaults to None. check_dtypes_order (bool, optiona): By default, this task will be used by all the flows. It can be set up to False for avoiding its application. Defaults to True. table (str, optional): Destination table. Defaults to None. schema (str, optional): Destination schema. Defaults to None. if_exists (Literal, optional): What to do if the table exists. Defaults to \"replace\". check_col_order (bool, optional): Whether to check column order. Defaults to True. sqldb_credentials_secret (str, optional): The name of the Azure Key Vault secret containing a dictionary with Azure SQL Database credentials. Defaults to None. on_bcp_error (Literal[\"skip\", \"fail\"], optional): What to do if error occurs. Defaults to \"skip\". max_download_retries (int, optional): How many times to retry the download. Defaults to 5. tags (List[str], optional): Flow tags to use, eg. to control flow concurrency. Defaults to [\"promotion\"]. vault_name (str, optional): The name of the vault from which to obtain the secrets. Defaults to None. timeout(int, optional): The amount of time (in seconds) to wait while running this task before a timeout occurs. Defaults to 3600. \"\"\" adls_path = adls_path . strip ( \"/\" ) # Read parquet if adls_path . split ( \".\" )[ - 1 ] in [ \"csv\" , \"parquet\" ]: self . adls_path = adls_path else : self . adls_path = get_key_value ( key = adls_path ) # Read schema self . dtypes = dtypes self . check_dtypes_order = check_dtypes_order self . adls_root_dir_path = os . path . split ( self . adls_path )[ 0 ] self . adls_file_name = os . path . split ( self . adls_path )[ - 1 ] extension = os . path . splitext ( self . adls_path )[ - 1 ] json_schema_file_name = self . adls_file_name . replace ( extension , \".json\" ) self . json_shema_path = os . path . join ( self . adls_root_dir_path , \"schema\" , json_schema_file_name , ) # AzureDataLakeUpload self . local_file_path = local_file_path or self . slugify ( name ) + \".csv\" self . local_json_path = self . slugify ( name ) + \".json\" self . read_sep = read_sep self . write_sep = write_sep self . overwrite_adls = overwrite_adls self . if_empty = if_empty self . adls_sp_credentials_secret = adls_sp_credentials_secret self . adls_path_conformed = self . get_promoted_path ( env = \"conformed\" ) self . adls_path_operations = self . get_promoted_path ( env = \"operations\" ) # AzureSQLCreateTable self . table = table self . schema = schema self . if_exists = if_exists self . check_col_order = check_col_order # Generate CSV self . remove_tab = remove_tab # BCPTask self . sqldb_credentials_secret = sqldb_credentials_secret self . on_bcp_error = on_bcp_error # Global self . max_download_retries = max_download_retries self . tags = tags self . vault_name = vault_name self . timeout = timeout super () . __init__ ( * args , name = name , ** kwargs ) # self.dtypes.update(METADATA_COLUMNS) self . gen_flow ()","title":"__init__()"},{"location":"references/flows_library/#viadot.flows.azure_sql_transform.AzureSQLTransform","text":"","title":"AzureSQLTransform"},{"location":"references/flows_library/#viadot.flows.azure_sql_transform.AzureSQLTransform.__init__","text":"Flow for running SQL queries on top of Azure SQL Database. Parameters: Name Type Description Default name str The name of the flow. required query str, required The query to execute on the database. required credentials_secret str The name of the Azure Key Vault secret containing a dictionary required vault_name str The name of the vault from which to obtain the secret. Defaults to None. None tags list Tag for marking flow. Defaults to \"transform\". ['transform'] timeout(int, optional The amount of time (in seconds) to wait while running this task before a timeout occurs. Defaults to 3600. required Source code in viadot/flows/azure_sql_transform.py def __init__ ( self , name : str , query : str , sqldb_credentials_secret : str = None , vault_name : str = None , tags : List [ str ] = [ \"transform\" ], timeout : int = 3600 , * args : List [ any ], ** kwargs : Dict [ str , Any ] ): \"\"\" Flow for running SQL queries on top of Azure SQL Database. Args: name (str): The name of the flow. query (str, required): The query to execute on the database. credentials_secret (str, optional): The name of the Azure Key Vault secret containing a dictionary with SQL db credentials (server, db_name, user, and password). vault_name (str, optional): The name of the vault from which to obtain the secret. Defaults to None. tags (list, optional): Tag for marking flow. Defaults to \"transform\". timeout(int, optional): The amount of time (in seconds) to wait while running this task before a timeout occurs. Defaults to 3600. \"\"\" self . query = query self . tags = tags self . sqldb_credentials_secret = sqldb_credentials_secret self . vault_name = vault_name self . timeout = timeout super () . __init__ ( * args , name = name , ** kwargs ) self . gen_flow ()","title":"__init__()"},{"location":"references/flows_library/#viadot.flows.supermetrics_to_adls.SupermetricsToADLS","text":"","title":"SupermetricsToADLS"},{"location":"references/flows_library/#viadot.flows.supermetrics_to_adls.SupermetricsToADLS.__init__","text":"Flow for downloading data from different marketing APIs to a local CSV using Supermetrics API, then uploading it to Azure Data Lake. Parameters: Name Type Description Default name str The name of the flow. required ds_id str A query parameter passed to the SupermetricsToCSV task required ds_accounts List[str] A query parameter passed to the SupermetricsToCSV task required ds_user str A query parameter passed to the SupermetricsToCSV task None fields List[str] A query parameter passed to the SupermetricsToCSV task required ds_segments List[str] A query parameter passed to the SupermetricsToCSV task. Defaults to None. None date_range_type str A query parameter passed to the SupermetricsToCSV task. Defaults to None. None start_date str A query parameter to pass start date to the date range filter. Defaults to None. None end_date str A query parameter to pass end date to the date range filter. Defaults to None. None settings Dict[str, Any] A query parameter passed to the SupermetricsToCSV task. Defaults to None. None filter str A query parameter passed to the SupermetricsToCSV task. Defaults to None. None max_rows int A query parameter passed to the SupermetricsToCSV task. Defaults to 1000000. 1000000 max_columns int A query parameter passed to the SupermetricsToCSV task. Defaults to None. None order_columns str A query parameter passed to the SupermetricsToCSV task. Defaults to None. None expectation_suite dict The Great Expectations suite used to valiaate the data. Defaults to None. None evaluation_parameters str A dictionary containing evaluation parameters for the validation. Defaults to None. None keep_validation_output bool Whether to keep the output files generated by the Great Expectations task. Defaults to False. False local_file_path str Local destination path. Defaults to None. None adls_file_name str Name of file in ADLS. Defaults to None. None output_file_extension str Output file extension - to allow selection of .csv for data which is not easy to handle with parquet. Defaults to \".parquet\".. '.parquet' adls_dir_path str Azure Data Lake destination folder/catalog path. Defaults to None. None sep str The separator to use in the CSV. Defaults to \" \". required overwrite_adls bool Whether to overwrite the file in ADLS. Defaults to True. True if_empty str What to do if the Supermetrics query returns no data. Defaults to \"warn\". 'warn' adls_sp_credentials_secret str The name of the Azure Key Vault secret containing a dictionary with None max_download_retries int How many times to retry the download. Defaults to 5. 5 supermetrics_task_timeout int The timeout for the download task. Defaults to 60*30. 1800 parallel bool Whether to parallelize the downloads. Defaults to True. True tags List[str] Flow tags to use, eg. to control flow concurrency. Defaults to [\"extract\"]. ['extract'] vault_name str The name of the vault from which to obtain the secrets. Defaults to None. None check_missing_data bool Whether to check missing data. Defaults to True. True timeout(int, optional The amount of time (in seconds) to wait while running this task before a timeout occurs. Defaults to 3600. required Source code in viadot/flows/supermetrics_to_adls.py def __init__ ( self , name : str , ds_id : str , ds_accounts : List [ str ], fields : List [ str ], ds_user : str = None , ds_segments : List [ str ] = None , date_range_type : str = None , start_date : str = None , end_date : str = None , settings : Dict [ str , Any ] = None , filter : str = None , max_rows : int = 1000000 , max_columns : int = None , order_columns : str = None , expectation_suite : dict = None , evaluation_parameters : dict = None , keep_validation_output : bool = False , output_file_extension : str = \".parquet\" , local_file_path : str = None , adls_file_name : str = None , adls_dir_path : str = None , overwrite_adls : bool = True , if_empty : str = \"warn\" , if_exists : str = \"replace\" , adls_sp_credentials_secret : str = None , max_download_retries : int = 5 , supermetrics_task_timeout : int = 60 * 30 , parallel : bool = True , tags : List [ str ] = [ \"extract\" ], vault_name : str = None , check_missing_data : bool = True , timeout : int = 3600 , * args : List [ any ], ** kwargs : Dict [ str , Any ], ): \"\"\" Flow for downloading data from different marketing APIs to a local CSV using Supermetrics API, then uploading it to Azure Data Lake. Args: name (str): The name of the flow. ds_id (str): A query parameter passed to the SupermetricsToCSV task ds_accounts (List[str]): A query parameter passed to the SupermetricsToCSV task ds_user (str): A query parameter passed to the SupermetricsToCSV task fields (List[str]): A query parameter passed to the SupermetricsToCSV task ds_segments (List[str], optional): A query parameter passed to the SupermetricsToCSV task. Defaults to None. date_range_type (str, optional): A query parameter passed to the SupermetricsToCSV task. Defaults to None. start_date (str, optional): A query parameter to pass start date to the date range filter. Defaults to None. end_date (str, optional): A query parameter to pass end date to the date range filter. Defaults to None. settings (Dict[str, Any], optional): A query parameter passed to the SupermetricsToCSV task. Defaults to None. filter (str, optional): A query parameter passed to the SupermetricsToCSV task. Defaults to None. max_rows (int, optional): A query parameter passed to the SupermetricsToCSV task. Defaults to 1000000. max_columns (int, optional): A query parameter passed to the SupermetricsToCSV task. Defaults to None. order_columns (str, optional): A query parameter passed to the SupermetricsToCSV task. Defaults to None. expectation_suite (dict, optional): The Great Expectations suite used to valiaate the data. Defaults to None. evaluation_parameters (str, optional): A dictionary containing evaluation parameters for the validation. Defaults to None. keep_validation_output (bool, optional): Whether to keep the output files generated by the Great Expectations task. Defaults to False. Currently, only GitHub URLs are supported. Defaults to None. local_file_path (str, optional): Local destination path. Defaults to None. adls_file_name (str, optional): Name of file in ADLS. Defaults to None. output_file_extension (str, optional): Output file extension - to allow selection of .csv for data which is not easy to handle with parquet. Defaults to \".parquet\".. adls_dir_path (str, optional): Azure Data Lake destination folder/catalog path. Defaults to None. sep (str, optional): The separator to use in the CSV. Defaults to \"\\t\". overwrite_adls (bool, optional): Whether to overwrite the file in ADLS. Defaults to True. if_empty (str, optional): What to do if the Supermetrics query returns no data. Defaults to \"warn\". adls_sp_credentials_secret (str, optional): The name of the Azure Key Vault secret containing a dictionary with ACCOUNT_NAME and Service Principal credentials (TENANT_ID, CLIENT_ID, CLIENT_SECRET) for the Azure Data Lake. Defaults to None. max_download_retries (int, optional): How many times to retry the download. Defaults to 5. supermetrics_task_timeout (int, optional): The timeout for the download task. Defaults to 60*30. parallel (bool, optional): Whether to parallelize the downloads. Defaults to True. tags (List[str], optional): Flow tags to use, eg. to control flow concurrency. Defaults to [\"extract\"]. vault_name (str, optional): The name of the vault from which to obtain the secrets. Defaults to None. check_missing_data (bool, optional): Whether to check missing data. Defaults to True. timeout(int, optional): The amount of time (in seconds) to wait while running this task before a timeout occurs. Defaults to 3600. \"\"\" if not ds_user : try : ds_user = PrefectSecret ( \"SUPERMETRICS_DEFAULT_USER\" ) . run () except ValueError as e : msg = \"Neither 'ds_user' parameter nor 'SUPERMETRICS_DEFAULT_USER' secret were not specified\" raise ValueError ( msg ) from e self . flow_name = name self . check_missing_data = check_missing_data self . timeout = timeout # SupermetricsToDF self . ds_id = ds_id self . ds_accounts = ds_accounts self . ds_segments = ds_segments self . ds_user = ds_user self . fields = fields self . date_range_type = date_range_type self . start_date = start_date self . end_date = end_date self . settings = settings self . filter = filter self . max_rows = max_rows self . max_columns = max_columns self . order_columns = order_columns self . if_exists = if_exists self . output_file_extension = output_file_extension # RunGreatExpectationsValidation self . expectation_suite = expectation_suite self . expectations_path = \"/home/viadot/tmp/expectations\" self . expectation_suite_name = expectation_suite [ \"expectation_suite_name\" ] self . evaluation_parameters = evaluation_parameters self . keep_validation_output = keep_validation_output # AzureDataLakeUpload self . local_file_path = ( local_file_path or self . slugify ( name ) + self . output_file_extension ) self . local_json_path = self . slugify ( name ) + \".json\" self . now = str ( pendulum . now ( \"utc\" )) self . adls_dir_path = adls_dir_path if adls_file_name is not None : self . adls_file_path = os . path . join ( adls_dir_path , adls_file_name ) self . adls_schema_file_dir_file = os . path . join ( adls_dir_path , \"schema\" , Path ( adls_file_name ) . stem + \".json\" ) else : self . adls_file_path = os . path . join ( adls_dir_path , self . now + self . output_file_extension ) self . adls_schema_file_dir_file = os . path . join ( adls_dir_path , \"schema\" , self . now + \".json\" ) self . overwrite_adls = overwrite_adls self . if_empty = if_empty self . adls_sp_credentials_secret = adls_sp_credentials_secret # Global self . max_download_retries = max_download_retries self . supermetrics_task_timeout = supermetrics_task_timeout self . parallel = parallel self . tags = tags self . vault_name = vault_name super () . __init__ ( * args , name = name , ** kwargs ) self . gen_flow ()","title":"__init__()"},{"location":"references/flows_library/#viadot.flows.supermetrics_to_azure_sql.SupermetricsToAzureSQL","text":"","title":"SupermetricsToAzureSQL"},{"location":"references/flows_library/#viadot.flows.cloud_for_customers_report_to_adls.CloudForCustomersReportToADLS","text":"","title":"CloudForCustomersReportToADLS"},{"location":"references/flows_library/#viadot.flows.cloud_for_customers_report_to_adls.CloudForCustomersReportToADLS.__init__","text":"Flow for downloading data from different marketing APIs to a local CSV using Cloud for Customers API, then uploading it to Azure Data Lake. Parameters: Name Type Description Default name str The name of the flow. None report_url str The url to the API. Defaults to None. None url str ??? None endpoint str ??? None params dict ??? {} fields list ??? None skip int Initial index value of reading row. Defaults to 0. 0 top int The value of top reading row. Defaults to 1000. 1000 channels List[str] Filtering parameters passed to the url. Defaults to None. None months List[str] Filtering parameters passed to the url. Defaults to None. None years List[str] Filtering parameters passed to the url. Defaults to None. None env str ??? 'QA' c4c_credentials_secret str The name of the Azure Key Vault secret containing a dictionary with None local_file_path str Local destination path. Defaults to None. None output_file_extension str Output file extension - to allow selection of .csv for data which is not easy '.csv' adls_dir_path str Azure Data Lake destination folder/catalog path. Defaults to None. None adls_file_path str Azure Data Lake destination file path. Defaults to None. None overwrite_adls bool Whether to overwrite the file in ADLS. Defaults to False. False adls_sp_credentials_secret str The name of the Azure Key Vault secret containing a dictionary with None if_empty str What to do if the Supermetrics query returns no data. Defaults to \"warn\". 'warn' if_exists str What to do if the local file already exists. Defaults to \"replace\". 'replace' timeout(int, optional The amount of time (in seconds) to wait while running this task before a timeout occurs. Defaults to 3600. required Source code in viadot/flows/cloud_for_customers_report_to_adls.py def __init__ ( self , name : str = None , report_url : str = None , url : str = None , endpoint : str = None , params : Dict [ str , Any ] = {}, fields : List [ str ] = None , skip : int = 0 , top : int = 1000 , channels : List [ str ] = None , months : List [ str ] = None , years : List [ str ] = None , env : str = \"QA\" , c4c_credentials_secret : str = None , local_file_path : str = None , output_file_extension : str = \".csv\" , adls_dir_path : str = None , adls_file_path : str = None , overwrite_adls : bool = False , adls_sp_credentials_secret : str = None , if_empty : str = \"warn\" , if_exists : str = \"replace\" , timeout : int = 3600 , * args : List [ any ], ** kwargs : Dict [ str , Any ], ): \"\"\" Flow for downloading data from different marketing APIs to a local CSV using Cloud for Customers API, then uploading it to Azure Data Lake. Args: name (str): The name of the flow. report_url (str, optional): The url to the API. Defaults to None. url (str, optional): ??? endpoint (str, optional): ??? params (dict, optional): ??? fields (list, optional): ??? skip (int, optional): Initial index value of reading row. Defaults to 0. top (int, optional): The value of top reading row. Defaults to 1000. channels (List[str], optional): Filtering parameters passed to the url. Defaults to None. months (List[str], optional): Filtering parameters passed to the url. Defaults to None. years (List[str], optional): Filtering parameters passed to the url. Defaults to None. env (str, optional): ??? c4c_credentials_secret (str, optional): The name of the Azure Key Vault secret containing a dictionary with username and password for the Cloud for Customers instance. local_file_path (str, optional): Local destination path. Defaults to None. output_file_extension (str, optional): Output file extension - to allow selection of .csv for data which is not easy to handle with parquet. Defaults to \".csv\". adls_dir_path (str, optional): Azure Data Lake destination folder/catalog path. Defaults to None. adls_file_path (str, optional): Azure Data Lake destination file path. Defaults to None. overwrite_adls (bool, optional): Whether to overwrite the file in ADLS. Defaults to False. adls_sp_credentials_secret (str, optional): The name of the Azure Key Vault secret containing a dictionary with ACCOUNT_NAME and Service Principal credentials (TENANT_ID, CLIENT_ID, CLIENT_SECRET) for the Azure Data Lake. Defaults to None. if_empty (str, optional): What to do if the Supermetrics query returns no data. Defaults to \"warn\". if_exists (str, optional): What to do if the local file already exists. Defaults to \"replace\". timeout(int, optional): The amount of time (in seconds) to wait while running this task before a timeout occurs. Defaults to 3600. \"\"\" self . report_url = report_url self . skip = skip self . top = top self . if_empty = if_empty self . env = env self . c4c_credentials_secret = c4c_credentials_secret self . timeout = timeout # AzureDataLakeUpload self . adls_sp_credentials_secret = adls_sp_credentials_secret self . if_exists = if_exists self . overwrite_adls = overwrite_adls self . output_file_extension = output_file_extension self . local_file_path = ( local_file_path or slugify ( name ) + self . output_file_extension ) self . now = str ( pendulum . now ( \"utc\" )) self . adls_dir_path = adls_dir_path self . adls_file_path = adls_file_path or os . path . join ( adls_dir_path , self . now + self . output_file_extension ) # in case of non-report invoking self . url = url self . endpoint = endpoint self . params = params self . fields = fields # filtering report_url for reports self . channels = channels self . months = months self . years = years self . report_urls_with_filters = [ self . report_url ] self . report_urls_with_filters = self . create_url_with_fields ( fields_list = self . channels , filter_code = \"CCHANNETZTEXT12CE6C2FA0D77995\" ) self . report_urls_with_filters = self . create_url_with_fields ( fields_list = self . months , filter_code = \"CMONTH_ID\" ) self . report_urls_with_filters = self . create_url_with_fields ( fields_list = self . years , filter_code = \"CYEAR_ID\" ) super () . __init__ ( * args , name = name , ** kwargs ) self . gen_flow ()","title":"__init__()"},{"location":"references/sql_sources/","text":"SQL Sources viadot.sources.base.Source to_arrow ( self , if_empty = 'warn' ) Creates a pyarrow table from source. Parameters: Name Type Description Default if_empty str : What to do if data sourse contains no data. Defaults to \"warn\". 'warn' Source code in viadot/sources/base.py def to_arrow ( self , if_empty : str = \"warn\" ) -> pa . Table : \"\"\" Creates a pyarrow table from source. Args: if_empty (str, optional): : What to do if data sourse contains no data. Defaults to \"warn\". \"\"\" try : df = self . to_df ( if_empty = if_empty ) except SKIP : return False table = pa . Table . from_pandas ( df ) return table to_csv ( self , path , if_exists = 'replace' , if_empty = 'warn' , sep = ' \\t ' , ** kwargs ) Write from source to a CSV file. Note that the source can be a particular file or table, but also a database in general. Therefore, some sources may require additional parameters to pull the right resource. Hence this method passes kwargs to the to_df() method implemented by the concrete source. Parameters: Name Type Description Default path str The destination path. required if_exists Literal[ What to do if the file exists. 'replace' if_empty str What to do if the source contains no data. 'warn' sep str The separator to use in the CSV. Defaults to \" \". '\\t' Exceptions: Type Description ValueError If the if_exists argument is incorrect. Returns: Type Description bool Whether the operation was successful. Source code in viadot/sources/base.py def to_csv ( self , path : str , if_exists : Literal [ \"append\" , \"replace\" ] = \"replace\" , if_empty : str = \"warn\" , sep = \" \\t \" , ** kwargs , ) -> bool : \"\"\" Write from source to a CSV file. Note that the source can be a particular file or table, but also a database in general. Therefore, some sources may require additional parameters to pull the right resource. Hence this method passes kwargs to the `to_df()` method implemented by the concrete source. Args: path (str): The destination path. if_exists (Literal[, optional): What to do if the file exists. Defaults to \"replace\". if_empty (str, optional): What to do if the source contains no data. Defaults to \"warn\". sep (str, optional): The separator to use in the CSV. Defaults to \"\\t\". Raises: ValueError: If the `if_exists` argument is incorrect. Returns: bool: Whether the operation was successful. \"\"\" try : df = self . to_df ( if_empty = if_empty , ** kwargs ) except SKIP : return False if if_exists == \"append\" : mode = \"a\" elif if_exists == \"replace\" : mode = \"w\" else : raise ValueError ( \"'if_exists' must be one of ['append', 'replace']\" ) df . to_csv ( path , sep = sep , mode = mode , index = False , header = not os . path . exists ( path ) ) return True to_excel ( self , path , if_exists = 'replace' , if_empty = 'warn' ) Write from source to a excel file. Parameters: Name Type Description Default path str The destination path. required if_exists str What to do if the file exists. Defaults to \"replace\". 'replace' if_empty str What to do if the source contains no data. 'warn' Source code in viadot/sources/base.py def to_excel ( self , path : str , if_exists : str = \"replace\" , if_empty : str = \"warn\" ) -> bool : \"\"\" Write from source to a excel file. Args: path (str): The destination path. if_exists (str, optional): What to do if the file exists. Defaults to \"replace\". if_empty (str, optional): What to do if the source contains no data. \"\"\" try : df = self . to_df ( if_empty = if_empty ) except SKIP : return False if if_exists == \"append\" : if os . path . isfile ( path ): excel_df = pd . read_excel ( path ) out_df = pd . concat ([ excel_df , df ]) else : out_df = df elif if_exists == \"replace\" : out_df = df out_df . to_excel ( path , index = False , encoding = \"utf8\" ) return True to_parquet ( self , path , if_exists = 'replace' , if_empty = 'warn' , ** kwargs ) Write from source to a Parquet file. Parameters: Name Type Description Default path str The destination path. required if_exists Literal[\"append\", \"replace\", \"skip\"] What to do if the file exists. Defaults to \"replace\". 'replace' if_empty Literal[\"warn\", \"fail\", \"skip\"] What to do if the source contains no data. Defaults to \"warn\". 'warn' Source code in viadot/sources/base.py def to_parquet ( self , path : str , if_exists : Literal [ \"append\" , \"replace\" , \"skip\" ] = \"replace\" , if_empty : Literal [ \"warn\" , \"fail\" , \"skip\" ] = \"warn\" , ** kwargs , ) -> None : \"\"\" Write from source to a Parquet file. Args: path (str): The destination path. if_exists (Literal[\"append\", \"replace\", \"skip\"], optional): What to do if the file exists. Defaults to \"replace\". if_empty (Literal[\"warn\", \"fail\", \"skip\"], optional): What to do if the source contains no data. Defaults to \"warn\". \"\"\" try : df = self . to_df ( if_empty = if_empty ) except SKIP : return False if if_exists == \"append\" and os . path . isfile ( path ): parquet_df = pd . read_parquet ( path ) out_df = pd . concat ([ parquet_df , df ]) elif if_exists == \"replace\" : out_df = df elif if_exists == \"skip\" : logger . info ( \"Skipped.\" ) return else : out_df = df # create directories if they don't exist try : if not os . path . isfile ( path ): directory = os . path . dirname ( path ) os . makedirs ( directory , exist_ok = True ) except FileNotFoundError : logger . info ( \"File not found.\" ) pass out_df . to_parquet ( path , index = False , ** kwargs ) viadot.sources.base.SQL ( Source ) con : Connection property readonly A singleton-like property for initiating a connection to the database. Returns: Type Description pyodbc.Connection database connection. conn_str : str property readonly Generate a connection string from params or config. Note that the user and password are escaped with '{}' characters. Returns: Type Description str The ODBC connection string. __init__ ( self , driver = None , config_key = None , credentials = None , query_timeout = 3600 , * args , ** kwargs ) special A base SQL source class. Parameters: Name Type Description Default driver str The SQL driver to use. Defaults to None. None config_key str The key inside local config containing the config. None credentials str Credentials for the connection. Defaults to None. None query_timeout int The timeout for executed queries. Defaults to 1 hour. 3600 Source code in viadot/sources/base.py def __init__ ( self , driver : str = None , config_key : str = None , credentials : str = None , query_timeout : int = 60 * 60 , * args , ** kwargs , ): \"\"\"A base SQL source class. Args: driver (str, optional): The SQL driver to use. Defaults to None. config_key (str, optional): The key inside local config containing the config. User can choose to use this or pass credentials directly to the `credentials` parameter. Defaults to None. credentials (str, optional): Credentials for the connection. Defaults to None. query_timeout (int, optional): The timeout for executed queries. Defaults to 1 hour. \"\"\" self . query_timeout = query_timeout if config_key : config_credentials = local_config . get ( config_key ) else : config_credentials = None credentials = credentials or config_credentials or {} if driver : credentials [ \"driver\" ] = driver super () . __init__ ( * args , credentials = credentials , ** kwargs ) self . _con = None create_table ( self , table , schema = None , dtypes = None , if_exists = 'fail' ) Create a table. Parameters: Name Type Description Default table str The destination table. Defaults to None. required schema str The destination schema. Defaults to None. None dtypes Dict[str, Any] The data types to use for the table. Defaults to None. None if_exists Literal What to do if the table already exists. Defaults to \"fail\". 'fail' Returns: Type Description bool Whether the operation was successful. Source code in viadot/sources/base.py def create_table ( self , table : str , schema : str = None , dtypes : Dict [ str , Any ] = None , if_exists : Literal [ \"fail\" , \"replace\" , \"skip\" , \"delete\" ] = \"fail\" , ) -> bool : \"\"\"Create a table. Args: table (str): The destination table. Defaults to None. schema (str, optional): The destination schema. Defaults to None. dtypes (Dict[str, Any], optional): The data types to use for the table. Defaults to None. if_exists (Literal, optional): What to do if the table already exists. Defaults to \"fail\". Returns: bool: Whether the operation was successful. \"\"\" fqn = f \" { schema } . { table } \" if schema is not None else table exists = self . _check_if_table_exists ( schema = schema , table = table ) if exists : if if_exists == \"replace\" : self . run ( f \"DROP TABLE { fqn } \" ) elif if_exists == \"delete\" : self . run ( f \"DELETE FROM { fqn } \" ) return True elif if_exists == \"fail\" : raise ValueError ( \"The table already exists and 'if_exists' is set to 'fail'.\" ) elif if_exists == \"skip\" : return False indent = \" \" dtypes_rows = [ indent + f '\" { col } \"' + \" \" + dtype for col , dtype in dtypes . items () ] dtypes_formatted = \", \\n \" . join ( dtypes_rows ) create_table_sql = f \"CREATE TABLE { fqn } ( \\n { dtypes_formatted } \\n )\" self . run ( create_table_sql ) return True insert_into ( self , table , df ) Insert values from a pandas DataFrame into an existing database table. Parameters: Name Type Description Default table str table name required df pd.DataFrame pandas dataframe required Returns: Type Description str The executed SQL insert query. Source code in viadot/sources/base.py def insert_into ( self , table : str , df : pd . DataFrame ) -> str : \"\"\"Insert values from a pandas DataFrame into an existing database table. Args: table (str): table name df (pd.DataFrame): pandas dataframe Returns: str: The executed SQL insert query. \"\"\" values = \"\" rows_count = df . shape [ 0 ] counter = 0 for row in df . values : counter += 1 out_row = \", \" . join ( map ( self . _sql_column , row )) comma = \", \\n \" if counter == rows_count : comma = \";\" out_row = f \"( { out_row } ) { comma } \" values += out_row columns = \", \" . join ( df . columns ) sql = f \"INSERT INTO { table } ( { columns } ) \\n VALUES { values } \" self . run ( sql ) return sql to_df ( self , query , con = None , if_empty = None ) Creates DataFrame form SQL query. Parameters: Name Type Description Default query str SQL query. If don't start with \"SELECT\" returns empty DataFrame. required con pyodbc.Connection The connection to use to pull the data. None if_empty str What to do if the query returns no data. Defaults to None. None Source code in viadot/sources/base.py def to_df ( self , query : str , con : pyodbc . Connection = None , if_empty : str = None ) -> pd . DataFrame : \"\"\"Creates DataFrame form SQL query. Args: query (str): SQL query. If don't start with \"SELECT\" returns empty DataFrame. con (pyodbc.Connection, optional): The connection to use to pull the data. if_empty (str, optional): What to do if the query returns no data. Defaults to None. \"\"\" conn = con or self . con query_sanitized = query . strip () . upper () if query_sanitized . startswith ( \"SELECT\" ) or query_sanitized . startswith ( \"WITH\" ): df = pd . read_sql_query ( query , conn ) if df . empty : self . _handle_if_empty ( if_empty = if_empty ) else : df = pd . DataFrame () return df viadot.sources.azure_data_lake.AzureDataLake ( Source ) A class for pulling data from the Azure Data Lakes (gen1 and gen2). You can either connect to the lake in general or to a particular path, eg. lake = AzureDataLake(); lake.exists(\"a/b/c.csv\") vs lake = AzureDataLake(path=\"a/b/c.csv\"); lake.exists() Parameters credentials : Dict[str, Any], optional A dictionary containing ACCOUNT_NAME and the following Service Principal credentials: - AZURE_TENANT_ID - AZURE_CLIENT_ID - AZURE_CLIENT_SECRET cp ( self , from_path = None , to_path = None , recursive = False ) Copies source to a destination. Parameters: Name Type Description Default from_path str Path form which to copy file. Defauls to None. None to_path str Path where to copy files. Defaults to None. None recursive bool Whether to copy files recursively or not. Defaults to False. False Source code in viadot/sources/azure_data_lake.py def cp ( self , from_path : str = None , to_path : str = None , recursive : bool = False ): \"\"\" Copies source to a destination. Args: from_path (str, optional): Path form which to copy file. Defauls to None. to_path (str, optional): Path where to copy files. Defaults to None. recursive (bool, optional): Whether to copy files recursively or not. Defaults to False. \"\"\" from_path = from_path or self . path to_path = to_path self . fs . cp ( from_path , to_path , recursive = recursive ) exists ( self , path = None ) Check if a location exists in Azure Data Lake. Parameters: Name Type Description Default path str The path to check. Can be a file or a directory. None Examples: from viadot.sources import AzureDataLake lake = AzureDataLake ( gen = 1 ) lake . exists ( \"tests/test.csv\" ) Returns: Type Description bool Whether the paths exists. Source code in viadot/sources/azure_data_lake.py def exists ( self , path : str = None ) -> bool : \"\"\" Check if a location exists in Azure Data Lake. Args: path (str): The path to check. Can be a file or a directory. Example: ```python from viadot.sources import AzureDataLake lake = AzureDataLake(gen=1) lake.exists(\"tests/test.csv\") ``` Returns: bool: Whether the paths exists. \"\"\" path = path or self . path return self . fs . exists ( path ) find ( self , path = None ) Returns list of files in a path using recursive method. Parameters: Name Type Description Default path str Path to a folder. Defaults to None. None Returns: Type Description List[str] List of paths. Source code in viadot/sources/azure_data_lake.py def find ( self , path : str = None ) -> List [ str ]: \"\"\" Returns list of files in a path using recursive method. Args: path (str, optional): Path to a folder. Defaults to None. Returns: List[str]: List of paths. \"\"\" path = path or self . path files_path_list_raw = self . fs . find ( path ) paths_list = [ p for p in files_path_list_raw if not p . endswith ( \"/\" )] return paths_list ls ( self , path = None ) Returns list of files in a path. Parameters: Name Type Description Default path str Path to a folder. Defaults to None. None Source code in viadot/sources/azure_data_lake.py def ls ( self , path : str = None ) -> List [ str ]: \"\"\" Returns list of files in a path. Args: path (str, optional): Path to a folder. Defaults to None. \"\"\" path = path or self . path return self . fs . ls ( path ) rm ( self , path = None , recursive = False ) Deletes a file or directory from the path. Parameters: Name Type Description Default path str Path to a folder or file. Defaults to None. None recursive bool Whether to delete files recursively or not. Defaults to False. False Source code in viadot/sources/azure_data_lake.py def rm ( self , path : str = None , recursive : bool = False ): \"\"\" Deletes a file or directory from the path. Args: path (str, optional): Path to a folder or file. Defaults to None. recursive (bool, optional): Whether to delete files recursively or not. Defaults to False. \"\"\" path = path or self . path self . fs . rm ( path , recursive = recursive ) upload ( self , from_path , to_path = None , recursive = False , overwrite = False ) Upload file(s) to the lake. Parameters: Name Type Description Default from_path str Path to the local file(s) to be uploaded. required to_path str Path to the destination file/folder None recursive bool Set this to true if working with directories. False overwrite bool Whether to overwrite the file(s) if they exist. False Examples: from viadot.sources import AzureDataLake lake = AzureDataLake () lake . upload ( from_path = 'tests/test.csv' , to_path = \"sandbox/test.csv\" ) Source code in viadot/sources/azure_data_lake.py def upload ( self , from_path : str , to_path : str = None , recursive : bool = False , overwrite : bool = False , ) -> None : \"\"\" Upload file(s) to the lake. Args: from_path (str): Path to the local file(s) to be uploaded. to_path (str): Path to the destination file/folder recursive (bool): Set this to true if working with directories. overwrite (bool): Whether to overwrite the file(s) if they exist. Example: ```python from viadot.sources import AzureDataLake lake = AzureDataLake() lake.upload(from_path='tests/test.csv', to_path=\"sandbox/test.csv\") ``` \"\"\" if self . gen == 1 : raise NotImplemented ( \"Azure Data Lake Gen1 does not support simple file upload.\" ) to_path = to_path or self . path self . fs . upload ( lpath = from_path , rpath = to_path , recursive = recursive , overwrite = overwrite , ) viadot.sources.azure_sql.AzureSQL ( SQLServer ) bulk_insert ( self , table , schema = None , source_path = None , sep = ' \\t ' , if_exists = 'append' ) Fuction to bulk insert. Parameters: Name Type Description Default table str Table name. required schema str Schema name. Defaults to None. None source_path str Full path to a data file. Defaults to one. None sep str field terminator to be used for char and widechar data files. Defaults to \" \". '\\t' if_exists Literal What to do if the table already exists. Defaults to \"append\". 'append' Source code in viadot/sources/azure_sql.py def bulk_insert ( self , table : str , schema : str = None , source_path : str = None , sep = \" \\t \" , if_exists : Literal = \"append\" , ): \"\"\"Fuction to bulk insert. Args: table (str): Table name. schema (str, optional): Schema name. Defaults to None. source_path (str, optional): Full path to a data file. Defaults to one. sep (str, optional): field terminator to be used for char and widechar data files. Defaults to \"\\t\". if_exists (Literal, optional): What to do if the table already exists. Defaults to \"append\". \"\"\" if schema is None : schema = self . DEFAULT_SCHEMA fqn = f \" { schema } . { table } \" insert_sql = f \"\"\" BULK INSERT { fqn } FROM ' { source_path } ' WITH ( CHECK_CONSTRAINTS, DATA_SOURCE=' { self . credentials [ 'data_source' ] } ', DATAFILETYPE='char', FIELDTERMINATOR=' { sep } ', ROWTERMINATOR='0x0a', FIRSTROW=2, KEEPIDENTITY, TABLOCK, CODEPAGE='65001' ); \"\"\" if if_exists == \"replace\" : self . run ( f \"DELETE FROM { schema } . { table } \" ) self . run ( insert_sql ) return True create_external_database ( self , external_database_name , storage_account_name , container_name , sas_token , master_key_password , credential_name = None ) Create an external database. Used to eg. execute BULK INSERT or OPENROWSET queries. Parameters: Name Type Description Default external_database_name str The name of the extrnal source (db) to be created. required storage_account_name str The name of the Azure storage account. required container_name str The name of the container which should become the \"database\". required sas_token str The SAS token to be used as the credential. Note that the auth required master_key_password str The password for the database master key of your required credential_name str How to name the SAS credential. This is really an Azure None Source code in viadot/sources/azure_sql.py def create_external_database ( self , external_database_name : str , storage_account_name : str , container_name : str , sas_token : str , master_key_password : str , credential_name : str = None , ): \"\"\"Create an external database. Used to eg. execute BULK INSERT or OPENROWSET queries. Args: external_database_name (str): The name of the extrnal source (db) to be created. storage_account_name (str): The name of the Azure storage account. container_name (str): The name of the container which should become the \"database\". sas_token (str): The SAS token to be used as the credential. Note that the auth system in Azure is pretty broken and you might need to paste here your storage account's account key instead. master_key_password (str): The password for the database master key of your Azure SQL Database. credential_name (str): How to name the SAS credential. This is really an Azure internal thing and can be anything. By default '{external_database_name}_credential`. \"\"\" # stupid Microsoft thing if sas_token . startswith ( \"?\" ): sas_token = sas_token [ 1 :] if credential_name is None : credential_name = f \" { external_database_name } _credential\" create_master_key_sql = ( f \"CREATE MASTER KEY ENCRYPTION BY PASSWORD = { master_key_password } \" ) create_external_db_credential_sql = f \"\"\" CREATE DATABASE SCOPED CREDENTIAL { credential_name } WITH IDENTITY = 'SHARED ACCESS SIGNATURE' SECRET = ' { sas_token } '; \"\"\" create_external_db_sql = f \"\"\" CREATE EXTERNAL DATA SOURCE { external_database_name } WITH ( LOCATION = f'https:// { storage_account_name } .blob.core.windows.net/ { container_name } ', CREDENTIAL = { credential_name } ); \"\"\" self . run ( create_master_key_sql ) self . run ( create_external_db_credential_sql ) self . run ( create_external_db_sql ) viadot.sources.sqlite.SQLite ( SQL ) A SQLite source Parameters: Name Type Description Default server str server string, usually localhost required db str the file path to the db e.g. /home/somedb.sqlite required conn_str property readonly Generate a connection string from params or config. Note that the user and password are escapedd with '{}' characters. Returns: Type Description str The ODBC connection string.","title":"SQL Sources"},{"location":"references/sql_sources/#sql-sources","text":"","title":"SQL Sources"},{"location":"references/sql_sources/#viadot.sources.base.Source","text":"","title":"Source"},{"location":"references/sql_sources/#viadot.sources.base.Source.to_arrow","text":"Creates a pyarrow table from source. Parameters: Name Type Description Default if_empty str : What to do if data sourse contains no data. Defaults to \"warn\". 'warn' Source code in viadot/sources/base.py def to_arrow ( self , if_empty : str = \"warn\" ) -> pa . Table : \"\"\" Creates a pyarrow table from source. Args: if_empty (str, optional): : What to do if data sourse contains no data. Defaults to \"warn\". \"\"\" try : df = self . to_df ( if_empty = if_empty ) except SKIP : return False table = pa . Table . from_pandas ( df ) return table","title":"to_arrow()"},{"location":"references/sql_sources/#viadot.sources.base.Source.to_csv","text":"Write from source to a CSV file. Note that the source can be a particular file or table, but also a database in general. Therefore, some sources may require additional parameters to pull the right resource. Hence this method passes kwargs to the to_df() method implemented by the concrete source. Parameters: Name Type Description Default path str The destination path. required if_exists Literal[ What to do if the file exists. 'replace' if_empty str What to do if the source contains no data. 'warn' sep str The separator to use in the CSV. Defaults to \" \". '\\t' Exceptions: Type Description ValueError If the if_exists argument is incorrect. Returns: Type Description bool Whether the operation was successful. Source code in viadot/sources/base.py def to_csv ( self , path : str , if_exists : Literal [ \"append\" , \"replace\" ] = \"replace\" , if_empty : str = \"warn\" , sep = \" \\t \" , ** kwargs , ) -> bool : \"\"\" Write from source to a CSV file. Note that the source can be a particular file or table, but also a database in general. Therefore, some sources may require additional parameters to pull the right resource. Hence this method passes kwargs to the `to_df()` method implemented by the concrete source. Args: path (str): The destination path. if_exists (Literal[, optional): What to do if the file exists. Defaults to \"replace\". if_empty (str, optional): What to do if the source contains no data. Defaults to \"warn\". sep (str, optional): The separator to use in the CSV. Defaults to \"\\t\". Raises: ValueError: If the `if_exists` argument is incorrect. Returns: bool: Whether the operation was successful. \"\"\" try : df = self . to_df ( if_empty = if_empty , ** kwargs ) except SKIP : return False if if_exists == \"append\" : mode = \"a\" elif if_exists == \"replace\" : mode = \"w\" else : raise ValueError ( \"'if_exists' must be one of ['append', 'replace']\" ) df . to_csv ( path , sep = sep , mode = mode , index = False , header = not os . path . exists ( path ) ) return True","title":"to_csv()"},{"location":"references/sql_sources/#viadot.sources.base.Source.to_excel","text":"Write from source to a excel file. Parameters: Name Type Description Default path str The destination path. required if_exists str What to do if the file exists. Defaults to \"replace\". 'replace' if_empty str What to do if the source contains no data. 'warn' Source code in viadot/sources/base.py def to_excel ( self , path : str , if_exists : str = \"replace\" , if_empty : str = \"warn\" ) -> bool : \"\"\" Write from source to a excel file. Args: path (str): The destination path. if_exists (str, optional): What to do if the file exists. Defaults to \"replace\". if_empty (str, optional): What to do if the source contains no data. \"\"\" try : df = self . to_df ( if_empty = if_empty ) except SKIP : return False if if_exists == \"append\" : if os . path . isfile ( path ): excel_df = pd . read_excel ( path ) out_df = pd . concat ([ excel_df , df ]) else : out_df = df elif if_exists == \"replace\" : out_df = df out_df . to_excel ( path , index = False , encoding = \"utf8\" ) return True","title":"to_excel()"},{"location":"references/sql_sources/#viadot.sources.base.Source.to_parquet","text":"Write from source to a Parquet file. Parameters: Name Type Description Default path str The destination path. required if_exists Literal[\"append\", \"replace\", \"skip\"] What to do if the file exists. Defaults to \"replace\". 'replace' if_empty Literal[\"warn\", \"fail\", \"skip\"] What to do if the source contains no data. Defaults to \"warn\". 'warn' Source code in viadot/sources/base.py def to_parquet ( self , path : str , if_exists : Literal [ \"append\" , \"replace\" , \"skip\" ] = \"replace\" , if_empty : Literal [ \"warn\" , \"fail\" , \"skip\" ] = \"warn\" , ** kwargs , ) -> None : \"\"\" Write from source to a Parquet file. Args: path (str): The destination path. if_exists (Literal[\"append\", \"replace\", \"skip\"], optional): What to do if the file exists. Defaults to \"replace\". if_empty (Literal[\"warn\", \"fail\", \"skip\"], optional): What to do if the source contains no data. Defaults to \"warn\". \"\"\" try : df = self . to_df ( if_empty = if_empty ) except SKIP : return False if if_exists == \"append\" and os . path . isfile ( path ): parquet_df = pd . read_parquet ( path ) out_df = pd . concat ([ parquet_df , df ]) elif if_exists == \"replace\" : out_df = df elif if_exists == \"skip\" : logger . info ( \"Skipped.\" ) return else : out_df = df # create directories if they don't exist try : if not os . path . isfile ( path ): directory = os . path . dirname ( path ) os . makedirs ( directory , exist_ok = True ) except FileNotFoundError : logger . info ( \"File not found.\" ) pass out_df . to_parquet ( path , index = False , ** kwargs )","title":"to_parquet()"},{"location":"references/sql_sources/#viadot.sources.base.SQL","text":"","title":"SQL"},{"location":"references/sql_sources/#viadot.sources.base.SQL.con","text":"A singleton-like property for initiating a connection to the database. Returns: Type Description pyodbc.Connection database connection.","title":"con"},{"location":"references/sql_sources/#viadot.sources.base.SQL.conn_str","text":"Generate a connection string from params or config. Note that the user and password are escaped with '{}' characters. Returns: Type Description str The ODBC connection string.","title":"conn_str"},{"location":"references/sql_sources/#viadot.sources.base.SQL.__init__","text":"A base SQL source class. Parameters: Name Type Description Default driver str The SQL driver to use. Defaults to None. None config_key str The key inside local config containing the config. None credentials str Credentials for the connection. Defaults to None. None query_timeout int The timeout for executed queries. Defaults to 1 hour. 3600 Source code in viadot/sources/base.py def __init__ ( self , driver : str = None , config_key : str = None , credentials : str = None , query_timeout : int = 60 * 60 , * args , ** kwargs , ): \"\"\"A base SQL source class. Args: driver (str, optional): The SQL driver to use. Defaults to None. config_key (str, optional): The key inside local config containing the config. User can choose to use this or pass credentials directly to the `credentials` parameter. Defaults to None. credentials (str, optional): Credentials for the connection. Defaults to None. query_timeout (int, optional): The timeout for executed queries. Defaults to 1 hour. \"\"\" self . query_timeout = query_timeout if config_key : config_credentials = local_config . get ( config_key ) else : config_credentials = None credentials = credentials or config_credentials or {} if driver : credentials [ \"driver\" ] = driver super () . __init__ ( * args , credentials = credentials , ** kwargs ) self . _con = None","title":"__init__()"},{"location":"references/sql_sources/#viadot.sources.base.SQL.create_table","text":"Create a table. Parameters: Name Type Description Default table str The destination table. Defaults to None. required schema str The destination schema. Defaults to None. None dtypes Dict[str, Any] The data types to use for the table. Defaults to None. None if_exists Literal What to do if the table already exists. Defaults to \"fail\". 'fail' Returns: Type Description bool Whether the operation was successful. Source code in viadot/sources/base.py def create_table ( self , table : str , schema : str = None , dtypes : Dict [ str , Any ] = None , if_exists : Literal [ \"fail\" , \"replace\" , \"skip\" , \"delete\" ] = \"fail\" , ) -> bool : \"\"\"Create a table. Args: table (str): The destination table. Defaults to None. schema (str, optional): The destination schema. Defaults to None. dtypes (Dict[str, Any], optional): The data types to use for the table. Defaults to None. if_exists (Literal, optional): What to do if the table already exists. Defaults to \"fail\". Returns: bool: Whether the operation was successful. \"\"\" fqn = f \" { schema } . { table } \" if schema is not None else table exists = self . _check_if_table_exists ( schema = schema , table = table ) if exists : if if_exists == \"replace\" : self . run ( f \"DROP TABLE { fqn } \" ) elif if_exists == \"delete\" : self . run ( f \"DELETE FROM { fqn } \" ) return True elif if_exists == \"fail\" : raise ValueError ( \"The table already exists and 'if_exists' is set to 'fail'.\" ) elif if_exists == \"skip\" : return False indent = \" \" dtypes_rows = [ indent + f '\" { col } \"' + \" \" + dtype for col , dtype in dtypes . items () ] dtypes_formatted = \", \\n \" . join ( dtypes_rows ) create_table_sql = f \"CREATE TABLE { fqn } ( \\n { dtypes_formatted } \\n )\" self . run ( create_table_sql ) return True","title":"create_table()"},{"location":"references/sql_sources/#viadot.sources.base.SQL.insert_into","text":"Insert values from a pandas DataFrame into an existing database table. Parameters: Name Type Description Default table str table name required df pd.DataFrame pandas dataframe required Returns: Type Description str The executed SQL insert query. Source code in viadot/sources/base.py def insert_into ( self , table : str , df : pd . DataFrame ) -> str : \"\"\"Insert values from a pandas DataFrame into an existing database table. Args: table (str): table name df (pd.DataFrame): pandas dataframe Returns: str: The executed SQL insert query. \"\"\" values = \"\" rows_count = df . shape [ 0 ] counter = 0 for row in df . values : counter += 1 out_row = \", \" . join ( map ( self . _sql_column , row )) comma = \", \\n \" if counter == rows_count : comma = \";\" out_row = f \"( { out_row } ) { comma } \" values += out_row columns = \", \" . join ( df . columns ) sql = f \"INSERT INTO { table } ( { columns } ) \\n VALUES { values } \" self . run ( sql ) return sql","title":"insert_into()"},{"location":"references/sql_sources/#viadot.sources.base.SQL.to_df","text":"Creates DataFrame form SQL query. Parameters: Name Type Description Default query str SQL query. If don't start with \"SELECT\" returns empty DataFrame. required con pyodbc.Connection The connection to use to pull the data. None if_empty str What to do if the query returns no data. Defaults to None. None Source code in viadot/sources/base.py def to_df ( self , query : str , con : pyodbc . Connection = None , if_empty : str = None ) -> pd . DataFrame : \"\"\"Creates DataFrame form SQL query. Args: query (str): SQL query. If don't start with \"SELECT\" returns empty DataFrame. con (pyodbc.Connection, optional): The connection to use to pull the data. if_empty (str, optional): What to do if the query returns no data. Defaults to None. \"\"\" conn = con or self . con query_sanitized = query . strip () . upper () if query_sanitized . startswith ( \"SELECT\" ) or query_sanitized . startswith ( \"WITH\" ): df = pd . read_sql_query ( query , conn ) if df . empty : self . _handle_if_empty ( if_empty = if_empty ) else : df = pd . DataFrame () return df","title":"to_df()"},{"location":"references/sql_sources/#viadot.sources.azure_data_lake.AzureDataLake","text":"A class for pulling data from the Azure Data Lakes (gen1 and gen2). You can either connect to the lake in general or to a particular path, eg. lake = AzureDataLake(); lake.exists(\"a/b/c.csv\") vs lake = AzureDataLake(path=\"a/b/c.csv\"); lake.exists()","title":"AzureDataLake"},{"location":"references/sql_sources/#viadot.sources.azure_data_lake.AzureDataLake--parameters","text":"credentials : Dict[str, Any], optional A dictionary containing ACCOUNT_NAME and the following Service Principal credentials: - AZURE_TENANT_ID - AZURE_CLIENT_ID - AZURE_CLIENT_SECRET","title":"Parameters"},{"location":"references/sql_sources/#viadot.sources.azure_data_lake.AzureDataLake.cp","text":"Copies source to a destination. Parameters: Name Type Description Default from_path str Path form which to copy file. Defauls to None. None to_path str Path where to copy files. Defaults to None. None recursive bool Whether to copy files recursively or not. Defaults to False. False Source code in viadot/sources/azure_data_lake.py def cp ( self , from_path : str = None , to_path : str = None , recursive : bool = False ): \"\"\" Copies source to a destination. Args: from_path (str, optional): Path form which to copy file. Defauls to None. to_path (str, optional): Path where to copy files. Defaults to None. recursive (bool, optional): Whether to copy files recursively or not. Defaults to False. \"\"\" from_path = from_path or self . path to_path = to_path self . fs . cp ( from_path , to_path , recursive = recursive )","title":"cp()"},{"location":"references/sql_sources/#viadot.sources.azure_data_lake.AzureDataLake.exists","text":"Check if a location exists in Azure Data Lake. Parameters: Name Type Description Default path str The path to check. Can be a file or a directory. None Examples: from viadot.sources import AzureDataLake lake = AzureDataLake ( gen = 1 ) lake . exists ( \"tests/test.csv\" ) Returns: Type Description bool Whether the paths exists. Source code in viadot/sources/azure_data_lake.py def exists ( self , path : str = None ) -> bool : \"\"\" Check if a location exists in Azure Data Lake. Args: path (str): The path to check. Can be a file or a directory. Example: ```python from viadot.sources import AzureDataLake lake = AzureDataLake(gen=1) lake.exists(\"tests/test.csv\") ``` Returns: bool: Whether the paths exists. \"\"\" path = path or self . path return self . fs . exists ( path )","title":"exists()"},{"location":"references/sql_sources/#viadot.sources.azure_data_lake.AzureDataLake.find","text":"Returns list of files in a path using recursive method. Parameters: Name Type Description Default path str Path to a folder. Defaults to None. None Returns: Type Description List[str] List of paths. Source code in viadot/sources/azure_data_lake.py def find ( self , path : str = None ) -> List [ str ]: \"\"\" Returns list of files in a path using recursive method. Args: path (str, optional): Path to a folder. Defaults to None. Returns: List[str]: List of paths. \"\"\" path = path or self . path files_path_list_raw = self . fs . find ( path ) paths_list = [ p for p in files_path_list_raw if not p . endswith ( \"/\" )] return paths_list","title":"find()"},{"location":"references/sql_sources/#viadot.sources.azure_data_lake.AzureDataLake.ls","text":"Returns list of files in a path. Parameters: Name Type Description Default path str Path to a folder. Defaults to None. None Source code in viadot/sources/azure_data_lake.py def ls ( self , path : str = None ) -> List [ str ]: \"\"\" Returns list of files in a path. Args: path (str, optional): Path to a folder. Defaults to None. \"\"\" path = path or self . path return self . fs . ls ( path )","title":"ls()"},{"location":"references/sql_sources/#viadot.sources.azure_data_lake.AzureDataLake.rm","text":"Deletes a file or directory from the path. Parameters: Name Type Description Default path str Path to a folder or file. Defaults to None. None recursive bool Whether to delete files recursively or not. Defaults to False. False Source code in viadot/sources/azure_data_lake.py def rm ( self , path : str = None , recursive : bool = False ): \"\"\" Deletes a file or directory from the path. Args: path (str, optional): Path to a folder or file. Defaults to None. recursive (bool, optional): Whether to delete files recursively or not. Defaults to False. \"\"\" path = path or self . path self . fs . rm ( path , recursive = recursive )","title":"rm()"},{"location":"references/sql_sources/#viadot.sources.azure_data_lake.AzureDataLake.upload","text":"Upload file(s) to the lake. Parameters: Name Type Description Default from_path str Path to the local file(s) to be uploaded. required to_path str Path to the destination file/folder None recursive bool Set this to true if working with directories. False overwrite bool Whether to overwrite the file(s) if they exist. False Examples: from viadot.sources import AzureDataLake lake = AzureDataLake () lake . upload ( from_path = 'tests/test.csv' , to_path = \"sandbox/test.csv\" ) Source code in viadot/sources/azure_data_lake.py def upload ( self , from_path : str , to_path : str = None , recursive : bool = False , overwrite : bool = False , ) -> None : \"\"\" Upload file(s) to the lake. Args: from_path (str): Path to the local file(s) to be uploaded. to_path (str): Path to the destination file/folder recursive (bool): Set this to true if working with directories. overwrite (bool): Whether to overwrite the file(s) if they exist. Example: ```python from viadot.sources import AzureDataLake lake = AzureDataLake() lake.upload(from_path='tests/test.csv', to_path=\"sandbox/test.csv\") ``` \"\"\" if self . gen == 1 : raise NotImplemented ( \"Azure Data Lake Gen1 does not support simple file upload.\" ) to_path = to_path or self . path self . fs . upload ( lpath = from_path , rpath = to_path , recursive = recursive , overwrite = overwrite , )","title":"upload()"},{"location":"references/sql_sources/#viadot.sources.azure_sql.AzureSQL","text":"","title":"AzureSQL"},{"location":"references/sql_sources/#viadot.sources.azure_sql.AzureSQL.bulk_insert","text":"Fuction to bulk insert. Parameters: Name Type Description Default table str Table name. required schema str Schema name. Defaults to None. None source_path str Full path to a data file. Defaults to one. None sep str field terminator to be used for char and widechar data files. Defaults to \" \". '\\t' if_exists Literal What to do if the table already exists. Defaults to \"append\". 'append' Source code in viadot/sources/azure_sql.py def bulk_insert ( self , table : str , schema : str = None , source_path : str = None , sep = \" \\t \" , if_exists : Literal = \"append\" , ): \"\"\"Fuction to bulk insert. Args: table (str): Table name. schema (str, optional): Schema name. Defaults to None. source_path (str, optional): Full path to a data file. Defaults to one. sep (str, optional): field terminator to be used for char and widechar data files. Defaults to \"\\t\". if_exists (Literal, optional): What to do if the table already exists. Defaults to \"append\". \"\"\" if schema is None : schema = self . DEFAULT_SCHEMA fqn = f \" { schema } . { table } \" insert_sql = f \"\"\" BULK INSERT { fqn } FROM ' { source_path } ' WITH ( CHECK_CONSTRAINTS, DATA_SOURCE=' { self . credentials [ 'data_source' ] } ', DATAFILETYPE='char', FIELDTERMINATOR=' { sep } ', ROWTERMINATOR='0x0a', FIRSTROW=2, KEEPIDENTITY, TABLOCK, CODEPAGE='65001' ); \"\"\" if if_exists == \"replace\" : self . run ( f \"DELETE FROM { schema } . { table } \" ) self . run ( insert_sql ) return True","title":"bulk_insert()"},{"location":"references/sql_sources/#viadot.sources.azure_sql.AzureSQL.create_external_database","text":"Create an external database. Used to eg. execute BULK INSERT or OPENROWSET queries. Parameters: Name Type Description Default external_database_name str The name of the extrnal source (db) to be created. required storage_account_name str The name of the Azure storage account. required container_name str The name of the container which should become the \"database\". required sas_token str The SAS token to be used as the credential. Note that the auth required master_key_password str The password for the database master key of your required credential_name str How to name the SAS credential. This is really an Azure None Source code in viadot/sources/azure_sql.py def create_external_database ( self , external_database_name : str , storage_account_name : str , container_name : str , sas_token : str , master_key_password : str , credential_name : str = None , ): \"\"\"Create an external database. Used to eg. execute BULK INSERT or OPENROWSET queries. Args: external_database_name (str): The name of the extrnal source (db) to be created. storage_account_name (str): The name of the Azure storage account. container_name (str): The name of the container which should become the \"database\". sas_token (str): The SAS token to be used as the credential. Note that the auth system in Azure is pretty broken and you might need to paste here your storage account's account key instead. master_key_password (str): The password for the database master key of your Azure SQL Database. credential_name (str): How to name the SAS credential. This is really an Azure internal thing and can be anything. By default '{external_database_name}_credential`. \"\"\" # stupid Microsoft thing if sas_token . startswith ( \"?\" ): sas_token = sas_token [ 1 :] if credential_name is None : credential_name = f \" { external_database_name } _credential\" create_master_key_sql = ( f \"CREATE MASTER KEY ENCRYPTION BY PASSWORD = { master_key_password } \" ) create_external_db_credential_sql = f \"\"\" CREATE DATABASE SCOPED CREDENTIAL { credential_name } WITH IDENTITY = 'SHARED ACCESS SIGNATURE' SECRET = ' { sas_token } '; \"\"\" create_external_db_sql = f \"\"\" CREATE EXTERNAL DATA SOURCE { external_database_name } WITH ( LOCATION = f'https:// { storage_account_name } .blob.core.windows.net/ { container_name } ', CREDENTIAL = { credential_name } ); \"\"\" self . run ( create_master_key_sql ) self . run ( create_external_db_credential_sql ) self . run ( create_external_db_sql )","title":"create_external_database()"},{"location":"references/sql_sources/#viadot.sources.sqlite.SQLite","text":"A SQLite source Parameters: Name Type Description Default server str server string, usually localhost required db str the file path to the db e.g. /home/somedb.sqlite required","title":"SQLite"},{"location":"references/sql_sources/#viadot.sources.sqlite.SQLite.conn_str","text":"Generate a connection string from params or config. Note that the user and password are escapedd with '{}' characters. Returns: Type Description str The ODBC connection string.","title":"conn_str"},{"location":"references/task_library/","text":"Task library viadot.tasks.open_apis.uk_carbon_intensity.StatsToCSV ( Task ) A Prefect task for downloading UK Carbon Instensity Statistics (stats) to a csv file. __call__ ( self ) special Run the task. Parameters path : str Path of the csv file created or edited by this task days_back : int, optional How many days of stats to download in the csv. UK Carbon Intensity statistics are available for up to 30 days, by default 10 Source code in viadot/tasks/open_apis/uk_carbon_intensity.py def __call__ ( self ): \"\"\" Run the task. Parameters ---------- path : str Path of the csv file created or edited by this task days_back : int, optional How many days of stats to download in the csv. UK Carbon Intensity statistics are available for up to 30 days, by default 10 \"\"\" __init__ ( self , * args , ** kwargs ) special Generate the task. Source code in viadot/tasks/open_apis/uk_carbon_intensity.py def __init__ ( self , * args , ** kwargs ): \"\"\"Generate the task.\"\"\" super () . __init__ ( name = \"uk_carbon_intensity_stats_to_csv\" , * args , ** kwargs ) run ( self , path , days_back = 10 ) Run the task. Parameters path : str Path of the csv file created or edited by this task days_back : int, optional How many days of stats to download in the csv. UK Carbon Intensity statistics are available for up to 30 days, by default 10 Source code in viadot/tasks/open_apis/uk_carbon_intensity.py def run ( self , path : str , days_back : int = 10 ): \"\"\" Run the task. Parameters ---------- path : str Path of the csv file created or edited by this task days_back : int, optional How many days of stats to download in the csv. UK Carbon Intensity statistics are available for up to 30 days, by default 10 \"\"\" logger = prefect . context . get ( \"logger\" ) carbon = UKCarbonIntensity () now = datetime . datetime . now () logger . info ( f \"Downloading data to { path } ...\" ) for i in range ( days_back ): from_delta = datetime . timedelta ( days = i + 1 ) to_delta = datetime . timedelta ( days = i ) to = now - to_delta from_ = now - from_delta carbon . query ( f \"/intensity/stats/ { from_ . isoformat () } / { to . isoformat () } \" ) carbon . to_csv ( path , if_exists = \"append\" ) # Download data to a local CSV file logger . info ( f \"Successfully downloaded data to { path } .\" ) viadot.tasks.open_apis.uk_carbon_intensity.StatsToExcel ( Task ) A Prefect task for downloading UK Carbon Instensity Statistics (stats) to a excel file. __call__ ( self ) special Run the task. Parameters path : str Path of the csv file created or edited by this task days_back : int, optional How many days of stats to download in the excel. UK Carbon Intensity statistics are available for up to 30 days, by default 10 Source code in viadot/tasks/open_apis/uk_carbon_intensity.py def __call__ ( self ): \"\"\" Run the task. Parameters ---------- path : str Path of the csv file created or edited by this task days_back : int, optional How many days of stats to download in the excel. UK Carbon Intensity statistics are available for up to 30 days, by default 10 \"\"\" __init__ ( self , * args , ** kwargs ) special Generate the task. Source code in viadot/tasks/open_apis/uk_carbon_intensity.py def __init__ ( self , * args , ** kwargs ): \"\"\"Generate the task.\"\"\" super () . __init__ ( name = \"uk_carbon_intensity_stats_to_excel\" , * args , ** kwargs ) run ( self , path , days_back = 10 ) Run the task. Parameters path : str Path of the excel file created or edited by this task days_back : int, optional How many days of stats to download in the excel. UK Carbon Intensity statistics are available for up to 30 days, by default 10 Source code in viadot/tasks/open_apis/uk_carbon_intensity.py def run ( self , path : str , days_back : int = 10 ): \"\"\" Run the task. Parameters ---------- path : str Path of the excel file created or edited by this task days_back : int, optional How many days of stats to download in the excel. UK Carbon Intensity statistics are available for up to 30 days, by default 10 \"\"\" logger = prefect . context . get ( \"logger\" ) carbon = UKCarbonIntensity () now = datetime . datetime . now () logger . info ( f \"Downloading data to { path } ...\" ) for i in range ( days_back ): from_delta = datetime . timedelta ( days = i + 1 ) to_delta = datetime . timedelta ( days = i ) to = now - to_delta from_ = now - from_delta carbon . query ( f \"/intensity/stats/ { from_ . isoformat () } / { to . isoformat () } \" ) carbon . to_excel ( path , if_exists = \"append\" ) # Download data to a local excel file logger . info ( f \"Successfully downloaded data to { path } .\" ) viadot.tasks.azure_data_lake.AzureDataLakeDownload ( Task ) Task for downloading data from the Azure Data lakes (gen1 and gen2). Parameters: Name Type Description Default from_path str The path from which to download the file(s). Defaults to None. required to_path str The destination path. Defaults to None. required recursive bool Set this to true if downloading entire directories. required gen int The generation of the Azure Data Lake. Defaults to 2. required vault_name str The name of the vault from which to fetch the secret. Defaults to None. required timeout(int, optional The amount of time (in seconds) to wait while running this task before a timeout occurs. Defaults to 3600. required max_retries int [description]. Defaults to 3. required retry_delay timedelta [description]. Defaults to timedelta(seconds=10). required __call__ ( self , * args , ** kwargs ) special Download file(s) from the Azure Data Lake Source code in viadot/tasks/azure_data_lake.py def __call__ ( self , * args , ** kwargs ): \"\"\"Download file(s) from the Azure Data Lake\"\"\" return super () . __call__ ( * args , ** kwargs ) run ( self , from_path = None , to_path = None , recursive = None , gen = None , sp_credentials_secret = None , vault_name = None , max_retries = None , retry_delay = None ) Task run method. Parameters: Name Type Description Default from_path str The path from which to download the file(s). None to_path str The destination path. None recursive bool Set this to true if downloading entire directories. None gen int The generation of the Azure Data Lake. None sp_credentials_secret str The name of the Azure Key Vault secret containing a dictionary with None vault_name str The name of the vault from which to obtain the secret. Defaults to None. None Source code in viadot/tasks/azure_data_lake.py @defaults_from_attrs ( \"from_path\" , \"to_path\" , \"recursive\" , \"gen\" , \"vault_name\" , \"max_retries\" , \"retry_delay\" , ) def run ( self , from_path : str = None , to_path : str = None , recursive : bool = None , gen : int = None , sp_credentials_secret : str = None , vault_name : str = None , max_retries : int = None , retry_delay : timedelta = None , ) -> None : \"\"\"Task run method. Args: from_path (str): The path from which to download the file(s). to_path (str): The destination path. recursive (bool): Set this to true if downloading entire directories. gen (int): The generation of the Azure Data Lake. sp_credentials_secret (str, optional): The name of the Azure Key Vault secret containing a dictionary with ACCOUNT_NAME and Service Principal credentials (TENANT_ID, CLIENT_ID, CLIENT_SECRET). Defaults to None. vault_name (str, optional): The name of the vault from which to obtain the secret. Defaults to None. \"\"\" file_name = from_path . split ( \"/\" )[ - 1 ] to_path = to_path or file_name if not sp_credentials_secret : # attempt to read a default for the service principal secret name try : sp_credentials_secret = PrefectSecret ( \"AZURE_DEFAULT_ADLS_SERVICE_PRINCIPAL_SECRET\" ) . run () except ValueError : pass if sp_credentials_secret : azure_secret_task = AzureKeyVaultSecret () credentials_str = azure_secret_task . run ( secret = sp_credentials_secret , vault_name = vault_name ) credentials = json . loads ( credentials_str ) else : credentials = { \"ACCOUNT_NAME\" : os . environ [ \"AZURE_ACCOUNT_NAME\" ], \"AZURE_TENANT_ID\" : os . environ [ \"AZURE_TENANT_ID\" ], \"AZURE_CLIENT_ID\" : os . environ [ \"AZURE_CLIENT_ID\" ], \"AZURE_CLIENT_SECRET\" : os . environ [ \"AZURE_CLIENT_SECRET\" ], } lake = AzureDataLake ( gen = gen , credentials = credentials ) full_dl_path = os . path . join ( credentials [ \"ACCOUNT_NAME\" ], from_path ) self . logger . info ( f \"Downloading data from { full_dl_path } to { to_path } ...\" ) lake . download ( from_path = from_path , to_path = to_path , recursive = recursive ) self . logger . info ( f \"Successfully downloaded data to { to_path } .\" ) viadot.tasks.azure_data_lake.AzureDataLakeUpload ( Task ) Upload file(s) to Azure Data Lake. Parameters: Name Type Description Default from_path str The local path from which to upload the file(s). Defaults to None. required to_path str The destination path. Defaults to None. required recursive bool Set this to true if uploading entire directories. Defaults to False. required overwrite bool Whether to overwrite files in the lake. Defaults to False. required gen int The generation of the Azure Data Lake. Defaults to 2. required vault_name str The name of the vault from which to obtain the secret. Defaults to None. required timeout(int, optional The amount of time (in seconds) to wait while running this task before a timeout occurs. Defaults to 3600. required __call__ ( self , * args , ** kwargs ) special Upload file(s) to the Azure Data Lake Source code in viadot/tasks/azure_data_lake.py def __call__ ( self , * args , ** kwargs ): \"\"\"Upload file(s) to the Azure Data Lake\"\"\" return super () . __call__ ( * args , ** kwargs ) run ( self , from_path = None , to_path = None , recursive = None , overwrite = None , gen = None , sp_credentials_secret = None , vault_name = None , max_retries = None , retry_delay = None ) Task run method. Parameters: Name Type Description Default from_path str The path from which to upload the file(s). None to_path str The destination path. None recursive bool Set to true if uploading entire directories. None overwrite bool Whether to overwrite the file(s) if they exist. None gen int The generation of the Azure Data Lake. None sp_credentials_secret str The name of the Azure Key Vault secret containing a dictionary with None vault_name str The name of the vault from which to obtain the secret. Defaults to None. None Source code in viadot/tasks/azure_data_lake.py @defaults_from_attrs ( \"from_path\" , \"to_path\" , \"recursive\" , \"overwrite\" , \"gen\" , \"vault_name\" , \"max_retries\" , \"retry_delay\" , ) def run ( self , from_path : str = None , to_path : str = None , recursive : bool = None , overwrite : bool = None , gen : int = None , sp_credentials_secret : str = None , vault_name : str = None , max_retries : int = None , retry_delay : timedelta = None , ) -> None : \"\"\"Task run method. Args: from_path (str): The path from which to upload the file(s). to_path (str): The destination path. recursive (bool): Set to true if uploading entire directories. overwrite (bool): Whether to overwrite the file(s) if they exist. gen (int): The generation of the Azure Data Lake. sp_credentials_secret (str, optional): The name of the Azure Key Vault secret containing a dictionary with ACCOUNT_NAME and Service Principal credentials (TENANT_ID, CLIENT_ID, CLIENT_SECRET). Defaults to None. vault_name (str, optional): The name of the vault from which to obtain the secret. Defaults to None. \"\"\" if not sp_credentials_secret : # attempt to read a default for the service principal secret name try : sp_credentials_secret = PrefectSecret ( \"AZURE_DEFAULT_ADLS_SERVICE_PRINCIPAL_SECRET\" ) . run () except ValueError : pass if sp_credentials_secret : azure_secret_task = AzureKeyVaultSecret () credentials_str = azure_secret_task . run ( secret = sp_credentials_secret , vault_name = vault_name ) credentials = json . loads ( credentials_str ) else : credentials = { \"ACCOUNT_NAME\" : os . environ [ \"AZURE_ACCOUNT_NAME\" ], \"AZURE_TENANT_ID\" : os . environ [ \"AZURE_TENANT_ID\" ], \"AZURE_CLIENT_ID\" : os . environ [ \"AZURE_CLIENT_ID\" ], \"AZURE_CLIENT_SECRET\" : os . environ [ \"AZURE_CLIENT_SECRET\" ], } lake = AzureDataLake ( gen = gen , credentials = credentials ) full_to_path = os . path . join ( credentials [ \"ACCOUNT_NAME\" ], to_path ) self . logger . info ( f \"Uploading data from { from_path } to { full_to_path } ...\" ) lake . upload ( from_path = from_path , to_path = to_path , recursive = recursive , overwrite = overwrite , ) self . logger . info ( f \"Successfully uploaded data to { full_to_path } .\" ) viadot.tasks.azure_data_lake.AzureDataLakeToDF ( Task ) __call__ ( self , * args , ** kwargs ) special Load file(s) from the Azure Data Lake to a pandas DataFrame. Source code in viadot/tasks/azure_data_lake.py def __call__ ( self , * args , ** kwargs ): \"\"\"Load file(s) from the Azure Data Lake to a pandas DataFrame.\"\"\" return super () . __call__ ( * args , ** kwargs ) __init__ ( self , path = None , sep = ' \\t ' , quoting = 0 , lineterminator = None , error_bad_lines = None , gen = 2 , vault_name = None , timeout = 3600 , max_retries = 3 , retry_delay = datetime . timedelta ( seconds = 10 ), * args , ** kwargs ) special Load file(s) from the Azure Data Lake to a pandas DataFrame. Currently supports CSV and parquet files. Parameters: Name Type Description Default path str The path from which to load the DataFrame. Defaults to None. None sep str The separator to use when reading a CSV file. Defaults to \" \". '\\t' quoting int The quoting mode to use when reading a CSV file. Defaults to 0. 0 lineterminator str The newline separator to use when reading a CSV file. Defaults to None. None error_bad_lines bool Whether to raise an exception on bad lines. Defaults to None. None gen int The generation of the Azure Data Lake. Defaults to 2. 2 vault_name str The name of the vault from which to obtain the secret. Defaults to None. None timeout(int, optional The amount of time (in seconds) to wait while running this task before a timeout occurs. Defaults to 3600. required Source code in viadot/tasks/azure_data_lake.py def __init__ ( self , path : str = None , sep : str = \" \\t \" , quoting : int = 0 , lineterminator : str = None , error_bad_lines : bool = None , gen : int = 2 , vault_name : str = None , timeout : int = 3600 , max_retries : int = 3 , retry_delay : timedelta = timedelta ( seconds = 10 ), * args , ** kwargs , ): \"\"\"Load file(s) from the Azure Data Lake to a pandas DataFrame. Currently supports CSV and parquet files. Args: path (str, optional): The path from which to load the DataFrame. Defaults to None. sep (str, optional): The separator to use when reading a CSV file. Defaults to \"\\t\". quoting (int, optional): The quoting mode to use when reading a CSV file. Defaults to 0. lineterminator (str, optional): The newline separator to use when reading a CSV file. Defaults to None. error_bad_lines (bool, optional): Whether to raise an exception on bad lines. Defaults to None. gen (int, optional): The generation of the Azure Data Lake. Defaults to 2. vault_name (str, optional): The name of the vault from which to obtain the secret. Defaults to None. timeout(int, optional): The amount of time (in seconds) to wait while running this task before a timeout occurs. Defaults to 3600. \"\"\" self . path = path self . sep = sep self . quoting = quoting self . lineterminator = lineterminator self . error_bad_lines = error_bad_lines self . gen = gen self . vault_name = vault_name super () . __init__ ( name = \"adls_to_df\" , max_retries = max_retries , retry_delay = retry_delay , timeout = timeout , * args , ** kwargs , ) run ( self , path = None , sep = None , quoting = None , lineterminator = None , error_bad_lines = None , gen = None , sp_credentials_secret = None , vault_name = None , max_retries = None , retry_delay = None ) Task run method. Parameters: Name Type Description Default path str The path to file(s) which should be loaded into a DataFrame. None sep str The field separator to use when loading the file to the DataFrame. None quoting int The quoting mode to use when reading a CSV file. Defaults to 0. None lineterminator str The newline separator to use when reading a CSV file. Defaults to None. None error_bad_lines bool Whether to raise an exception on bad lines. Defaults to None. None gen int The generation of the Azure Data Lake. None sp_credentials_secret str The name of the Azure Key Vault secret containing a dictionary with None vault_name str The name of the vault from which to obtain the secret. Defaults to None. None Source code in viadot/tasks/azure_data_lake.py @defaults_from_attrs ( \"path\" , \"sep\" , \"quoting\" , \"lineterminator\" , \"error_bad_lines\" , \"gen\" , \"vault_name\" , \"max_retries\" , \"retry_delay\" , ) def run ( self , path : str = None , sep : str = None , quoting : int = None , lineterminator : str = None , error_bad_lines : bool = None , gen : int = None , sp_credentials_secret : str = None , vault_name : str = None , max_retries : int = None , retry_delay : timedelta = None , ) -> pd . DataFrame : \"\"\"Task run method. Args: path (str): The path to file(s) which should be loaded into a DataFrame. sep (str): The field separator to use when loading the file to the DataFrame. quoting (int, optional): The quoting mode to use when reading a CSV file. Defaults to 0. lineterminator (str, optional): The newline separator to use when reading a CSV file. Defaults to None. error_bad_lines (bool, optional): Whether to raise an exception on bad lines. Defaults to None. gen (int): The generation of the Azure Data Lake. sp_credentials_secret (str, optional): The name of the Azure Key Vault secret containing a dictionary with ACCOUNT_NAME and Service Principal credentials (TENANT_ID, CLIENT_ID, CLIENT_SECRET). Defaults to None. vault_name (str, optional): The name of the vault from which to obtain the secret. Defaults to None. \"\"\" if quoting is None : quoting = 0 if path is None : raise ValueError ( \"Please provide the path to the file to be downloaded.\" ) if not sp_credentials_secret : # attempt to read a default for the service principal secret name try : sp_credentials_secret = PrefectSecret ( \"AZURE_DEFAULT_ADLS_SERVICE_PRINCIPAL_SECRET\" ) . run () except ValueError : pass if sp_credentials_secret : azure_secret_task = AzureKeyVaultSecret () credentials_str = azure_secret_task . run ( secret = sp_credentials_secret , vault_name = vault_name ) credentials = json . loads ( credentials_str ) else : credentials = { \"ACCOUNT_NAME\" : os . environ [ \"AZURE_ACCOUNT_NAME\" ], \"AZURE_TENANT_ID\" : os . environ [ \"AZURE_TENANT_ID\" ], \"AZURE_CLIENT_ID\" : os . environ [ \"AZURE_CLIENT_ID\" ], \"AZURE_CLIENT_SECRET\" : os . environ [ \"AZURE_CLIENT_SECRET\" ], } lake = AzureDataLake ( gen = gen , credentials = credentials , path = path ) full_dl_path = os . path . join ( credentials [ \"ACCOUNT_NAME\" ], path ) self . logger . info ( f \"Downloading data from { full_dl_path } to a DataFrame...\" ) df = lake . to_df ( sep = sep , quoting = quoting , lineterminator = lineterminator , error_bad_lines = error_bad_lines , ) self . logger . info ( f \"Successfully loaded data.\" ) return df viadot.tasks.azure_data_lake.AzureDataLakeCopy ( Task ) Task for copying data between the Azure Data lakes files. Parameters: Name Type Description Default from_path str The path from which to copy the file(s). Defaults to None. required to_path str The destination path. Defaults to None. required recursive bool Set this to true if copy entire directories. required gen int The generation of the Azure Data Lake. Defaults to 2. required vault_name str The name of the vault from which to fetch the secret. Defaults to None. required timeout(int, optional The amount of time (in seconds) to wait while running this task before a timeout occurs. Defaults to 3600. required max_retries int [description]. Defaults to 3. required retry_delay timedelta [description]. Defaults to timedelta(seconds=10). required __call__ ( self , * args , ** kwargs ) special Copy file(s) from the Azure Data Lake Source code in viadot/tasks/azure_data_lake.py def __call__ ( self , * args , ** kwargs ): \"\"\"Copy file(s) from the Azure Data Lake\"\"\" return super () . __call__ ( * args , ** kwargs ) run ( self , from_path = None , to_path = None , recursive = None , gen = None , sp_credentials_secret = None , vault_name = None , max_retries = None , retry_delay = None ) Task run method. Parameters: Name Type Description Default from_path str The path from which to copy the file(s). None to_path str The destination path. None recursive bool Set this to true if copying entire directories. None gen int The generation of the Azure Data Lake. None sp_credentials_secret str The name of the Azure Key Vault secret containing a dictionary with None vault_name str The name of the vault from which to obtain the secret. Defaults to None. None Source code in viadot/tasks/azure_data_lake.py @defaults_from_attrs ( \"from_path\" , \"to_path\" , \"recursive\" , \"gen\" , \"vault_name\" , \"max_retries\" , \"retry_delay\" , ) def run ( self , from_path : str = None , to_path : str = None , recursive : bool = None , gen : int = None , sp_credentials_secret : str = None , vault_name : str = None , max_retries : int = None , retry_delay : timedelta = None , ) -> None : \"\"\"Task run method. Args: from_path (str): The path from which to copy the file(s). to_path (str): The destination path. recursive (bool): Set this to true if copying entire directories. gen (int): The generation of the Azure Data Lake. sp_credentials_secret (str, optional): The name of the Azure Key Vault secret containing a dictionary with ACCOUNT_NAME and Service Principal credentials (TENANT_ID, CLIENT_ID, CLIENT_SECRET). Defaults to None. vault_name (str, optional): The name of the vault from which to obtain the secret. Defaults to None. \"\"\" file_name = from_path . split ( \"/\" )[ - 1 ] to_path = to_path or file_name if not sp_credentials_secret : # attempt to read a default for the service principal secret name try : sp_credentials_secret = PrefectSecret ( \"AZURE_DEFAULT_ADLS_SERVICE_PRINCIPAL_SECRET\" ) . run () except ValueError : pass if sp_credentials_secret : azure_secret_task = AzureKeyVaultSecret () credentials_str = azure_secret_task . run ( secret = sp_credentials_secret , vault_name = vault_name ) credentials = json . loads ( credentials_str ) else : credentials = { \"ACCOUNT_NAME\" : os . environ [ \"AZURE_ACCOUNT_NAME\" ], \"AZURE_TENANT_ID\" : os . environ [ \"AZURE_TENANT_ID\" ], \"AZURE_CLIENT_ID\" : os . environ [ \"AZURE_CLIENT_ID\" ], \"AZURE_CLIENT_SECRET\" : os . environ [ \"AZURE_CLIENT_SECRET\" ], } lake = AzureDataLake ( gen = gen , credentials = credentials ) full_dl_path = os . path . join ( credentials [ \"ACCOUNT_NAME\" ], from_path ) self . logger . info ( f \"Copying data from { full_dl_path } to { to_path } ...\" ) lake . cp ( from_path = from_path , to_path = to_path , recursive = recursive ) self . logger . info ( f \"Successfully copied data to { to_path } .\" ) viadot.tasks.azure_data_lake.AzureDataLakeList ( Task ) Task for listing files in Azure Data Lake. Parameters: Name Type Description Default path str The path to the directory which contents you want to list. Defaults to None. required gen int The generation of the Azure Data Lake. Defaults to 2. required vault_name str The name of the vault from which to fetch the secret. Defaults to None. required timeout(int, optional The amount of time (in seconds) to wait while running this task before a timeout occurs. Defaults to 3600. required max_retries int [description]. Defaults to 3. required retry_delay timedelta [description]. Defaults to timedelta(seconds=10). required Returns: Type Description List[str] The list of paths to the contents of path . These paths do not include the container, eg. the path to the file located at \"https://my_storage_acc.blob.core.windows.net/raw/supermetrics/test_file.txt\" will be shown as \"raw/supermetrics/test_file.txt\". run ( self , path = None , recursive = False , file_to_match = None , gen = None , sp_credentials_secret = None , vault_name = None , max_retries = None , retry_delay = None ) Task run method. Parameters: Name Type Description Default path str The path to the directory which contents you want to list. Defaults to None. None recursive bool If True, recursively list all subdirectories and files. Defaults to False. False file_to_match str If exist it only returns files with that name. Defaults to None. None gen int The generation of the Azure Data Lake. Defaults to None. None sp_credentials_secret str The name of the Azure Key Vault secret containing a dictionary with None vault_name str The name of the vault from which to obtain the secret. Defaults to None. None Returns: Type Description List[str] The list of paths to the contents of path . These paths do not include the container, eg. the path to the file located at \"https://my_storage_acc.blob.core.windows.net/raw/supermetrics/test_file.txt\" will be shown as \"raw/supermetrics/test_file.txt\". Source code in viadot/tasks/azure_data_lake.py @defaults_from_attrs ( \"path\" , \"gen\" , \"vault_name\" , \"max_retries\" , \"retry_delay\" , ) def run ( self , path : str = None , recursive : bool = False , file_to_match : str = None , gen : int = None , sp_credentials_secret : str = None , vault_name : str = None , max_retries : int = None , retry_delay : timedelta = None , ) -> List [ str ]: \"\"\"Task run method. Args: path (str, optional): The path to the directory which contents you want to list. Defaults to None. recursive (bool, optional): If True, recursively list all subdirectories and files. Defaults to False. file_to_match (str, optional): If exist it only returns files with that name. Defaults to None. gen (int): The generation of the Azure Data Lake. Defaults to None. sp_credentials_secret (str, optional): The name of the Azure Key Vault secret containing a dictionary with ACCOUNT_NAME and Service Principal credentials (TENANT_ID, CLIENT_ID, CLIENT_SECRET). Defaults to None. vault_name (str, optional): The name of the vault from which to obtain the secret. Defaults to None. Returns: List[str]: The list of paths to the contents of `path`. These paths do not include the container, eg. the path to the file located at \"https://my_storage_acc.blob.core.windows.net/raw/supermetrics/test_file.txt\" will be shown as \"raw/supermetrics/test_file.txt\". \"\"\" if not sp_credentials_secret : # attempt to read a default for the service principal secret name try : sp_credentials_secret = PrefectSecret ( \"AZURE_DEFAULT_ADLS_SERVICE_PRINCIPAL_SECRET\" ) . run () except ValueError : pass if sp_credentials_secret : azure_secret_task = AzureKeyVaultSecret () credentials_str = azure_secret_task . run ( secret = sp_credentials_secret , vault_name = vault_name ) credentials = json . loads ( credentials_str ) else : credentials = { \"ACCOUNT_NAME\" : os . environ [ \"AZURE_ACCOUNT_NAME\" ], \"AZURE_TENANT_ID\" : os . environ [ \"AZURE_TENANT_ID\" ], \"AZURE_CLIENT_ID\" : os . environ [ \"AZURE_CLIENT_ID\" ], \"AZURE_CLIENT_SECRET\" : os . environ [ \"AZURE_CLIENT_SECRET\" ], } lake = AzureDataLake ( gen = gen , credentials = credentials ) full_dl_path = os . path . join ( credentials [ \"ACCOUNT_NAME\" ], path ) self . logger . info ( f \"Listing files in { full_dl_path } .\" ) if recursive : self . logger . info ( \"Loading ADLS directories recursively.\" ) files = lake . find ( path ) if file_to_match : conditions = [ file_to_match in item for item in files ] valid_files = np . array ([]) if any ( conditions ): index = np . where ( conditions )[ 0 ] files = list ( np . append ( valid_files , [ files [ i ] for i in index ])) else : raise FileExistsError ( f \"There are not any available file named { file_to_match } .\" ) else : files = lake . ls ( path ) self . logger . info ( f \"Successfully listed files in { full_dl_path } .\" ) return files viadot.tasks.azure_key_vault.AzureKeyVaultSecret ( SecretBase ) Task for retrieving secrets from an Azure Key Vault and returning it as a dictionary. Note that all initialization arguments can optionally be provided or overwritten at runtime. For authentication, there are two options: you can set the AZURE_CREDENTIALS Prefect Secret containing your Azure Key Vault credentials which will be passed directly to SecretClient , or you can configure your flow's runtime environment for EnvironmentCredential . Parameters: Name Type Description Default - secret (str the name of the secret to retrieve required - vault_name (str the name of the vault from which to fetch the secret required - secret_client_kwargs (dict additional keyword arguments to forward to the SecretClient. required - **kwargs (dict additional keyword arguments to pass to the Task constructor required run ( self , secret = None , vault_name = None , credentials = None , max_retries = None , retry_delay = None ) Task run method. Parameters: Name Type Description Default - secret (str the name of the secret to retrieve required - vault_name (str the name of the vault from which to fetch the secret required - credentials (dict your Azure Key Vault credentials passed from an upstream Secret task. By default, credentials are read from the AZURE_CREDENTIALS Prefect Secret; this Secret must be a JSON string with the subkey KEY_VAULT and then vault_name containing three keys: AZURE_TENANT_ID , AZURE_CLIENT_ID , and AZURE_CLIENT_SECRET , which will be passed directly to SecretClient . If not provided here or in context, the task will fall back to Azure credentials discovery using EnvironmentCredential() . Example AZURE_CREDENTIALS environment variable: export AZURE_CREDENTIALS = '{\"KEY_VAULT\": {\"test_key_vault\": {\"AZURE_TENANT_ID\": \"a\", \"AZURE_CLIENT_ID\": \"b\", \"AZURE_CLIENT_SECRET\": \"c\"}}}' required Examples: from prefect import Flow from viadot.tasks import AzureKeyVaultSecret azure_secret_task = AzureKeyVaultSecret () with Flow ( name = \"example\" ) as f : secret = azure_secret_task ( secret = \"test\" , vault_name = \"my_vault_name\" ) out = f . run () Returns: Type Description - str the contents of this secret, as a string Source code in viadot/tasks/azure_key_vault.py @defaults_from_attrs ( \"secret\" , \"vault_name\" ) def run ( self , secret : str = None , vault_name : str = None , credentials : dict = None , max_retries : int = None , retry_delay : timedelta = None , ) -> str : \"\"\" Task run method. Args: - secret (str): the name of the secret to retrieve - vault_name (str): the name of the vault from which to fetch the secret - credentials (dict, optional): your Azure Key Vault credentials passed from an upstream Secret task. By default, credentials are read from the `AZURE_CREDENTIALS` Prefect Secret; this Secret must be a JSON string with the subkey `KEY_VAULT` and then vault_name containing three keys: `AZURE_TENANT_ID`, `AZURE_CLIENT_ID`, and `AZURE_CLIENT_SECRET`, which will be passed directly to `SecretClient`. If not provided here or in context, the task will fall back to Azure credentials discovery using `EnvironmentCredential()`. Example `AZURE_CREDENTIALS` environment variable: `export AZURE_CREDENTIALS = '{\"KEY_VAULT\": {\"test_key_vault\": {\"AZURE_TENANT_ID\": \"a\", \"AZURE_CLIENT_ID\": \"b\", \"AZURE_CLIENT_SECRET\": \"c\"}}}'` Example: ```python from prefect import Flow from viadot.tasks import AzureKeyVaultSecret azure_secret_task = AzureKeyVaultSecret() with Flow(name=\"example\") as f: secret = azure_secret_task(secret=\"test\", vault_name=\"my_vault_name\") out = f.run() ``` Returns: - str: the contents of this secret, as a string \"\"\" if secret is None : raise ValueError ( \"A secret name must be provided.\" ) key_vault = get_key_vault ( vault_name = vault_name , credentials = credentials , secret_client_kwargs = self . secret_client_kwargs , ) secret_string = key_vault . get_secret ( secret ) . value return secret_string viadot.tasks.azure_key_vault.CreateAzureKeyVaultSecret ( SecretBase ) Task for creating secrets in an Azure Key Vault. Note that all initialization arguments can optionally be provided or overwritten at runtime. For authentication, there are two options: you can set the AZURE_CREDENTIALS Prefect Secret containing your Azure Key Vault credentials which will be passed directly to SecretClient , or you can configure your flow's runtime environment for EnvironmentCredential . Parameters: Name Type Description Default - secret (str the name of the secret to retrieve required - vault_name (str the name of the vault from which to fetch the secret required - secret_client_kwargs (dict additional keyword arguments to forward to the SecretClient. required - **kwargs (dict additional keyword arguments to pass to the Task constructor required run ( self , secret = None , value = None , lifetime = None , vault_name = None , credentials = None , max_retries = None , retry_delay = None ) Task run method. Parameters: Name Type Description Default - secret (str the name of the secret to set required - value (str the value which the secret will hold required - lifetime (int The number of days after which the secret should expire. required - vault_name (str the name of the vault from which to fetch the secret required - credentials (dict your Azure Key Vault credentials passed from an upstream Secret task; this Secret must be a JSON string with the subkey KEY_VAULT and then vault_name containing three keys: AZURE_TENANT_ID , AZURE_CLIENT_ID , and AZURE_CLIENT_SECRET , which will be passed directly to SecretClient . If not provided here or in context, the task will fall back to Azure credentials discovery using EnvironmentCredential() . Example AZURE_CREDENTIALS environment variable: export AZURE_CREDENTIALS = '{\"KEY_VAULT\": {\"test_key_vault\": {\"AZURE_TENANT_ID\": \"a\", \"AZURE_CLIENT_ID\": \"b\", \"AZURE_CLIENT_SECRET\": \"c\"}}}' required Examples: from prefect import Flow from prefect.tasks.secrets import PrefectSecret from viadot.tasks import CreateAzureKeyVaultSecret create_secret_task = CreateAzureKeyVaultSecret () with Flow ( name = \"example\" ) as f : azure_credentials = PrefectSecret ( \"AZURE_CREDENTIALS\" ) secret = create_secret_task ( secret = \"test2\" , value = 42 , vault_name = \"my_vault_name\" , credentials = azure_credentials ) out = f . run () Returns: Type Description - bool Whether the secret was created successfully. Source code in viadot/tasks/azure_key_vault.py @defaults_from_attrs ( \"secret\" , \"value\" , \"lifetime\" , \"vault_name\" , ) def run ( self , secret : str = None , value : str = None , lifetime : int = None , vault_name : str = None , credentials : dict = None , max_retries : int = None , retry_delay : timedelta = None , ) -> bool : \"\"\" Task run method. Args: - secret (str): the name of the secret to set - value (str): the value which the secret will hold - lifetime (int): The number of days after which the secret should expire. - vault_name (str): the name of the vault from which to fetch the secret - credentials (dict, optional): your Azure Key Vault credentials passed from an upstream Secret task; this Secret must be a JSON string with the subkey `KEY_VAULT` and then vault_name containing three keys: `AZURE_TENANT_ID`, `AZURE_CLIENT_ID`, and `AZURE_CLIENT_SECRET`, which will be passed directly to `SecretClient`. If not provided here or in context, the task will fall back to Azure credentials discovery using `EnvironmentCredential()`. Example `AZURE_CREDENTIALS` environment variable: `export AZURE_CREDENTIALS = '{\"KEY_VAULT\": {\"test_key_vault\": {\"AZURE_TENANT_ID\": \"a\", \"AZURE_CLIENT_ID\": \"b\", \"AZURE_CLIENT_SECRET\": \"c\"}}}'` Example: ```python from prefect import Flow from prefect.tasks.secrets import PrefectSecret from viadot.tasks import CreateAzureKeyVaultSecret create_secret_task = CreateAzureKeyVaultSecret() with Flow(name=\"example\") as f: azure_credentials = PrefectSecret(\"AZURE_CREDENTIALS\") secret = create_secret_task(secret=\"test2\", value=42, vault_name=\"my_vault_name\", credentials=azure_credentials) out = f.run() ``` Returns: - bool: Whether the secret was created successfully. \"\"\" if secret is None : raise ValueError ( \"A secret name must be provided.\" ) key_vault = get_key_vault ( vault_name = vault_name , credentials = credentials , secret_client_kwargs = self . secret_client_kwargs , ) expires_on = pendulum . now ( \"UTC\" ) . add ( days = lifetime ) secret_obj = key_vault . set_secret ( secret , value , expires_on = expires_on ) was_successful = secret_obj . name == secret return was_successful viadot.tasks.azure_key_vault.DeleteAzureKeyVaultSecret ( SecretBase ) Task for removing (\"soft delete\") a secret from an Azure Key Vault. Note that all initialization arguments can optionally be provided or overwritten at runtime. For authentication, there are two options: you can set the AZURE_CREDENTIALS Prefect Secret containing your Azure Key Vault credentials which will be passed directly to SecretClient , or you can configure your flow's runtime environment for EnvironmentCredential . Parameters: Name Type Description Default - secret (str the name of the secret to retrieve required - vault_name (str the name of the vault from which to fetch the secret required - secret_client_kwargs (dict additional keyword arguments to forward to the SecretClient. required - **kwargs (dict additional keyword arguments to pass to the Task constructor required run ( self , secret = None , vault_name = None , credentials = None , max_retries = None , retry_delay = None ) Task run method. Parameters: Name Type Description Default - secret (str the name of the secret to delete required - vault_name (str the name of the vault whethe the secret is located required - credentials (dict your Azure Key Vault credentials passed from an upstream Secret task. By default, credentials are read from the AZURE_CREDENTIALS Prefect Secret; this Secret must be a JSON string with the subkey KEY_VAULT and then vault_name containing three keys: AZURE_TENANT_ID , AZURE_CLIENT_ID , and AZURE_CLIENT_SECRET , which will be passed directly to SecretClient . If not provided here or in context, the task will fall back to Azure credentials discovery using EnvironmentCredential() . Example AZURE_CREDENTIALS environment variable: export AZURE_CREDENTIALS = '{\"KEY_VAULT\": {\"test_key_vault\": {\"AZURE_TENANT_ID\": \"a\", \"AZURE_CLIENT_ID\": \"b\", \"AZURE_CLIENT_SECRET\": \"c\"}}}' required Examples: from prefect import Flow from viadot.tasks import DeleteAzureKeyVaultSecret azure_secret_task = DeleteAzureKeyVaultSecret () with Flow ( name = \"example\" ) as f : secret = azure_secret_task ( secret = \"test\" , vault_name = \"my_vault_name\" ) out = f . run () Returns: Type Description - bool Whether the secret was deleted successfully. Source code in viadot/tasks/azure_key_vault.py @defaults_from_attrs ( \"secret\" , \"vault_name\" ) def run ( self , secret : str = None , vault_name : str = None , credentials : dict = None , max_retries : int = None , retry_delay : timedelta = None , ) -> bool : \"\"\" Task run method. Args: - secret (str): the name of the secret to delete - vault_name (str): the name of the vault whethe the secret is located - credentials (dict, optional): your Azure Key Vault credentials passed from an upstream Secret task. By default, credentials are read from the `AZURE_CREDENTIALS` Prefect Secret; this Secret must be a JSON string with the subkey `KEY_VAULT` and then vault_name containing three keys: `AZURE_TENANT_ID`, `AZURE_CLIENT_ID`, and `AZURE_CLIENT_SECRET`, which will be passed directly to `SecretClient`. If not provided here or in context, the task will fall back to Azure credentials discovery using `EnvironmentCredential()`. Example `AZURE_CREDENTIALS` environment variable: `export AZURE_CREDENTIALS = '{\"KEY_VAULT\": {\"test_key_vault\": {\"AZURE_TENANT_ID\": \"a\", \"AZURE_CLIENT_ID\": \"b\", \"AZURE_CLIENT_SECRET\": \"c\"}}}'` Example: ```python from prefect import Flow from viadot.tasks import DeleteAzureKeyVaultSecret azure_secret_task = DeleteAzureKeyVaultSecret() with Flow(name=\"example\") as f: secret = azure_secret_task(secret=\"test\", vault_name=\"my_vault_name\") out = f.run() ``` Returns: - bool: Whether the secret was deleted successfully. \"\"\" if secret is None : raise ValueError ( \"A secret name must be provided.\" ) key_vault = get_key_vault ( vault_name = vault_name , credentials = credentials , secret_client_kwargs = self . secret_client_kwargs , ) poller = key_vault . begin_delete_secret ( secret ) poller . wait ( timeout = 60 * 5 ) was_successful = poller . status () == \"finished\" return was_successful viadot.tasks.azure_sql.AzureSQLBulkInsert ( Task ) run ( self , from_path = None , schema = None , table = None , dtypes = None , sep = None , if_exists = None , credentials_secret = None , vault_name = None ) Bulk insert data from Azure Data Lake into an Azure SQL Database table. This task also creates the table if it doesn't exist. Currently, only CSV files are supported. from_path (str): Path to the file to be inserted. schema (str): Destination schema. table (str): Destination table. dtypes (Dict[str, Any]): Data types to force. sep (str): The separator to use to read the CSV file. if_exists (Literal, optional): What to do if the table already exists. credentials_secret (str, optional): The name of the Azure Key Vault secret containing a dictionary with SQL db credentials (server, db_name, user, and password). vault_name (str, optional): The name of the vault from which to obtain the secret. Defaults to None. Source code in viadot/tasks/azure_sql.py @defaults_from_attrs ( \"sep\" , \"if_exists\" , \"credentials_secret\" ) def run ( self , from_path : str = None , schema : str = None , table : str = None , dtypes : Dict [ str , Any ] = None , sep : str = None , if_exists : Literal [ \"fail\" , \"replace\" , \"append\" , \"delete\" ] = None , credentials_secret : str = None , vault_name : str = None , ): \"\"\" Bulk insert data from Azure Data Lake into an Azure SQL Database table. This task also creates the table if it doesn't exist. Currently, only CSV files are supported. Args: from_path (str): Path to the file to be inserted. schema (str): Destination schema. table (str): Destination table. dtypes (Dict[str, Any]): Data types to force. sep (str): The separator to use to read the CSV file. if_exists (Literal, optional): What to do if the table already exists. credentials_secret (str, optional): The name of the Azure Key Vault secret containing a dictionary with SQL db credentials (server, db_name, user, and password). vault_name (str, optional): The name of the vault from which to obtain the secret. Defaults to None. \"\"\" fqn = f \" { schema } . { table } \" if schema else table credentials = get_credentials ( credentials_secret , vault_name = vault_name ) azure_sql = AzureSQL ( credentials = credentials ) if if_exists == \"replace\" : azure_sql . create_table ( schema = schema , table = table , dtypes = dtypes , if_exists = if_exists ) self . logger . info ( f \"Successfully created table { fqn } .\" ) azure_sql . bulk_insert ( schema = schema , table = table , source_path = from_path , sep = sep , if_exists = if_exists , ) self . logger . info ( f \"Successfully inserted data into { fqn } .\" ) viadot.tasks.azure_sql.AzureSQLCreateTable ( Task ) run ( self , schema = None , table = None , dtypes = None , if_exists = None , credentials_secret = None , vault_name = None , max_retries = None , retry_delay = None ) Create a table in Azure SQL Database. Parameters: Name Type Description Default schema str Destination schema. None table str Destination table. None dtypes Dict[str, Any] Data types to force. None if_exists Literal What to do if the table already exists. None credentials_secret str The name of the Azure Key Vault secret containing a dictionary None vault_name str The name of the vault from which to obtain the secret. Defaults to None. None Source code in viadot/tasks/azure_sql.py @defaults_from_attrs ( \"if_exists\" ) def run ( self , schema : str = None , table : str = None , dtypes : Dict [ str , Any ] = None , if_exists : Literal [ \"fail\" , \"replace\" , \"skip\" , \"delete\" ] = None , credentials_secret : str = None , vault_name : str = None , max_retries : int = None , retry_delay : timedelta = None , ): \"\"\" Create a table in Azure SQL Database. Args: schema (str, optional): Destination schema. table (str, optional): Destination table. dtypes (Dict[str, Any], optional): Data types to force. if_exists (Literal, optional): What to do if the table already exists. credentials_secret (str, optional): The name of the Azure Key Vault secret containing a dictionary with SQL db credentials (server, db_name, user, and password). vault_name (str, optional): The name of the vault from which to obtain the secret. Defaults to None. \"\"\" credentials = get_credentials ( credentials_secret , vault_name = vault_name ) azure_sql = AzureSQL ( credentials = credentials ) fqn = f \" { schema } . { table } \" if schema is not None else table created = azure_sql . create_table ( schema = schema , table = table , dtypes = dtypes , if_exists = if_exists ) if created : self . logger . info ( f \"Successfully created table { fqn } .\" ) else : self . logger . info ( f \"Table { fqn } has not been created as if_exists is set to { if_exists } .\" ) viadot.tasks.azure_sql.AzureSQLDBQuery ( Task ) Task for running an Azure SQL Database query. Parameters: Name Type Description Default query str, required The query to execute on the database. required credentials_secret str The name of the Azure Key Vault secret containing a dictionary required vault_name str The name of the vault from which to obtain the secret. Defaults to None. required timeout(int, optional The amount of time (in seconds) to wait while running this task before a timeout occurs. Defaults to 3600. required run ( self , query , credentials_secret = None , vault_name = None ) Run an Azure SQL Database query Parameters: Name Type Description Default query str, required The query to execute on the database. required credentials_secret str The name of the Azure Key Vault secret containing a dictionary None vault_name str The name of the vault from which to obtain the secret. Defaults to None. None Source code in viadot/tasks/azure_sql.py def run ( self , query : str , credentials_secret : str = None , vault_name : str = None , ): \"\"\"Run an Azure SQL Database query Args: query (str, required): The query to execute on the database. credentials_secret (str, optional): The name of the Azure Key Vault secret containing a dictionary with SQL db credentials (server, db_name, user, and password). vault_name (str, optional): The name of the vault from which to obtain the secret. Defaults to None. \"\"\" credentials = get_credentials ( credentials_secret , vault_name = vault_name ) azure_sql = AzureSQL ( credentials = credentials ) # run the query and fetch the results if it's a select result = azure_sql . run ( query ) self . logger . info ( f \"Successfully ran the query.\" ) return result viadot.tasks.bcp.BCPTask ( ShellTask ) Task for bulk inserting data into SQL Server-compatible databases. Parameters: Name Type Description Default - path (str The path to the local CSV file to be inserted. required - schema (str The destination schema. required - table (str The destination table. required - chunksize (int The chunk size to use. required - error_log_file_path (string Full path of an error file. Defaults to \"log_file.log\". required - on_error (Literal[\"skip\", \"fail\"] What to do if error occurs. Defaults to \"skip\". required - credentials (dict The credentials to use for connecting with the database. required - vault_name (str The name of the vault from which to fetch the secret. required - timeout(int The amount of time (in seconds) to wait while running this task before a timeout occurs. Defaults to 3600. required - **kwargs (dict Additional keyword arguments to pass to the Task constructor. required run ( self , path = None , schema = None , table = None , chunksize = None , error_log_file_path = None , on_error = None , credentials = None , credentials_secret = None , vault_name = None , max_retries = None , retry_delay = None , ** kwargs ) Task run method. path (str, optional): The path to the local CSV file to be inserted. schema (str, optional): The destination schema. table (str, optional): The destination table. chunksize (int, optional): The chunk size to use. By default 5000. error_log_file_path (string, optional): Full path of an error file. Defaults to \"log_file.log\". on_error (Literal, optional): What to do if error occur. Defaults to None. credentials (dict, optional): The credentials to use for connecting with SQL Server. credentials_secret (str, optional): The name of the Key Vault secret containing database credentials. (server, db_name, user, password) vault_name (str): The name of the vault from which to fetch the secret. Returns: Type Description str The output of the bcp CLI command. Source code in viadot/tasks/bcp.py @defaults_from_attrs ( \"path\" , \"schema\" , \"table\" , \"chunksize\" , \"error_log_file_path\" , \"on_error\" , \"credentials\" , \"vault_name\" , \"max_retries\" , \"retry_delay\" , ) def run ( self , path : str = None , schema : str = None , table : str = None , chunksize : int = None , error_log_file_path : str = None , on_error : Literal = None , credentials : dict = None , credentials_secret : str = None , vault_name : str = None , max_retries : int = None , retry_delay : timedelta = None , ** kwargs , ) -> str : \"\"\" Task run method. Args: - path (str, optional): The path to the local CSV file to be inserted. - schema (str, optional): The destination schema. - table (str, optional): The destination table. - chunksize (int, optional): The chunk size to use. By default 5000. - error_log_file_path (string, optional): Full path of an error file. Defaults to \"log_file.log\". - on_error (Literal, optional): What to do if error occur. Defaults to None. - credentials (dict, optional): The credentials to use for connecting with SQL Server. - credentials_secret (str, optional): The name of the Key Vault secret containing database credentials. (server, db_name, user, password) - vault_name (str): The name of the vault from which to fetch the secret. Returns: str: The output of the bcp CLI command. \"\"\" if not credentials : if not credentials_secret : # attempt to read a default for the service principal secret name try : credentials_secret = PrefectSecret ( \"AZURE_DEFAULT_SQLDB_SERVICE_PRINCIPAL_SECRET\" ) . run () except ValueError : pass if credentials_secret : credentials_str = AzureKeyVaultSecret ( credentials_secret , vault_name = vault_name ) . run () credentials = json . loads ( credentials_str ) fqn = f \" { schema } . { table } \" if schema else table server = credentials [ \"server\" ] db_name = credentials [ \"db_name\" ] uid = credentials [ \"user\" ] pwd = credentials [ \"password\" ] if \",\" in server : # A space after the comma is allowed in the ODBC connection string # but not in BCP's 'server' argument. server = server . replace ( \" \" , \"\" ) if on_error == \"skip\" : max_error = 0 elif on_error == \"fail\" : max_error = 1 else : raise ValueError ( \"Please provide correct 'on_error' parameter value - 'skip' or 'fail'. \" ) command = f \"/opt/mssql-tools/bin/bcp { fqn } in ' { path } ' -S { server } -d { db_name } -U { uid } -P ' { pwd } ' -c -F 2 -b { chunksize } -h 'TABLOCK' -e ' { error_log_file_path } ' -m { max_error } \" run_command = super () . run ( command = command , ** kwargs ) try : parse_logs ( error_log_file_path ) except : logger . warning ( \"BCP logs couldn't be parsed.\" ) return run_command viadot.tasks.great_expectations.RunGreatExpectationsValidation ( RunGreatExpectationsValidation ) Task for running data validation with Great Expectations on a pandas DataFrame. See https://docs.prefect.io/api/latest/tasks/great_expectations.html#rungreatexpectationsvalidation for full documentation. Parameters: Name Type Description Default expectations_path str The path to the directory containing the expectation suites. required df pd.DataFrame The DataFrame to validate. required viadot.tasks.sqlite.SQLiteInsert ( Task ) Task for inserting data from a pandas DataFrame into SQLite. Parameters: Name Type Description Default db_path str The path to the database to be used. Defaults to None. required sql_path str The path to the text file containing the query. Defaults to None. required timeout(int, optional The amount of time (in seconds) to wait while running this task before a timeout occurs. Defaults to 3600. required viadot.tasks.sqlite.SQLiteSQLtoDF ( Task ) Task for downloading data from the SQLite to a pandas DataFrame. SQLite will create a new database in the directory specified by the 'db_path' parameter. Parameters: Name Type Description Default db_path str The path to the database to be used. Defaults to None. required sql_path str The path to the text file containing the query. Defaults to None. required timeout(int, optional The amount of time (in seconds) to wait while running this task before a timeout occurs. Defaults to 3600. required __call__ ( self ) special Generate a DataFrame from a SQLite SQL query Source code in viadot/tasks/sqlite.py def __call__ ( self ): \"\"\"Generate a DataFrame from a SQLite SQL query\"\"\" viadot.tasks.supermetrics.SupermetricsToCSV ( Task ) Task to downloading data from Supermetrics API to CSV file. Parameters: Name Type Description Default path str The destination path. Defaults to \"supermetrics_extract.csv\". required max_retries int The maximum number of retries. Defaults to 5. required retry_delay timedelta The delay between task retries. Defaults to 10 seconds. required timeout int The amount of time (in seconds) to wait while running this task before a timeout occurs. Defaults to 3600. required max_rows int Maximum number of rows the query results should contain. Defaults to 1 000 000. required max_cols int Maximum number of columns the query results should contain. Defaults to None. required if_exists str What to do if file already exists. Defaults to \"replace\". required if_empty str What to do if query returns no data. Defaults to \"warn\". required sep str The separator in a target csv file. Defaults to \"/t\". required __call__ ( self ) special Download Supermetrics data to a CSV Source code in viadot/tasks/supermetrics.py def __call__ ( self ): \"\"\"Download Supermetrics data to a CSV\"\"\" super () . __call__ ( self ) run ( self , path = None , ds_id = None , ds_accounts = None , ds_segments = None , ds_user = None , fields = None , date_range_type = None , start_date = None , end_date = None , settings = None , filter = None , max_rows = None , max_columns = None , order_columns = None , if_exists = None , if_empty = None , max_retries = None , retry_delay = None , timeout = None , sep = None ) Task run method. Parameters: Name Type Description Default path str The destination path. Defaulrs to None None ds_id str A Supermetrics query parameter. None ds_accounts Union[str, List[str]] A Supermetrics query parameter. Defaults to None. None ds_segments List[str] A Supermetrics query parameter. Defaults to None. None ds_user str A Supermetrics query parameter. Defaults to None. None fields List[str] A Supermetrics query parameter. Defaults to None. None date_range_type str A Supermetrics query parameter. Defaults to None. None start_date str A Supermetrics query parameter. Defaults to None. None settings Dict[str, Any] A Supermetrics query parameter. Defaults to None. None filter str A Supermetrics query parameter. Defaults to None. None max_rows int A Supermetrics query parameter. Defaults to None. None max_columns int A Supermetrics query parameter. Defaults to None. None order_columns str A Supermetrics query parameter. Defaults to None. None if_exists str What to do if file already exists. Defaults to \"replace\". None if_empty str What to do if query returns no data. Defaults to \"warn\". None max_retries int The maximum number of retries. Defaults to 5. None retry_delay timedelta The delay between task retries. Defaults to 10 seconds. None timeout int Task timeout. Defaults to 30 minuntes. None Source code in viadot/tasks/supermetrics.py @defaults_from_attrs ( \"path\" , \"max_rows\" , \"if_exists\" , \"if_empty\" , \"max_retries\" , \"retry_delay\" , \"timeout\" , \"sep\" , ) def run ( self , path : str = None , ds_id : str = None , ds_accounts : Union [ str , List [ str ]] = None , ds_segments : List [ str ] = None , ds_user : str = None , fields : List [ str ] = None , date_range_type : str = None , start_date : str = None , end_date : str = None , settings : Dict [ str , Any ] = None , filter : str = None , max_rows : int = None , max_columns : int = None , order_columns : str = None , if_exists : str = None , if_empty : str = None , max_retries : int = None , retry_delay : timedelta = None , timeout : int = None , sep : str = None , ): \"\"\" Task run method. Args: path (str, optional): The destination path. Defaulrs to None ds_id (str, optional): A Supermetrics query parameter. ds_accounts (Union[str, List[str]], optional): A Supermetrics query parameter. Defaults to None. ds_segments (List[str], optional): A Supermetrics query parameter. Defaults to None. ds_user (str, optional): A Supermetrics query parameter. Defaults to None. fields (List[str], optional): A Supermetrics query parameter. Defaults to None. date_range_type (str, optional): A Supermetrics query parameter. Defaults to None. start_date (str, optional): A Supermetrics query parameter. Defaults to None. end_date (str, optional) A Supermetrics query parameter. Defaults to None. settings (Dict[str, Any], optional): A Supermetrics query parameter. Defaults to None. filter (str, optional): A Supermetrics query parameter. Defaults to None. max_rows (int, optional): A Supermetrics query parameter. Defaults to None. max_columns (int, optional): A Supermetrics query parameter. Defaults to None. order_columns (str, optional): A Supermetrics query parameter. Defaults to None. if_exists (str, optional): What to do if file already exists. Defaults to \"replace\". if_empty (str, optional): What to do if query returns no data. Defaults to \"warn\". max_retries (int, optional): The maximum number of retries. Defaults to 5. retry_delay (timedelta, optional): The delay between task retries. Defaults to 10 seconds. timeout (int, optional): Task timeout. Defaults to 30 minuntes. sep (str, optional) \"\"\" if max_retries : self . max_retries = max_retries if retry_delay : self . retry_delay = retry_delay if isinstance ( ds_accounts , str ): ds_accounts = [ ds_accounts ] # Build the URL # Note the task accepts only one account per query query = dict ( ds_id = ds_id , ds_accounts = ds_accounts , ds_segments = ds_segments , ds_user = ds_user , fields = fields , date_range_type = date_range_type , start_date = start_date , end_date = end_date , settings = settings , filter = filter , max_rows = max_rows , max_columns = max_columns , order_columns = order_columns , ) query = { param : val for param , val in query . items () if val is not None } supermetrics = Supermetrics () supermetrics . query ( query ) # Download data to a local CSV file self . logger . info ( f \"Downloading data to { path } ...\" ) supermetrics . to_csv ( path , if_exists = if_exists , if_empty = if_empty , sep = sep ) self . logger . info ( f \"Successfully downloaded data to { path } .\" ) viadot.tasks.supermetrics.SupermetricsToDF ( Task ) Task for downloading data from the Supermetrics API to a pandas DataFrame. Parameters: Name Type Description Default ds_id str A Supermetrics query parameter. required ds_accounts Union[str, List[str]] A Supermetrics query parameter. Defaults to None. required ds_segments List[str] A Supermetrics query parameter. Defaults to None. required ds_user str A Supermetrics query parameter. Defaults to None. required fields List[str] A Supermetrics query parameter. Defaults to None. required date_range_type str A Supermetrics query parameter. Defaults to None. required settings Dict[str, Any] A Supermetrics query parameter. Defaults to None. required filter str A Supermetrics query parameter. Defaults to None. required max_rows int A Supermetrics query parameter. Defaults to None. required max_columns int A Supermetrics query parameter. Defaults to None. required order_columns str A Supermetrics query parameter. Defaults to None. required if_empty str What to do if query returns no data. Defaults to \"warn\". required max_retries int The maximum number of retries. Defaults to 5. required retry_delay timedelta The delay between task retries. Defaults to 10 seconds. required timeout int The amount of time (in seconds) to wait while running this task before a timeout occurs. Defaults to 3600. required run ( self , ds_id = None , ds_accounts = None , ds_segments = None , ds_user = None , fields = None , date_range_type = None , start_date = None , end_date = None , settings = None , filter = None , max_rows = None , max_columns = None , order_columns = None , if_empty = None , max_retries = None , retry_delay = None , timeout = None ) Task run method. Parameters: Name Type Description Default ds_id str A Supermetrics query parameter. None ds_accounts Union[str, List[str]] A Supermetrics query parameter. Defaults to None. None ds_segments List[str] A Supermetrics query parameter. Defaults to None. None ds_user str A Supermetrics query parameter. Defaults to None. None fields List[str] A Supermetrics query parameter. Defaults to None. None date_range_type str A Supermetrics query parameter. Defaults to None. None start_date str A query paramter to pass start date to the date range filter. Defaults to None. None end_date str A query paramter to pass end date to the date range filter. Defaults to None. None settings Dict[str, Any] A Supermetrics query parameter. Defaults to None. None filter str A Supermetrics query parameter. Defaults to None. None max_rows int A Supermetrics query parameter. Defaults to None. None max_columns int A Supermetrics query parameter. Defaults to None. None order_columns str A Supermetrics query parameter. Defaults to None. None if_empty str What to do if query returns no data. Defaults to \"warn\". None max_retries int The maximum number of retries. Defaults to 5. None retry_delay timedelta The delay between task retries. Defaults to 10 seconds. None timeout int Task timeout. Defaults to 30 minuntes. None Returns: Type Description pd.DataFrame The query result as a pandas DataFrame. Source code in viadot/tasks/supermetrics.py @defaults_from_attrs ( \"if_empty\" , \"max_rows\" , \"max_retries\" , \"retry_delay\" , \"timeout\" , ) def run ( self , ds_id : str = None , ds_accounts : Union [ str , List [ str ]] = None , ds_segments : List [ str ] = None , ds_user : str = None , fields : List [ str ] = None , date_range_type : str = None , start_date : str = None , end_date : str = None , settings : Dict [ str , Any ] = None , filter : str = None , max_rows : int = None , max_columns : int = None , order_columns : str = None , if_empty : str = None , max_retries : int = None , retry_delay : timedelta = None , timeout : int = None , ) -> pd . DataFrame : \"\"\" Task run method. Args: ds_id (str, optional): A Supermetrics query parameter. ds_accounts (Union[str, List[str]], optional): A Supermetrics query parameter. Defaults to None. ds_segments (List[str], optional): A Supermetrics query parameter. Defaults to None. ds_user (str, optional): A Supermetrics query parameter. Defaults to None. fields (List[str], optional): A Supermetrics query parameter. Defaults to None. date_range_type (str, optional): A Supermetrics query parameter. Defaults to None. start_date (str, optional): A query paramter to pass start date to the date range filter. Defaults to None. end_date (str, optional): A query paramter to pass end date to the date range filter. Defaults to None. settings (Dict[str, Any], optional): A Supermetrics query parameter. Defaults to None. filter (str, optional): A Supermetrics query parameter. Defaults to None. max_rows (int, optional): A Supermetrics query parameter. Defaults to None. max_columns (int, optional): A Supermetrics query parameter. Defaults to None. order_columns (str, optional): A Supermetrics query parameter. Defaults to None. if_empty (str, optional): What to do if query returns no data. Defaults to \"warn\". max_retries (int, optional): The maximum number of retries. Defaults to 5. retry_delay (timedelta, optional): The delay between task retries. Defaults to 10 seconds. timeout (int, optional): Task timeout. Defaults to 30 minuntes. Returns: pd.DataFrame: The query result as a pandas DataFrame. \"\"\" if max_retries : self . max_retries = max_retries if retry_delay : self . retry_delay = retry_delay if isinstance ( ds_accounts , str ): ds_accounts = [ ds_accounts ] # Build the URL # Note the task accepts only one account per query query = dict ( ds_id = ds_id , ds_accounts = ds_accounts , ds_segments = ds_segments , ds_user = ds_user , fields = fields , date_range_type = date_range_type , start_date = start_date , end_date = end_date , settings = settings , filter = filter , max_rows = max_rows , max_columns = max_columns , order_columns = order_columns , ) query = { param : val for param , val in query . items () if val is not None } supermetrics = Supermetrics () supermetrics . query ( query ) # Download data to a local CSV file self . logger . info ( f \"Downloading data to a DataFrame...\" ) df = supermetrics . to_df ( if_empty = if_empty ) self . logger . info ( f \"Successfully downloaded data to a DataFrame.\" ) return df viadot . task_utils . add_ingestion_metadata_task ( df ) Add ingestion metadata columns, eg. data download date Parameters: Name Type Description Default df pd.DataFrame input DataFrame. required Source code in viadot/task_utils.py @task ( timeout = 3600 ) def add_ingestion_metadata_task ( df : pd . DataFrame , ): \"\"\"Add ingestion metadata columns, eg. data download date Args: df (pd.DataFrame): input DataFrame. \"\"\" # Don't skip when df has columns but has no data if len ( df . columns ) == 0 : return df else : df2 = df . copy ( deep = True ) df2 [ \"_viadot_downloaded_at_utc\" ] = datetime . now ( timezone . utc ) . replace ( microsecond = 0 ) return df2 viadot . task_utils . get_latest_timestamp_file_path ( files ) Return the name of the latest file in a given data lake directory, given a list of paths in that directory. Such list can be obtained using the AzureDataLakeList task. This task is useful for working with immutable data lakes as the data is often written in the format /path/table_name/TIMESTAMP.parquet. Source code in viadot/task_utils.py @task ( timeout = 3600 ) def get_latest_timestamp_file_path ( files : List [ str ]) -> str : \"\"\" Return the name of the latest file in a given data lake directory, given a list of paths in that directory. Such list can be obtained using the `AzureDataLakeList` task. This task is useful for working with immutable data lakes as the data is often written in the format /path/table_name/TIMESTAMP.parquet. \"\"\" logger = prefect . context . get ( \"logger\" ) extract_fname = ( lambda f : os . path . basename ( f ) . replace ( \".csv\" , \"\" ) . replace ( \".parquet\" , \"\" ) ) file_names = [ extract_fname ( file ) for file in files ] latest_file_name = max ( file_names , key = lambda d : datetime . fromisoformat ( d )) latest_file = files [ file_names . index ( latest_file_name )] logger . debug ( f \"Latest file: { latest_file } \" ) return latest_file viadot.tasks.cloud_for_customers.C4CToDF ( Task ) run ( self , url = None , env = 'QA' , endpoint = None , fields = None , params = None , chunksize = None , if_empty = 'warn' , credentials_secret = None , vault_name = None ) Task for downloading data from the Cloud for Customers to a pandas DataFrame using normal URL (with query parameters). This task grab data from table from 'scratch' with passing table name in url or endpoint. It is rocommended to add some filters parameters in this case. Examples: url = \"https://mysource.com/sap/c4c/odata/v1/c4codataapi\" endpoint = \"ServiceRequestCollection\" params = {\"$filter\": \"CreationDateTime ge 2021-12-21T00:00:00Z\"} Parameters: Name Type Description Default url str The url to the API in case of prepared report. Defaults to None. None env str The environment to use. Defaults to 'QA'. 'QA' endpoint str The endpoint of the API. Defaults to None. None fields List[str] The C4C Table fields. Defaults to None. None params Dict[str, str] Query parameters. Defaults to $format=json. None chunksize int How many rows to retrieve from C4C at a time. Uses a server-side cursor. None if_empty str What to do if query returns no data. Defaults to \"warn\". 'warn' credentials_secret str The name of the Azure Key Vault secret containing a dictionary None vault_name str The name of the vault from which to obtain the secret. Defaults to None. None Returns: Type Description pd.DataFrame The query result as a pandas DataFrame. Source code in viadot/tasks/cloud_for_customers.py @defaults_from_attrs ( \"url\" , \"endpoint\" , \"fields\" , \"params\" , \"chunksize\" , \"env\" , \"if_empty\" ) def run ( self , url : str = None , env : str = \"QA\" , endpoint : str = None , fields : List [ str ] = None , params : Dict [ str , str ] = None , chunksize : int = None , if_empty : str = \"warn\" , credentials_secret : str = None , vault_name : str = None , ): \"\"\" Task for downloading data from the Cloud for Customers to a pandas DataFrame using normal URL (with query parameters). This task grab data from table from 'scratch' with passing table name in url or endpoint. It is rocommended to add some filters parameters in this case. Example: url = \"https://mysource.com/sap/c4c/odata/v1/c4codataapi\" endpoint = \"ServiceRequestCollection\" params = {\"$filter\": \"CreationDateTime ge 2021-12-21T00:00:00Z\"} Args: url (str, optional): The url to the API in case of prepared report. Defaults to None. env (str, optional): The environment to use. Defaults to 'QA'. endpoint (str, optional): The endpoint of the API. Defaults to None. fields (List[str], optional): The C4C Table fields. Defaults to None. params (Dict[str, str]): Query parameters. Defaults to $format=json. chunksize (int, optional): How many rows to retrieve from C4C at a time. Uses a server-side cursor. if_empty (str, optional): What to do if query returns no data. Defaults to \"warn\". credentials_secret (str, optional): The name of the Azure Key Vault secret containing a dictionary with C4C credentials. Defaults to None. vault_name (str, optional): The name of the vault from which to obtain the secret. Defaults to None. Returns: pd.DataFrame: The query result as a pandas DataFrame. \"\"\" if not credentials_secret : try : credentials_secret = PrefectSecret ( \"C4C_KV\" ) . run () except ValueError : pass if credentials_secret : credentials_str = AzureKeyVaultSecret ( credentials_secret , vault_name = vault_name ) . run () credentials = json . loads ( credentials_str ) else : credentials = local_config . get ( \"CLOUD_FOR_CUSTOMERS\" )[ env ] self . logger . info ( f \"Downloading data from { url + endpoint } ...\" ) # If we get any of these in params, we don't perform any chunking if any ([ \"$skip\" in params , \"$top\" in params ]): return CloudForCustomers ( url = url , endpoint = endpoint , params = params , env = env , credentials = credentials , ) . to_df ( if_empty = if_empty , fields = fields ) def _generate_chunks () -> Generator [ pd . DataFrame , None , None ]: \"\"\" Util returning chunks as a generator to save memory. \"\"\" offset = 0 total_record_count = 0 while True : boundaries = { \"$skip\" : offset , \"$top\" : chunksize } params . update ( boundaries ) chunk = CloudForCustomers ( url = url , endpoint = endpoint , params = params , env = env , credentials = credentials , ) . to_df ( if_empty = if_empty , fields = fields ) chunk_record_count = chunk . shape [ 0 ] total_record_count += chunk_record_count self . logger . info ( f \"Successfully downloaded { total_record_count } records.\" ) yield chunk if chunk . shape [ 0 ] < chunksize : break offset += chunksize self . logger . info ( f \"Data from { url + endpoint } has been downloaded successfully.\" ) chunks = _generate_chunks () df = pd . concat ( chunks ) return df viadot.tasks.cloud_for_customers.C4CReportToDF ( Task ) __call__ ( self , * args , ** kwargs ) special Download report to DF Source code in viadot/tasks/cloud_for_customers.py def __call__ ( self , * args , ** kwargs ): \"\"\"Download report to DF\"\"\" return super () . __call__ ( * args , ** kwargs ) run ( self , report_url = None , env = 'QA' , skip = 0 , top = 1000 , credentials_secret = None , vault_name = None , max_retries = 3 , retry_delay = datetime . timedelta ( seconds = 10 )) Task for downloading data from the Cloud for Customers to a pandas DataFrame using report URL. Parameters: Name Type Description Default report_url str The URL to the report. Defaults to None. None env str The environment to use. Defaults to 'QA'. 'QA' skip int Initial index value of reading row. Defaults to 0. 0 top int The value of top reading row. Defaults to 1000. 1000 credentials_secret str The name of the Azure Key Vault secret containing a dictionary None vault_name str The name of the vault from which to obtain the secret. Defaults to None. None Returns: Type Description pd.DataFrame The query result as a pandas DataFrame. Source code in viadot/tasks/cloud_for_customers.py @defaults_from_attrs ( \"report_url\" , \"env\" , \"skip\" , \"top\" , ) def run ( self , report_url : str = None , env : str = \"QA\" , skip : int = 0 , top : int = 1000 , credentials_secret : str = None , vault_name : str = None , max_retries : int = 3 , retry_delay : timedelta = timedelta ( seconds = 10 ), ): \"\"\" Task for downloading data from the Cloud for Customers to a pandas DataFrame using report URL. Args: report_url (str, optional): The URL to the report. Defaults to None. env (str, optional): The environment to use. Defaults to 'QA'. skip (int, optional): Initial index value of reading row. Defaults to 0. top (int, optional): The value of top reading row. Defaults to 1000. credentials_secret (str, optional): The name of the Azure Key Vault secret containing a dictionary with C4C credentials (username & password). Defaults to None. vault_name (str, optional): The name of the vault from which to obtain the secret. Defaults to None. Returns: pd.DataFrame: The query result as a pandas DataFrame. \"\"\" if not credentials_secret : try : credentials_secret = PrefectSecret ( \"C4C_KV\" ) . run () except ValueError : pass if credentials_secret : credentials_str = AzureKeyVaultSecret ( credentials_secret , vault_name = vault_name ) . run () credentials = json . loads ( credentials_str ) else : credentials = local_config . get ( \"CLOUD_FOR_CUSTOMERS\" )[ env ] final_df = pd . DataFrame () next_batch = True while next_batch : new_url = f \" { report_url } &$top= { top } &$skip= { skip } \" chunk_from_url = CloudForCustomers ( report_url = new_url , env = env , credentials = credentials ) df = chunk_from_url . to_df () final_df = final_df . append ( df ) if not final_df . empty : df_count = df . count ()[ 1 ] if df_count != top : next_batch = False skip += top else : break return final_df","title":"Task library"},{"location":"references/task_library/#task-library","text":"","title":"Task library"},{"location":"references/task_library/#viadot.tasks.open_apis.uk_carbon_intensity.StatsToCSV","text":"A Prefect task for downloading UK Carbon Instensity Statistics (stats) to a csv file.","title":"StatsToCSV"},{"location":"references/task_library/#viadot.tasks.open_apis.uk_carbon_intensity.StatsToCSV.__call__","text":"Run the task.","title":"__call__()"},{"location":"references/task_library/#viadot.tasks.open_apis.uk_carbon_intensity.StatsToCSV.__call__--parameters","text":"path : str Path of the csv file created or edited by this task days_back : int, optional How many days of stats to download in the csv. UK Carbon Intensity statistics are available for up to 30 days, by default 10 Source code in viadot/tasks/open_apis/uk_carbon_intensity.py def __call__ ( self ): \"\"\" Run the task. Parameters ---------- path : str Path of the csv file created or edited by this task days_back : int, optional How many days of stats to download in the csv. UK Carbon Intensity statistics are available for up to 30 days, by default 10 \"\"\"","title":"Parameters"},{"location":"references/task_library/#viadot.tasks.open_apis.uk_carbon_intensity.StatsToCSV.__init__","text":"Generate the task. Source code in viadot/tasks/open_apis/uk_carbon_intensity.py def __init__ ( self , * args , ** kwargs ): \"\"\"Generate the task.\"\"\" super () . __init__ ( name = \"uk_carbon_intensity_stats_to_csv\" , * args , ** kwargs )","title":"__init__()"},{"location":"references/task_library/#viadot.tasks.open_apis.uk_carbon_intensity.StatsToCSV.run","text":"Run the task.","title":"run()"},{"location":"references/task_library/#viadot.tasks.open_apis.uk_carbon_intensity.StatsToCSV.run--parameters","text":"path : str Path of the csv file created or edited by this task days_back : int, optional How many days of stats to download in the csv. UK Carbon Intensity statistics are available for up to 30 days, by default 10 Source code in viadot/tasks/open_apis/uk_carbon_intensity.py def run ( self , path : str , days_back : int = 10 ): \"\"\" Run the task. Parameters ---------- path : str Path of the csv file created or edited by this task days_back : int, optional How many days of stats to download in the csv. UK Carbon Intensity statistics are available for up to 30 days, by default 10 \"\"\" logger = prefect . context . get ( \"logger\" ) carbon = UKCarbonIntensity () now = datetime . datetime . now () logger . info ( f \"Downloading data to { path } ...\" ) for i in range ( days_back ): from_delta = datetime . timedelta ( days = i + 1 ) to_delta = datetime . timedelta ( days = i ) to = now - to_delta from_ = now - from_delta carbon . query ( f \"/intensity/stats/ { from_ . isoformat () } / { to . isoformat () } \" ) carbon . to_csv ( path , if_exists = \"append\" ) # Download data to a local CSV file logger . info ( f \"Successfully downloaded data to { path } .\" )","title":"Parameters"},{"location":"references/task_library/#viadot.tasks.open_apis.uk_carbon_intensity.StatsToExcel","text":"A Prefect task for downloading UK Carbon Instensity Statistics (stats) to a excel file.","title":"StatsToExcel"},{"location":"references/task_library/#viadot.tasks.open_apis.uk_carbon_intensity.StatsToExcel.__call__","text":"Run the task.","title":"__call__()"},{"location":"references/task_library/#viadot.tasks.open_apis.uk_carbon_intensity.StatsToExcel.__call__--parameters","text":"path : str Path of the csv file created or edited by this task days_back : int, optional How many days of stats to download in the excel. UK Carbon Intensity statistics are available for up to 30 days, by default 10 Source code in viadot/tasks/open_apis/uk_carbon_intensity.py def __call__ ( self ): \"\"\" Run the task. Parameters ---------- path : str Path of the csv file created or edited by this task days_back : int, optional How many days of stats to download in the excel. UK Carbon Intensity statistics are available for up to 30 days, by default 10 \"\"\"","title":"Parameters"},{"location":"references/task_library/#viadot.tasks.open_apis.uk_carbon_intensity.StatsToExcel.__init__","text":"Generate the task. Source code in viadot/tasks/open_apis/uk_carbon_intensity.py def __init__ ( self , * args , ** kwargs ): \"\"\"Generate the task.\"\"\" super () . __init__ ( name = \"uk_carbon_intensity_stats_to_excel\" , * args , ** kwargs )","title":"__init__()"},{"location":"references/task_library/#viadot.tasks.open_apis.uk_carbon_intensity.StatsToExcel.run","text":"Run the task.","title":"run()"},{"location":"references/task_library/#viadot.tasks.open_apis.uk_carbon_intensity.StatsToExcel.run--parameters","text":"path : str Path of the excel file created or edited by this task days_back : int, optional How many days of stats to download in the excel. UK Carbon Intensity statistics are available for up to 30 days, by default 10 Source code in viadot/tasks/open_apis/uk_carbon_intensity.py def run ( self , path : str , days_back : int = 10 ): \"\"\" Run the task. Parameters ---------- path : str Path of the excel file created or edited by this task days_back : int, optional How many days of stats to download in the excel. UK Carbon Intensity statistics are available for up to 30 days, by default 10 \"\"\" logger = prefect . context . get ( \"logger\" ) carbon = UKCarbonIntensity () now = datetime . datetime . now () logger . info ( f \"Downloading data to { path } ...\" ) for i in range ( days_back ): from_delta = datetime . timedelta ( days = i + 1 ) to_delta = datetime . timedelta ( days = i ) to = now - to_delta from_ = now - from_delta carbon . query ( f \"/intensity/stats/ { from_ . isoformat () } / { to . isoformat () } \" ) carbon . to_excel ( path , if_exists = \"append\" ) # Download data to a local excel file logger . info ( f \"Successfully downloaded data to { path } .\" )","title":"Parameters"},{"location":"references/task_library/#viadot.tasks.azure_data_lake.AzureDataLakeDownload","text":"Task for downloading data from the Azure Data lakes (gen1 and gen2). Parameters: Name Type Description Default from_path str The path from which to download the file(s). Defaults to None. required to_path str The destination path. Defaults to None. required recursive bool Set this to true if downloading entire directories. required gen int The generation of the Azure Data Lake. Defaults to 2. required vault_name str The name of the vault from which to fetch the secret. Defaults to None. required timeout(int, optional The amount of time (in seconds) to wait while running this task before a timeout occurs. Defaults to 3600. required max_retries int [description]. Defaults to 3. required retry_delay timedelta [description]. Defaults to timedelta(seconds=10). required","title":"AzureDataLakeDownload"},{"location":"references/task_library/#viadot.tasks.azure_data_lake.AzureDataLakeDownload.__call__","text":"Download file(s) from the Azure Data Lake Source code in viadot/tasks/azure_data_lake.py def __call__ ( self , * args , ** kwargs ): \"\"\"Download file(s) from the Azure Data Lake\"\"\" return super () . __call__ ( * args , ** kwargs )","title":"__call__()"},{"location":"references/task_library/#viadot.tasks.azure_data_lake.AzureDataLakeDownload.run","text":"Task run method. Parameters: Name Type Description Default from_path str The path from which to download the file(s). None to_path str The destination path. None recursive bool Set this to true if downloading entire directories. None gen int The generation of the Azure Data Lake. None sp_credentials_secret str The name of the Azure Key Vault secret containing a dictionary with None vault_name str The name of the vault from which to obtain the secret. Defaults to None. None Source code in viadot/tasks/azure_data_lake.py @defaults_from_attrs ( \"from_path\" , \"to_path\" , \"recursive\" , \"gen\" , \"vault_name\" , \"max_retries\" , \"retry_delay\" , ) def run ( self , from_path : str = None , to_path : str = None , recursive : bool = None , gen : int = None , sp_credentials_secret : str = None , vault_name : str = None , max_retries : int = None , retry_delay : timedelta = None , ) -> None : \"\"\"Task run method. Args: from_path (str): The path from which to download the file(s). to_path (str): The destination path. recursive (bool): Set this to true if downloading entire directories. gen (int): The generation of the Azure Data Lake. sp_credentials_secret (str, optional): The name of the Azure Key Vault secret containing a dictionary with ACCOUNT_NAME and Service Principal credentials (TENANT_ID, CLIENT_ID, CLIENT_SECRET). Defaults to None. vault_name (str, optional): The name of the vault from which to obtain the secret. Defaults to None. \"\"\" file_name = from_path . split ( \"/\" )[ - 1 ] to_path = to_path or file_name if not sp_credentials_secret : # attempt to read a default for the service principal secret name try : sp_credentials_secret = PrefectSecret ( \"AZURE_DEFAULT_ADLS_SERVICE_PRINCIPAL_SECRET\" ) . run () except ValueError : pass if sp_credentials_secret : azure_secret_task = AzureKeyVaultSecret () credentials_str = azure_secret_task . run ( secret = sp_credentials_secret , vault_name = vault_name ) credentials = json . loads ( credentials_str ) else : credentials = { \"ACCOUNT_NAME\" : os . environ [ \"AZURE_ACCOUNT_NAME\" ], \"AZURE_TENANT_ID\" : os . environ [ \"AZURE_TENANT_ID\" ], \"AZURE_CLIENT_ID\" : os . environ [ \"AZURE_CLIENT_ID\" ], \"AZURE_CLIENT_SECRET\" : os . environ [ \"AZURE_CLIENT_SECRET\" ], } lake = AzureDataLake ( gen = gen , credentials = credentials ) full_dl_path = os . path . join ( credentials [ \"ACCOUNT_NAME\" ], from_path ) self . logger . info ( f \"Downloading data from { full_dl_path } to { to_path } ...\" ) lake . download ( from_path = from_path , to_path = to_path , recursive = recursive ) self . logger . info ( f \"Successfully downloaded data to { to_path } .\" )","title":"run()"},{"location":"references/task_library/#viadot.tasks.azure_data_lake.AzureDataLakeUpload","text":"Upload file(s) to Azure Data Lake. Parameters: Name Type Description Default from_path str The local path from which to upload the file(s). Defaults to None. required to_path str The destination path. Defaults to None. required recursive bool Set this to true if uploading entire directories. Defaults to False. required overwrite bool Whether to overwrite files in the lake. Defaults to False. required gen int The generation of the Azure Data Lake. Defaults to 2. required vault_name str The name of the vault from which to obtain the secret. Defaults to None. required timeout(int, optional The amount of time (in seconds) to wait while running this task before a timeout occurs. Defaults to 3600. required","title":"AzureDataLakeUpload"},{"location":"references/task_library/#viadot.tasks.azure_data_lake.AzureDataLakeUpload.__call__","text":"Upload file(s) to the Azure Data Lake Source code in viadot/tasks/azure_data_lake.py def __call__ ( self , * args , ** kwargs ): \"\"\"Upload file(s) to the Azure Data Lake\"\"\" return super () . __call__ ( * args , ** kwargs )","title":"__call__()"},{"location":"references/task_library/#viadot.tasks.azure_data_lake.AzureDataLakeUpload.run","text":"Task run method. Parameters: Name Type Description Default from_path str The path from which to upload the file(s). None to_path str The destination path. None recursive bool Set to true if uploading entire directories. None overwrite bool Whether to overwrite the file(s) if they exist. None gen int The generation of the Azure Data Lake. None sp_credentials_secret str The name of the Azure Key Vault secret containing a dictionary with None vault_name str The name of the vault from which to obtain the secret. Defaults to None. None Source code in viadot/tasks/azure_data_lake.py @defaults_from_attrs ( \"from_path\" , \"to_path\" , \"recursive\" , \"overwrite\" , \"gen\" , \"vault_name\" , \"max_retries\" , \"retry_delay\" , ) def run ( self , from_path : str = None , to_path : str = None , recursive : bool = None , overwrite : bool = None , gen : int = None , sp_credentials_secret : str = None , vault_name : str = None , max_retries : int = None , retry_delay : timedelta = None , ) -> None : \"\"\"Task run method. Args: from_path (str): The path from which to upload the file(s). to_path (str): The destination path. recursive (bool): Set to true if uploading entire directories. overwrite (bool): Whether to overwrite the file(s) if they exist. gen (int): The generation of the Azure Data Lake. sp_credentials_secret (str, optional): The name of the Azure Key Vault secret containing a dictionary with ACCOUNT_NAME and Service Principal credentials (TENANT_ID, CLIENT_ID, CLIENT_SECRET). Defaults to None. vault_name (str, optional): The name of the vault from which to obtain the secret. Defaults to None. \"\"\" if not sp_credentials_secret : # attempt to read a default for the service principal secret name try : sp_credentials_secret = PrefectSecret ( \"AZURE_DEFAULT_ADLS_SERVICE_PRINCIPAL_SECRET\" ) . run () except ValueError : pass if sp_credentials_secret : azure_secret_task = AzureKeyVaultSecret () credentials_str = azure_secret_task . run ( secret = sp_credentials_secret , vault_name = vault_name ) credentials = json . loads ( credentials_str ) else : credentials = { \"ACCOUNT_NAME\" : os . environ [ \"AZURE_ACCOUNT_NAME\" ], \"AZURE_TENANT_ID\" : os . environ [ \"AZURE_TENANT_ID\" ], \"AZURE_CLIENT_ID\" : os . environ [ \"AZURE_CLIENT_ID\" ], \"AZURE_CLIENT_SECRET\" : os . environ [ \"AZURE_CLIENT_SECRET\" ], } lake = AzureDataLake ( gen = gen , credentials = credentials ) full_to_path = os . path . join ( credentials [ \"ACCOUNT_NAME\" ], to_path ) self . logger . info ( f \"Uploading data from { from_path } to { full_to_path } ...\" ) lake . upload ( from_path = from_path , to_path = to_path , recursive = recursive , overwrite = overwrite , ) self . logger . info ( f \"Successfully uploaded data to { full_to_path } .\" )","title":"run()"},{"location":"references/task_library/#viadot.tasks.azure_data_lake.AzureDataLakeToDF","text":"","title":"AzureDataLakeToDF"},{"location":"references/task_library/#viadot.tasks.azure_data_lake.AzureDataLakeToDF.__call__","text":"Load file(s) from the Azure Data Lake to a pandas DataFrame. Source code in viadot/tasks/azure_data_lake.py def __call__ ( self , * args , ** kwargs ): \"\"\"Load file(s) from the Azure Data Lake to a pandas DataFrame.\"\"\" return super () . __call__ ( * args , ** kwargs )","title":"__call__()"},{"location":"references/task_library/#viadot.tasks.azure_data_lake.AzureDataLakeToDF.__init__","text":"Load file(s) from the Azure Data Lake to a pandas DataFrame. Currently supports CSV and parquet files. Parameters: Name Type Description Default path str The path from which to load the DataFrame. Defaults to None. None sep str The separator to use when reading a CSV file. Defaults to \" \". '\\t' quoting int The quoting mode to use when reading a CSV file. Defaults to 0. 0 lineterminator str The newline separator to use when reading a CSV file. Defaults to None. None error_bad_lines bool Whether to raise an exception on bad lines. Defaults to None. None gen int The generation of the Azure Data Lake. Defaults to 2. 2 vault_name str The name of the vault from which to obtain the secret. Defaults to None. None timeout(int, optional The amount of time (in seconds) to wait while running this task before a timeout occurs. Defaults to 3600. required Source code in viadot/tasks/azure_data_lake.py def __init__ ( self , path : str = None , sep : str = \" \\t \" , quoting : int = 0 , lineterminator : str = None , error_bad_lines : bool = None , gen : int = 2 , vault_name : str = None , timeout : int = 3600 , max_retries : int = 3 , retry_delay : timedelta = timedelta ( seconds = 10 ), * args , ** kwargs , ): \"\"\"Load file(s) from the Azure Data Lake to a pandas DataFrame. Currently supports CSV and parquet files. Args: path (str, optional): The path from which to load the DataFrame. Defaults to None. sep (str, optional): The separator to use when reading a CSV file. Defaults to \"\\t\". quoting (int, optional): The quoting mode to use when reading a CSV file. Defaults to 0. lineterminator (str, optional): The newline separator to use when reading a CSV file. Defaults to None. error_bad_lines (bool, optional): Whether to raise an exception on bad lines. Defaults to None. gen (int, optional): The generation of the Azure Data Lake. Defaults to 2. vault_name (str, optional): The name of the vault from which to obtain the secret. Defaults to None. timeout(int, optional): The amount of time (in seconds) to wait while running this task before a timeout occurs. Defaults to 3600. \"\"\" self . path = path self . sep = sep self . quoting = quoting self . lineterminator = lineterminator self . error_bad_lines = error_bad_lines self . gen = gen self . vault_name = vault_name super () . __init__ ( name = \"adls_to_df\" , max_retries = max_retries , retry_delay = retry_delay , timeout = timeout , * args , ** kwargs , )","title":"__init__()"},{"location":"references/task_library/#viadot.tasks.azure_data_lake.AzureDataLakeToDF.run","text":"Task run method. Parameters: Name Type Description Default path str The path to file(s) which should be loaded into a DataFrame. None sep str The field separator to use when loading the file to the DataFrame. None quoting int The quoting mode to use when reading a CSV file. Defaults to 0. None lineterminator str The newline separator to use when reading a CSV file. Defaults to None. None error_bad_lines bool Whether to raise an exception on bad lines. Defaults to None. None gen int The generation of the Azure Data Lake. None sp_credentials_secret str The name of the Azure Key Vault secret containing a dictionary with None vault_name str The name of the vault from which to obtain the secret. Defaults to None. None Source code in viadot/tasks/azure_data_lake.py @defaults_from_attrs ( \"path\" , \"sep\" , \"quoting\" , \"lineterminator\" , \"error_bad_lines\" , \"gen\" , \"vault_name\" , \"max_retries\" , \"retry_delay\" , ) def run ( self , path : str = None , sep : str = None , quoting : int = None , lineterminator : str = None , error_bad_lines : bool = None , gen : int = None , sp_credentials_secret : str = None , vault_name : str = None , max_retries : int = None , retry_delay : timedelta = None , ) -> pd . DataFrame : \"\"\"Task run method. Args: path (str): The path to file(s) which should be loaded into a DataFrame. sep (str): The field separator to use when loading the file to the DataFrame. quoting (int, optional): The quoting mode to use when reading a CSV file. Defaults to 0. lineterminator (str, optional): The newline separator to use when reading a CSV file. Defaults to None. error_bad_lines (bool, optional): Whether to raise an exception on bad lines. Defaults to None. gen (int): The generation of the Azure Data Lake. sp_credentials_secret (str, optional): The name of the Azure Key Vault secret containing a dictionary with ACCOUNT_NAME and Service Principal credentials (TENANT_ID, CLIENT_ID, CLIENT_SECRET). Defaults to None. vault_name (str, optional): The name of the vault from which to obtain the secret. Defaults to None. \"\"\" if quoting is None : quoting = 0 if path is None : raise ValueError ( \"Please provide the path to the file to be downloaded.\" ) if not sp_credentials_secret : # attempt to read a default for the service principal secret name try : sp_credentials_secret = PrefectSecret ( \"AZURE_DEFAULT_ADLS_SERVICE_PRINCIPAL_SECRET\" ) . run () except ValueError : pass if sp_credentials_secret : azure_secret_task = AzureKeyVaultSecret () credentials_str = azure_secret_task . run ( secret = sp_credentials_secret , vault_name = vault_name ) credentials = json . loads ( credentials_str ) else : credentials = { \"ACCOUNT_NAME\" : os . environ [ \"AZURE_ACCOUNT_NAME\" ], \"AZURE_TENANT_ID\" : os . environ [ \"AZURE_TENANT_ID\" ], \"AZURE_CLIENT_ID\" : os . environ [ \"AZURE_CLIENT_ID\" ], \"AZURE_CLIENT_SECRET\" : os . environ [ \"AZURE_CLIENT_SECRET\" ], } lake = AzureDataLake ( gen = gen , credentials = credentials , path = path ) full_dl_path = os . path . join ( credentials [ \"ACCOUNT_NAME\" ], path ) self . logger . info ( f \"Downloading data from { full_dl_path } to a DataFrame...\" ) df = lake . to_df ( sep = sep , quoting = quoting , lineterminator = lineterminator , error_bad_lines = error_bad_lines , ) self . logger . info ( f \"Successfully loaded data.\" ) return df","title":"run()"},{"location":"references/task_library/#viadot.tasks.azure_data_lake.AzureDataLakeCopy","text":"Task for copying data between the Azure Data lakes files. Parameters: Name Type Description Default from_path str The path from which to copy the file(s). Defaults to None. required to_path str The destination path. Defaults to None. required recursive bool Set this to true if copy entire directories. required gen int The generation of the Azure Data Lake. Defaults to 2. required vault_name str The name of the vault from which to fetch the secret. Defaults to None. required timeout(int, optional The amount of time (in seconds) to wait while running this task before a timeout occurs. Defaults to 3600. required max_retries int [description]. Defaults to 3. required retry_delay timedelta [description]. Defaults to timedelta(seconds=10). required","title":"AzureDataLakeCopy"},{"location":"references/task_library/#viadot.tasks.azure_data_lake.AzureDataLakeCopy.__call__","text":"Copy file(s) from the Azure Data Lake Source code in viadot/tasks/azure_data_lake.py def __call__ ( self , * args , ** kwargs ): \"\"\"Copy file(s) from the Azure Data Lake\"\"\" return super () . __call__ ( * args , ** kwargs )","title":"__call__()"},{"location":"references/task_library/#viadot.tasks.azure_data_lake.AzureDataLakeCopy.run","text":"Task run method. Parameters: Name Type Description Default from_path str The path from which to copy the file(s). None to_path str The destination path. None recursive bool Set this to true if copying entire directories. None gen int The generation of the Azure Data Lake. None sp_credentials_secret str The name of the Azure Key Vault secret containing a dictionary with None vault_name str The name of the vault from which to obtain the secret. Defaults to None. None Source code in viadot/tasks/azure_data_lake.py @defaults_from_attrs ( \"from_path\" , \"to_path\" , \"recursive\" , \"gen\" , \"vault_name\" , \"max_retries\" , \"retry_delay\" , ) def run ( self , from_path : str = None , to_path : str = None , recursive : bool = None , gen : int = None , sp_credentials_secret : str = None , vault_name : str = None , max_retries : int = None , retry_delay : timedelta = None , ) -> None : \"\"\"Task run method. Args: from_path (str): The path from which to copy the file(s). to_path (str): The destination path. recursive (bool): Set this to true if copying entire directories. gen (int): The generation of the Azure Data Lake. sp_credentials_secret (str, optional): The name of the Azure Key Vault secret containing a dictionary with ACCOUNT_NAME and Service Principal credentials (TENANT_ID, CLIENT_ID, CLIENT_SECRET). Defaults to None. vault_name (str, optional): The name of the vault from which to obtain the secret. Defaults to None. \"\"\" file_name = from_path . split ( \"/\" )[ - 1 ] to_path = to_path or file_name if not sp_credentials_secret : # attempt to read a default for the service principal secret name try : sp_credentials_secret = PrefectSecret ( \"AZURE_DEFAULT_ADLS_SERVICE_PRINCIPAL_SECRET\" ) . run () except ValueError : pass if sp_credentials_secret : azure_secret_task = AzureKeyVaultSecret () credentials_str = azure_secret_task . run ( secret = sp_credentials_secret , vault_name = vault_name ) credentials = json . loads ( credentials_str ) else : credentials = { \"ACCOUNT_NAME\" : os . environ [ \"AZURE_ACCOUNT_NAME\" ], \"AZURE_TENANT_ID\" : os . environ [ \"AZURE_TENANT_ID\" ], \"AZURE_CLIENT_ID\" : os . environ [ \"AZURE_CLIENT_ID\" ], \"AZURE_CLIENT_SECRET\" : os . environ [ \"AZURE_CLIENT_SECRET\" ], } lake = AzureDataLake ( gen = gen , credentials = credentials ) full_dl_path = os . path . join ( credentials [ \"ACCOUNT_NAME\" ], from_path ) self . logger . info ( f \"Copying data from { full_dl_path } to { to_path } ...\" ) lake . cp ( from_path = from_path , to_path = to_path , recursive = recursive ) self . logger . info ( f \"Successfully copied data to { to_path } .\" )","title":"run()"},{"location":"references/task_library/#viadot.tasks.azure_data_lake.AzureDataLakeList","text":"Task for listing files in Azure Data Lake. Parameters: Name Type Description Default path str The path to the directory which contents you want to list. Defaults to None. required gen int The generation of the Azure Data Lake. Defaults to 2. required vault_name str The name of the vault from which to fetch the secret. Defaults to None. required timeout(int, optional The amount of time (in seconds) to wait while running this task before a timeout occurs. Defaults to 3600. required max_retries int [description]. Defaults to 3. required retry_delay timedelta [description]. Defaults to timedelta(seconds=10). required Returns: Type Description List[str] The list of paths to the contents of path . These paths do not include the container, eg. the path to the file located at \"https://my_storage_acc.blob.core.windows.net/raw/supermetrics/test_file.txt\" will be shown as \"raw/supermetrics/test_file.txt\".","title":"AzureDataLakeList"},{"location":"references/task_library/#viadot.tasks.azure_data_lake.AzureDataLakeList.run","text":"Task run method. Parameters: Name Type Description Default path str The path to the directory which contents you want to list. Defaults to None. None recursive bool If True, recursively list all subdirectories and files. Defaults to False. False file_to_match str If exist it only returns files with that name. Defaults to None. None gen int The generation of the Azure Data Lake. Defaults to None. None sp_credentials_secret str The name of the Azure Key Vault secret containing a dictionary with None vault_name str The name of the vault from which to obtain the secret. Defaults to None. None Returns: Type Description List[str] The list of paths to the contents of path . These paths do not include the container, eg. the path to the file located at \"https://my_storage_acc.blob.core.windows.net/raw/supermetrics/test_file.txt\" will be shown as \"raw/supermetrics/test_file.txt\". Source code in viadot/tasks/azure_data_lake.py @defaults_from_attrs ( \"path\" , \"gen\" , \"vault_name\" , \"max_retries\" , \"retry_delay\" , ) def run ( self , path : str = None , recursive : bool = False , file_to_match : str = None , gen : int = None , sp_credentials_secret : str = None , vault_name : str = None , max_retries : int = None , retry_delay : timedelta = None , ) -> List [ str ]: \"\"\"Task run method. Args: path (str, optional): The path to the directory which contents you want to list. Defaults to None. recursive (bool, optional): If True, recursively list all subdirectories and files. Defaults to False. file_to_match (str, optional): If exist it only returns files with that name. Defaults to None. gen (int): The generation of the Azure Data Lake. Defaults to None. sp_credentials_secret (str, optional): The name of the Azure Key Vault secret containing a dictionary with ACCOUNT_NAME and Service Principal credentials (TENANT_ID, CLIENT_ID, CLIENT_SECRET). Defaults to None. vault_name (str, optional): The name of the vault from which to obtain the secret. Defaults to None. Returns: List[str]: The list of paths to the contents of `path`. These paths do not include the container, eg. the path to the file located at \"https://my_storage_acc.blob.core.windows.net/raw/supermetrics/test_file.txt\" will be shown as \"raw/supermetrics/test_file.txt\". \"\"\" if not sp_credentials_secret : # attempt to read a default for the service principal secret name try : sp_credentials_secret = PrefectSecret ( \"AZURE_DEFAULT_ADLS_SERVICE_PRINCIPAL_SECRET\" ) . run () except ValueError : pass if sp_credentials_secret : azure_secret_task = AzureKeyVaultSecret () credentials_str = azure_secret_task . run ( secret = sp_credentials_secret , vault_name = vault_name ) credentials = json . loads ( credentials_str ) else : credentials = { \"ACCOUNT_NAME\" : os . environ [ \"AZURE_ACCOUNT_NAME\" ], \"AZURE_TENANT_ID\" : os . environ [ \"AZURE_TENANT_ID\" ], \"AZURE_CLIENT_ID\" : os . environ [ \"AZURE_CLIENT_ID\" ], \"AZURE_CLIENT_SECRET\" : os . environ [ \"AZURE_CLIENT_SECRET\" ], } lake = AzureDataLake ( gen = gen , credentials = credentials ) full_dl_path = os . path . join ( credentials [ \"ACCOUNT_NAME\" ], path ) self . logger . info ( f \"Listing files in { full_dl_path } .\" ) if recursive : self . logger . info ( \"Loading ADLS directories recursively.\" ) files = lake . find ( path ) if file_to_match : conditions = [ file_to_match in item for item in files ] valid_files = np . array ([]) if any ( conditions ): index = np . where ( conditions )[ 0 ] files = list ( np . append ( valid_files , [ files [ i ] for i in index ])) else : raise FileExistsError ( f \"There are not any available file named { file_to_match } .\" ) else : files = lake . ls ( path ) self . logger . info ( f \"Successfully listed files in { full_dl_path } .\" ) return files","title":"run()"},{"location":"references/task_library/#viadot.tasks.azure_key_vault.AzureKeyVaultSecret","text":"Task for retrieving secrets from an Azure Key Vault and returning it as a dictionary. Note that all initialization arguments can optionally be provided or overwritten at runtime. For authentication, there are two options: you can set the AZURE_CREDENTIALS Prefect Secret containing your Azure Key Vault credentials which will be passed directly to SecretClient , or you can configure your flow's runtime environment for EnvironmentCredential . Parameters: Name Type Description Default - secret (str the name of the secret to retrieve required - vault_name (str the name of the vault from which to fetch the secret required - secret_client_kwargs (dict additional keyword arguments to forward to the SecretClient. required - **kwargs (dict additional keyword arguments to pass to the Task constructor required","title":"AzureKeyVaultSecret"},{"location":"references/task_library/#viadot.tasks.azure_key_vault.AzureKeyVaultSecret.run","text":"Task run method. Parameters: Name Type Description Default - secret (str the name of the secret to retrieve required - vault_name (str the name of the vault from which to fetch the secret required - credentials (dict your Azure Key Vault credentials passed from an upstream Secret task. By default, credentials are read from the AZURE_CREDENTIALS Prefect Secret; this Secret must be a JSON string with the subkey KEY_VAULT and then vault_name containing three keys: AZURE_TENANT_ID , AZURE_CLIENT_ID , and AZURE_CLIENT_SECRET , which will be passed directly to SecretClient . If not provided here or in context, the task will fall back to Azure credentials discovery using EnvironmentCredential() . Example AZURE_CREDENTIALS environment variable: export AZURE_CREDENTIALS = '{\"KEY_VAULT\": {\"test_key_vault\": {\"AZURE_TENANT_ID\": \"a\", \"AZURE_CLIENT_ID\": \"b\", \"AZURE_CLIENT_SECRET\": \"c\"}}}' required Examples: from prefect import Flow from viadot.tasks import AzureKeyVaultSecret azure_secret_task = AzureKeyVaultSecret () with Flow ( name = \"example\" ) as f : secret = azure_secret_task ( secret = \"test\" , vault_name = \"my_vault_name\" ) out = f . run () Returns: Type Description - str the contents of this secret, as a string Source code in viadot/tasks/azure_key_vault.py @defaults_from_attrs ( \"secret\" , \"vault_name\" ) def run ( self , secret : str = None , vault_name : str = None , credentials : dict = None , max_retries : int = None , retry_delay : timedelta = None , ) -> str : \"\"\" Task run method. Args: - secret (str): the name of the secret to retrieve - vault_name (str): the name of the vault from which to fetch the secret - credentials (dict, optional): your Azure Key Vault credentials passed from an upstream Secret task. By default, credentials are read from the `AZURE_CREDENTIALS` Prefect Secret; this Secret must be a JSON string with the subkey `KEY_VAULT` and then vault_name containing three keys: `AZURE_TENANT_ID`, `AZURE_CLIENT_ID`, and `AZURE_CLIENT_SECRET`, which will be passed directly to `SecretClient`. If not provided here or in context, the task will fall back to Azure credentials discovery using `EnvironmentCredential()`. Example `AZURE_CREDENTIALS` environment variable: `export AZURE_CREDENTIALS = '{\"KEY_VAULT\": {\"test_key_vault\": {\"AZURE_TENANT_ID\": \"a\", \"AZURE_CLIENT_ID\": \"b\", \"AZURE_CLIENT_SECRET\": \"c\"}}}'` Example: ```python from prefect import Flow from viadot.tasks import AzureKeyVaultSecret azure_secret_task = AzureKeyVaultSecret() with Flow(name=\"example\") as f: secret = azure_secret_task(secret=\"test\", vault_name=\"my_vault_name\") out = f.run() ``` Returns: - str: the contents of this secret, as a string \"\"\" if secret is None : raise ValueError ( \"A secret name must be provided.\" ) key_vault = get_key_vault ( vault_name = vault_name , credentials = credentials , secret_client_kwargs = self . secret_client_kwargs , ) secret_string = key_vault . get_secret ( secret ) . value return secret_string","title":"run()"},{"location":"references/task_library/#viadot.tasks.azure_key_vault.CreateAzureKeyVaultSecret","text":"Task for creating secrets in an Azure Key Vault. Note that all initialization arguments can optionally be provided or overwritten at runtime. For authentication, there are two options: you can set the AZURE_CREDENTIALS Prefect Secret containing your Azure Key Vault credentials which will be passed directly to SecretClient , or you can configure your flow's runtime environment for EnvironmentCredential . Parameters: Name Type Description Default - secret (str the name of the secret to retrieve required - vault_name (str the name of the vault from which to fetch the secret required - secret_client_kwargs (dict additional keyword arguments to forward to the SecretClient. required - **kwargs (dict additional keyword arguments to pass to the Task constructor required","title":"CreateAzureKeyVaultSecret"},{"location":"references/task_library/#viadot.tasks.azure_key_vault.CreateAzureKeyVaultSecret.run","text":"Task run method. Parameters: Name Type Description Default - secret (str the name of the secret to set required - value (str the value which the secret will hold required - lifetime (int The number of days after which the secret should expire. required - vault_name (str the name of the vault from which to fetch the secret required - credentials (dict your Azure Key Vault credentials passed from an upstream Secret task; this Secret must be a JSON string with the subkey KEY_VAULT and then vault_name containing three keys: AZURE_TENANT_ID , AZURE_CLIENT_ID , and AZURE_CLIENT_SECRET , which will be passed directly to SecretClient . If not provided here or in context, the task will fall back to Azure credentials discovery using EnvironmentCredential() . Example AZURE_CREDENTIALS environment variable: export AZURE_CREDENTIALS = '{\"KEY_VAULT\": {\"test_key_vault\": {\"AZURE_TENANT_ID\": \"a\", \"AZURE_CLIENT_ID\": \"b\", \"AZURE_CLIENT_SECRET\": \"c\"}}}' required Examples: from prefect import Flow from prefect.tasks.secrets import PrefectSecret from viadot.tasks import CreateAzureKeyVaultSecret create_secret_task = CreateAzureKeyVaultSecret () with Flow ( name = \"example\" ) as f : azure_credentials = PrefectSecret ( \"AZURE_CREDENTIALS\" ) secret = create_secret_task ( secret = \"test2\" , value = 42 , vault_name = \"my_vault_name\" , credentials = azure_credentials ) out = f . run () Returns: Type Description - bool Whether the secret was created successfully. Source code in viadot/tasks/azure_key_vault.py @defaults_from_attrs ( \"secret\" , \"value\" , \"lifetime\" , \"vault_name\" , ) def run ( self , secret : str = None , value : str = None , lifetime : int = None , vault_name : str = None , credentials : dict = None , max_retries : int = None , retry_delay : timedelta = None , ) -> bool : \"\"\" Task run method. Args: - secret (str): the name of the secret to set - value (str): the value which the secret will hold - lifetime (int): The number of days after which the secret should expire. - vault_name (str): the name of the vault from which to fetch the secret - credentials (dict, optional): your Azure Key Vault credentials passed from an upstream Secret task; this Secret must be a JSON string with the subkey `KEY_VAULT` and then vault_name containing three keys: `AZURE_TENANT_ID`, `AZURE_CLIENT_ID`, and `AZURE_CLIENT_SECRET`, which will be passed directly to `SecretClient`. If not provided here or in context, the task will fall back to Azure credentials discovery using `EnvironmentCredential()`. Example `AZURE_CREDENTIALS` environment variable: `export AZURE_CREDENTIALS = '{\"KEY_VAULT\": {\"test_key_vault\": {\"AZURE_TENANT_ID\": \"a\", \"AZURE_CLIENT_ID\": \"b\", \"AZURE_CLIENT_SECRET\": \"c\"}}}'` Example: ```python from prefect import Flow from prefect.tasks.secrets import PrefectSecret from viadot.tasks import CreateAzureKeyVaultSecret create_secret_task = CreateAzureKeyVaultSecret() with Flow(name=\"example\") as f: azure_credentials = PrefectSecret(\"AZURE_CREDENTIALS\") secret = create_secret_task(secret=\"test2\", value=42, vault_name=\"my_vault_name\", credentials=azure_credentials) out = f.run() ``` Returns: - bool: Whether the secret was created successfully. \"\"\" if secret is None : raise ValueError ( \"A secret name must be provided.\" ) key_vault = get_key_vault ( vault_name = vault_name , credentials = credentials , secret_client_kwargs = self . secret_client_kwargs , ) expires_on = pendulum . now ( \"UTC\" ) . add ( days = lifetime ) secret_obj = key_vault . set_secret ( secret , value , expires_on = expires_on ) was_successful = secret_obj . name == secret return was_successful","title":"run()"},{"location":"references/task_library/#viadot.tasks.azure_key_vault.DeleteAzureKeyVaultSecret","text":"Task for removing (\"soft delete\") a secret from an Azure Key Vault. Note that all initialization arguments can optionally be provided or overwritten at runtime. For authentication, there are two options: you can set the AZURE_CREDENTIALS Prefect Secret containing your Azure Key Vault credentials which will be passed directly to SecretClient , or you can configure your flow's runtime environment for EnvironmentCredential . Parameters: Name Type Description Default - secret (str the name of the secret to retrieve required - vault_name (str the name of the vault from which to fetch the secret required - secret_client_kwargs (dict additional keyword arguments to forward to the SecretClient. required - **kwargs (dict additional keyword arguments to pass to the Task constructor required","title":"DeleteAzureKeyVaultSecret"},{"location":"references/task_library/#viadot.tasks.azure_key_vault.DeleteAzureKeyVaultSecret.run","text":"Task run method. Parameters: Name Type Description Default - secret (str the name of the secret to delete required - vault_name (str the name of the vault whethe the secret is located required - credentials (dict your Azure Key Vault credentials passed from an upstream Secret task. By default, credentials are read from the AZURE_CREDENTIALS Prefect Secret; this Secret must be a JSON string with the subkey KEY_VAULT and then vault_name containing three keys: AZURE_TENANT_ID , AZURE_CLIENT_ID , and AZURE_CLIENT_SECRET , which will be passed directly to SecretClient . If not provided here or in context, the task will fall back to Azure credentials discovery using EnvironmentCredential() . Example AZURE_CREDENTIALS environment variable: export AZURE_CREDENTIALS = '{\"KEY_VAULT\": {\"test_key_vault\": {\"AZURE_TENANT_ID\": \"a\", \"AZURE_CLIENT_ID\": \"b\", \"AZURE_CLIENT_SECRET\": \"c\"}}}' required Examples: from prefect import Flow from viadot.tasks import DeleteAzureKeyVaultSecret azure_secret_task = DeleteAzureKeyVaultSecret () with Flow ( name = \"example\" ) as f : secret = azure_secret_task ( secret = \"test\" , vault_name = \"my_vault_name\" ) out = f . run () Returns: Type Description - bool Whether the secret was deleted successfully. Source code in viadot/tasks/azure_key_vault.py @defaults_from_attrs ( \"secret\" , \"vault_name\" ) def run ( self , secret : str = None , vault_name : str = None , credentials : dict = None , max_retries : int = None , retry_delay : timedelta = None , ) -> bool : \"\"\" Task run method. Args: - secret (str): the name of the secret to delete - vault_name (str): the name of the vault whethe the secret is located - credentials (dict, optional): your Azure Key Vault credentials passed from an upstream Secret task. By default, credentials are read from the `AZURE_CREDENTIALS` Prefect Secret; this Secret must be a JSON string with the subkey `KEY_VAULT` and then vault_name containing three keys: `AZURE_TENANT_ID`, `AZURE_CLIENT_ID`, and `AZURE_CLIENT_SECRET`, which will be passed directly to `SecretClient`. If not provided here or in context, the task will fall back to Azure credentials discovery using `EnvironmentCredential()`. Example `AZURE_CREDENTIALS` environment variable: `export AZURE_CREDENTIALS = '{\"KEY_VAULT\": {\"test_key_vault\": {\"AZURE_TENANT_ID\": \"a\", \"AZURE_CLIENT_ID\": \"b\", \"AZURE_CLIENT_SECRET\": \"c\"}}}'` Example: ```python from prefect import Flow from viadot.tasks import DeleteAzureKeyVaultSecret azure_secret_task = DeleteAzureKeyVaultSecret() with Flow(name=\"example\") as f: secret = azure_secret_task(secret=\"test\", vault_name=\"my_vault_name\") out = f.run() ``` Returns: - bool: Whether the secret was deleted successfully. \"\"\" if secret is None : raise ValueError ( \"A secret name must be provided.\" ) key_vault = get_key_vault ( vault_name = vault_name , credentials = credentials , secret_client_kwargs = self . secret_client_kwargs , ) poller = key_vault . begin_delete_secret ( secret ) poller . wait ( timeout = 60 * 5 ) was_successful = poller . status () == \"finished\" return was_successful","title":"run()"},{"location":"references/task_library/#viadot.tasks.azure_sql.AzureSQLBulkInsert","text":"","title":"AzureSQLBulkInsert"},{"location":"references/task_library/#viadot.tasks.azure_sql.AzureSQLBulkInsert.run","text":"Bulk insert data from Azure Data Lake into an Azure SQL Database table. This task also creates the table if it doesn't exist. Currently, only CSV files are supported. from_path (str): Path to the file to be inserted. schema (str): Destination schema. table (str): Destination table. dtypes (Dict[str, Any]): Data types to force. sep (str): The separator to use to read the CSV file. if_exists (Literal, optional): What to do if the table already exists. credentials_secret (str, optional): The name of the Azure Key Vault secret containing a dictionary with SQL db credentials (server, db_name, user, and password). vault_name (str, optional): The name of the vault from which to obtain the secret. Defaults to None. Source code in viadot/tasks/azure_sql.py @defaults_from_attrs ( \"sep\" , \"if_exists\" , \"credentials_secret\" ) def run ( self , from_path : str = None , schema : str = None , table : str = None , dtypes : Dict [ str , Any ] = None , sep : str = None , if_exists : Literal [ \"fail\" , \"replace\" , \"append\" , \"delete\" ] = None , credentials_secret : str = None , vault_name : str = None , ): \"\"\" Bulk insert data from Azure Data Lake into an Azure SQL Database table. This task also creates the table if it doesn't exist. Currently, only CSV files are supported. Args: from_path (str): Path to the file to be inserted. schema (str): Destination schema. table (str): Destination table. dtypes (Dict[str, Any]): Data types to force. sep (str): The separator to use to read the CSV file. if_exists (Literal, optional): What to do if the table already exists. credentials_secret (str, optional): The name of the Azure Key Vault secret containing a dictionary with SQL db credentials (server, db_name, user, and password). vault_name (str, optional): The name of the vault from which to obtain the secret. Defaults to None. \"\"\" fqn = f \" { schema } . { table } \" if schema else table credentials = get_credentials ( credentials_secret , vault_name = vault_name ) azure_sql = AzureSQL ( credentials = credentials ) if if_exists == \"replace\" : azure_sql . create_table ( schema = schema , table = table , dtypes = dtypes , if_exists = if_exists ) self . logger . info ( f \"Successfully created table { fqn } .\" ) azure_sql . bulk_insert ( schema = schema , table = table , source_path = from_path , sep = sep , if_exists = if_exists , ) self . logger . info ( f \"Successfully inserted data into { fqn } .\" )","title":"run()"},{"location":"references/task_library/#viadot.tasks.azure_sql.AzureSQLCreateTable","text":"","title":"AzureSQLCreateTable"},{"location":"references/task_library/#viadot.tasks.azure_sql.AzureSQLCreateTable.run","text":"Create a table in Azure SQL Database. Parameters: Name Type Description Default schema str Destination schema. None table str Destination table. None dtypes Dict[str, Any] Data types to force. None if_exists Literal What to do if the table already exists. None credentials_secret str The name of the Azure Key Vault secret containing a dictionary None vault_name str The name of the vault from which to obtain the secret. Defaults to None. None Source code in viadot/tasks/azure_sql.py @defaults_from_attrs ( \"if_exists\" ) def run ( self , schema : str = None , table : str = None , dtypes : Dict [ str , Any ] = None , if_exists : Literal [ \"fail\" , \"replace\" , \"skip\" , \"delete\" ] = None , credentials_secret : str = None , vault_name : str = None , max_retries : int = None , retry_delay : timedelta = None , ): \"\"\" Create a table in Azure SQL Database. Args: schema (str, optional): Destination schema. table (str, optional): Destination table. dtypes (Dict[str, Any], optional): Data types to force. if_exists (Literal, optional): What to do if the table already exists. credentials_secret (str, optional): The name of the Azure Key Vault secret containing a dictionary with SQL db credentials (server, db_name, user, and password). vault_name (str, optional): The name of the vault from which to obtain the secret. Defaults to None. \"\"\" credentials = get_credentials ( credentials_secret , vault_name = vault_name ) azure_sql = AzureSQL ( credentials = credentials ) fqn = f \" { schema } . { table } \" if schema is not None else table created = azure_sql . create_table ( schema = schema , table = table , dtypes = dtypes , if_exists = if_exists ) if created : self . logger . info ( f \"Successfully created table { fqn } .\" ) else : self . logger . info ( f \"Table { fqn } has not been created as if_exists is set to { if_exists } .\" )","title":"run()"},{"location":"references/task_library/#viadot.tasks.azure_sql.AzureSQLDBQuery","text":"Task for running an Azure SQL Database query. Parameters: Name Type Description Default query str, required The query to execute on the database. required credentials_secret str The name of the Azure Key Vault secret containing a dictionary required vault_name str The name of the vault from which to obtain the secret. Defaults to None. required timeout(int, optional The amount of time (in seconds) to wait while running this task before a timeout occurs. Defaults to 3600. required","title":"AzureSQLDBQuery"},{"location":"references/task_library/#viadot.tasks.azure_sql.AzureSQLDBQuery.run","text":"Run an Azure SQL Database query Parameters: Name Type Description Default query str, required The query to execute on the database. required credentials_secret str The name of the Azure Key Vault secret containing a dictionary None vault_name str The name of the vault from which to obtain the secret. Defaults to None. None Source code in viadot/tasks/azure_sql.py def run ( self , query : str , credentials_secret : str = None , vault_name : str = None , ): \"\"\"Run an Azure SQL Database query Args: query (str, required): The query to execute on the database. credentials_secret (str, optional): The name of the Azure Key Vault secret containing a dictionary with SQL db credentials (server, db_name, user, and password). vault_name (str, optional): The name of the vault from which to obtain the secret. Defaults to None. \"\"\" credentials = get_credentials ( credentials_secret , vault_name = vault_name ) azure_sql = AzureSQL ( credentials = credentials ) # run the query and fetch the results if it's a select result = azure_sql . run ( query ) self . logger . info ( f \"Successfully ran the query.\" ) return result","title":"run()"},{"location":"references/task_library/#viadot.tasks.bcp.BCPTask","text":"Task for bulk inserting data into SQL Server-compatible databases. Parameters: Name Type Description Default - path (str The path to the local CSV file to be inserted. required - schema (str The destination schema. required - table (str The destination table. required - chunksize (int The chunk size to use. required - error_log_file_path (string Full path of an error file. Defaults to \"log_file.log\". required - on_error (Literal[\"skip\", \"fail\"] What to do if error occurs. Defaults to \"skip\". required - credentials (dict The credentials to use for connecting with the database. required - vault_name (str The name of the vault from which to fetch the secret. required - timeout(int The amount of time (in seconds) to wait while running this task before a timeout occurs. Defaults to 3600. required - **kwargs (dict Additional keyword arguments to pass to the Task constructor. required","title":"BCPTask"},{"location":"references/task_library/#viadot.tasks.bcp.BCPTask.run","text":"Task run method. path (str, optional): The path to the local CSV file to be inserted. schema (str, optional): The destination schema. table (str, optional): The destination table. chunksize (int, optional): The chunk size to use. By default 5000. error_log_file_path (string, optional): Full path of an error file. Defaults to \"log_file.log\". on_error (Literal, optional): What to do if error occur. Defaults to None. credentials (dict, optional): The credentials to use for connecting with SQL Server. credentials_secret (str, optional): The name of the Key Vault secret containing database credentials. (server, db_name, user, password) vault_name (str): The name of the vault from which to fetch the secret. Returns: Type Description str The output of the bcp CLI command. Source code in viadot/tasks/bcp.py @defaults_from_attrs ( \"path\" , \"schema\" , \"table\" , \"chunksize\" , \"error_log_file_path\" , \"on_error\" , \"credentials\" , \"vault_name\" , \"max_retries\" , \"retry_delay\" , ) def run ( self , path : str = None , schema : str = None , table : str = None , chunksize : int = None , error_log_file_path : str = None , on_error : Literal = None , credentials : dict = None , credentials_secret : str = None , vault_name : str = None , max_retries : int = None , retry_delay : timedelta = None , ** kwargs , ) -> str : \"\"\" Task run method. Args: - path (str, optional): The path to the local CSV file to be inserted. - schema (str, optional): The destination schema. - table (str, optional): The destination table. - chunksize (int, optional): The chunk size to use. By default 5000. - error_log_file_path (string, optional): Full path of an error file. Defaults to \"log_file.log\". - on_error (Literal, optional): What to do if error occur. Defaults to None. - credentials (dict, optional): The credentials to use for connecting with SQL Server. - credentials_secret (str, optional): The name of the Key Vault secret containing database credentials. (server, db_name, user, password) - vault_name (str): The name of the vault from which to fetch the secret. Returns: str: The output of the bcp CLI command. \"\"\" if not credentials : if not credentials_secret : # attempt to read a default for the service principal secret name try : credentials_secret = PrefectSecret ( \"AZURE_DEFAULT_SQLDB_SERVICE_PRINCIPAL_SECRET\" ) . run () except ValueError : pass if credentials_secret : credentials_str = AzureKeyVaultSecret ( credentials_secret , vault_name = vault_name ) . run () credentials = json . loads ( credentials_str ) fqn = f \" { schema } . { table } \" if schema else table server = credentials [ \"server\" ] db_name = credentials [ \"db_name\" ] uid = credentials [ \"user\" ] pwd = credentials [ \"password\" ] if \",\" in server : # A space after the comma is allowed in the ODBC connection string # but not in BCP's 'server' argument. server = server . replace ( \" \" , \"\" ) if on_error == \"skip\" : max_error = 0 elif on_error == \"fail\" : max_error = 1 else : raise ValueError ( \"Please provide correct 'on_error' parameter value - 'skip' or 'fail'. \" ) command = f \"/opt/mssql-tools/bin/bcp { fqn } in ' { path } ' -S { server } -d { db_name } -U { uid } -P ' { pwd } ' -c -F 2 -b { chunksize } -h 'TABLOCK' -e ' { error_log_file_path } ' -m { max_error } \" run_command = super () . run ( command = command , ** kwargs ) try : parse_logs ( error_log_file_path ) except : logger . warning ( \"BCP logs couldn't be parsed.\" ) return run_command","title":"run()"},{"location":"references/task_library/#viadot.tasks.great_expectations.RunGreatExpectationsValidation","text":"Task for running data validation with Great Expectations on a pandas DataFrame. See https://docs.prefect.io/api/latest/tasks/great_expectations.html#rungreatexpectationsvalidation for full documentation. Parameters: Name Type Description Default expectations_path str The path to the directory containing the expectation suites. required df pd.DataFrame The DataFrame to validate. required","title":"RunGreatExpectationsValidation"},{"location":"references/task_library/#viadot.tasks.sqlite.SQLiteInsert","text":"Task for inserting data from a pandas DataFrame into SQLite. Parameters: Name Type Description Default db_path str The path to the database to be used. Defaults to None. required sql_path str The path to the text file containing the query. Defaults to None. required timeout(int, optional The amount of time (in seconds) to wait while running this task before a timeout occurs. Defaults to 3600. required","title":"SQLiteInsert"},{"location":"references/task_library/#viadot.tasks.sqlite.SQLiteSQLtoDF","text":"Task for downloading data from the SQLite to a pandas DataFrame. SQLite will create a new database in the directory specified by the 'db_path' parameter. Parameters: Name Type Description Default db_path str The path to the database to be used. Defaults to None. required sql_path str The path to the text file containing the query. Defaults to None. required timeout(int, optional The amount of time (in seconds) to wait while running this task before a timeout occurs. Defaults to 3600. required","title":"SQLiteSQLtoDF"},{"location":"references/task_library/#viadot.tasks.sqlite.SQLiteSQLtoDF.__call__","text":"Generate a DataFrame from a SQLite SQL query Source code in viadot/tasks/sqlite.py def __call__ ( self ): \"\"\"Generate a DataFrame from a SQLite SQL query\"\"\"","title":"__call__()"},{"location":"references/task_library/#viadot.tasks.supermetrics.SupermetricsToCSV","text":"Task to downloading data from Supermetrics API to CSV file. Parameters: Name Type Description Default path str The destination path. Defaults to \"supermetrics_extract.csv\". required max_retries int The maximum number of retries. Defaults to 5. required retry_delay timedelta The delay between task retries. Defaults to 10 seconds. required timeout int The amount of time (in seconds) to wait while running this task before a timeout occurs. Defaults to 3600. required max_rows int Maximum number of rows the query results should contain. Defaults to 1 000 000. required max_cols int Maximum number of columns the query results should contain. Defaults to None. required if_exists str What to do if file already exists. Defaults to \"replace\". required if_empty str What to do if query returns no data. Defaults to \"warn\". required sep str The separator in a target csv file. Defaults to \"/t\". required","title":"SupermetricsToCSV"},{"location":"references/task_library/#viadot.tasks.supermetrics.SupermetricsToCSV.__call__","text":"Download Supermetrics data to a CSV Source code in viadot/tasks/supermetrics.py def __call__ ( self ): \"\"\"Download Supermetrics data to a CSV\"\"\" super () . __call__ ( self )","title":"__call__()"},{"location":"references/task_library/#viadot.tasks.supermetrics.SupermetricsToCSV.run","text":"Task run method. Parameters: Name Type Description Default path str The destination path. Defaulrs to None None ds_id str A Supermetrics query parameter. None ds_accounts Union[str, List[str]] A Supermetrics query parameter. Defaults to None. None ds_segments List[str] A Supermetrics query parameter. Defaults to None. None ds_user str A Supermetrics query parameter. Defaults to None. None fields List[str] A Supermetrics query parameter. Defaults to None. None date_range_type str A Supermetrics query parameter. Defaults to None. None start_date str A Supermetrics query parameter. Defaults to None. None settings Dict[str, Any] A Supermetrics query parameter. Defaults to None. None filter str A Supermetrics query parameter. Defaults to None. None max_rows int A Supermetrics query parameter. Defaults to None. None max_columns int A Supermetrics query parameter. Defaults to None. None order_columns str A Supermetrics query parameter. Defaults to None. None if_exists str What to do if file already exists. Defaults to \"replace\". None if_empty str What to do if query returns no data. Defaults to \"warn\". None max_retries int The maximum number of retries. Defaults to 5. None retry_delay timedelta The delay between task retries. Defaults to 10 seconds. None timeout int Task timeout. Defaults to 30 minuntes. None Source code in viadot/tasks/supermetrics.py @defaults_from_attrs ( \"path\" , \"max_rows\" , \"if_exists\" , \"if_empty\" , \"max_retries\" , \"retry_delay\" , \"timeout\" , \"sep\" , ) def run ( self , path : str = None , ds_id : str = None , ds_accounts : Union [ str , List [ str ]] = None , ds_segments : List [ str ] = None , ds_user : str = None , fields : List [ str ] = None , date_range_type : str = None , start_date : str = None , end_date : str = None , settings : Dict [ str , Any ] = None , filter : str = None , max_rows : int = None , max_columns : int = None , order_columns : str = None , if_exists : str = None , if_empty : str = None , max_retries : int = None , retry_delay : timedelta = None , timeout : int = None , sep : str = None , ): \"\"\" Task run method. Args: path (str, optional): The destination path. Defaulrs to None ds_id (str, optional): A Supermetrics query parameter. ds_accounts (Union[str, List[str]], optional): A Supermetrics query parameter. Defaults to None. ds_segments (List[str], optional): A Supermetrics query parameter. Defaults to None. ds_user (str, optional): A Supermetrics query parameter. Defaults to None. fields (List[str], optional): A Supermetrics query parameter. Defaults to None. date_range_type (str, optional): A Supermetrics query parameter. Defaults to None. start_date (str, optional): A Supermetrics query parameter. Defaults to None. end_date (str, optional) A Supermetrics query parameter. Defaults to None. settings (Dict[str, Any], optional): A Supermetrics query parameter. Defaults to None. filter (str, optional): A Supermetrics query parameter. Defaults to None. max_rows (int, optional): A Supermetrics query parameter. Defaults to None. max_columns (int, optional): A Supermetrics query parameter. Defaults to None. order_columns (str, optional): A Supermetrics query parameter. Defaults to None. if_exists (str, optional): What to do if file already exists. Defaults to \"replace\". if_empty (str, optional): What to do if query returns no data. Defaults to \"warn\". max_retries (int, optional): The maximum number of retries. Defaults to 5. retry_delay (timedelta, optional): The delay between task retries. Defaults to 10 seconds. timeout (int, optional): Task timeout. Defaults to 30 minuntes. sep (str, optional) \"\"\" if max_retries : self . max_retries = max_retries if retry_delay : self . retry_delay = retry_delay if isinstance ( ds_accounts , str ): ds_accounts = [ ds_accounts ] # Build the URL # Note the task accepts only one account per query query = dict ( ds_id = ds_id , ds_accounts = ds_accounts , ds_segments = ds_segments , ds_user = ds_user , fields = fields , date_range_type = date_range_type , start_date = start_date , end_date = end_date , settings = settings , filter = filter , max_rows = max_rows , max_columns = max_columns , order_columns = order_columns , ) query = { param : val for param , val in query . items () if val is not None } supermetrics = Supermetrics () supermetrics . query ( query ) # Download data to a local CSV file self . logger . info ( f \"Downloading data to { path } ...\" ) supermetrics . to_csv ( path , if_exists = if_exists , if_empty = if_empty , sep = sep ) self . logger . info ( f \"Successfully downloaded data to { path } .\" )","title":"run()"},{"location":"references/task_library/#viadot.tasks.supermetrics.SupermetricsToDF","text":"Task for downloading data from the Supermetrics API to a pandas DataFrame. Parameters: Name Type Description Default ds_id str A Supermetrics query parameter. required ds_accounts Union[str, List[str]] A Supermetrics query parameter. Defaults to None. required ds_segments List[str] A Supermetrics query parameter. Defaults to None. required ds_user str A Supermetrics query parameter. Defaults to None. required fields List[str] A Supermetrics query parameter. Defaults to None. required date_range_type str A Supermetrics query parameter. Defaults to None. required settings Dict[str, Any] A Supermetrics query parameter. Defaults to None. required filter str A Supermetrics query parameter. Defaults to None. required max_rows int A Supermetrics query parameter. Defaults to None. required max_columns int A Supermetrics query parameter. Defaults to None. required order_columns str A Supermetrics query parameter. Defaults to None. required if_empty str What to do if query returns no data. Defaults to \"warn\". required max_retries int The maximum number of retries. Defaults to 5. required retry_delay timedelta The delay between task retries. Defaults to 10 seconds. required timeout int The amount of time (in seconds) to wait while running this task before a timeout occurs. Defaults to 3600. required","title":"SupermetricsToDF"},{"location":"references/task_library/#viadot.tasks.supermetrics.SupermetricsToDF.run","text":"Task run method. Parameters: Name Type Description Default ds_id str A Supermetrics query parameter. None ds_accounts Union[str, List[str]] A Supermetrics query parameter. Defaults to None. None ds_segments List[str] A Supermetrics query parameter. Defaults to None. None ds_user str A Supermetrics query parameter. Defaults to None. None fields List[str] A Supermetrics query parameter. Defaults to None. None date_range_type str A Supermetrics query parameter. Defaults to None. None start_date str A query paramter to pass start date to the date range filter. Defaults to None. None end_date str A query paramter to pass end date to the date range filter. Defaults to None. None settings Dict[str, Any] A Supermetrics query parameter. Defaults to None. None filter str A Supermetrics query parameter. Defaults to None. None max_rows int A Supermetrics query parameter. Defaults to None. None max_columns int A Supermetrics query parameter. Defaults to None. None order_columns str A Supermetrics query parameter. Defaults to None. None if_empty str What to do if query returns no data. Defaults to \"warn\". None max_retries int The maximum number of retries. Defaults to 5. None retry_delay timedelta The delay between task retries. Defaults to 10 seconds. None timeout int Task timeout. Defaults to 30 minuntes. None Returns: Type Description pd.DataFrame The query result as a pandas DataFrame. Source code in viadot/tasks/supermetrics.py @defaults_from_attrs ( \"if_empty\" , \"max_rows\" , \"max_retries\" , \"retry_delay\" , \"timeout\" , ) def run ( self , ds_id : str = None , ds_accounts : Union [ str , List [ str ]] = None , ds_segments : List [ str ] = None , ds_user : str = None , fields : List [ str ] = None , date_range_type : str = None , start_date : str = None , end_date : str = None , settings : Dict [ str , Any ] = None , filter : str = None , max_rows : int = None , max_columns : int = None , order_columns : str = None , if_empty : str = None , max_retries : int = None , retry_delay : timedelta = None , timeout : int = None , ) -> pd . DataFrame : \"\"\" Task run method. Args: ds_id (str, optional): A Supermetrics query parameter. ds_accounts (Union[str, List[str]], optional): A Supermetrics query parameter. Defaults to None. ds_segments (List[str], optional): A Supermetrics query parameter. Defaults to None. ds_user (str, optional): A Supermetrics query parameter. Defaults to None. fields (List[str], optional): A Supermetrics query parameter. Defaults to None. date_range_type (str, optional): A Supermetrics query parameter. Defaults to None. start_date (str, optional): A query paramter to pass start date to the date range filter. Defaults to None. end_date (str, optional): A query paramter to pass end date to the date range filter. Defaults to None. settings (Dict[str, Any], optional): A Supermetrics query parameter. Defaults to None. filter (str, optional): A Supermetrics query parameter. Defaults to None. max_rows (int, optional): A Supermetrics query parameter. Defaults to None. max_columns (int, optional): A Supermetrics query parameter. Defaults to None. order_columns (str, optional): A Supermetrics query parameter. Defaults to None. if_empty (str, optional): What to do if query returns no data. Defaults to \"warn\". max_retries (int, optional): The maximum number of retries. Defaults to 5. retry_delay (timedelta, optional): The delay between task retries. Defaults to 10 seconds. timeout (int, optional): Task timeout. Defaults to 30 minuntes. Returns: pd.DataFrame: The query result as a pandas DataFrame. \"\"\" if max_retries : self . max_retries = max_retries if retry_delay : self . retry_delay = retry_delay if isinstance ( ds_accounts , str ): ds_accounts = [ ds_accounts ] # Build the URL # Note the task accepts only one account per query query = dict ( ds_id = ds_id , ds_accounts = ds_accounts , ds_segments = ds_segments , ds_user = ds_user , fields = fields , date_range_type = date_range_type , start_date = start_date , end_date = end_date , settings = settings , filter = filter , max_rows = max_rows , max_columns = max_columns , order_columns = order_columns , ) query = { param : val for param , val in query . items () if val is not None } supermetrics = Supermetrics () supermetrics . query ( query ) # Download data to a local CSV file self . logger . info ( f \"Downloading data to a DataFrame...\" ) df = supermetrics . to_df ( if_empty = if_empty ) self . logger . info ( f \"Successfully downloaded data to a DataFrame.\" ) return df","title":"run()"},{"location":"references/task_library/#viadot.task_utils.add_ingestion_metadata_task","text":"Add ingestion metadata columns, eg. data download date Parameters: Name Type Description Default df pd.DataFrame input DataFrame. required Source code in viadot/task_utils.py @task ( timeout = 3600 ) def add_ingestion_metadata_task ( df : pd . DataFrame , ): \"\"\"Add ingestion metadata columns, eg. data download date Args: df (pd.DataFrame): input DataFrame. \"\"\" # Don't skip when df has columns but has no data if len ( df . columns ) == 0 : return df else : df2 = df . copy ( deep = True ) df2 [ \"_viadot_downloaded_at_utc\" ] = datetime . now ( timezone . utc ) . replace ( microsecond = 0 ) return df2","title":"add_ingestion_metadata_task()"},{"location":"references/task_library/#viadot.task_utils.get_latest_timestamp_file_path","text":"Return the name of the latest file in a given data lake directory, given a list of paths in that directory. Such list can be obtained using the AzureDataLakeList task. This task is useful for working with immutable data lakes as the data is often written in the format /path/table_name/TIMESTAMP.parquet. Source code in viadot/task_utils.py @task ( timeout = 3600 ) def get_latest_timestamp_file_path ( files : List [ str ]) -> str : \"\"\" Return the name of the latest file in a given data lake directory, given a list of paths in that directory. Such list can be obtained using the `AzureDataLakeList` task. This task is useful for working with immutable data lakes as the data is often written in the format /path/table_name/TIMESTAMP.parquet. \"\"\" logger = prefect . context . get ( \"logger\" ) extract_fname = ( lambda f : os . path . basename ( f ) . replace ( \".csv\" , \"\" ) . replace ( \".parquet\" , \"\" ) ) file_names = [ extract_fname ( file ) for file in files ] latest_file_name = max ( file_names , key = lambda d : datetime . fromisoformat ( d )) latest_file = files [ file_names . index ( latest_file_name )] logger . debug ( f \"Latest file: { latest_file } \" ) return latest_file","title":"get_latest_timestamp_file_path()"},{"location":"references/task_library/#viadot.tasks.cloud_for_customers.C4CToDF","text":"","title":"C4CToDF"},{"location":"references/task_library/#viadot.tasks.cloud_for_customers.C4CToDF.run","text":"Task for downloading data from the Cloud for Customers to a pandas DataFrame using normal URL (with query parameters). This task grab data from table from 'scratch' with passing table name in url or endpoint. It is rocommended to add some filters parameters in this case. Examples: url = \"https://mysource.com/sap/c4c/odata/v1/c4codataapi\" endpoint = \"ServiceRequestCollection\" params = {\"$filter\": \"CreationDateTime ge 2021-12-21T00:00:00Z\"} Parameters: Name Type Description Default url str The url to the API in case of prepared report. Defaults to None. None env str The environment to use. Defaults to 'QA'. 'QA' endpoint str The endpoint of the API. Defaults to None. None fields List[str] The C4C Table fields. Defaults to None. None params Dict[str, str] Query parameters. Defaults to $format=json. None chunksize int How many rows to retrieve from C4C at a time. Uses a server-side cursor. None if_empty str What to do if query returns no data. Defaults to \"warn\". 'warn' credentials_secret str The name of the Azure Key Vault secret containing a dictionary None vault_name str The name of the vault from which to obtain the secret. Defaults to None. None Returns: Type Description pd.DataFrame The query result as a pandas DataFrame. Source code in viadot/tasks/cloud_for_customers.py @defaults_from_attrs ( \"url\" , \"endpoint\" , \"fields\" , \"params\" , \"chunksize\" , \"env\" , \"if_empty\" ) def run ( self , url : str = None , env : str = \"QA\" , endpoint : str = None , fields : List [ str ] = None , params : Dict [ str , str ] = None , chunksize : int = None , if_empty : str = \"warn\" , credentials_secret : str = None , vault_name : str = None , ): \"\"\" Task for downloading data from the Cloud for Customers to a pandas DataFrame using normal URL (with query parameters). This task grab data from table from 'scratch' with passing table name in url or endpoint. It is rocommended to add some filters parameters in this case. Example: url = \"https://mysource.com/sap/c4c/odata/v1/c4codataapi\" endpoint = \"ServiceRequestCollection\" params = {\"$filter\": \"CreationDateTime ge 2021-12-21T00:00:00Z\"} Args: url (str, optional): The url to the API in case of prepared report. Defaults to None. env (str, optional): The environment to use. Defaults to 'QA'. endpoint (str, optional): The endpoint of the API. Defaults to None. fields (List[str], optional): The C4C Table fields. Defaults to None. params (Dict[str, str]): Query parameters. Defaults to $format=json. chunksize (int, optional): How many rows to retrieve from C4C at a time. Uses a server-side cursor. if_empty (str, optional): What to do if query returns no data. Defaults to \"warn\". credentials_secret (str, optional): The name of the Azure Key Vault secret containing a dictionary with C4C credentials. Defaults to None. vault_name (str, optional): The name of the vault from which to obtain the secret. Defaults to None. Returns: pd.DataFrame: The query result as a pandas DataFrame. \"\"\" if not credentials_secret : try : credentials_secret = PrefectSecret ( \"C4C_KV\" ) . run () except ValueError : pass if credentials_secret : credentials_str = AzureKeyVaultSecret ( credentials_secret , vault_name = vault_name ) . run () credentials = json . loads ( credentials_str ) else : credentials = local_config . get ( \"CLOUD_FOR_CUSTOMERS\" )[ env ] self . logger . info ( f \"Downloading data from { url + endpoint } ...\" ) # If we get any of these in params, we don't perform any chunking if any ([ \"$skip\" in params , \"$top\" in params ]): return CloudForCustomers ( url = url , endpoint = endpoint , params = params , env = env , credentials = credentials , ) . to_df ( if_empty = if_empty , fields = fields ) def _generate_chunks () -> Generator [ pd . DataFrame , None , None ]: \"\"\" Util returning chunks as a generator to save memory. \"\"\" offset = 0 total_record_count = 0 while True : boundaries = { \"$skip\" : offset , \"$top\" : chunksize } params . update ( boundaries ) chunk = CloudForCustomers ( url = url , endpoint = endpoint , params = params , env = env , credentials = credentials , ) . to_df ( if_empty = if_empty , fields = fields ) chunk_record_count = chunk . shape [ 0 ] total_record_count += chunk_record_count self . logger . info ( f \"Successfully downloaded { total_record_count } records.\" ) yield chunk if chunk . shape [ 0 ] < chunksize : break offset += chunksize self . logger . info ( f \"Data from { url + endpoint } has been downloaded successfully.\" ) chunks = _generate_chunks () df = pd . concat ( chunks ) return df","title":"run()"},{"location":"references/task_library/#viadot.tasks.cloud_for_customers.C4CReportToDF","text":"","title":"C4CReportToDF"},{"location":"references/task_library/#viadot.tasks.cloud_for_customers.C4CReportToDF.__call__","text":"Download report to DF Source code in viadot/tasks/cloud_for_customers.py def __call__ ( self , * args , ** kwargs ): \"\"\"Download report to DF\"\"\" return super () . __call__ ( * args , ** kwargs )","title":"__call__()"},{"location":"references/task_library/#viadot.tasks.cloud_for_customers.C4CReportToDF.run","text":"Task for downloading data from the Cloud for Customers to a pandas DataFrame using report URL. Parameters: Name Type Description Default report_url str The URL to the report. Defaults to None. None env str The environment to use. Defaults to 'QA'. 'QA' skip int Initial index value of reading row. Defaults to 0. 0 top int The value of top reading row. Defaults to 1000. 1000 credentials_secret str The name of the Azure Key Vault secret containing a dictionary None vault_name str The name of the vault from which to obtain the secret. Defaults to None. None Returns: Type Description pd.DataFrame The query result as a pandas DataFrame. Source code in viadot/tasks/cloud_for_customers.py @defaults_from_attrs ( \"report_url\" , \"env\" , \"skip\" , \"top\" , ) def run ( self , report_url : str = None , env : str = \"QA\" , skip : int = 0 , top : int = 1000 , credentials_secret : str = None , vault_name : str = None , max_retries : int = 3 , retry_delay : timedelta = timedelta ( seconds = 10 ), ): \"\"\" Task for downloading data from the Cloud for Customers to a pandas DataFrame using report URL. Args: report_url (str, optional): The URL to the report. Defaults to None. env (str, optional): The environment to use. Defaults to 'QA'. skip (int, optional): Initial index value of reading row. Defaults to 0. top (int, optional): The value of top reading row. Defaults to 1000. credentials_secret (str, optional): The name of the Azure Key Vault secret containing a dictionary with C4C credentials (username & password). Defaults to None. vault_name (str, optional): The name of the vault from which to obtain the secret. Defaults to None. Returns: pd.DataFrame: The query result as a pandas DataFrame. \"\"\" if not credentials_secret : try : credentials_secret = PrefectSecret ( \"C4C_KV\" ) . run () except ValueError : pass if credentials_secret : credentials_str = AzureKeyVaultSecret ( credentials_secret , vault_name = vault_name ) . run () credentials = json . loads ( credentials_str ) else : credentials = local_config . get ( \"CLOUD_FOR_CUSTOMERS\" )[ env ] final_df = pd . DataFrame () next_batch = True while next_batch : new_url = f \" { report_url } &$top= { top } &$skip= { skip } \" chunk_from_url = CloudForCustomers ( report_url = new_url , env = env , credentials = credentials ) df = chunk_from_url . to_df () final_df = final_df . append ( df ) if not final_df . empty : df_count = df . count ()[ 1 ] if df_count != top : next_batch = False skip += top else : break return final_df","title":"run()"},{"location":"tutorials/adding_source/","text":"Adding a source 1. Add a source To add a source, create a new file in viadot/sources . The source must inherit from the Source base class and accept a credentials parameter. 2. Add a task Within the task, you should handle the authentication to the source. For this, utilize either a Prefect secret or the Azure Key Vault secret. See existing tasks, eg. AzureDataLakeDownload , for reference. Note that we sometimes also provide a default value for the secret name which is stored as a Prefect secret itself. This is so that you can safely publish your flow code in the \"infrastructure as code\" spirit, without revealing the names of the actual keys used in your vault. You can instead only provide the name of the Prefect secret holding the actual name. These defaults can be configured in your local Prefect config ( .prefect/config.toml ) or in Prefect cloud. For example, let's say you have a secret set by another department in your organization called my_service_principal , storing the credentials of the service account used to authenticate to the data lake. Let's assume the name of this service account should be protected. With the implementation used eg. in AzureDataLakeDownload , you can create Prefect secret called eg. my_service_account_1_name and only refer to this secret in your flow, eg. in this task, by setting sp_credentials_secret to my_service_account_1_name . 3. Integrate into a flow Now you can finally integrate the source into a full flow. See Adding a flow","title":"Tutorials"},{"location":"tutorials/adding_source/#adding-a-source","text":"","title":"Adding a source"},{"location":"tutorials/adding_source/#1-add-a-source","text":"To add a source, create a new file in viadot/sources . The source must inherit from the Source base class and accept a credentials parameter.","title":"1. Add a source"},{"location":"tutorials/adding_source/#2-add-a-task","text":"Within the task, you should handle the authentication to the source. For this, utilize either a Prefect secret or the Azure Key Vault secret. See existing tasks, eg. AzureDataLakeDownload , for reference. Note that we sometimes also provide a default value for the secret name which is stored as a Prefect secret itself. This is so that you can safely publish your flow code in the \"infrastructure as code\" spirit, without revealing the names of the actual keys used in your vault. You can instead only provide the name of the Prefect secret holding the actual name. These defaults can be configured in your local Prefect config ( .prefect/config.toml ) or in Prefect cloud. For example, let's say you have a secret set by another department in your organization called my_service_principal , storing the credentials of the service account used to authenticate to the data lake. Let's assume the name of this service account should be protected. With the implementation used eg. in AzureDataLakeDownload , you can create Prefect secret called eg. my_service_account_1_name and only refer to this secret in your flow, eg. in this task, by setting sp_credentials_secret to my_service_account_1_name .","title":"2. Add a task"},{"location":"tutorials/adding_source/#3-integrate-into-a-flow","text":"Now you can finally integrate the source into a full flow. See Adding a flow","title":"3. Integrate into a flow"},{"location":"tutorials/cloud_for_customers/","text":"How to pull CloudForCustomers data With Viadot you can pull data from CloudForCustomers API, save it in csv and parquet format on the Azure Data Lake. You can connect directly with prepeard report URL adress or with table adding special parameters to fetch the data. Pull data from Supermetrics and save output as a csv file on Azure Data Lake To pull data from CloudForCustomers we will create flow basing on CloudForCustomersReportToADLS viadot.flows.cloud_for_customers_report_to_adls.CloudForCustomersReportToADLS ( Flow ) __init__ ( self , name = None , report_url = None , url = None , endpoint = None , params = {}, fields = None , skip = 0 , top = 1000 , channels = None , months = None , years = None , env = 'QA' , c4c_credentials_secret = None , local_file_path = None , output_file_extension = '.csv' , adls_dir_path = None , adls_file_path = None , overwrite_adls = False , adls_sp_credentials_secret = None , if_empty = 'warn' , if_exists = 'replace' , timeout = 3600 , * args , ** kwargs ) special Flow for downloading data from different marketing APIs to a local CSV using Cloud for Customers API, then uploading it to Azure Data Lake. Parameters: Name Type Description Default name str The name of the flow. None report_url str The url to the API. Defaults to None. None url str ??? None endpoint str ??? None params dict ??? {} fields list ??? None skip int Initial index value of reading row. Defaults to 0. 0 top int The value of top reading row. Defaults to 1000. 1000 channels List[str] Filtering parameters passed to the url. Defaults to None. None months List[str] Filtering parameters passed to the url. Defaults to None. None years List[str] Filtering parameters passed to the url. Defaults to None. None env str ??? 'QA' c4c_credentials_secret str The name of the Azure Key Vault secret containing a dictionary with None local_file_path str Local destination path. Defaults to None. None output_file_extension str Output file extension - to allow selection of .csv for data which is not easy '.csv' adls_dir_path str Azure Data Lake destination folder/catalog path. Defaults to None. None adls_file_path str Azure Data Lake destination file path. Defaults to None. None overwrite_adls bool Whether to overwrite the file in ADLS. Defaults to False. False adls_sp_credentials_secret str The name of the Azure Key Vault secret containing a dictionary with None if_empty str What to do if the Supermetrics query returns no data. Defaults to \"warn\". 'warn' if_exists str What to do if the local file already exists. Defaults to \"replace\". 'replace' timeout(int, optional The amount of time (in seconds) to wait while running this task before a timeout occurs. Defaults to 3600. required Source code in viadot/flows/cloud_for_customers_report_to_adls.py def __init__ ( self , name : str = None , report_url : str = None , url : str = None , endpoint : str = None , params : Dict [ str , Any ] = {}, fields : List [ str ] = None , skip : int = 0 , top : int = 1000 , channels : List [ str ] = None , months : List [ str ] = None , years : List [ str ] = None , env : str = \"QA\" , c4c_credentials_secret : str = None , local_file_path : str = None , output_file_extension : str = \".csv\" , adls_dir_path : str = None , adls_file_path : str = None , overwrite_adls : bool = False , adls_sp_credentials_secret : str = None , if_empty : str = \"warn\" , if_exists : str = \"replace\" , timeout : int = 3600 , * args : List [ any ], ** kwargs : Dict [ str , Any ], ): \"\"\" Flow for downloading data from different marketing APIs to a local CSV using Cloud for Customers API, then uploading it to Azure Data Lake. Args: name (str): The name of the flow. report_url (str, optional): The url to the API. Defaults to None. url (str, optional): ??? endpoint (str, optional): ??? params (dict, optional): ??? fields (list, optional): ??? skip (int, optional): Initial index value of reading row. Defaults to 0. top (int, optional): The value of top reading row. Defaults to 1000. channels (List[str], optional): Filtering parameters passed to the url. Defaults to None. months (List[str], optional): Filtering parameters passed to the url. Defaults to None. years (List[str], optional): Filtering parameters passed to the url. Defaults to None. env (str, optional): ??? c4c_credentials_secret (str, optional): The name of the Azure Key Vault secret containing a dictionary with username and password for the Cloud for Customers instance. local_file_path (str, optional): Local destination path. Defaults to None. output_file_extension (str, optional): Output file extension - to allow selection of .csv for data which is not easy to handle with parquet. Defaults to \".csv\". adls_dir_path (str, optional): Azure Data Lake destination folder/catalog path. Defaults to None. adls_file_path (str, optional): Azure Data Lake destination file path. Defaults to None. overwrite_adls (bool, optional): Whether to overwrite the file in ADLS. Defaults to False. adls_sp_credentials_secret (str, optional): The name of the Azure Key Vault secret containing a dictionary with ACCOUNT_NAME and Service Principal credentials (TENANT_ID, CLIENT_ID, CLIENT_SECRET) for the Azure Data Lake. Defaults to None. if_empty (str, optional): What to do if the Supermetrics query returns no data. Defaults to \"warn\". if_exists (str, optional): What to do if the local file already exists. Defaults to \"replace\". timeout(int, optional): The amount of time (in seconds) to wait while running this task before a timeout occurs. Defaults to 3600. \"\"\" self . report_url = report_url self . skip = skip self . top = top self . if_empty = if_empty self . env = env self . c4c_credentials_secret = c4c_credentials_secret self . timeout = timeout # AzureDataLakeUpload self . adls_sp_credentials_secret = adls_sp_credentials_secret self . if_exists = if_exists self . overwrite_adls = overwrite_adls self . output_file_extension = output_file_extension self . local_file_path = ( local_file_path or slugify ( name ) + self . output_file_extension ) self . now = str ( pendulum . now ( \"utc\" )) self . adls_dir_path = adls_dir_path self . adls_file_path = adls_file_path or os . path . join ( adls_dir_path , self . now + self . output_file_extension ) # in case of non-report invoking self . url = url self . endpoint = endpoint self . params = params self . fields = fields # filtering report_url for reports self . channels = channels self . months = months self . years = years self . report_urls_with_filters = [ self . report_url ] self . report_urls_with_filters = self . create_url_with_fields ( fields_list = self . channels , filter_code = \"CCHANNETZTEXT12CE6C2FA0D77995\" ) self . report_urls_with_filters = self . create_url_with_fields ( fields_list = self . months , filter_code = \"CMONTH_ID\" ) self . report_urls_with_filters = self . create_url_with_fields ( fields_list = self . years , filter_code = \"CYEAR_ID\" ) super () . __init__ ( * args , name = name , ** kwargs ) self . gen_flow ()","title":"How to pull CloudForCustomers data"},{"location":"tutorials/cloud_for_customers/#how-to-pull-cloudforcustomers-data","text":"With Viadot you can pull data from CloudForCustomers API, save it in csv and parquet format on the Azure Data Lake. You can connect directly with prepeard report URL adress or with table adding special parameters to fetch the data.","title":"How to pull CloudForCustomers data"},{"location":"tutorials/cloud_for_customers/#pull-data-from-supermetrics-and-save-output-as-a-csv-file-on-azure-data-lake","text":"To pull data from CloudForCustomers we will create flow basing on CloudForCustomersReportToADLS","title":"Pull data from Supermetrics and save output as a csv file on Azure Data Lake"},{"location":"tutorials/cloud_for_customers/#viadot.flows.cloud_for_customers_report_to_adls.CloudForCustomersReportToADLS","text":"","title":"CloudForCustomersReportToADLS"},{"location":"tutorials/cloud_for_customers/#viadot.flows.cloud_for_customers_report_to_adls.CloudForCustomersReportToADLS.__init__","text":"Flow for downloading data from different marketing APIs to a local CSV using Cloud for Customers API, then uploading it to Azure Data Lake. Parameters: Name Type Description Default name str The name of the flow. None report_url str The url to the API. Defaults to None. None url str ??? None endpoint str ??? None params dict ??? {} fields list ??? None skip int Initial index value of reading row. Defaults to 0. 0 top int The value of top reading row. Defaults to 1000. 1000 channels List[str] Filtering parameters passed to the url. Defaults to None. None months List[str] Filtering parameters passed to the url. Defaults to None. None years List[str] Filtering parameters passed to the url. Defaults to None. None env str ??? 'QA' c4c_credentials_secret str The name of the Azure Key Vault secret containing a dictionary with None local_file_path str Local destination path. Defaults to None. None output_file_extension str Output file extension - to allow selection of .csv for data which is not easy '.csv' adls_dir_path str Azure Data Lake destination folder/catalog path. Defaults to None. None adls_file_path str Azure Data Lake destination file path. Defaults to None. None overwrite_adls bool Whether to overwrite the file in ADLS. Defaults to False. False adls_sp_credentials_secret str The name of the Azure Key Vault secret containing a dictionary with None if_empty str What to do if the Supermetrics query returns no data. Defaults to \"warn\". 'warn' if_exists str What to do if the local file already exists. Defaults to \"replace\". 'replace' timeout(int, optional The amount of time (in seconds) to wait while running this task before a timeout occurs. Defaults to 3600. required Source code in viadot/flows/cloud_for_customers_report_to_adls.py def __init__ ( self , name : str = None , report_url : str = None , url : str = None , endpoint : str = None , params : Dict [ str , Any ] = {}, fields : List [ str ] = None , skip : int = 0 , top : int = 1000 , channels : List [ str ] = None , months : List [ str ] = None , years : List [ str ] = None , env : str = \"QA\" , c4c_credentials_secret : str = None , local_file_path : str = None , output_file_extension : str = \".csv\" , adls_dir_path : str = None , adls_file_path : str = None , overwrite_adls : bool = False , adls_sp_credentials_secret : str = None , if_empty : str = \"warn\" , if_exists : str = \"replace\" , timeout : int = 3600 , * args : List [ any ], ** kwargs : Dict [ str , Any ], ): \"\"\" Flow for downloading data from different marketing APIs to a local CSV using Cloud for Customers API, then uploading it to Azure Data Lake. Args: name (str): The name of the flow. report_url (str, optional): The url to the API. Defaults to None. url (str, optional): ??? endpoint (str, optional): ??? params (dict, optional): ??? fields (list, optional): ??? skip (int, optional): Initial index value of reading row. Defaults to 0. top (int, optional): The value of top reading row. Defaults to 1000. channels (List[str], optional): Filtering parameters passed to the url. Defaults to None. months (List[str], optional): Filtering parameters passed to the url. Defaults to None. years (List[str], optional): Filtering parameters passed to the url. Defaults to None. env (str, optional): ??? c4c_credentials_secret (str, optional): The name of the Azure Key Vault secret containing a dictionary with username and password for the Cloud for Customers instance. local_file_path (str, optional): Local destination path. Defaults to None. output_file_extension (str, optional): Output file extension - to allow selection of .csv for data which is not easy to handle with parquet. Defaults to \".csv\". adls_dir_path (str, optional): Azure Data Lake destination folder/catalog path. Defaults to None. adls_file_path (str, optional): Azure Data Lake destination file path. Defaults to None. overwrite_adls (bool, optional): Whether to overwrite the file in ADLS. Defaults to False. adls_sp_credentials_secret (str, optional): The name of the Azure Key Vault secret containing a dictionary with ACCOUNT_NAME and Service Principal credentials (TENANT_ID, CLIENT_ID, CLIENT_SECRET) for the Azure Data Lake. Defaults to None. if_empty (str, optional): What to do if the Supermetrics query returns no data. Defaults to \"warn\". if_exists (str, optional): What to do if the local file already exists. Defaults to \"replace\". timeout(int, optional): The amount of time (in seconds) to wait while running this task before a timeout occurs. Defaults to 3600. \"\"\" self . report_url = report_url self . skip = skip self . top = top self . if_empty = if_empty self . env = env self . c4c_credentials_secret = c4c_credentials_secret self . timeout = timeout # AzureDataLakeUpload self . adls_sp_credentials_secret = adls_sp_credentials_secret self . if_exists = if_exists self . overwrite_adls = overwrite_adls self . output_file_extension = output_file_extension self . local_file_path = ( local_file_path or slugify ( name ) + self . output_file_extension ) self . now = str ( pendulum . now ( \"utc\" )) self . adls_dir_path = adls_dir_path self . adls_file_path = adls_file_path or os . path . join ( adls_dir_path , self . now + self . output_file_extension ) # in case of non-report invoking self . url = url self . endpoint = endpoint self . params = params self . fields = fields # filtering report_url for reports self . channels = channels self . months = months self . years = years self . report_urls_with_filters = [ self . report_url ] self . report_urls_with_filters = self . create_url_with_fields ( fields_list = self . channels , filter_code = \"CCHANNETZTEXT12CE6C2FA0D77995\" ) self . report_urls_with_filters = self . create_url_with_fields ( fields_list = self . months , filter_code = \"CMONTH_ID\" ) self . report_urls_with_filters = self . create_url_with_fields ( fields_list = self . years , filter_code = \"CYEAR_ID\" ) super () . __init__ ( * args , name = name , ** kwargs ) self . gen_flow ()","title":"__init__()"},{"location":"tutorials/sharepoint/","text":"How to pull excel file from Sharepoint With Viadot you can download Excel file from Sharepoint and then upload it to Azure Data Lake. You can set a URL to file on Sharepoint an specify parameters such as path to local Excel file, number of rows and sheet number to be extracted. Pull data from Sharepoint and save output as a csv file on Azure Data Lake To pull Excel file from Sharepint we create flow basing on SharepointToADLS viadot.flows.sharepoint_to_adls.SharepointToADLS ( Flow ) __init__ ( self , name , url_to_file = None , nrows_to_df = None , path_to_file = None , sheet_number = None , validate_excel_file = False , output_file_extension = '.csv' , local_dir_path = None , adls_dir_path = None , adls_file_name = None , adls_sp_credentials_secret = None , overwrite_adls = False , if_empty = 'warn' , if_exists = 'replace' , timeout = 3600 , * args , ** kwargs ) special Flow for downloading Excel file from Sharepoint then uploading it to Azure Data Lake. Parameters: Name Type Description Default name str The name of the flow. required url_to_file str Link to a file on Sharepoint. Defaults to None. (e.g : https://{tenant_name}.sharepoint.com/sites/{folder}/Shared%20Documents/Dashboard/file). None nrows_to_df int Number of rows to read at a time. Defaults to 50000. Defaults to None. None path_to_file str Path to local Excel file. Defaults to None. None sheet_number int Sheet number to be extracted from file. Counting from 0, if None all sheets are axtracted. Defaults to None. None validate_excel_file bool Check if columns in separate sheets are the same. Defaults to False. False output_file_extension str Output file extension - to allow selection of .csv for data which is not easy to handle with parquet. Defaults to \".csv\". '.csv' local_dir_path str File directory. Defaults to None. None adls_dir_path str Azure Data Lake destination folder/catalog path. Defaults to None. None adls_file_name str Name of file in ADLS. Defaults to None. None adls_sp_credentials_secret str The name of the Azure Key Vault secret containing a dictionary with None overwrite_adls bool Whether to overwrite files in the lake. Defaults to False. False if_empty str What to do if query returns no data. Defaults to \"warn\". 'warn' timeout(int, optional The amount of time (in seconds) to wait while running this task before a timeout occurs. Defaults to 3600. required Source code in viadot/flows/sharepoint_to_adls.py def __init__ ( self , name : str , url_to_file : str = None , nrows_to_df : int = None , path_to_file : str = None , sheet_number : int = None , validate_excel_file : bool = False , output_file_extension : str = \".csv\" , local_dir_path : str = None , adls_dir_path : str = None , adls_file_name : str = None , adls_sp_credentials_secret : str = None , overwrite_adls : bool = False , if_empty : str = \"warn\" , if_exists : str = \"replace\" , timeout : int = 3600 , * args : List [ any ], ** kwargs : Dict [ str , Any ], ): \"\"\" Flow for downloading Excel file from Sharepoint then uploading it to Azure Data Lake. Args: name (str): The name of the flow. url_to_file (str, optional): Link to a file on Sharepoint. Defaults to None. (e.g : https://{tenant_name}.sharepoint.com/sites/{folder}/Shared%20Documents/Dashboard/file). nrows_to_df (int, optional): Number of rows to read at a time. Defaults to 50000. Defaults to None. path_to_file (str, optional): Path to local Excel file. Defaults to None. sheet_number (int, optional): Sheet number to be extracted from file. Counting from 0, if None all sheets are axtracted. Defaults to None. validate_excel_file (bool, optional): Check if columns in separate sheets are the same. Defaults to False. output_file_extension (str, optional): Output file extension - to allow selection of .csv for data which is not easy to handle with parquet. Defaults to \".csv\". local_dir_path (str, optional): File directory. Defaults to None. adls_dir_path (str, optional): Azure Data Lake destination folder/catalog path. Defaults to None. adls_file_name (str, optional): Name of file in ADLS. Defaults to None. adls_sp_credentials_secret (str, optional): The name of the Azure Key Vault secret containing a dictionary with ACCOUNT_NAME and Service Principal credentials (TENANT_ID, CLIENT_ID, CLIENT_SECRET) for the Azure Data Lake. Defaults to None. overwrite_adls (bool, optional): Whether to overwrite files in the lake. Defaults to False. if_empty (str, optional): What to do if query returns no data. Defaults to \"warn\". timeout(int, optional): The amount of time (in seconds) to wait while running this task before a timeout occurs. Defaults to 3600. \"\"\" # SharepointToDF self . if_empty = if_empty self . nrows = nrows_to_df self . path_to_file = path_to_file self . url_to_file = url_to_file self . local_dir_path = local_dir_path self . sheet_number = sheet_number self . validate_excel_file = validate_excel_file self . timeout = timeout # AzureDataLakeUpload self . overwrite = overwrite_adls self . adls_sp_credentials_secret = adls_sp_credentials_secret self . if_exists = if_exists self . output_file_extension = output_file_extension self . now = str ( pendulum . now ( \"utc\" )) if self . local_dir_path is not None : self . local_file_path = ( self . local_dir_path + self . slugify ( name ) + self . output_file_extension ) else : self . local_file_path = self . slugify ( name ) + self . output_file_extension self . local_json_path = self . slugify ( name ) + \".json\" self . adls_dir_path = adls_dir_path if adls_file_name is not None : self . adls_file_path = os . path . join ( adls_dir_path , adls_file_name ) self . adls_schema_file_dir_file = os . path . join ( adls_dir_path , \"schema\" , Path ( adls_file_name ) . stem + \".json\" ) else : self . adls_file_path = os . path . join ( adls_dir_path , self . now + self . output_file_extension ) self . adls_schema_file_dir_file = os . path . join ( adls_dir_path , \"schema\" , self . now + \".json\" ) super () . __init__ ( * args , name = name , ** kwargs ) self . gen_flow ()","title":"How to pull excel file from Sharepoint"},{"location":"tutorials/sharepoint/#how-to-pull-excel-file-from-sharepoint","text":"With Viadot you can download Excel file from Sharepoint and then upload it to Azure Data Lake. You can set a URL to file on Sharepoint an specify parameters such as path to local Excel file, number of rows and sheet number to be extracted.","title":"How to pull excel file from Sharepoint"},{"location":"tutorials/sharepoint/#pull-data-from-sharepoint-and-save-output-as-a-csv-file-on-azure-data-lake","text":"To pull Excel file from Sharepint we create flow basing on SharepointToADLS","title":"Pull data from Sharepoint and save output as a csv file on Azure Data Lake"},{"location":"tutorials/sharepoint/#viadot.flows.sharepoint_to_adls.SharepointToADLS","text":"","title":"SharepointToADLS"},{"location":"tutorials/sharepoint/#viadot.flows.sharepoint_to_adls.SharepointToADLS.__init__","text":"Flow for downloading Excel file from Sharepoint then uploading it to Azure Data Lake. Parameters: Name Type Description Default name str The name of the flow. required url_to_file str Link to a file on Sharepoint. Defaults to None. (e.g : https://{tenant_name}.sharepoint.com/sites/{folder}/Shared%20Documents/Dashboard/file). None nrows_to_df int Number of rows to read at a time. Defaults to 50000. Defaults to None. None path_to_file str Path to local Excel file. Defaults to None. None sheet_number int Sheet number to be extracted from file. Counting from 0, if None all sheets are axtracted. Defaults to None. None validate_excel_file bool Check if columns in separate sheets are the same. Defaults to False. False output_file_extension str Output file extension - to allow selection of .csv for data which is not easy to handle with parquet. Defaults to \".csv\". '.csv' local_dir_path str File directory. Defaults to None. None adls_dir_path str Azure Data Lake destination folder/catalog path. Defaults to None. None adls_file_name str Name of file in ADLS. Defaults to None. None adls_sp_credentials_secret str The name of the Azure Key Vault secret containing a dictionary with None overwrite_adls bool Whether to overwrite files in the lake. Defaults to False. False if_empty str What to do if query returns no data. Defaults to \"warn\". 'warn' timeout(int, optional The amount of time (in seconds) to wait while running this task before a timeout occurs. Defaults to 3600. required Source code in viadot/flows/sharepoint_to_adls.py def __init__ ( self , name : str , url_to_file : str = None , nrows_to_df : int = None , path_to_file : str = None , sheet_number : int = None , validate_excel_file : bool = False , output_file_extension : str = \".csv\" , local_dir_path : str = None , adls_dir_path : str = None , adls_file_name : str = None , adls_sp_credentials_secret : str = None , overwrite_adls : bool = False , if_empty : str = \"warn\" , if_exists : str = \"replace\" , timeout : int = 3600 , * args : List [ any ], ** kwargs : Dict [ str , Any ], ): \"\"\" Flow for downloading Excel file from Sharepoint then uploading it to Azure Data Lake. Args: name (str): The name of the flow. url_to_file (str, optional): Link to a file on Sharepoint. Defaults to None. (e.g : https://{tenant_name}.sharepoint.com/sites/{folder}/Shared%20Documents/Dashboard/file). nrows_to_df (int, optional): Number of rows to read at a time. Defaults to 50000. Defaults to None. path_to_file (str, optional): Path to local Excel file. Defaults to None. sheet_number (int, optional): Sheet number to be extracted from file. Counting from 0, if None all sheets are axtracted. Defaults to None. validate_excel_file (bool, optional): Check if columns in separate sheets are the same. Defaults to False. output_file_extension (str, optional): Output file extension - to allow selection of .csv for data which is not easy to handle with parquet. Defaults to \".csv\". local_dir_path (str, optional): File directory. Defaults to None. adls_dir_path (str, optional): Azure Data Lake destination folder/catalog path. Defaults to None. adls_file_name (str, optional): Name of file in ADLS. Defaults to None. adls_sp_credentials_secret (str, optional): The name of the Azure Key Vault secret containing a dictionary with ACCOUNT_NAME and Service Principal credentials (TENANT_ID, CLIENT_ID, CLIENT_SECRET) for the Azure Data Lake. Defaults to None. overwrite_adls (bool, optional): Whether to overwrite files in the lake. Defaults to False. if_empty (str, optional): What to do if query returns no data. Defaults to \"warn\". timeout(int, optional): The amount of time (in seconds) to wait while running this task before a timeout occurs. Defaults to 3600. \"\"\" # SharepointToDF self . if_empty = if_empty self . nrows = nrows_to_df self . path_to_file = path_to_file self . url_to_file = url_to_file self . local_dir_path = local_dir_path self . sheet_number = sheet_number self . validate_excel_file = validate_excel_file self . timeout = timeout # AzureDataLakeUpload self . overwrite = overwrite_adls self . adls_sp_credentials_secret = adls_sp_credentials_secret self . if_exists = if_exists self . output_file_extension = output_file_extension self . now = str ( pendulum . now ( \"utc\" )) if self . local_dir_path is not None : self . local_file_path = ( self . local_dir_path + self . slugify ( name ) + self . output_file_extension ) else : self . local_file_path = self . slugify ( name ) + self . output_file_extension self . local_json_path = self . slugify ( name ) + \".json\" self . adls_dir_path = adls_dir_path if adls_file_name is not None : self . adls_file_path = os . path . join ( adls_dir_path , adls_file_name ) self . adls_schema_file_dir_file = os . path . join ( adls_dir_path , \"schema\" , Path ( adls_file_name ) . stem + \".json\" ) else : self . adls_file_path = os . path . join ( adls_dir_path , self . now + self . output_file_extension ) self . adls_schema_file_dir_file = os . path . join ( adls_dir_path , \"schema\" , self . now + \".json\" ) super () . __init__ ( * args , name = name , ** kwargs ) self . gen_flow ()","title":"__init__()"},{"location":"tutorials/supermetrics/","text":"How to pull Supermetrics data With Viadot you have opportunity to pull data from Supermetrics API, save it in parquet format on the Azure Data Lake. You can also load this data do Azure SQL DB. If you need more info about Supermetric API please visit https://supermetrics.com/docs/product-api-getting-started/ Pull data from Supermetrics and save output as a parquet file on Azure Data Lake To pull data from Supermetrics we will create flow basing on SupermetricsToADLS viadot.flows.supermetrics_to_adls.SupermetricsToADLS ( Flow ) __init__ ( self , name , ds_id , ds_accounts , fields , ds_user = None , ds_segments = None , date_range_type = None , start_date = None , end_date = None , settings = None , filter = None , max_rows = 1000000 , max_columns = None , order_columns = None , expectation_suite = None , evaluation_parameters = None , keep_validation_output = False , output_file_extension = '.parquet' , local_file_path = None , adls_file_name = None , adls_dir_path = None , overwrite_adls = True , if_empty = 'warn' , if_exists = 'replace' , adls_sp_credentials_secret = None , max_download_retries = 5 , supermetrics_task_timeout = 1800 , parallel = True , tags = [ 'extract' ], vault_name = None , check_missing_data = True , timeout = 3600 , * args , ** kwargs ) special Flow for downloading data from different marketing APIs to a local CSV using Supermetrics API, then uploading it to Azure Data Lake. Parameters: Name Type Description Default name str The name of the flow. required ds_id str A query parameter passed to the SupermetricsToCSV task required ds_accounts List[str] A query parameter passed to the SupermetricsToCSV task required ds_user str A query parameter passed to the SupermetricsToCSV task None fields List[str] A query parameter passed to the SupermetricsToCSV task required ds_segments List[str] A query parameter passed to the SupermetricsToCSV task. Defaults to None. None date_range_type str A query parameter passed to the SupermetricsToCSV task. Defaults to None. None start_date str A query parameter to pass start date to the date range filter. Defaults to None. None end_date str A query parameter to pass end date to the date range filter. Defaults to None. None settings Dict[str, Any] A query parameter passed to the SupermetricsToCSV task. Defaults to None. None filter str A query parameter passed to the SupermetricsToCSV task. Defaults to None. None max_rows int A query parameter passed to the SupermetricsToCSV task. Defaults to 1000000. 1000000 max_columns int A query parameter passed to the SupermetricsToCSV task. Defaults to None. None order_columns str A query parameter passed to the SupermetricsToCSV task. Defaults to None. None expectation_suite dict The Great Expectations suite used to valiaate the data. Defaults to None. None evaluation_parameters str A dictionary containing evaluation parameters for the validation. Defaults to None. None keep_validation_output bool Whether to keep the output files generated by the Great Expectations task. Defaults to False. False local_file_path str Local destination path. Defaults to None. None adls_file_name str Name of file in ADLS. Defaults to None. None output_file_extension str Output file extension - to allow selection of .csv for data which is not easy to handle with parquet. Defaults to \".parquet\".. '.parquet' adls_dir_path str Azure Data Lake destination folder/catalog path. Defaults to None. None sep str The separator to use in the CSV. Defaults to \" \". required overwrite_adls bool Whether to overwrite the file in ADLS. Defaults to True. True if_empty str What to do if the Supermetrics query returns no data. Defaults to \"warn\". 'warn' adls_sp_credentials_secret str The name of the Azure Key Vault secret containing a dictionary with None max_download_retries int How many times to retry the download. Defaults to 5. 5 supermetrics_task_timeout int The timeout for the download task. Defaults to 60*30. 1800 parallel bool Whether to parallelize the downloads. Defaults to True. True tags List[str] Flow tags to use, eg. to control flow concurrency. Defaults to [\"extract\"]. ['extract'] vault_name str The name of the vault from which to obtain the secrets. Defaults to None. None check_missing_data bool Whether to check missing data. Defaults to True. True timeout(int, optional The amount of time (in seconds) to wait while running this task before a timeout occurs. Defaults to 3600. required Source code in viadot/flows/supermetrics_to_adls.py def __init__ ( self , name : str , ds_id : str , ds_accounts : List [ str ], fields : List [ str ], ds_user : str = None , ds_segments : List [ str ] = None , date_range_type : str = None , start_date : str = None , end_date : str = None , settings : Dict [ str , Any ] = None , filter : str = None , max_rows : int = 1000000 , max_columns : int = None , order_columns : str = None , expectation_suite : dict = None , evaluation_parameters : dict = None , keep_validation_output : bool = False , output_file_extension : str = \".parquet\" , local_file_path : str = None , adls_file_name : str = None , adls_dir_path : str = None , overwrite_adls : bool = True , if_empty : str = \"warn\" , if_exists : str = \"replace\" , adls_sp_credentials_secret : str = None , max_download_retries : int = 5 , supermetrics_task_timeout : int = 60 * 30 , parallel : bool = True , tags : List [ str ] = [ \"extract\" ], vault_name : str = None , check_missing_data : bool = True , timeout : int = 3600 , * args : List [ any ], ** kwargs : Dict [ str , Any ], ): \"\"\" Flow for downloading data from different marketing APIs to a local CSV using Supermetrics API, then uploading it to Azure Data Lake. Args: name (str): The name of the flow. ds_id (str): A query parameter passed to the SupermetricsToCSV task ds_accounts (List[str]): A query parameter passed to the SupermetricsToCSV task ds_user (str): A query parameter passed to the SupermetricsToCSV task fields (List[str]): A query parameter passed to the SupermetricsToCSV task ds_segments (List[str], optional): A query parameter passed to the SupermetricsToCSV task. Defaults to None. date_range_type (str, optional): A query parameter passed to the SupermetricsToCSV task. Defaults to None. start_date (str, optional): A query parameter to pass start date to the date range filter. Defaults to None. end_date (str, optional): A query parameter to pass end date to the date range filter. Defaults to None. settings (Dict[str, Any], optional): A query parameter passed to the SupermetricsToCSV task. Defaults to None. filter (str, optional): A query parameter passed to the SupermetricsToCSV task. Defaults to None. max_rows (int, optional): A query parameter passed to the SupermetricsToCSV task. Defaults to 1000000. max_columns (int, optional): A query parameter passed to the SupermetricsToCSV task. Defaults to None. order_columns (str, optional): A query parameter passed to the SupermetricsToCSV task. Defaults to None. expectation_suite (dict, optional): The Great Expectations suite used to valiaate the data. Defaults to None. evaluation_parameters (str, optional): A dictionary containing evaluation parameters for the validation. Defaults to None. keep_validation_output (bool, optional): Whether to keep the output files generated by the Great Expectations task. Defaults to False. Currently, only GitHub URLs are supported. Defaults to None. local_file_path (str, optional): Local destination path. Defaults to None. adls_file_name (str, optional): Name of file in ADLS. Defaults to None. output_file_extension (str, optional): Output file extension - to allow selection of .csv for data which is not easy to handle with parquet. Defaults to \".parquet\".. adls_dir_path (str, optional): Azure Data Lake destination folder/catalog path. Defaults to None. sep (str, optional): The separator to use in the CSV. Defaults to \"\\t\". overwrite_adls (bool, optional): Whether to overwrite the file in ADLS. Defaults to True. if_empty (str, optional): What to do if the Supermetrics query returns no data. Defaults to \"warn\". adls_sp_credentials_secret (str, optional): The name of the Azure Key Vault secret containing a dictionary with ACCOUNT_NAME and Service Principal credentials (TENANT_ID, CLIENT_ID, CLIENT_SECRET) for the Azure Data Lake. Defaults to None. max_download_retries (int, optional): How many times to retry the download. Defaults to 5. supermetrics_task_timeout (int, optional): The timeout for the download task. Defaults to 60*30. parallel (bool, optional): Whether to parallelize the downloads. Defaults to True. tags (List[str], optional): Flow tags to use, eg. to control flow concurrency. Defaults to [\"extract\"]. vault_name (str, optional): The name of the vault from which to obtain the secrets. Defaults to None. check_missing_data (bool, optional): Whether to check missing data. Defaults to True. timeout(int, optional): The amount of time (in seconds) to wait while running this task before a timeout occurs. Defaults to 3600. \"\"\" if not ds_user : try : ds_user = PrefectSecret ( \"SUPERMETRICS_DEFAULT_USER\" ) . run () except ValueError as e : msg = \"Neither 'ds_user' parameter nor 'SUPERMETRICS_DEFAULT_USER' secret were not specified\" raise ValueError ( msg ) from e self . flow_name = name self . check_missing_data = check_missing_data self . timeout = timeout # SupermetricsToDF self . ds_id = ds_id self . ds_accounts = ds_accounts self . ds_segments = ds_segments self . ds_user = ds_user self . fields = fields self . date_range_type = date_range_type self . start_date = start_date self . end_date = end_date self . settings = settings self . filter = filter self . max_rows = max_rows self . max_columns = max_columns self . order_columns = order_columns self . if_exists = if_exists self . output_file_extension = output_file_extension # RunGreatExpectationsValidation self . expectation_suite = expectation_suite self . expectations_path = \"/home/viadot/tmp/expectations\" self . expectation_suite_name = expectation_suite [ \"expectation_suite_name\" ] self . evaluation_parameters = evaluation_parameters self . keep_validation_output = keep_validation_output # AzureDataLakeUpload self . local_file_path = ( local_file_path or self . slugify ( name ) + self . output_file_extension ) self . local_json_path = self . slugify ( name ) + \".json\" self . now = str ( pendulum . now ( \"utc\" )) self . adls_dir_path = adls_dir_path if adls_file_name is not None : self . adls_file_path = os . path . join ( adls_dir_path , adls_file_name ) self . adls_schema_file_dir_file = os . path . join ( adls_dir_path , \"schema\" , Path ( adls_file_name ) . stem + \".json\" ) else : self . adls_file_path = os . path . join ( adls_dir_path , self . now + self . output_file_extension ) self . adls_schema_file_dir_file = os . path . join ( adls_dir_path , \"schema\" , self . now + \".json\" ) self . overwrite_adls = overwrite_adls self . if_empty = if_empty self . adls_sp_credentials_secret = adls_sp_credentials_secret # Global self . max_download_retries = max_download_retries self . supermetrics_task_timeout = supermetrics_task_timeout self . parallel = parallel self . tags = tags self . vault_name = vault_name super () . __init__ ( * args , name = name , ** kwargs ) self . gen_flow () Data types are automaticly detected and mapped to meet Microsoft Azure SQL Database requirments. Schema json will be stored in the data lake (parquet_file_directory/schema)","title":"How to pull Supermetrics data"},{"location":"tutorials/supermetrics/#how-to-pull-supermetrics-data","text":"With Viadot you have opportunity to pull data from Supermetrics API, save it in parquet format on the Azure Data Lake. You can also load this data do Azure SQL DB. If you need more info about Supermetric API please visit https://supermetrics.com/docs/product-api-getting-started/","title":"How to pull Supermetrics data"},{"location":"tutorials/supermetrics/#pull-data-from-supermetrics-and-save-output-as-a-parquet-file-on-azure-data-lake","text":"To pull data from Supermetrics we will create flow basing on SupermetricsToADLS","title":"Pull data from Supermetrics and save output as a parquet file on Azure Data Lake"},{"location":"tutorials/supermetrics/#viadot.flows.supermetrics_to_adls.SupermetricsToADLS","text":"","title":"SupermetricsToADLS"},{"location":"tutorials/supermetrics/#viadot.flows.supermetrics_to_adls.SupermetricsToADLS.__init__","text":"Flow for downloading data from different marketing APIs to a local CSV using Supermetrics API, then uploading it to Azure Data Lake. Parameters: Name Type Description Default name str The name of the flow. required ds_id str A query parameter passed to the SupermetricsToCSV task required ds_accounts List[str] A query parameter passed to the SupermetricsToCSV task required ds_user str A query parameter passed to the SupermetricsToCSV task None fields List[str] A query parameter passed to the SupermetricsToCSV task required ds_segments List[str] A query parameter passed to the SupermetricsToCSV task. Defaults to None. None date_range_type str A query parameter passed to the SupermetricsToCSV task. Defaults to None. None start_date str A query parameter to pass start date to the date range filter. Defaults to None. None end_date str A query parameter to pass end date to the date range filter. Defaults to None. None settings Dict[str, Any] A query parameter passed to the SupermetricsToCSV task. Defaults to None. None filter str A query parameter passed to the SupermetricsToCSV task. Defaults to None. None max_rows int A query parameter passed to the SupermetricsToCSV task. Defaults to 1000000. 1000000 max_columns int A query parameter passed to the SupermetricsToCSV task. Defaults to None. None order_columns str A query parameter passed to the SupermetricsToCSV task. Defaults to None. None expectation_suite dict The Great Expectations suite used to valiaate the data. Defaults to None. None evaluation_parameters str A dictionary containing evaluation parameters for the validation. Defaults to None. None keep_validation_output bool Whether to keep the output files generated by the Great Expectations task. Defaults to False. False local_file_path str Local destination path. Defaults to None. None adls_file_name str Name of file in ADLS. Defaults to None. None output_file_extension str Output file extension - to allow selection of .csv for data which is not easy to handle with parquet. Defaults to \".parquet\".. '.parquet' adls_dir_path str Azure Data Lake destination folder/catalog path. Defaults to None. None sep str The separator to use in the CSV. Defaults to \" \". required overwrite_adls bool Whether to overwrite the file in ADLS. Defaults to True. True if_empty str What to do if the Supermetrics query returns no data. Defaults to \"warn\". 'warn' adls_sp_credentials_secret str The name of the Azure Key Vault secret containing a dictionary with None max_download_retries int How many times to retry the download. Defaults to 5. 5 supermetrics_task_timeout int The timeout for the download task. Defaults to 60*30. 1800 parallel bool Whether to parallelize the downloads. Defaults to True. True tags List[str] Flow tags to use, eg. to control flow concurrency. Defaults to [\"extract\"]. ['extract'] vault_name str The name of the vault from which to obtain the secrets. Defaults to None. None check_missing_data bool Whether to check missing data. Defaults to True. True timeout(int, optional The amount of time (in seconds) to wait while running this task before a timeout occurs. Defaults to 3600. required Source code in viadot/flows/supermetrics_to_adls.py def __init__ ( self , name : str , ds_id : str , ds_accounts : List [ str ], fields : List [ str ], ds_user : str = None , ds_segments : List [ str ] = None , date_range_type : str = None , start_date : str = None , end_date : str = None , settings : Dict [ str , Any ] = None , filter : str = None , max_rows : int = 1000000 , max_columns : int = None , order_columns : str = None , expectation_suite : dict = None , evaluation_parameters : dict = None , keep_validation_output : bool = False , output_file_extension : str = \".parquet\" , local_file_path : str = None , adls_file_name : str = None , adls_dir_path : str = None , overwrite_adls : bool = True , if_empty : str = \"warn\" , if_exists : str = \"replace\" , adls_sp_credentials_secret : str = None , max_download_retries : int = 5 , supermetrics_task_timeout : int = 60 * 30 , parallel : bool = True , tags : List [ str ] = [ \"extract\" ], vault_name : str = None , check_missing_data : bool = True , timeout : int = 3600 , * args : List [ any ], ** kwargs : Dict [ str , Any ], ): \"\"\" Flow for downloading data from different marketing APIs to a local CSV using Supermetrics API, then uploading it to Azure Data Lake. Args: name (str): The name of the flow. ds_id (str): A query parameter passed to the SupermetricsToCSV task ds_accounts (List[str]): A query parameter passed to the SupermetricsToCSV task ds_user (str): A query parameter passed to the SupermetricsToCSV task fields (List[str]): A query parameter passed to the SupermetricsToCSV task ds_segments (List[str], optional): A query parameter passed to the SupermetricsToCSV task. Defaults to None. date_range_type (str, optional): A query parameter passed to the SupermetricsToCSV task. Defaults to None. start_date (str, optional): A query parameter to pass start date to the date range filter. Defaults to None. end_date (str, optional): A query parameter to pass end date to the date range filter. Defaults to None. settings (Dict[str, Any], optional): A query parameter passed to the SupermetricsToCSV task. Defaults to None. filter (str, optional): A query parameter passed to the SupermetricsToCSV task. Defaults to None. max_rows (int, optional): A query parameter passed to the SupermetricsToCSV task. Defaults to 1000000. max_columns (int, optional): A query parameter passed to the SupermetricsToCSV task. Defaults to None. order_columns (str, optional): A query parameter passed to the SupermetricsToCSV task. Defaults to None. expectation_suite (dict, optional): The Great Expectations suite used to valiaate the data. Defaults to None. evaluation_parameters (str, optional): A dictionary containing evaluation parameters for the validation. Defaults to None. keep_validation_output (bool, optional): Whether to keep the output files generated by the Great Expectations task. Defaults to False. Currently, only GitHub URLs are supported. Defaults to None. local_file_path (str, optional): Local destination path. Defaults to None. adls_file_name (str, optional): Name of file in ADLS. Defaults to None. output_file_extension (str, optional): Output file extension - to allow selection of .csv for data which is not easy to handle with parquet. Defaults to \".parquet\".. adls_dir_path (str, optional): Azure Data Lake destination folder/catalog path. Defaults to None. sep (str, optional): The separator to use in the CSV. Defaults to \"\\t\". overwrite_adls (bool, optional): Whether to overwrite the file in ADLS. Defaults to True. if_empty (str, optional): What to do if the Supermetrics query returns no data. Defaults to \"warn\". adls_sp_credentials_secret (str, optional): The name of the Azure Key Vault secret containing a dictionary with ACCOUNT_NAME and Service Principal credentials (TENANT_ID, CLIENT_ID, CLIENT_SECRET) for the Azure Data Lake. Defaults to None. max_download_retries (int, optional): How many times to retry the download. Defaults to 5. supermetrics_task_timeout (int, optional): The timeout for the download task. Defaults to 60*30. parallel (bool, optional): Whether to parallelize the downloads. Defaults to True. tags (List[str], optional): Flow tags to use, eg. to control flow concurrency. Defaults to [\"extract\"]. vault_name (str, optional): The name of the vault from which to obtain the secrets. Defaults to None. check_missing_data (bool, optional): Whether to check missing data. Defaults to True. timeout(int, optional): The amount of time (in seconds) to wait while running this task before a timeout occurs. Defaults to 3600. \"\"\" if not ds_user : try : ds_user = PrefectSecret ( \"SUPERMETRICS_DEFAULT_USER\" ) . run () except ValueError as e : msg = \"Neither 'ds_user' parameter nor 'SUPERMETRICS_DEFAULT_USER' secret were not specified\" raise ValueError ( msg ) from e self . flow_name = name self . check_missing_data = check_missing_data self . timeout = timeout # SupermetricsToDF self . ds_id = ds_id self . ds_accounts = ds_accounts self . ds_segments = ds_segments self . ds_user = ds_user self . fields = fields self . date_range_type = date_range_type self . start_date = start_date self . end_date = end_date self . settings = settings self . filter = filter self . max_rows = max_rows self . max_columns = max_columns self . order_columns = order_columns self . if_exists = if_exists self . output_file_extension = output_file_extension # RunGreatExpectationsValidation self . expectation_suite = expectation_suite self . expectations_path = \"/home/viadot/tmp/expectations\" self . expectation_suite_name = expectation_suite [ \"expectation_suite_name\" ] self . evaluation_parameters = evaluation_parameters self . keep_validation_output = keep_validation_output # AzureDataLakeUpload self . local_file_path = ( local_file_path or self . slugify ( name ) + self . output_file_extension ) self . local_json_path = self . slugify ( name ) + \".json\" self . now = str ( pendulum . now ( \"utc\" )) self . adls_dir_path = adls_dir_path if adls_file_name is not None : self . adls_file_path = os . path . join ( adls_dir_path , adls_file_name ) self . adls_schema_file_dir_file = os . path . join ( adls_dir_path , \"schema\" , Path ( adls_file_name ) . stem + \".json\" ) else : self . adls_file_path = os . path . join ( adls_dir_path , self . now + self . output_file_extension ) self . adls_schema_file_dir_file = os . path . join ( adls_dir_path , \"schema\" , self . now + \".json\" ) self . overwrite_adls = overwrite_adls self . if_empty = if_empty self . adls_sp_credentials_secret = adls_sp_credentials_secret # Global self . max_download_retries = max_download_retries self . supermetrics_task_timeout = supermetrics_task_timeout self . parallel = parallel self . tags = tags self . vault_name = vault_name super () . __init__ ( * args , name = name , ** kwargs ) self . gen_flow () Data types are automaticly detected and mapped to meet Microsoft Azure SQL Database requirments. Schema json will be stored in the data lake (parquet_file_directory/schema)","title":"__init__()"}]}